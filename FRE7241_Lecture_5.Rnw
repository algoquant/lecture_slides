% FRE7241_Lecture_5

% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='tiny', fig.width=4, fig.height=4)
options(width=80, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#5]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#5, Spring 2021}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{April 27, 2021}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Plotting Time Series Using Package \protect\emph{dygraphs}}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using Package \protect\emph{dygraphs}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dygraph()} from package \emph{dygraphs} creates interactive plots for \emph{xts} time series.
      \vskip1ex
      The function \texttt{dyCandlestick()} creates a \emph{candlestick} plot object for \emph{OHLC} data, and uses the first four columns to plot \emph{candlesticks}, and it plots any additional columns as lines.
      \vskip1ex
      The function \texttt{dyOptions()} adds options (like colors, etc.) to a \emph{dygraph} plot.
      <<echo=TRUE,eval=FALSE>>=
library(dygraphs)
# Calculate volume-weighted average price
oh_lc <- rutils::etf_env$VTI
v_wap <- TTR::VWAP(price=quantmod::Cl(oh_lc),
    volume=quantmod::Vo(oh_lc), n=20)
# Add VWAP to OHLC data
da_ta <- cbind(oh_lc[, 1:4], v_wap)["2009-01/2009-04"]
# Create dygraphs object
dy_graph <- dygraphs::dygraph(da_ta)
# Increase line width and color
dy_graph <- dygraphs::dyOptions(dy_graph, 
  colors="red", strokeWidth=3)
# Convert dygraphs object to candlestick plot
dy_graph <- dygraphs::dyCandlestick(dy_graph)
# Render candlestick plot
dy_graph
# Candlestick plot using pipes syntax
dygraphs::dygraph(da_ta) %>% dyCandlestick() %>% 
  dyOptions(colors="red", strokeWidth=3)
# Candlestick plot without using pipes syntax
dygraphs::dyCandlestick(dygraphs::dyOptions(dygraphs::dygraph(da_ta), 
  colors="red", strokeWidth=3))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick.png}
      Each \emph{candlestick} displays one period of data, and consists of a box representing the \emph{Open} and \emph{Close} prices, and a vertical line representing the \emph{High} and \emph{Low} prices.
      \vskip1ex
      The color of the box signifies whether the \emph{Close} price was higher or lower than the \emph{Open},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} \protect\emph{OHLC} Plots With Background Shading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyShading()} adds shading to a \emph{dygraphs} plot object.
      \vskip1ex
      The function \texttt{dyShading()} requires a vector of dates for shading.
      <<echo=TRUE,eval=FALSE>>=
# Create candlestick plot with background shading
in_dic <- (Cl(da_ta) > da_ta[, "VWAP"])
whi_ch <- which(rutils::diff_it(in_dic) != 0)
in_dic <- rbind(first(in_dic), in_dic[whi_ch, ], last(in_dic))
date_s <- index(in_dic)
in_dic <- ifelse(drop(coredata(in_dic)), "lightgreen", "antiquewhite")
# Create dygraph object without rendering it
dy_graph <- dygraphs::dygraph(da_ta) %>% dyCandlestick() %>% 
  dyOptions(colors="red", strokeWidth=3)
# Add shading
for (i in 1:(NROW(in_dic)-1)) {
    dy_graph <- dy_graph %>% 
      dyShading(from=date_s[i], to=date_s[i+1], color=in_dic[i])
}  # end for
# Render the dygraph object
dy_graph
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick_shaded.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} Plots With Two \texttt{"y"} Axes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyAxis()} from package \emph{dygraphs} plots customized axes to a \emph{dygraphs} plot object.
      \vskip1ex
      The function \texttt{dySeries()} adds a time series to a \emph{dygraphs} plot object.
      <<echo=TRUE,eval=FALSE>>=
library(dygraphs)
# Prepare VTI and IEF prices
price_s <- cbind(Cl(rutils::etf_env$VTI), Cl(rutils::etf_env$IEF))
price_s <- na.omit(price_s)
col_names <- rutils::get_name(colnames(price_s))
colnames(price_s) <- col_names
# dygraphs plot with two y-axes
library(dygraphs)
dygraphs::dygraph(price_s, main=paste(col_names, collapse=" and ")) %>%
  dyAxis(name="y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis(name="y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", strokeWidth=2, col="red") %>%
  dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth, height=0.35\paperwidth]{figure/dygraphs_2yaxis.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Moving Average Crossover Strategies}


%%%%%%%%%%%%%%%
\subsection{Volume-Weighted Average Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes.
      \vskip1ex
      Moving averages (such as \emph{VWAP}) are often used to define technical indicators (trading signals).
      \vskip1ex
      The \emph{VWAP} is an indicator used for trend following strategies.
      <<echo=TRUE,eval=FALSE>>=
# Calculate log OHLC prices
sym_bol <- "VTI"
# oh_lc <- rutils::etf_env$VTI
oh_lc <- get(sym_bol, rutils::etf_env)
oh_lc[, 1:4] <- log(oh_lc[, 1:4])
n_rows <- NROW(oh_lc)
op_en <- quantmod::Op(oh_lc)
clos_e <- quantmod::Cl(oh_lc)
colnames(clos_e) <- sym_bol
vol_ume <- quantmod::Vo(oh_lc)
re_turns <- rutils::diff_it(clos_e)
cum_rets <- cumsum(re_turns)
# Define aggregation interval and calculate VWAP
look_back <- 200
v_wap <- rutils::roll_sum(x_ts=cum_rets*vol_ume, look_back=look_back)
volume_roll <- rutils::roll_sum(x_ts=vol_ume, look_back=look_back)
v_wap <- v_wap/volume_roll
v_wap[is.na(v_wap)] <- 0
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_indic.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot VTI and VWAP using dygraphs
weal_th <- cbind(cum_rets, v_wap)
colnames(weal_th) <- c(sym_bol, "VWAP")
dygraphs::dygraph(weal_th, main="VTI and VWAP") %>% 
  dyOptions(colors=c("blue", "red"), strokeWidth=3)
# Plot VTI and VWAP using quantmod
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue", "red")
quantmod::chart_Series(x=cbind(clos_e, v_wap), name="VTI and VWAP", 
             theme=plot_theme)
legend("top", legend=c("VTI", "VWAP"), lty=1, lwd=6, cex=0.9,
       bg="white", col=c("blue", "red"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Moving Average Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a \emph{Moving Average Crossover} strategy, when the current price crosses above the \emph{VWAP}, then the strategy switches its position to long risk, and vice versa.
      \vskip1ex
      A single-period time lag is applied to the \emph{VWAP indicator}, so that the strategy trades immediately after the \emph{VWAP indicator} is evaluated at the end of the day.
      \vskip1ex
      This assumption may be too optimistic because in practice it's difficult to trade immediately just before the close of markets.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VWAP indicator
in_dic <- sign(cum_rets - v_wap)
# Calculate positions as lagged indicator
position_s <- rutils::lag_it(in_dic)
# Calculate daily profits and losses of strategy
pnl_s <- re_turns*position_s
colnames(pnl_s) <- "Strategy"
cum_pnls <- cumsum(pnl_s)
weal_th <- cbind(cum_rets, cum_pnls, v_wap)
colnames(weal_th) <- c(sym_bol, "Strategy", "VWAP")
# Annualized Sharpe ratios of VTI and VWAP strategy
sharp_e <- sqrt(252)*sapply(cbind(re_turns, pnl_s), function (x) mean(x)/sd(x))
# Calculate index for background shading
in_dic <- (cum_rets > v_wap)
whi_ch <- which(rutils::diff_it(in_dic) != 0)
in_dic <- rbind(first(in_dic), in_dic[whi_ch, ], last(in_dic))
date_s <- index(in_dic)
in_dic <- ifelse(drop(coredata(in_dic)), "lightgreen", "antiquewhite")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_strat.png}
      <<echo=TRUE,eval=FALSE>>=
# Create dygraph object without rendering it
dy_graph <- dygraphs::dygraph(weal_th, main=paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red", "purple"), strokeWidth=2)
# Add shading
for (i in 1:(NROW(in_dic)-1)) {
    dy_graph <- dy_graph %>% 
      dyShading(from=date_s[i], to=date_s[i+1], color=in_dic[i])
}  # end for
# Render the dygraph object
dy_graph
# Plot VTI and VWAP strategy using quantmod
quantmod::chart_Series(x=cbind(cum_rets, cum_pnls), 
  name="VWAP Crossover Strategy for VTI", theme=plot_theme)
add_TA(position_s > 0, on=-1, col="lightgreen", border="lightgreen")
add_TA(position_s < 0, on=-1, col="lightgrey", border="lightgrey")
legend("top", legend=c(sym_bol, "VWAP strategy"), lty=1, lwd=6, 
       cex=0.9, inset=0.1, bg="white", col=c("blue", "red"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{MA Crossover Strategy With Lag}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{MA Crossover} strategy suffers losses when prices are range-bound without a trend, because whenever it switches position the prices soon change direction.  (This is called a "whipsaw".)
      \vskip1ex
      To prevent whipsaws and over-trading, the \emph{MA Crossover} strategy may choose to delay switching positions until the indicator repeats the same value for several periods.
      \vskip1ex
      There's a tradeoff between switching positions too early and risking a whipsaw, and waiting too long and missing a trend.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions from lagged indicator
lagg <- 2
in_dic <- sign(cum_rets - v_wap)
indic_sum <- roll::roll_sum(in_dic, width=lagg)
indic_sum[1:lagg] <- 0
position_s <- rep(NA_integer_, NROW(clos_e))
position_s[1] <- 0
position_s <- ifelse(indic_sum == lagg, 1, position_s)
position_s <- ifelse(indic_sum == (-lagg), -1, position_s)
position_s <- zoo::na.locf(position_s, na.rm=FALSE)
# Lag the positions to trade in next period
position_s <- rutils::lag_it(position_s, lagg=1)
# Calculate PnLs of lagged strategy
pnl_s <- re_turns*position_s
colnames(pnl_s) <- "Strategy"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_strat_lag.png}
      <<echo=TRUE,eval=FALSE>>=
cum_pnls_lag <- cumsum(pnl_s)
weal_th <- cbind(cum_pnls, cum_pnls_lag)
colnames(weal_th) <- c("Strategy", "Strategy_lag")
# Annualized Sharpe ratios of VWAP strategies
sharp_e <- sqrt(252)*sapply(cbind(re_turns, pnl_s), 
  function (x) mean(x)/sd(x))
# Plot both strategies
dygraphs::dygraph(weal_th, main=paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red"), strokeWidth=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Dual VWAP Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The fast-moving \emph{VWAP} is calculated over a short look-back interval, while the slow-moving \emph{VWAP} is calculated over a longer interval.
      \vskip1ex
      The trend following reverses direction when the fast-moving \emph{VWAP} crosses the slow-moving one.
      <<echo=TRUE,eval=FALSE>>=
# Calculate fast and slow VWAPs
vwap_fast <- TTR::VWAP(cum_rets, volume=vol_ume, n=20)
vwap_fast[1:20] <- 0
vwap_slow <- TTR::VWAP(cum_rets, volume=vol_ume, n=200)
vwap_slow[1:200] <- 0
# Calculate VWAP indicator
in_dic <- sign(vwap_fast - vwap_slow)
# Calculate positions as lagged indicator
position_s <- rutils::lag_it(in_dic)
# Calculate daily profits and losses of strategy
pnl_s <- re_turns*position_s
colnames(pnl_s) <- "Strategy"
cum_pnls <- cumsum(pnl_s)
weal_th <- cbind(cum_rets, cum_pnls, vwap_fast, vwap_slow)
colnames(weal_th) <- c(sym_bol, "Strategy", "VWAP_fast", "VWAP_slow")
# Annualized Sharpe ratios of VTI and VWAP strategy
sharp_e <- sqrt(252)*sapply(cbind(re_turns, pnl_s), 
  function (x) mean(x)/sd(x))
# Calculate index for background shading
in_dic <- (vwap_fast > vwap_slow)
whi_ch <- which(rutils::diff_it(in_dic) != 0)
in_dic <- rbind(first(in_dic), in_dic[whi_ch, ], last(in_dic))
date_s <- index(in_dic)
in_dic <- ifelse(drop(coredata(in_dic)), "lightgreen", "antiquewhite")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_dual_strat.png}
      <<echo=TRUE,eval=FALSE>>=
# Create dygraph object without rendering it
dy_graph <- dygraphs::dygraph(weal_th, main=paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red", "purple", "lightpurple"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Add shading
for (i in 1:(NROW(in_dic)-1)) {
    dy_graph <- dy_graph %>% 
      dyShading(from=date_s[i], to=date_s[i+1], color=in_dic[i])
}  # end for
# Render the dygraph object
dy_graph
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Modeling Volatility and Skewness}


%%%%%%%%%%%%%%%
\subsection{Estimating the Rolling Variance of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The variance of asset returns exhibits \emph{heteroskedasticity}, i.e. it changes over time.
      \vskip1ex
      The rolling variance of returns is given by:
      \begin{flalign*}
        \sigma_i^2 &= \frac{1}{k-1} \sum_{j=0}^{k-1} (r_{i-j}-\bar{r_i})^2 \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} r_{i-j}}
      \end{flalign*}
      Where \texttt{k} is the \emph{look-back interval} equal to the number of data points
      for performing aggregations over the past.
      \vskip1ex
      It's also possible to calculate the rolling variance in \texttt{R} using vectorized functions, without using an \texttt{apply()} loop.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
n_rows <- NROW(re_turns)
# Define end points
end_p <- 1:NROW(re_turns)
# Start points are multi-period lag of end_p
look_back <- 11
start_p <- c(rep_len(0, look_back-1), end_p[1:(n_rows-look_back+1)])
# Calculate rolling variance in sapply() loop - takes very long
vari_ance <- sapply(1:n_rows, function(in_dex) {
  ret_s <- re_turns[start_p[in_dex]:end_p[in_dex]]
  sum((ret_s - mean(ret_s))^2)
}) / (look_back-1)  # end sapply
# Use only vectorized functions
rolling_rets <- cumsum(re_turns)
rolling_rets <- (rolling_rets -
  c(rep_len(0, look_back), rolling_rets[1:(n_rows-look_back)]))
rolling_rets2 <- cumsum(re_turns^2)
rolling_rets2 <- (rolling_rets2 -
  c(rep_len(0, look_back), rolling_rets2[1:(n_rows-look_back)]))
vari_ance2 <- (rolling_rets2 - rolling_rets^2/look_back)/(look_back-1)
all.equal(vari_ance[-(1:look_back)], as.numeric(vari_ance2)[-(1:look_back)])
# Same, using package rutils
rolling_rets <- rutils::roll_sum(re_turns, look_back=look_back)
rolling_rets2 <- rutils::roll_sum(re_turns^2, look_back=look_back)
vari_ance2 <- (rolling_rets2 - rolling_rets^2/look_back)/(look_back-1)
# Coerce vari_ance into xts
tail(vari_ance)
class(vari_ance)
vari_ance <- xts(vari_ance, order.by=index(re_turns))
colnames(vari_ance) <- "VTI.variance"
head(vari_ance)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Rolling Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_var()} for \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for rolling scaling and centering of time series,
        \item \texttt{roll\_pcr()} for rolling principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling VTI variance using package roll
library(roll)  # Load roll
vari_ance <-
  roll::roll_var(re_turns, width=look_back)
colnames(vari_ance) <- "VTI.variance"
head(vari_ance)
sum(is.na(vari_ance))
vari_ance[1:(look_back-1)] <- 0
# Benchmark calculation of rolling variance
library(microbenchmark)
summary(microbenchmark(
  roll_sapply=sapply(2:n_rows, function(in_dex) {
    ret_s <- re_turns[start_p[in_dex]:end_p[in_dex]]
    sum((ret_s - mean(ret_s))^2)
  }),
  ro_ll=roll::roll_var(re_turns, width=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{EWMA} Realized Variance Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time-varying volatility can be more accurately estimated using an \emph{Exponentially Weighted Moving Average} (\emph{EWMA}) variance estimator.
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      \begin{displaymath}
        \sigma_i^2 = (1-\lambda) {r_i}^2 + \lambda \sigma_{i-1}^2 = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j {r_{i-j}}^2
      \end{displaymath}
      $\sigma_i^2$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period \texttt{i} and the past \emph{realized} variance.
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa.
      \vskip1ex
      The function \texttt{filter()} calculates the convolution of a vector or time series with a vector of filter coefficients (weights).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/vol_ewma.png}\\
      <<echo=TRUE,eval=FALSE>>=
# Calculate EWMA VTI variance using filter()
look_back <- 51
weight_s <- exp(-0.1*1:look_back)
weight_s <- weight_s/sum(weight_s)
vari_ance <- stats::filter(re_turns^2,
    filter=weight_s, sides=1)
vari_ance[1:(look_back-1)] <- vari_ance[look_back]
class(vari_ance)
vari_ance <- as.numeric(vari_ance)
x_ts <- xts:::xts(sqrt(vari_ance), order.by=index(re_turns))
# Plot EWMA standard deviation
chart_Series(x_ts,
  name="EWMA standard deviation")
dygraphs::dygraph(x_ts, main="EWMA standard deviation")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating \protect\emph{EWMA} Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the \emph{time series} has non-zero \emph{expected} mean, then the rolling \emph{EWMA} variance is a vector given by the estimator:
      \begin{flalign*}
        \sigma_i^2 &= \frac{1}{k-1} \sum_{j=0}^{k-1} {w_j (r_{i-j}-\bar{r_i})^2} \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} {w_j r_{i-j}}}
      \end{flalign*}
      Where $w_j$ is the vector of weights:
      \begin{displaymath}
        w_j = \frac{\lambda^j}{\sum_{j=0}^{k-1} \lambda^j}
      \end{displaymath}
      The function \texttt{roll\_var()} from package \emph{roll} calculates the rolling \emph{EWMA} variance.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling VTI variance using package roll
library(roll)  # Load roll
vari_ance <- roll::roll_var(re_turns,
  weights=rev(weight_s), width=look_back)
colnames(vari_ance) <- "VTI.variance"
class(vari_ance)
head(vari_ance)
sum(is.na(vari_ance))
vari_ance[1:(look_back-1)] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variance calculated over non-overlapping intervals has very statistically significant autocorrelations.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(4, 3, 1, 1), oma=c(0, 0, 0, 0))
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Calculate rolling VTI variance using package roll
look_back <- 22
vari_ance <- roll::roll_var(re_turns, width=look_back)
vari_ance[1:(look_back-1)] <- 0
colnames(vari_ance) <- "VTI.variance"
# Number of look_backs that fit over re_turns
n_rows <- NROW(re_turns)
n_agg <- n_rows %/% look_back
# Define end_p with beginning stub
end_p <- c(0, n_rows-look_back*n_agg + (0:n_agg)*look_back)
n_rows <- NROW(end_p)
# Subset vari_ance to end_p
vari_ance <- vari_ance[end_p]
# Plot autocorrelation function
rutils::plot_acf(vari_ance, lag=10, main="ACF of Variance")
# Plot partial autocorrelation
pacf(vari_ance, lag=10, main="PACF of Variance", ylab=NA)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_var.png}\\
      \includegraphics[width=0.5\paperwidth]{figure/pacf_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} model is a volatility model defined by two coupled equations:
      \begin{flalign*}
        r_i &= \mu + \sigma_{i-1} \xi_i \\
        \sigma_i^2 &= \omega + \alpha r_i^2 + \beta \sigma_{i-1}^2
      \end{flalign*}
      Where $\sigma_i^2$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance ${r_{i-1}}^2$, and the past variance $\sigma_{i-1}^2$.
      \vskip1ex
      The return process $r_i$ follows a normal distribution with time-dependent variance $\sigma_i^2$.
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance.
      \vskip1ex
      The parameter $\omega$ is proportional to the long-term average level of variance, which is given by:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      The sum of $\alpha$ plus $\beta$ should be less than $1$, otherwise the volatility is explosive.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define GARCH parameters
om_ega <- 0.01 ; al_pha <- 0.2
be_ta <- 0.79 ; n_rows <- 1000
re_turns <- numeric(n_rows)
vari_ance <- numeric(n_rows)
vari_ance[1] <- om_ega/(1-al_pha-be_ta)
re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
# Simulate GARCH model
set.seed(1121)  # Reset random numbers
for (i in 2:n_rows) {
  re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
  vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 +
    be_ta*vari_ance[i-1]
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} volatility model exhibits sharp spikes in the volatility, followed by a quick decay of volatility.
      \vskip1ex
      But the decay of volatility in the \emph{GARCH} model is faster than what is observed in practice.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=5, height=3.5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0))
# Plot GARCH cumulative returns
plot(cumsum(re_turns/100), t="l",
  lwd=2, col="blue", xlab="", ylab="",
  main="GARCH cumulative returns")
# Plot dygraphs GARCH standard deviation
date_s <- seq.Date(from=Sys.Date()-n_rows+1,
  to=Sys.Date(), length.out=n_rows)
x_ts <- xts:::xts(cumsum(re_turns/100), order.by=date_s)
dygraphs::dygraph(x_ts, main="GARCH cumulative returns")
# Plot GARCH standard deviation
plot(sqrt(vari_ance), t="l",
  col="blue", xlab="", ylab="",
  main="GARCH standard deviation")
# Plot dygraphsGARCH standard deviation
x_ts <- xts:::xts(sqrt(vari_ance), order.by=date_s)
dygraphs::dygraph(x_ts, main="GARCH standard deviation")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_returns.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/garch_stdev.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Properties}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Greater values of $\alpha$ produce a stronger feedback between the realized returns and variance, causing stronger variance spikes and higher kurtosis.
      <<echo=TRUE,eval=FALSE>>=
# Define GARCH parameters
om_ega <- 0.0001 ; al_pha <- 0.5
be_ta <- 0.1 ; n_rows <- 10000
re_turns <- numeric(n_rows)
vari_ance <- numeric(n_rows)
vari_ance[1] <- om_ega/(1-al_pha-be_ta)
re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
# Simulate GARCH model
set.seed(1121)  # Reset random numbers
for (i in 2:n_rows) {
  re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
  vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 +
    be_ta*vari_ance[i-1]
}  # end for
# Calculate kurtosis of GARCH returns
moments::moment(re_turns, order=4) /
  moments::moment(re_turns, order=2)^2
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(re_turns)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_hist.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of GARCH returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH returns histogram")
lines(density(re_turns, adjust=1.5),
      lwd=3, col="blue")
optim_fit <- MASS::fitdistr(re_turns,
  densfun="t", df=2, lower=c(-1, 1e-7))
lo_cation <- optim_fit$estimate[1]
scal_e <- optim_fit$estimate[2]
curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05, bty="n",
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Calibration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GARCH} models can be calibrated on returns using the \emph{maximum-likelihood} method, but it's a complex optimization procedure.
      \vskip1ex
      The package \emph{fGarch} contains functions for applying \emph{GARCH} models.
      \vskip1ex
      The function \texttt{garchFit()} calibrates a \emph{GARCH} model on a time series of returns.
      \vskip1ex
      The function \texttt{garchFit()} returns an \texttt{S4} object of class \emph{fGARCH}, with multiple slots containing the \emph{GARCH} model outputs and diagnostic information.
      <<echo=(-(1:2)),eval=FALSE>>=
# use fixed notation instead of exponential notation
options(scipen=999)
library(fGarch)
# Fit returns into GARCH
garch_fit <- fGarch::garchFit(data=re_turns)
# Fitted GARCH parameters
round(garch_fit@fit$coef, 5)
# Actual GARCH parameters
round(c(mu=mean(re_turns), omega=om_ega,
  alpha=al_pha, beta=be_ta), 5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_fGarch_fitted.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot GARCH fitted standard deviation
plot(sqrt(garch_fit@fit$series$h), t="l",
  col="blue", xlab="", ylab="",
  main="GARCH fitted standard deviation")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{garchSpec()} from package \emph{fGarch} specifies a \emph{GARCH} model.
      \vskip1ex
      The function \texttt{garchSim()} simulates a \emph{GARCH} model.
      <<echo=TRUE,eval=FALSE>>=
# Specify GARCH model
garch_spec <- fGarch::garchSpec(
  model=list(omega=om_ega, alpha=al_pha, beta=be_ta))
# Simulate GARCH model
garch_sim <-
  fGarch::garchSim(spec=garch_spec, n=n_rows)
re_turns <- as.numeric(garch_sim)
# Calculate kurtosis of GARCH returns
moments::moment(re_turns, order=4) /
  moments::moment(re_turns, order=2)^2
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(re_turns)
# Plot histogram of GARCH returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH returns histogram")
lines(density(re_turns, adjust=1.5),
      lwd=3, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/garch_fGarch_hist.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Fit t-distribution into GARCH returns
optim_fit <- MASS::fitdistr(re_turns,
  densfun="t", df=2, lower=c(-1, 1e-7))
lo_cation <- optim_fit$estimate[1]
scal_e <- optim_fit$estimate[2]
curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05, bty="n",
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Volatility From Intraday Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{close-to-close} volatility $\sigma$ depends on the \emph{Close} prices $C_i$ from \emph{OHLC} data:
      \begin{flalign*}
        \sigma^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (r_i - \bar{r})^2 \\
        \bar{r} &= \frac{1}{n}{\sum_{i=0}^{n} r_i} \quad r_i = \log(\frac{C_i}{C_{i-1}})
      \end{flalign*}
      \vskip1ex
      But intraday time series of prices (for example \texttt{HighFreq::SPY} prices), can have large overnight jumps which inflate the volatility estimates.
      \vskip1ex
      So the overnight returns must be divided by the overnight time interval (in seconds), which produces per second returns.
      \vskip1ex
      The per second returns can be multiplied by \texttt{60} to scale them back up to per minute returns.
      \vskip1ex
      The function \texttt{zoo::index()} extracts the time index of a time series.
      \vskip1ex
      The function \texttt{zoo::.index()} extracts the time index expressed in the number of seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely SPY returns (unit per minute) single day
# Minutely SPY volatility (unit per minute)
re_turns <- rutils::diff_it(log(SPY["2012-02-13", 4]))
sd(re_turns)
# SPY returns multiple days (includes overnight jumps)
re_turns <- rutils::diff_it(log(SPY[, 4]))
sd(re_turns)
# Table of time intervals - 60 second is most frequent
in_dex <- rutils::diff_it(.index(SPY))
table(in_dex)
# SPY returns divided by the overnight time intervals (unit per second)
re_turns <- re_turns / in_dex
re_turns[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility as Function of Aggregation Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimated volatility $\sigma$ scales as the \emph{power} of the length of the aggregation time interval $\Delta t$:
      \begin{displaymath}
        \frac{\sigma_t}{\sigma} = {\Delta t} ^ H
      \end{displaymath}
      Where \texttt{H} is the \emph{Hurst} exponent, $\sigma$ is the return volatility, and $\sigma_t$ is the volatility of the aggregated returns.
      \vskip1ex
      If returns follow \emph{Brownian motion} then the volatility scales as the \emph{square root} of the length of the aggregation interval (\texttt{H = 0.5}).
      \vskip1ex
      If returns are \emph{mean reverting} then the volatility scales slower than the \emph{square root} (\texttt{H < 0.5}).
      \vskip1ex
      If returns are \emph{trending} then the volatility scales faster than the \emph{square root} (\texttt{H > 0.5}).
      \vskip1ex
      The length of the daily time interval is often approximated to be equal to \texttt{390 = 6.5*60} minutes, since the exchange trading session is equal to \texttt{6.5} hours, and daily volatility is dominated by the trading session.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely OHLC SPY prices aggregated to daily prices
SPY_daily <- rutils::to_period(oh_lc=HighFreq::SPY, period="days")
# Daily SPY volatility from daily returns
sd(rutils::diff_it(log(SPY_daily[, 4])))
# Minutely SPY returns (unit per minute)
re_turns <- rutils::diff_it(log(SPY[, 4]))
# Minutely SPY volatility scaled to daily interval
sqrt(6.5*60)*sd(re_turns)
# Minutely SPY returns with overnight scaling (unit per second)
re_turns <- rutils::diff_it(log(SPY[, 4]))
in_dex <- rutils::diff_it(.index(SPY))
re_turns <- re_turns / in_dex
re_turns[1] <- 0
# Daily SPY volatility from minutely returns
sqrt(6.5*60)*60*sd(re_turns)
# Daily SPY volatility
# Including extra time over weekends and holidays
24*60*60*sd(rutils::diff_it(log(SPY_daily[, 4]))[-1] /
            rutils::diff_it(.index(SPY_daily))[-1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For a single aggregation interval, the \emph{Hurst exponent} \texttt{H} is equal to:
      \begin{displaymath}
        H = \frac{\log{\sigma_t} - \log{\sigma}}{\log{\Delta t}}
      \end{displaymath}
      \vskip1ex
      For a vector of aggregation intervals, the \emph{Hurst exponent} \texttt{H} can be calculated by regressing the volatility against the aggregation intervals.
        <<echo=TRUE,eval=FALSE>>=
# Calculate SPY prices adjusted for overnight jumps
price_s <- log(as.numeric(Cl(HighFreq::SPY[, 4])))
re_turns <- rutils::diff_it(price_s) /
  rutils::diff_it(.index(HighFreq::SPY))
re_turns[1] <- 0
price_s <- cumsum(re_turns)
# Calculate volatilities for vector of aggregation intervals
interval_s <- round(seq.int(from=3, to=30, length.out=9)^2)
vol_s <- sapply(interval_s, function(inter_val) {
  end_p <- rutils::calc_endpoints(price_s, inter_val=inter_val)
  sd(rutils::diff_it(price_s[end_p]))
})  # end sapply
vol_log <- log(vol_s)
vol_log <- vol_log - mean(vol_log)
inter_log <- log(interval_s)
inter_log <- inter_log - mean(inter_log)
mod_el <- lm(vol_log ~ inter_log)
hurs_t <- summary(mod_el)$coeff[2, 1]
# Or directly from formula
hurst_form <- sum(vol_log*inter_log)/sum(inter_log^2)
all.equal(hurst_form, hurs_t)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hurst_vol.png}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(1, 1, 1, 1))
plot(vol_log ~ inter_log, lwd=6, col="red",
     xlab="aggregation intervals (log)",
     ylab="volatility (log)",
     main="Hurst Exponent for SPY From Volatilities")
abline(mod_el, lwd=3, col="blue")
text(-2, 0.5, paste0("Hurst = ", round(hurs_t, 4)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rescaled Range Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The range $R_{\Delta t}$ of prices $p_t$ over an interval $\Delta t$, is the difference between the highest attained price minus the lowest:
      \begin{displaymath}
        R_t = \max_{\Delta t}{[p_{\tau}]} - \min_{\Delta t}{[p_{\tau}]}
      \end{displaymath}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is equal to the range $R_{\Delta t}$ divided by the standard deviation of the price differences $\sigma_t$: $RS_{\Delta t} = R_t / \sigma_t$.
      \vskip1ex
      The \emph{Rescaled Range} $RS_{\Delta t}$ for a time series of prices is calculated by:
      \begin{itemize}
        \item Dividing the time series into non-overlapping intervals of length $\Delta t$,
        \item Calculating the \emph{rescaled range} $RS_{\Delta t}$ for each interval,
        \item Calculating the average of the \emph{rescaled ranges} $RS_{\Delta t}$ for all the intervals.
      \end{itemize}
      \emph{Rescaled Range Analysis} (R/S) consists of calculating the average \emph{rescaled range} $RS_{\Delta t}$ as a function of the length of the aggregation interval $\Delta t$.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate the rescaled range
inter_val <- 500
end_p <- rutils::calc_endpoints(price_s,
  inter_val=inter_val)
r_s <- sapply(2:NROW(end_p), function(ep) {
  in_dex <- end_p[ep-1]:end_p[ep]
  diff(range(price_s[in_dex]))/sd(re_turns[in_dex])
})  # end sapply
mean(r_s)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Rescaled Range}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is proportional to the length of the aggregation interval $\Delta t$ raised to the power of the \emph{Hurst exponent} \texttt{H}:
      \begin{displaymath}
        RS_{\Delta t} \propto {\Delta t}^H
      \end{displaymath}
      The \emph{Hurst exponents} calculated from the \emph{rescaled range} and the \emph{volatility} are similar because they both measure the dependence of returns over time, but they're not equal because they use different methods to estimate price dispersion.
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate rescaled range for vector of aggregation intervals
r_s <- sapply(interval_s, function(inter_val) {
  end_p <- rutils::calc_endpoints(price_s,
    inter_val=inter_val)
  r_s <- sapply(2:NROW(end_p), function(ep) {
    in_dex <- end_p[ep-1]:end_p[ep]
    diff(range(price_s[in_dex]))/sd(re_turns[in_dex])
  })  # end sapply
  mean(na.omit(r_s))
})  # end sapply
rs_log <- log(r_s)
rs_log <- rs_log - mean(rs_log)
inter_log <- log(interval_s)
inter_log <- inter_log - mean(inter_log)
mod_el <- lm(rs_log ~ inter_log)
hurs_t <- summary(mod_el)$coeff[2, 1]
# Or directly from formula
hurst_form <- sum(rs_log*inter_log)/sum(inter_log^2)
all.equal(hurst_form, hurs_t)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hurst_reg.png}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(1, 1, 1, 1))
plot(rs_log ~ inter_log, lwd=6, col="red",
     xlab="aggregation intervals (log)",
     ylab="rescaled range (log)",
     main="Rescaled Range Analysis for SPY")
abline(mod_el, lwd=3, col="blue")
text(-2, 0.5, paste0("Hurst = ", round(hurs_t, 4)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Range} Volatility Estimators of \protect\emph{OHLC} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Range} volatility estimators utilize the \texttt{high} and \texttt{low} prices, and therefore have lower standard errors than the standard \emph{close-to-close} estimator.
      \vskip1ex
      The \emph{Garman-Klass} estimator uses the \emph{low-to-high} price range, but it underestimates volatility because it doesn't account for \emph{close-to-open} price jumps:
      \begin{displaymath}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)\log(\frac{C_i}{O_i})^2)
      \end{displaymath}
      The \emph{Yang-Zhang} estimator is the most efficient (has the lowest standard error) among unbiased estimators, and also accounts for \emph{close-to-open} price jumps:
      \vspace{-1em}
      \begin{multline*}
        \hspace{-1em}\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{O_i}{C_{i-1}})-\bar{r}_{co})^2 + \\
        0.134(\log(\frac{C_i}{O_i})-\bar{r}_{oc})^2 + \\
        \frac{0.866}{n} \sum_{i=1}^{n} (\log(\frac{H_i}{O_i})\log(\frac{H_i}{C_i}) + \log(\frac{L_i}{O_i})\log(\frac{L_i}{C_i}))
      \end{multline*}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Daily SPY volatility from minutely prices using package TTR
library(TTR)
sqrt((6.5*60)*mean(na.omit(
  TTR::volatility(HighFreq::SPY, N=1,
                  calc="yang.zhang"))^2))
# SPY volatility using package HighFreq
60*sqrt((6.5*60)*agg_regate(oh_lc=HighFreq::SPY,
            weight_ed=FALSE, mo_ment="run_variance",
            calc_method="yang_zhang"))
      @
      \vspace{-1em}
      Theoretically, the \emph{Yang-Zhang} (\emph{YZ}) and \emph{Garman-Klass-Yang-Zhang} (\emph{GKYZ}) range variance estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator.
      \vskip1ex
      But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the \emph{YZ} and \emph{GKYZ} range estimators.
      \vskip1ex
      Therefore in practice the \emph{YZ} and \emph{GKYZ} range estimators underestimate volatility.
      \vskip1ex
      In addition, their standard errors are reduced less than by the theoretical amount, for the same reason.
      \vskip1ex
      The \emph{Garman-Klass-Yang-Zhang} estimator is another very efficient and unbiased estimator, and also accounts for \emph{close-to-open} price jumps:
      \vspace{-1em}
      \begin{multline*}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} ((\log(\frac{O_i}{C_{i-1}})-\bar{r})^2 + \\
        0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)(\log(\frac{C_i}{O_i})^2))
      \end{multline*}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Performing Aggregations Over Time Series}


%%%%%%%%%%%%%%%
\subsection{Defining Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A time \emph{period} is the time between two neighboring points in time.
      \vskip1ex
      A time \emph{interval} is the time spanned by one or more time \emph{periods}.
      \vskip1ex
      A \emph{look-back interval} is a time \emph{interval} for performing aggregations over the past, starting from a \emph{start point} and ending at an \emph{end point}.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      The look-back \emph{intervals} may or may not \emph{overlap} with their neighboring intervals.
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{end points} at each point in time.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
      \vskip1ex
      An \emph{interval aggregation} is specified by \emph{end points} separated by many time \emph{periods}.
      \vskip1ex
      Examples of interval aggregations are monthly asset returns, or trailing 12-month asset returns calculated every month.
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Rolling} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{end points} at each point in time.
      \vskip1ex
      The first \emph{end point} is equal to zero $0$.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
    \column{0.5\textwidth}
\vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
oh_lc <- rutils::etf_env$VTI
# Number of data points
n_rows <- NROW(oh_lc["2018-06/"])
# Define end_p at each point in time
end_p <- 0:n_rows
# Number of data points in look_back interval
look_back <- 22
# start_p are end_p lagged by look_back
start_p <- c(rep_len(0, look_back - 1),
    end_p[1:(NROW(end_p)- look_back + 1)])
head(start_p, 33)
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_rolling.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Equally Spaced \protect\emph{end points} of a Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The neighboring \emph{end points} may be separated by a fixed number of periods, equal to \texttt{n\_points}.
      \vskip1ex
      If the total number of data points is not an integer multiple of \texttt{n\_points}, then a stub interval must be added either at the beginning or at the end of the \emph{end points}.
      \vskip1ex
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
price_s <- quantmod::Cl(oh_lc["2018/"])
n_rows <- NROW(price_s)
# Number of periods between endpoints
n_points <- 22
# Number of n_points that fit over n_rows
n_agg <- n_rows %/% n_points
# If n_rows==n_points*n_agg then whole number
end_p <- (0:n_agg)*n_points
# Stub interval at beginning
end_p <- c(0, n_rows-n_points*n_agg +
                  (0:n_agg)*n_points)
# Else stub interval at end
end_p <- c((0:n_agg)*n_points, n_rows)
# Or use xts::endpoints()
end_p <- xts::endpoints(price_s, on="months")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/intervals_end_points.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot data and endpoints as vertical lines
plot.xts(price_s, col="blue", lwd=2, xlab="", ylab="",
         main="Prices with Endpoints as Vertical Lines")
addEventLines(xts(rep("endpoint", NROW(end_p)-1), index(price_s)[end_p]),
              col="red", lwd=2, pos=4)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- "blue"
chart_Series(price_s, theme=plot_theme,
  name="prices with endpoints as vertical lines")
abline(v=end_p, col="red", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Overlapping} time intervals can be defined if the \emph{start points} are equal to the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of an overlapping interval aggregation are trailing 12-month asset returns calculated every month.
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
n_rows <- NROW(rutils::etf_env$VTI["2019/"])
# Number of n_points that fit over n_rows
n_points <- 22
n_agg <- n_rows %/% n_points
# Stub interval at beginning
end_p <- c(0, n_rows-n_points*n_agg +
                  (0:n_agg)*n_points)
      @
    \column{0.5\textwidth}
      The length of the \emph{look-back interval} can be defined either as the number of data points, or as the number of \emph{end points} to look back over.
      <<echo=TRUE,eval=FALSE>>=
# look_back defined as number of data points
look_back <- 252
# start_p are end_p lagged by look_back
start_p <- (end_p - look_back + 1)
start_p <- ifelse(start_p < 0, 0,
                       start_p)
# look_back defined as number of end_p
look_back <- 12
start_p <- c(rep_len(0, look_back-1),
    end_p[1:(NROW(end_p)- look_back + 1)])
# Bind start_p with end_p
cbind(start_p, end_p)
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Non-overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Non-overlapping} time intervals can be defined if \emph{start points} are equal to the previous \emph{end points}.
      \vskip1ex
      In that case the look-back \emph{intervals} are non-overlapping and \emph{contiguous} (each \emph{start point} is the \emph{end point} of the previous interval).
      \vskip1ex
      If the \emph{start points} are defined as the previous \emph{end points} plus $1$, then the \emph{intervals} are \emph{exclusive}.
      \vskip1ex
      \emph{Exclusive intervals} are used for calculating \emph{out-of-sample} aggregations over future intervals.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
n_rows <- NROW(rutils::etf_env$VTI["2019/"])
# Number of data points per interval
n_points <- 22
# Number of n_pointss that fit over n_rows
n_agg <- n_rows %/% n_points
# Define end_p with beginning stub
end_p <- c(0, n_rows-n_points*n_agg +
                  (0:n_agg)*n_points)
# Define contiguous start_p
start_p <- c(0, end_p[1:(NROW(end_p)-1)])
# Define exclusive start_p
start_p <- c(0, end_p[1:(NROW(end_p)-1)]+1)
      @
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_non_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Aggregations performed over time series can be extremely slow if done improperly, therefore it's very important to find the fastest methods of performing aggregations.
      \vskip1ex
      The \texttt{sapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The variable \texttt{look\_back} is the size of the look-back interval, equal to the number of data points used for applying the aggregation function (including the current point).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<TRUE,eval=FALSE>>=
price_s <- quantmod::Cl(rutils::etf_env$VTI)
end_p <- 0:NROW(price_s)  # End points at each point
n_rows <- NROW(end_p)
look_back <- 22  # Number of data points per look-back interval
# start_p are multi-period lag of end_p
start_p <- c(rep_len(0, look_back - 1),
    end_p[1:(n_rows - look_back + 1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(2:n_rows, function(in_dex) {
    start_p[in_dex]:end_p[in_dex]
})  # end lapply
# Define aggregation function
agg_regate <- function(x_ts) c(max=max(x_ts), min=min(x_ts))
# Perform aggregations over look_backs list
agg_regations <- sapply(look_backs,
    function(look_back) agg_regate(price_s[look_back])
)  # end sapply
# Coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations,
                     order.by=index(price_s[end_p]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{lapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Perform aggregations over look_backs list
agg_regations <- lapply(look_backs,
    function(look_back) agg_regate(price_s[look_back])
)  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call(rbind, agg_regations)
# Convert into xts
agg_regations <- xts::xts(agg_regations,
    order.by=index(price_s))
agg_regations <- cbind(agg_regations, price_s)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
x11(width=6, height=5)
chart_Series(agg_regations, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(agg_regations),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{roll\_agg()} performs rolling aggregations of its function argument \texttt{FUN}, over an \emph{xts} series (\texttt{x\_ts}), and a look-back interval (\texttt{look\_back}).
      \vskip1ex
      The argument \texttt{FUN} is an aggregation function over a subset of \texttt{x\_ts} series.
      \vskip1ex
      The dots \texttt{"..."} argument is passed into \texttt{FUN} as additional arguments.
      \vskip1ex
      The argument \texttt{look\_back} is equal to the number of periods of \texttt{x\_ts} series which are passed to the aggregation function \texttt{FUN}.
      \vskip1ex
      The functional \texttt{roll\_agg()} calls \texttt{lapply()}, which loops over the length of series \texttt{x\_ts}.
      \vskip1ex
      Note that two different intervals may be used with \texttt{roll\_agg()}.
      \vskip1ex
      The first interval is the argument \texttt{look\_back}.
      \vskip1ex
      A second interval may be one of the variables bound to the dots \texttt{"..."} argument, and passed to the aggregation function \texttt{FUN} (for example, an \emph{EWMA} window).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define functional for rolling aggregations
roll_agg <- function(x_ts, look_back, FUN, ...) {
# Define end points at every period
  end_p <- 0:NROW(x_ts)
  n_rows <- NROW(end_p)
# Define starting points as lag of end_p
  start_p <- c(rep_len(0, look_back - 1),
    end_p[1:(n_rows- look_back + 1)])
# Perform aggregations over look_backs list
  agg_regations <- lapply(2:n_rows, function(in_dex)
    FUN(x_ts[start_p[in_dex]:end_p[in_dex]], ...)
  )  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call(rbind, agg_regations)
# Coerce agg_regations into xts series
  if (!is.xts(agg_regations))
    agg_regations <- xts(agg_regations, order.by=index(x_ts))
  agg_regations
}  # end roll_agg
# Define aggregation function
agg_regate <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# Perform aggregations over rolling interval
agg_regations <- roll_agg(price_s, look_back=look_back,
                    FUN=agg_regate)
class(agg_regations)
dim(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Speed of Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of rolling aggregations using \texttt{apply()} loops can be greatly increased by simplifying the aggregation function
      \vskip1ex
      For example, an aggregation function that returns a vector is over \texttt{13} times faster than a function that returns an \emph{xts} object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define aggregation function that returns a vector
agg_vector <- function(x_ts)
  c(max=max(x_ts), min=min(x_ts))
# Define aggregation function that returns an xts
agg_xts <- function(x_ts)
  xts(t(c(max=max(x_ts), min=min(x_ts))), order.by=end(x_ts))
# Benchmark the speed of aggregation functions
library(microbenchmark)
summary(microbenchmark(
  agg_vector=roll_agg(price_s, look_back=look_back, FUN=agg_vector),
  agg_xts=roll_agg(price_s, look_back=look_back, FUN=agg_xts),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      Several packages contain functionals designed for performing rolling aggregations:
      \begin{itemize}
        \item \texttt{rollapply.zoo()} from package \emph{zoo},
        \item \texttt{rollapply.xts()} from package \emph{xts},
        \item \texttt{apply.rolling()} from package \emph{PerformanceAnalytics},
      \end{itemize}
      These functionals don't require specifying the \emph{end points}, and instead calculate the \emph{end points} from the rolling interval width.
      \vskip1ex
      These functionals can only apply functions that return a single value, not a vector.
      \vskip1ex
      These functionals return an \emph{xts} series with leading \texttt{NA} values at points before the rolling interval can fit over the data.
      \vskip1ex
      The argument \texttt{align="right"} of \texttt{rollapply()} determines that aggregations are taken from the past.
      \vskip1ex
      The functional \texttt{rollapply.xts} is the fastest, about as fast as performing an \texttt{lapply()} loop directly.
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define aggregation function that returns a single value
agg_regate <- function(x_ts)  max(x_ts)
# Perform aggregations over a rolling interval
agg_regations <- xts:::rollapply.xts(price_s, width=look_back,
                    FUN=agg_regate, align="right")
# Perform aggregations over a rolling interval
library(PerformanceAnalytics)  # Load package PerformanceAnalytics
agg_regations <- apply.rolling(price_s,
                    width=look_back, FUN=agg_regate)
# Benchmark the speed of the functionals
library(microbenchmark)
summary(microbenchmark(
  roll_agg=roll_agg(price_s, look_back=look_back, FUN=max),
  roll_xts=xts:::rollapply.xts(price_s, width=look_back, FUN=max, align="right"),
  apply_rolling=apply.rolling(price_s, width=look_back, FUN=max),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Rolling Aggregations Using \protect\emph{Vectorized} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic functions \texttt{cumsum()}, \texttt{cummax()}, and \texttt{cummin()} return the cumulative sums, minima, and maxima of \emph{vectors} and \emph{time series} objects.
      \vskip1ex
      The methods for these functions are implemented as \emph{vectorized compiled} functions, and are therefore much faster than \texttt{apply()} loops.
      \vskip1ex
      The \texttt{cumsum()} function can be used to efficiently calculate the rolling sum of an an \emph{xts} series.
      \vskip1ex
      Using the function \texttt{cumsum()} is over \texttt{25} times faster than using \texttt{apply()} loops.
      \vskip1ex
      But rolling standard deviations and higher moments can't be easily calculated using \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Rolling sum using cumsum()
roll_sum <- function(x_ts, look_back) {
  cum_sum <- cumsum(na.omit(x_ts))
  out_put <- cum_sum - lag(x=cum_sum, k=look_back)
  out_put[1:look_back, ] <- cum_sum[1:look_back, ]
  colnames(out_put) <- paste0(colnames(x_ts), "_stdev")
  out_put
}  # end roll_sum
agg_regations <- roll_sum(price_s, look_back=look_back)
# Perform rolling aggregations using lapply loop
agg_regations <- lapply(2:n_rows, function(in_dex)
    sum(price_s[start_p[in_dex]:end_p[in_dex]])
)  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call(rbind, agg_regations)
head(agg_regations)
tail(agg_regations)
# Benchmark the speed of both methods
library(microbenchmark)
summary(microbenchmark(
  roll_sum=roll_sum(price_s, look_back=look_back),
  s_apply=sapply(look_backs,
    function(look_back) sum(price_s[look_back])),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series Using Function \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calculates a \emph{recursive} filter over the vector of random \emph{innovations} $\xi_i$ as follows:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI prices
price_s <- quantmod::Cl(rutils::etf_env$VTI)
# Calculate EWMA prices using filter()
look_back <- 21
weight_s <- exp(-0.1*1:look_back)
weight_s <- weight_s/sum(weight_s)
filter_ed <- stats::filter(price_s, filter=weight_s,
                         method="convolution", sides=1)
# filter() returns time series of class "ts"
class(filter_ed)
# Filter using compiled C++ function directly
getAnywhere(C_cfilter)
str(stats:::C_cfilter)
filter_fast <- .Call(stats:::C_cfilter, price_s, filter=weight_s, sides=1, circular=FALSE)
all.equal(as.numeric(filter_ed), filter_fast, check.attributes=FALSE)
# Calculate EWMA prices using roll::roll_sum()
weights_rev <- rev(weight_s)
roll_ed <- roll::roll_sum(price_s, width=look_back, weights=weights_rev)
all.equal(filter_ed[-(1:look_back)],
          as.numeric(roll_ed)[-(1:look_back)],
          check.attributes=FALSE)
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  filter=filter(price_s, filter=weight_s, method="convolution", sides=1),
  filter_fast=.Call(stats:::C_cfilter, price_s, filter=weight_s, sides=1, circular=FALSE),
  cum_sum=cumsum(price_s),
  roll=roll::roll_sum(price_s, width=look_back, weights=weights_rev)
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{TTR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{TTR} contains functions for calculating rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{runSum()} for rolling sums,
        \item \texttt{runMin()} and \texttt{runMax()} for rolling minima and maxima,
        \item \texttt{runSD()} for rolling standard deviations,
        \item \texttt{runMedian()} and \texttt{runMAD()} for rolling medians and Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runCor()} for rolling correlations,
      \end{itemize}
      The rolling \emph{TTR} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} or \texttt{Fortran} code).
      \vskip1ex
      But the rolling \emph{TTR} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the rolling maximum and minimum over a vector of data
roll_maxminr <- function(vec_tor, look_back) {
  n_rows <- NROW(vec_tor)
  max_min <- matrix(numeric(2*n_rows), nc=2)
  # Loop over periods
  for (it in 1:n_rows) {
    sub_vec <- vec_tor[max(1, it-look_back+1):it]
    max_min[it, 1] <- max(sub_vec)
    max_min[it, 2] <- min(sub_vec)
  }  # end for
  return(max_min)
}  # end roll_maxminr
max_minr <- roll_maxminr(price_s, look_back)
max_minr <- xts::xts(max_minr, index(price_s))
library(TTR)  # Load package TTR
max_min <- cbind(TTR::runMax(x=price_s, n=look_back),
                 TTR::runMin(x=price_s, n=look_back))
all.equal(max_min[-(1:look_back), ], max_minr[-(1:look_back), ], check.attributes=FALSE)
# Benchmark the speed of TTR::runMax
library(microbenchmark)
summary(microbenchmark(
  pure_r=roll_maxminr(price_s, look_back),
  ttr=TTR::runMax(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
# Benchmark the speed of TTR::runSum
summary(microbenchmark(
  vector_r=cumsum(coredata(price_s)),
  rutils=rutils::roll_sum(price_s, look_back=look_back),
  ttr=TTR::runSum(price_s, n=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()}, \texttt{roll\_max()}, \texttt{roll\_mean()}, and \texttt{roll\_median()} for \emph{weighted} rolling sums, maximums, means, and medians,
        \item \texttt{roll\_var()} for \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for rolling scaling and centering of time series,
        \item \texttt{roll\_lm()} for rolling regression,
        \item \texttt{roll\_pcr()} for rolling principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate rolling VTI variance using package roll
library(roll)  # Load roll
re_turns <- na.omit(rutils::etf_env$re_turns[, "VTI"])
look_back <- 22
# Calculate rolling sum using RcppRoll
sum_roll <- roll::roll_sum(re_turns, width=look_back)
# Calculate rolling sum using rutils
sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
all.equal(sum_roll[-(1:look_back), ], sum_rutils[-(1:look_back), ], check.attributes=FALSE)
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(re_turns),
  roll=roll::roll_sum(re_turns, width=look_back),
  RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
  rutils=rutils::roll_sum(re_turns, look_back=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{RcppRoll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{RcppRoll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for \emph{weighted} rolling sums,
        \item \texttt{roll\_min()} and \texttt{roll\_max()} for \emph{weighted} rolling minima and maxima,
        \item \texttt{roll\_sd()} for \emph{weighted} rolling standard deviations,
        \item \texttt{roll\_median()} for \emph{weighted} rolling medians,
      \end{itemize}
      The \emph{RcppRoll} functions accept \emph{xts} objects, but they return matrices, not \emph{xts} objects.
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(RcppRoll)  # Load package RcppRoll
# Calculate rolling sum using RcppRoll
sum_roll <- RcppRoll::roll_sum(re_turns, align="right", n=look_back)
# Calculate rolling sum using rutils
sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
all.equal(sum_roll, coredata(sum_rutils[-(1:(look_back-1))]), check.attributes=FALSE)
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  cum_sum=cumsum(re_turns),
  RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
  rutils=rutils::roll_sum(re_turns, look_back=look_back),
  times=10))[, c(1, 4, 5)]
# Calculate EWMA prices using RcppRoll
price_s <- na.omit(rutils::etf_env$VTI[, 4])
weight_s <- exp(0.1*1:look_back)
prices_ewma <- RcppRoll::roll_mean(price_s,
      align="right", n=look_back, weights=weight_s)
prices_ewma <- cbind(price_s,
  rbind(coredata(price_s[1:(look_back-1), ]), prices_ewma))
colnames(prices_ewma) <- c("VTI", "VTI EWMA")
# Plot an interactive dygraph plot
dygraphs::dygraph(prices_ewma)
# Or static plot of EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
chart_Series(prices_ewma, theme=plot_theme,
             name="EWMA prices")
legend("top", legend=colnames(prices_ewma),
       bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{caTools}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{caTools} contains functions for calculating rolling interval aggregations over a \texttt{vector} of data:
      \begin{itemize}
        \item \texttt{runmin()} and \texttt{runmax()} for rolling minima and maxima,
        \item \texttt{runsd()} for rolling standard deviations,
        \item \texttt{runmad()} for rolling Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runquantile()} for rolling quantiles,
      \end{itemize}
      Time series need to be coerced to \emph{vectors} before they are passed to \emph{caTools} functions.
      \vskip1ex
      The rolling \emph{caTools} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      The argument \texttt{"endrule"} determines how the end values of the data are treated.
      \vskip1ex
      The argument \texttt{"align"} determines whether the interval is centered (default), left-aligned or right-aligned, with \texttt{align="center"} the fastest option.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
library(caTools)  # Load package "caTools"
# Get documentation for package "caTools"
packageDescription("caTools")  # Get short description
help(package="caTools")  # Load help page
data(package="caTools")  # List all datasets in "caTools"
ls("package:caTools")  # List all objects in "caTools"
detach("package:caTools")  # Remove caTools from search path
# Median filter
look_back <- 2
price_s <- quantmod::Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
med_ian <- runmed(x=price_s, k=look_back)
# Vector of rolling volatility
sigma_r <- runsd(x=price_s, k=look_back,
                endrule="constant", align="center")
# Vector of rolling quantiles
quan_tiles <- runquantile(x=price_s, k=look_back,
  probs=0.9, endrule="constant", align="center")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Calendar \protect\emph{end points} of \protect\emph{xts} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vskip1ex
      For example:\\ \-\ \texttt{endpoints(x, on="hours")}\\
      extracts the indices of the last observations in each hour.
      \vskip1ex
      The \emph{end points} calculated by \texttt{endpoints()} aren't always equally spaced, and aren't the same as those calculated from fixed intervals.
      \vskip1ex
      For example, the last observations in each day aren't equally spaced due to weekends and holidays.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Indices of last observations in each hour
end_p <- xts::endpoints(price_s, on="hours")
head(end_p)
# extract the last observations in each hour
head(price_s[end_p, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
price_s <- quantmod::Cl(rutils::etf_env$VTI)
# Number of data points
n_rows <- NROW(price_s)
# Number of data points per interval
look_back <- 22
# Number of look_backs that fit over n_rows
n_agg <- n_rows %/% look_back
# Define end_p with beginning stub
end_p <- c(0, n_rows-look_back*n_agg + (0:n_agg)*look_back)
# Define contiguous start_p
start_p <- c(0, end_p[1:(NROW(end_p)-1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(2:NROW(end_p), function(in_dex) {
    start_p[in_dex]:end_p[in_dex]
})  # end lapply
look_backs[[1]]
look_backs[[2]]
# Perform sapply() loop over look_backs list
agg_regations <- sapply(look_backs,
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
  })  # end sapply
# Coerce agg_regations into matrix and transpose it
if (is.vector(agg_regations))
  agg_regations <- t(agg_regations)
agg_regations <- t(agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations,
    order.by=index(price_s[end_p]))
head(agg_regations)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(agg_regations),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Perform lapply() loop over look_backs list
agg_regations <- lapply(look_backs,
    function(look_back) {
      x_ts <- price_s[look_back]
      c(max=max(x_ts), min=min(x_ts))
    })  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call(rbind, agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations,
    order.by=index(price_s[end_p]))
head(agg_regations)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(agg_regations),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations Using \texttt{period.apply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} from package \emph{xts} performs \emph{aggregations} over non-overlapping intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      Internally \texttt{period.apply()} performs an \texttt{sapply()} loop, and is therefore about as fast as an \texttt{sapply()} loop.
      \vskip1ex
      The package \emph{xts} also has several specialized functionals for aggregating data over \emph{end points}:
      \begin{itemize}
        \item \texttt{period.sum()} calculate the sum for each period,
        \item \texttt{period.max()} calculate the maximum for each period,
        \item \texttt{period.min()} calculate the minimum for each period,
        \item \texttt{period.prod()} calculate the product for each period,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define functional for rolling aggregations over end_p
roll_agg <- function(x_ts, end_p, FUN, ...) {
  n_rows <- NROW(end_p)
# start_p are single-period lag of end_p
  start_p <- c(1, end_p[1:(n_rows-1)])
# Perform aggregations over look_backs list
  agg_regations <- lapply(look_backs,
    function(look_back) FUN(x_ts[look_back], ...))  # end lapply
# rbind list into single xts or matrix
  agg_regations <- rutils::do_call(rbind, agg_regations)
  if (!is.xts(agg_regations))
    agg_regations <-  # Coerce agg_regations into xts series
    xts(agg_regations, order.by=index(x_ts[end_p]))
  agg_regations
}  # end roll_agg
# Apply sum() over end_p
agg_regations <-
  roll_agg(price_s, end_p=end_p, FUN=sum)
agg_regations <-
  period.apply(price_s, INDEX=end_p, FUN=sum)
# Benchmark the speed of aggregation functions
summary(microbenchmark(
  roll_agg=roll_agg(price_s, end_p=end_p, FUN=sum),
  period_apply=period.apply(price_s, INDEX=end_p, FUN=sum),
  times=10))[, c(1, 4, 5)]
agg_regations <- period.sum(price_s, INDEX=end_p)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations of \protect\emph{xts} Over Calendar Periods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{xts} has convenience wrapper functionals for \texttt{period.apply()}, that apply functions over calendar periods:
      \begin{itemize}
        \item \texttt{apply.daily()} applies functions over daily periods,
        \item \texttt{apply.weekly()} applies functions over weekly periods,
        \item \texttt{apply.monthly()} applies functions over monthly periods,
        \item \texttt{apply.quarterly()} applies functions over quarterly periods,
        \item \texttt{apply.yearly()} applies functions over yearly periods,
      \end{itemize}
      These functionals don't require specifying a vector of \emph{end points}, because they determine the \emph{end points} from the calendar periods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Load package HighFreq
library(HighFreq)
# Extract closing minutely prices
price_s <- quantmod::Cl(rutils::etf_env$VTI["2019"])
# Apply "mean" over daily periods
agg_regations <- apply.daily(price_s, FUN=sum)
head(agg_regations)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations Over Overlapping Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} performs aggregations over \emph{non-overlapping} intervals.
      \vskip1ex
      But it's often necessary to perform aggregations over \emph{overlapping} intervals, defined by a vector of \emph{end points} and a \emph{look-back interval}.
      \vskip1ex
      The \emph{start points} are defined as the \emph{end points} lagged by the interval width (number of periods in the \emph{look-back interval}).
      \vskip1ex
      Each point in time has an associated \emph{look-back interval}, which starts at a certain number of periods in the past (\emph{start\_point}) and ends at that point (\emph{end\_point}).
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the \emph{look-back interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end_p with beginning stub
n_points <- 5
n_rows <- NROW(price_s)
n_agg <- n_rows %/% n_points
end_p <- c(0, n_rows-n_points*n_agg + (0:n_agg)*n_points)
# Number of data points in look_back interval
look_back <- 22
# start_p are end_p lagged by look_back
start_p <- (end_p - look_back + 1)
start_p <- ifelse(start_p < 0, 0, start_p)
# Perform lapply() loop over look_backs list
agg_regations <- lapply(2:NROW(end_p), function(in_dex) {
      x_ts <- price_s[start_p[in_dex]:end_p[in_dex]]
      c(max=max(x_ts), min=min(x_ts))
})  # end lapply
# rbind list into single xts or matrix
agg_regations <- rutils::do_call(rbind, agg_regations)
# Coerce agg_regations into xts series
agg_regations <- xts(agg_regations,
    order.by=index(price_s[end_p]))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
chart_Series(agg_regations, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(agg_regations),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extending Interval Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Interval aggregations produce values only at the \emph{end points}, but they can be carried forward in time using the function \texttt{na.locf.xts()} from package \emph{xts}.
      <<echo=TRUE,eval=FALSE>>=
agg_regations <- cbind(price_s, agg_regations)
tail(agg_regations)
agg_regations <- na.omit(xts:::na.locf.xts(agg_regations))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
chart_Series(agg_regations, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(agg_regations),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/agg_interval_carryfwd.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations of \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The method \texttt{aggregate.zoo()} performs aggregations of \emph{zoo} series over non-overlapping intervals defined by a vector of aggregation groups (minutes, hours, days, etc.).
      \vskip1ex
      For example, \texttt{aggregate.zoo()} can calculate the average monthly returns.
      <<echo=(-(1:3)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(zoo)  # Load package zoo
# Create zoo time series of random returns
date_s <- Sys.Date() + 0:365
zoo_series <- zoo(rnorm(NROW(date_s)), order.by=date_s)
# Create monthly dates
dates_agg <- as.Date(as.yearmon(index(zoo_series)))
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, FUN=mean)
# Merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# Replace NA's using locf
zoo_agg <- na.locf(zoo_agg, na.rm=FALSE)
# Extract aggregated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_agg-1}
      \vspace{-7em}
      <<zoo_agg,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
# library(rutils)  # Load package rutils
# Plot original and aggregated cumulative returns
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, bty="n",
       title="Aggregated Prices",
       leg=c("orig prices", "agg prices"),
       lwd=2, bg="white", col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpolating \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has two functions for replacing \texttt{NA} values using interpolation:
      \begin{itemize}
        \item \texttt{na.approx()} performs linear interpolation,
        \item \texttt{na.spline()} performs spline interpolation,
      \end{itemize}
      \vspace{-1em}
      <<zoo_interpol,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=dates_agg, FUN=mean)
# Merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# Replace NA's using linear interpolation
zoo_agg <- na.approx(zoo_agg)
# Extract interpolated zoo
zoo_agg <- zoo_agg[index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Interpolated Prices",
       leg=c("orig prices", "interpol prices"), lwd=2, bg="white",
       col=c("black", "red"), bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_interpol-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Over \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has several functions for rolling calculations:
      \begin{itemize}
        \item \texttt{rollapply()} performing aggregations over a rolling (sliding) interval,
        \item \texttt{rollmean()} calculating rolling means,
        \item \texttt{rollmedian()} calculating rolling median,
        \item \texttt{rollmax()} calculating rolling max,
      \end{itemize}
      \vspace{-1em}
      <<zoo_roll,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# "mean" aggregation over interval with width=11
zoo_mean <- rollapply(zoo_series, width=11,
                      FUN=mean, align="right")
# Merge with original zoo - union of dates
zoo_mean <- cbind(zoo_series, zoo_mean)
# Replace NA's using na.locf
zoo_mean <- na.locf(zoo_mean, na.rm=FALSE, fromLast=TRUE)
# Extract mean zoo
zoo_mean <- zoo_mean[index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_mean), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Mean Prices",
       leg=c("orig prices", "mean prices"), lwd=2, bg="white",
       col=c("black", "red"), bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/zoo_roll-1}
      \vspace{-3em}
      The argument \texttt{align="right"} determines that aggregations are taken from the past,
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\section{Time Series Modeling}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimator of \emph{autocorrelation} of a time series is equal to:
      \begin{displaymath}
        \rho_k = \frac{\sum_{i=k+1}^n (x_i-\bar{x})(x_{i-k}-\bar{x})}{(n-k) \, \sigma^2}
      \end{displaymath}
      The \emph{autocorrelation function} (\emph{ACF}) is the vector of autocorrelation coefficients.
      \vskip1ex
      The function \texttt{stats::acf()} calculates and plots the autocorrelation function of a time series.
      \vskip1ex
      The function \texttt{stats::acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
      \vskip1ex
      The function \texttt{as.numeric()} coerces complex data objects into \texttt{numeric} vectors, and removes all their \emph{attributes}.
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 2, 1, 1), oma=c(0, 0, 0, 0), mgp=c(2, 0.5, 0))
re_turns <- as.numeric(na.omit(rutils::etf_env$re_turns$VTI))
# Plot autocorrelations using stats::acf()
stats::acf(re_turns, lag=10, xlab="lag", main="")
title(main="ACF of VTI Returns", line=-1)
# Two-tailed 95% confidence interval
qnorm(0.975)/sqrt(NROW(re_turns))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_vti.png}\\
      The \emph{VTI} time series of returns does not appear to have statistically significant autocorrelations.
      \vskip1ex
      The horizontal dashed lines are two-tailed confidence intervals of the autocorrelation estimator at \texttt{95\%} significance level: $\frac{\Phi^{-1}(0.975)}{\sqrt{n}}$.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ljung-Box Test for Autocorrelations of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test, tests if the autocorrelations of a time series are \emph{statistically significant}.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Ljung-Box} test is that the autocorrelations are equal to zero.
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where \texttt{n} is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations.
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom.
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that have \emph{statistically insignificant} autocorrelations.
      \vskip1ex
      The function \texttt{Box.test()} calculates the \emph{Ljung-Box} test and returns the test statistic and its p-value.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Ljung-Box test for VTI returns
# 'lag' is the number of autocorrelation coefficients
Box.test(re_turns, lag=10, type="Ljung")
library(Ecdat)  # Load Ecdat
macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
macro_diff <- na.omit(diff(macro_zoo))
# Changes in 3 month T-bill rate are autocorrelated
Box.test(macro_diff[, "3mTbill"], lag=10, type="Ljung")
# Changes in unemployment rate are autocorrelated
Box.test(macro_diff[, "unemprate"], lag=10, type="Ljung")
      @
      The \emph{p}-value for \emph{VTI} returns is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and that \emph{VTI} returns are \emph{not} autocorrelated.
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
      \vskip1ex
      Inspection of the data returned by \texttt{acf()} shows how to omit the lag zero autocorrelation.
      \vskip1ex
      The function \texttt{acf()} returns the \emph{ACF} data invisibly, i.e. the return value can be assigned to a variable, but otherwise it isn't automatically printed to the console.
      \vskip1ex
      The function \texttt{rutils::plot\_acf()} from package \emph{rutils} is a wrapper for \texttt{acf()}, and it omits the lag zero autocorrelation.
      <<echo=TRUE,eval=FALSE>>=
# Get the ACF data returned invisibly
acf_data <- acf(re_turns, plot=FALSE)
summary(acf_data)
# Print the ACF data
print(acf_data)
dim(acf_data$acf)
dim(acf_data$lag)
head(acf_data$acf)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot_acf <- function(x_ts, lagg=10,
                     plo_t=TRUE,
                     xlab="Lag", ylab="",
                     main="", ...) {
  # Calculate the ACF without a plot
  acf_data <- acf(x=x_ts, lag.max=lagg, plot=FALSE, ...)
  # Remove first element of ACF data
  acf_data$acf <- array(data=acf_data$acf[-1],
    dim=c((dim(acf_data$acf)[1]-1), 1, 1))
  acf_data$lag <- array(data=acf_data$lag[-1],
    dim=c((dim(acf_data$lag)[1]-1), 1, 1))
  # Plot ACF
  if (plo_t) {
    ci <- qnorm((1+0.95)/2)*sqrt(1/NROW(x_ts))
    ylim <- c(min(-ci, range(acf_data$acf[-1])),
              max(ci, range(acf_data$acf[-1])))
    plot(acf_data, xlab=xlab, ylab=ylab,
         ylim=ylim, main="", ci=0)
    title(main=main, line=0.5)
    abline(h=c(-ci, ci), col="blue", lty=2)
  }
  # Return the ACF data invisibly
  invisible(acf_data)
}  # end plot_acf
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of \protect\emph{VTI} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VTI} time series of returns does not appear to have statistically significant autocorrelations.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
        <<echo=TRUE,eval=FALSE>>=
# Improved autocorrelation function
rutils::plot_acf(re_turns, lag=10, main="")
title(main="ACF of VTI returns", line=-1)
# Ljung-Box test for VTI returns
Box.test(re_turns, lag=10, type="Ljung")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_vti_bis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Squared \protect\emph{VTI} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Squared random returns are not autocorrelated.
      \vskip1ex
      But squared \emph{VTI} returns do have statistically significant autocorrelations.
      \vskip1ex
      The autocorrelations of squared asset returns are a very important feature.
      <<VTI_squared_acf,echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=7)
par(mfrow=c(2,1))  # Set plot panels
par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
# Autocorrelation of squared random returns
rutils::plot_acf(rnorm(NROW(re_turns))^2, lag=10, main="")
title(main="ACF of Squared Random Returns", line=-1)
# Autocorrelation of squared VTI returns
rutils::plot_acf(re_turns^2, lag=10, main="")
title(main="ACF of Squared VTI Returns", line=-1)
# Ljung-Box test for squared VTI returns
Box.test(re_turns^2, lag=10, type="Ljung")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_vti_squared.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{U.S. Macroeconomic Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{Ecdat} contains the \texttt{Macrodat} U.S. macroeconomic data.
      \vskip1ex
      \texttt{"lhur"} is the unemployment rate (average of months in quarter).
      \vskip1ex
      \texttt{"fygm3"} 3 month treasury bill interest rate (last month in quarter)
      <<macro_data,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
library(rutils)  # Load package rutils
library(Ecdat)  # Load Ecdat
colnames(Macrodat)  # United States Macroeconomic Time Series
macro_zoo <- as.zoo(  # Coerce to "zoo"
          Macrodat[, c("lhur", "fygm3")])
colnames(macro_zoo) <- c("unemprate", "3mTbill")
# ggplot2 in multiple panes
autoplot(  # Generic ggplot2 for "zoo"
  object=macro_zoo, main="US Macro",
  facets=Series ~ .) + # end autoplot
  xlab("") +
theme(  # modify plot theme
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank()
)  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/macro_data-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Econometric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most econometric data displays a high degree of autocorrelation.
      \vskip1ex
      But the time series of asset returns display very low autocorrelations.
      <<macro_corr,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
macro_diff <- na.omit(diff(macro_zoo))
rutils::plot_acf(coredata(macro_diff[, "unemprate"]),
  lag=10, main="quarterly unemployment rate")
rutils::plot_acf(coredata(macro_diff[, "3mTbill"]),
  lag=10, main="3 month T-bill EOQ")
      @
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/macro_corr-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series Using Function \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI prices
price_s <- na.omit(rutils::etf_env$price_s$VTI)
n_rows <- NROW(price_s)
# Apply convolution filter over past values (sides=1)
co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
filter_ed <- filter(price_s, filter=co_eff,
                    method="convolution", sides=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Filtering Using Compiled Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
      \vskip1ex
      \texttt{filter()} with \texttt{method="convolution"} calls the function \texttt{stats:::C\_cfilter} to calculate the \emph{convolution}.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calculates a \emph{recursive} filter over the vector of random \emph{innovations} $\xi_i$ as follows:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calls the function \texttt{stats:::C\_rfilter} to calculate the \emph{recursive filter}.
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled \texttt{C++} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Inspect the R code of the function filter()
filter
# Get information about C_cfilter()
getAnywhere(C_cfilter)
# Filter over past values (sides=1)
filter_ed <- filter(price_s, filter=co_eff,
                    method="convolution", sides=1)
# Filter using C_cfilter() compiled C++ function directly
filter_fast <- .Call(stats:::C_cfilter, price_s, filter=co_eff,
                     sides=1, circular=FALSE)
all.equal(as.numeric(filter_ed), filter_fast,
          check.attributes=FALSE)
# Benchmark speed of the two methods
library(microbenchmark)
summary(microbenchmark(
  fast_filter=.Call(stats:::C_cfilter, price_s, filter=co_eff, sides=1, circular=FALSE),
  R_filter=filter(price_s, filter=co_eff, method="convolution", sides=1)
  ), times=10)[, c(1, 4, 5)]
# Simulate AR process using filter()
in_nov <- rnorm(n_rows)
ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
# Get information about C_rfilter()
getAnywhere(C_rfilter)
# Filter using C_rfilter() compiled C++ function directly
arima_fast <- .Call(stats:::C_rfilter, in_nov, co_eff,
                    double(n_coeff + n_rows))
all.equal(as.numeric(ari_ma), arima_fast[-(1:n_coeff)], check.attributes=FALSE)
# Benchmark speed of the two methods
summary(microbenchmark(
  fast_filter=.Call(stats:::C_rfilter, in_nov, co_eff, double(NROW(co_eff) + NROW(in_nov))),
  R_filter=filter(x=in_nov, filter=co_eff, method="recursive")
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<VTI_filter,eval=FALSE,fig.width=6,fig.height=5,fig.show='hide'>>=
library(rutils)  # Load package rutils
library(ggplot2)  # Load ggplot2
library(gridExtra)  # Load gridExtra
# Coerce to zoo and merge the time series
filter_ed <- cbind(price_s, as.numeric(filter_ed))
colnames(filter_ed) <- c("VTI", "VTI filtered")
# Plot ggplot2
autoplot(filter_ed["2008/2010"],
    main="Filtered VTI", facets=NULL) +  # end autoplot
xlab("") + ylab("") +
theme(  # modify plot theme
    legend.position=c(0.1, 0.5),
    plot.title=element_text(vjust=-2.0),
    plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
    plot.background=element_blank(),
    axis.text.y=element_blank()
    )  # end theme
# end ggplot2
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ggplot_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering a time series creates autocorrelations.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window
x11(width=6, height=7)
# Set plot parameters
par(oma=c(1, 1, 0, 1), mar=c(1, 1, 1, 1), mgp=c(0, 0.5, 0),
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Set two plot panels
par(mfrow=c(2,1))
# Calculate VTI returns
re_turns <- na.omit(diff(log(filter_ed)))
# Plot ACF of VTI returns
rutils::plot_acf(re_turns[, 1], lag=10, xlab="")
title(main="ACF of VTI Returns", line=-1)
# Plot ACF of VTI returns
rutils::plot_acf(re_turns[, 2], lag=10, xlab="")
title(main="ACF of VTI Filtered Returns", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/acf_vti_filter.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} of order \emph{p} for a time series $r_i$ is defined as:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $\varphi_i$ are the \emph{AR(p)} coefficients, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{AR(p)} process is a special case of an \emph{ARIMA} process, and is simply called an \emph{AR(p)} process.
      \vskip1ex
      If the \emph{AR(p)} process is \emph{stationary} then the time series $r_i$ is mean reverting to zero.
      \vskip1ex
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes, with the \texttt{"model"} argument accepting a \texttt{list} of \emph{AR(p)} coefficients $\varphi_i$.
    <<echo=(-(1:2)),eval=FALSE>>=
# Simulate AR processes
set.seed(1121)  # Reset random numbers
date_s <- Sys.Date() + 0:728  # Two year daily series
ari_ma <- xts(  # AR time series of returns
  x=arima.sim(n=NROW(date_s), model=list(ar=0.2)),
  order.by=date_s)
ari_ma <- cbind(ari_ma, cumsum(ari_ma))
colnames(ari_ma) <- c("AR returns", "AR prices")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      % \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_process.png}
      \vspace{-3em}
    <<echo=(-(1:2)),eval=FALSE>>=
library(ggplot2)  # Load ggplot2
library(gridExtra)  # Load gridExtra
autoplot(object=ari_ma, # ggplot AR process
     facets="Series ~ .",
     main="Autoregressive process (phi=0.2)") +
  facet_grid("Series ~ .", scales="free_y") +
  xlab("") + ylab("") +
theme(legend.position=c(0.1, 0.5),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Examples of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of mean reversion of an \emph{AR(1)} process depends on the \emph{AR(p)} coefficient $\varphi_1$, with a negative coefficient producing faster mean reversion, and a positive coefficient producing stronger diversion.
      \vskip1ex
      A positive coefficient $\varphi_1$ produces a diversion away from the mean, so that the time series $r_i$ wanders away from the mean for longer periods of time.
      <<echo=TRUE,eval=FALSE>>=
ar_coeff <- c(-0.9, 0.01, 0.9)  # AR coefficients
# Create three AR time series
ari_ma <- sapply(ar_coeff, function(phi) {
  set.seed(1121)  # Reset random numbers
  arima.sim(n=NROW(date_s), model=list(ar=phi))
})  # end sapply
colnames(ari_ma) <- paste("autocorr", ar_coeff)
plot.zoo(ari_ma, main="AR(1) prices", xlab=NA)
# Or plot using ggplot
ari_ma <- xts(x=ari_ma, order.by=date_s)
library(ggplot)
autoplot(ari_ma, main="AR(1) prices",
         facets=Series ~ .) +
    facet_grid(Series ~ ., scales="free_y") +
xlab("") +
theme(
  legend.position=c(0.1, 0.5),
  plot.title=element_text(vjust=-2.0),
  plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
  plot.background=element_blank(),
  axis.text.y=element_blank())
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_processes.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated by using an explicit recursive loop in \texttt{R}.
      \vskip1ex
      \emph{AR(p)} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      The function \texttt{filter()} applies a linear filter to a vector, and returns a time series of class \texttt{"ts"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define AR(3) coefficients and innovations
co_eff <- c(0.1, 0.39, 0.5)
n_rows <- 1e2
set.seed(1121); in_nov <- rnorm(n_rows)
# Simulate AR process using recursive loop in R
ari_ma <- numeric(NROW(in_nov))
ari_ma[1] <- in_nov[1]
ari_ma[2] <- co_eff[1]*ari_ma[1] + in_nov[2]
ari_ma[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1] + in_nov[3]
for (it in 4:NROW(ari_ma)) {
  ari_ma[it] <- ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]
}  # End for
# Simulate AR process using filter()
arima_faster <- filter(x=in_nov,
  filter=co_eff, method="recursive")
class(arima_faster)
all.equal(ari_ma, as.numeric(arima_faster))
# Fast simulation of AR process using C_rfilter()
arima_fastest <- .Call(stats:::C_rfilter, in_nov, co_eff,
                       double(NROW(co_eff) + NROW(in_nov)))[-(1:3)]
all.equal(ari_ma, arima_fastest)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes Using \texttt{arima.sim()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes by calling the function \texttt{filter()}.
      \vskip1ex
      \emph{ARIMA} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      Simulating stationary \emph{autoregressive} processes requires a \emph{warmup period}, to allow the process to reach its stationary state.
      \vskip1ex
      The required length of the \emph{warmup period} depends on the smallest root of the characteristic equation, with a longer \emph{warmup period} needed for smaller roots, that are closer to $1$.
      \vskip1ex
      The \emph{rule of thumb} (heuristic rule, guideline) is for the \emph{warmup period} to be equal to \texttt{6} divided  by the logarithm of the smallest characteristic root plus the number of \emph{AR(p)} coefficients: $\frac{6}{\log(minroot)} + \operatorname{numcoeff}$
    \column{0.5\textwidth}
      \vspace{-1em}
    <<echo=TRUE,eval=FALSE>>=
# Calculate modulus of roots of characteristic equation
root_s <- Mod(polyroot(c(1, -co_eff)))
# Calculate warmup period
warm_up <- NROW(co_eff) + ceiling(6/log(min(root_s)))
set.seed(1121)
n_rows <- 1e4
in_nov <- rnorm(n_rows + warm_up)
# Simulate AR process using arima.sim()
ari_ma <- arima.sim(n=n_rows,
  model=list(ar=co_eff),
  start.innov=in_nov[1:warm_up],
  innov=in_nov[(warm_up+1):NROW(in_nov)])
# Simulate AR process using filter()
arima_fast <- filter(x=in_nov,
  filter=co_eff, method="recursive")
all.equal(arima_fast[-(1:warm_up)],
  as.numeric(ari_ma))
# Benchmark the speed of the three methods of simulating AR process
library(microbenchmark)
summary(microbenchmark(
  filter=filter(x=in_nov, filter=co_eff, method="recursive"),
  arima_sim=arima.sim(n=n_rows,
                      model=list(ar=co_eff),
                      start.innov=in_nov[1:warm_up],
                      innov=in_nov[(warm_up+1):NROW(in_nov)]),
  arima_loop={for (it in 4:NROW(ari_ma)) {
  ari_ma[it] <- ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]}}
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stationary Processes and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A process is \emph{stationary} if its probability distribution does not change with time, which means that it has constant mean and variance.
      \vskip1ex
      The \emph{autoregressive} process \emph{AR(p)}:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Has the following characteristic equation:
      $1 - \varphi_1 z - \varphi_2 z^2 - \ldots - \varphi_p z^p = 0$
      \vskip1ex
      An autoregressive process is \emph{stationary} only if the absolute values of all the roots of its characteristic equation are greater than $1$.
      \vskip1ex
      If the sum of the autoregressive coefficients is equal to $1$: $\sum_{i=1}^p \varphi_i = 1$, then the process has a root equal to $1$ (it has a \emph{unit root}), so it's not \emph{stationary}.
      \vskip1ex
      Non-stationary processes with unit roots are called \emph{unit-root} processes.
      \vskip1ex
      A simple example of a \emph{unit-root} process is the \emph{Wiener process}:
      $p_i = p_{i-1} + \xi_i$
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/stat_unit_root-1}
      \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
library(rutils)  # Load rutils
library(ggplot2)  # Load ggplot2
set.seed(1121)  # Initialize random number generator
rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
                  order.by=(Sys.Date()+0:99)))
colnames(rand_walk) <- paste("rand_walk", 1:3, sep="_")
plot.zoo(rand_walk, main="Random walks",
     xlab="", ylab="", plot.type="single",
     col=c("black", "red", "blue"))
# Add legend
legend(x="topleft", legend=colnames(rand_walk),
       col=c("black", "red", "blue"), lty=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Integrated and Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_n = {\sum_{i=1}^n r_i}$.
      \vskip1ex
      If returns follow an \emph{AR(p)} process:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Then asset prices follow the process:
      $p_i = (1 + \varphi_1) p_{i-1} + (\varphi_2 - \varphi_1) p_{i-2} + \ldots + (\varphi_p - \varphi_{p-1}) p_{i-p} - \varphi_p p_{i-p-1} + \xi_i$
      \vskip1ex
      The sum of the coefficients of the price process is equal to $1$, so it has a \emph{unit root} for all values of the $\varphi_i$ coefficients.
      \vskip1ex
      The \emph{integrated} process of an \emph{AR(p)} process is always a \emph{unit root} process.
    \column{0.5\textwidth}
      In particular, if returns follow an \emph{AR(1)} process: $r_i = \varphi r_{i-1} + \xi_i$.
      \vskip1ex
      Then asset prices follow the process: $p_i = (1 + \varphi) p_{i-1} - \varphi p_{i-2} + \xi_i$
      \vskip1ex
      Which is a \emph{unit root} process for all values of $\varphi$, because the sum of its coefficients is equal to $1$.
      \vskip1ex
      If $\varphi = 0$ then the above process is a \emph{Wiener process} (random walk).
      <<echo=TRUE,eval=FALSE>>=
# Simulate arima with large AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=n_rows, model=list(ar=0.99))
tseries::adf.test(ari_ma)
# Integrated series has unit root
tseries::adf.test(cumsum(ari_ma))
# Simulate arima with negative AR coefficient
set.seed(1121)
ari_ma <- arima.sim(n=n_rows, model=list(ar=-0.99))
tseries::adf.test(ari_ma)
# Integrated series has unit root
tseries::adf.test(cumsum(ari_ma))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Unit Roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Augmented Dickey-Fuller} \emph{ADF} test is designed to test the \emph{null hypothesis} that a time series has a \emph{unit root}.
      \vskip1ex
      The \emph{ADF} test fits an autoregressive model with an extra mean reversion term:
      \begin{displaymath}
        r_i = \gamma (p_{i-1} - p_{eq}) + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $p_{eq}$ is the equilibrium price, towards which prices revert.
      \vskip1ex
      If the mean reversion parameter $\gamma$ is negative: $\gamma < 0$, then the time series $p_i$ exhibits mean reversion and has no \emph{unit root}.
      \vskip1ex
      The \emph{null hypothesis} is that prices have a unit root ($\gamma = 0$, no mean reversion), while the alternative hypothesis is that it's \emph{stationary} ($\gamma < 0$, mean reversion).
      \vskip1ex
      The \emph{ADF} test statistic is equal to the \emph{t}-value of the $\gamma$ parameter: $t_{\gamma} = \hat\gamma / SE_{\gamma}$ (which follows a distribution different from the \texttt{t}-distribution).
    \column{0.5\textwidth}
      The common practice is to use a small number of lags in the \emph{ADF} test, and if the residuals are autocorrelated, then to increase them until the correlations are no longer significant.
      \vskip1ex
      If the number of lags in the regression is zero: $p = 0$ then the \emph{ADF} test becomes the standard \emph{Dickey-Fuller} test: $r_i = \gamma p_{i-1} + \xi_i$.
      \vskip1ex
      The function \texttt{tseries::adf.test()} performs the \emph{ADF} test.
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121); vol_at <- 0.01; in_nov <- vol_at*rnorm(1e4)
# Simulate AR(1) process with coefficient=1, with unit root
ari_ma <- filter(x=in_nov, filter=1.0, method="recursive")
x11(); plot(ari_ma, t="l", main="AR(1) coefficient = 1.0")
# Perform ADF test with lag = 1
tseries::adf.test(ari_ma, k=1)
# Perform standard Dickey-Fuller test
tseries::adf.test(ari_ma, k=0)
# Simulate AR(1) with coefficient close to 1, without unit root
ari_ma <- filter(x=in_nov, filter=0.99, method="recursive")
x11(); plot(ari_ma, t="l", main="AR(1) coefficient = 0.99")
tseries::adf.test(ari_ma, k=1)
# Simulate Ornstein-Uhlenbeck OU process with mean reversion
eq_price <- 0.0; the_ta <- 0.001
price_s <- HighFreq::sim_ou(eq_price=eq_price, vol_at=1.0, the_ta=the_ta, in_nov=in_nov)
x11(); plot(price_s, t="l", main=paste("OU coefficient =", the_ta))
tseries::adf.test(price_s, k=1)
# Simulate Ornstein-Uhlenbeck OU process with zero reversion
the_ta <- 0.0
price_s <- HighFreq::sim_ou(eq_price=eq_price, vol_at=1.0, the_ta=the_ta, in_nov=in_nov)
x11(); plot(price_s, t="l", main=paste("OU coefficient =", the_ta))
tseries::adf.test(price_s, k=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Engle-Granger Two-step Procedure for Cointegration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ADF} test can be applied to test for the cointegration of time series of prices.
      \vskip1ex
      The Engle-Granger two-step procedure for two time series consists of:
      \begin{itemize}
        \item Performing a regression to calculate the cointegrating factor $\beta$,
        \item Applying the \emph{ADF} test to the residuals of the regression to determine that they don't have a unit root (they are mean-reverting).
      \end{itemize}
      The regression of prices is not statistically valid because they are not normally distributed.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate XLB and XLE prices
price_s <- na.omit(rutils::etf_env$price_s[, c("XLB", "XLE")])
xl_b <- drop(zoo::coredata(price_s$XLB))
xl_e <- drop(zoo::coredata(price_s$XLE))
# Calculate regression coefficients of XLB ~ XLE
be_ta <- cov(xl_b, xl_e)/var(xl_e)
al_pha <- (mean(xl_b) - be_ta*mean(xl_e))
# Calculate regression residuals
fit_ted <- (al_pha + be_ta*xl_e)
residual_s <- (xl_b - fit_ted)
# Perform ADF test on residuals
tseries::adf.test(residual_s, k=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Variance of Unit-root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR(1)} process:
      $r_i = \varphi r_{i-1} + \xi_i$
      has the following characteristic equation:
      $1 - \varphi z = 0$,
      with a root equal to:
      $z = 1 / \varphi$
      \vskip1ex
      If $\varphi = 1$, then the characteristic equation has a \emph{unit root} (and therefore it isn't \emph{stationary}), and the process follows:
      $r_i = r_{i-1} + \xi_i$
      \vskip1ex
      The above is called a \emph{Wiener process}, and it's an example of a \emph{unit-root} process.
      \vskip1ex
      The expected value of the \emph{AR(1)} process $r_i = \varphi r_{i-1} + \xi_i$ is equal to zero: $\mathbb{E}[r_i] = \frac{\mathbb{E}[\xi_i]}{1 - \varphi} = 0$.
      \vskip1ex
      And its variance is equal to: $\sigma^2 = \mathbb{E}[r_i^2] = \frac{\sigma_{\xi}^2}{1 - \varphi^2}$.
      \vskip1ex
      If $\varphi = 1$, then the \emph{variance} grows over time and becomes infinite over time, so the process isn't \emph{stationary}.
      \vskip1ex
      The variance of the \emph{Wiener process} $r_i = r_{i-1} + \xi$ is proportional to time: $\sigma_i^2 = \mathbb{E}[r_i^2] = i \sigma_{\xi}^2$
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/brownian_var.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate random walks using apply() loops
set.seed(1121)  # Initialize random number generator
rand_walks <- matrix(rnorm(1000*100), ncol=1000)
rand_walks <- apply(rand_walks, 2, cumsum)
vari_ance <- apply(rand_walks, 1, var)
# Simulate random walks using vectorized functions
set.seed(1121)  # Initialize random number generator
rand_walks <- matrixStats::colCumsums(matrix(rnorm(1000*100), ncol=1000))
vari_ance <- matrixStats::rowVars(rand_walks)
par(mar=c(5, 3, 2, 2), oma=c(0, 0, 0, 0))
plot(vari_ance, xlab="time steps", ylab="",
     t="l", col="blue", lwd=2,
     main="Variance of Random Walk")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The autocorrelation $\rho_i$ of an \emph{AR(1)} process (defined as $r_i = \varphi r_{i-1} + \xi_i$), satisfies the recursive equation: $\rho_i = \varphi \rho_{i-1}$, with $\rho_1 = \varphi$.
      \vskip1ex
      Therefore \emph{AR(1)} processes have exponentially decaying autocorrelations: $\rho_i = \varphi^i$.
      \vskip1ex
      The \emph{AR(1)} process can be solved recursively:
      \begin{align*}
        r_1 &= \xi_1 \\
        r_2 &= \varphi r_1 + \xi_2 = \xi_2 + \varphi \xi_1 \\
        r_3 &= \xi_3 + \varphi \xi_2 + \varphi^2 \xi_1 \\
        r_4 &= \xi_4 + \varphi \xi_3 + \varphi^2 \xi_2 + \varphi^3 \xi_1
      \end{align*}
      Therefore the \emph{AR(1)} process can be expressed as a \emph{moving average} (\emph{MA}) of the \emph{innovations} $\xi_i$: $r_i = \sum_{i=1}^n {\varphi^{i-1} \xi_i}$.
      \vskip1ex
      If $\varphi < 1.0$ then the influence of the innovation $\xi_i$ decays exponentially.
      \vskip1ex
      If $\varphi = 1.0$ then the influence of the random innovations $\xi_i$ persists indefinitely, so that the variance of $r_i$ increases linearly with time.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_acf.png}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF}.
      % \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0))
# Simulate AR(1) process
ari_ma <- arima.sim(n=1e3, model=list(ar=0.8))
# ACF of AR(1) process
ac_f <- rutils::plot_acf(ari_ma, lag=10, xlab="", ylab="",
  main="Autocorrelations of AR(1) process")
ac_f$acf[1:5]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If two random variables are both correlated to a third variable, then they are indirectly correlated with each other.
      \vskip1ex
      The indirect correlation can be removed by defining new variables with no correlation to the third variable.
      \vskip1ex
      The \emph{partial correlation} is the correlation after the correlations to the common variables are removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{align*}
        \varrho_1 &= \rho_1 \\
        \varrho_2 &= \rho_2 - \varrho_1 \rho_1 \\
        \varrho_3 &= \rho_3 - \varrho_1 \rho_2 - \varrho_2 \rho_1
      \end{align*}
      The function \texttt{pacf()} calculates and plots the \emph{partial autocorrelations}, but it performs regressions instead of using the Durbin-Levinson algorithm.
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_pacf.png}
      % \vspace{-2em}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} at lag one.
      <<echo=TRUE,eval=FALSE>>=
# PACF of AR(1) process
pac_f <- pacf(ari_ma, lag=10, xlab="", ylab="", main="")
title("Partial autocorrelations of AR(1) process", line=1)
pac_f <- drop(pac_f$acf)
pac_f[1:5]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations of \protect\emph{AR(1)} Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autocorrelation of lag $1$ induces higher order autocorrelations of lag \texttt{2, 3, ...}, which may obscure the true higher order autocorrelations.
      \vskip1ex
      A linear combination of the time series and its own lag can be created, such that its lag $1$ autocorrelation is zero.
      \vskip1ex
      The lag $2$ autocorrelation of this new series is called the \emph{partial autocorrelation} of lag $2$, and represents the true second order autocorrelation.
      \vskip1ex
      The \emph{partial autocorrelation} of lag \texttt{k} is the autocorrelation of lag \texttt{k}, after all the autocorrelations of lag \texttt{1, ..., k-1} have been removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{displaymath}
        \varrho_k = \rho_k - \sum_{i=1}^{k-1} {\varrho_i \rho_{k-i}}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compute pacf recursively from acf
ac_f <- rutils::plot_acf(ari_ma, lag=10, plo_t=FALSE)
ac_f <- drop(ac_f$acf)
pac_f <- numeric(3)
pac_f[1] <- ac_f[1]
pac_f[2] <- ac_f[2] - ac_f[1]^2
pac_f[3] <- ac_f[3] - pac_f[2]*ac_f[1] - ac_f[2]*pac_f[1]
# Compute pacf recursively in a loop
pac_f <- numeric(NROW(ac_f))
pac_f[1] <- ac_f[1]
for (it in 2:NROW(pac_f)) {
  pac_f[it] <- ac_f[it] - pac_f[1:(it-1)] %*% ac_f[(it-1):1]
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Order Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR(3)} process of order \emph{three} is defined by the formula:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \varphi_3 r_{i-3} + \xi_i
      \end{displaymath}
      Autoregressive processes \emph{AR(p)} of order \emph{p} have an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} up to lag \emph{p}.
      \vskip1ex
      The number of non-zero \emph{partial autocorrelations} is equal to the \emph{order} parameter $p$ of the \emph{AR(p)} process.
      <<ar_pacf,echo=(-(1:2)),eval=FALSE,fig.height=8,fig.show='hide'>>=
par(oma=c(15, 1, 1, 1), mgp=c(0, 0.5, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
# Simulate AR process of returns
ari_ma <- arima.sim(n=1e3, model=list(ar=c(0.1, 0.5, 0.1)))
# ACF of AR(3) process
rutils::plot_acf(ari_ma, lag=10, xlab="", ylab="",
         main="ACF of AR(3) process")
# PACF of AR(3) process
pacf(ari_ma, lag=10, xlab="", ylab="",
     main="PACF of AR(3) process")
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/ar_pacf-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calibrating Time Series Models}


%%%%%%%%%%%%%%%
\subsection{Fitting Time Series to Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} for the time series of returns $r_i$:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be solved as a \emph{multivariate} linear regression, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      An intercept term can be added to the above formula by adding a unit column to the regression design matrix.
      \vskip1ex
      Adding the intercept term produces slightly different coefficients, depending on the mean of the returns.
      \vskip1ex
      The function \texttt{stats::ar.ols()} fits an \emph{AR(p)} model, but it produces slightly different coefficients than linear regression, because it uses a different calibration procedure.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Specify AR process parameters
n_rows <- 1e3
co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
set.seed(1121); in_nov <- rnorm(n_rows)
# ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
# Simulate AR process using C_rfilter()
ari_ma <- .Call(stats:::C_rfilter, in_nov, co_eff,
  double(n_rows + n_coeff))[-(1:n_coeff)]
# Fit AR model using ar.ols()
ar_fit <- ar.ols(ari_ma, order.max=n_coeff, aic=FALSE)
class(ar_fit)
is.list(ar_fit)
drop(ar_fit$ar)
# Define design matrix with intercept column
de_sign <- sapply(1:n_coeff, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
de_sign <- cbind(rep(1, n_rows), de_sign)
# Fit AR model using regression
design_inv <- MASS::ginv(de_sign)
coeff_fit <- drop(design_inv %*% ari_ma)
all.equal(drop(ar_fit$ar), coeff_fit[-1], check.attributes=FALSE)
# Define design matrix without intercept column
de_sign <- sapply(1:n_coeff, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
# Fit AR model using regression
design_inv <- MASS::ginv(de_sign)
coeff_fit <- drop(design_inv %*% ari_ma)
all.equal(drop(ar_fit$ar), coeff_fit, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Standard Errors of the \protect\emph{AR(p)} Coefficients}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{standard errors} of the fitted \emph{AR(p)} coefficients are proportional to the standard deviation of the fitted residuals.
      \vskip1ex
      Their \emph{t}-values are equal to the ratio of the fitted coefficients divided by their standard errors.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the regression residuals
fit_ted <- drop(de_sign %*% coeff_fit)
residual_s <- drop(ari_ma - fit_ted)
# Variance of residuals
var_resid <- sum(residual_s^2)/(n_rows - NROW(coeff_fit))
# Design matrix squared
design_2 <- crossprod(de_sign)
# Calculate covariance matrix of AR coefficients
co_var <- var_resid*MASS::ginv(design_2)
coeff_fitd <- sqrt(diag(co_var))
# Calculate t-values of AR coefficients
coeff_tvals <- drop(coeff_fit)/coeff_fitd
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Order Selection of \protect\emph{AR(p)} Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Order selection means determining the \emph{order parameter} $p$ of the \emph{AR(p)} model that best fits the time series.
      \vskip1ex
      The order parameter $p$ can be set equal to the number of significantly non-zero \emph{partial autocorrelations} of the time series.
      \vskip1ex
      The order parameter can also be determined by only selecting coefficients with statistically significant \emph{t}-values.
      \vskip1ex
      Fitting an \emph{AR(p)} model can be performed by first determining the order $p$, and then calculating the coefficients.
      \vskip1ex
      The function \texttt{stats::arima()} calibrates (fits) an \emph{ARIMA} model to a univariate time series.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} performs order selection, and calibrates an \emph{AR(p)} model to a univariate time series.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Fit AR(5) model into AR(3) process
de_sign <- sapply(1:5, function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
de_sign <- cbind(rep(1, n_rows), de_sign)
design_inv <- MASS::ginv(de_sign)
coeff_fit <- drop(design_inv %*% ari_ma)
# Calculate t-values of AR(5) coefficients
residual_s <- drop(ari_ma - drop(de_sign %*% coeff_fit))
var_resid <- sum(residual_s^2)/(n_rows - NROW(coeff_fit))
co_var <- var_resid*MASS::ginv(crossprod(de_sign))
coeff_fitd <- sqrt(diag(co_var))
coeff_tvals <- drop(coeff_fit)/coeff_fitd
# Fit AR(5) model using arima()
arima_fit <- arima(ari_ma, order=c(5, 0, 0), include.mean=FALSE)
arima_fit$coef
# Fit AR(5) model using auto.arima()
library(forecast)  # Load forecast
arima_fit <- forecast::auto.arima(ari_ma, max.p=5, max.q=0, max.d=0)
# Fit AR(5) model into VTI returns
re_turns <- drop(zoo::coredata(na.omit(rutils::etf_env$re_turns$VTI)))
de_sign <- sapply(1:5, function(lagg) {
  rutils::lag_it(re_turns, lagg=lagg)
})  # end sapply
de_sign <- cbind(rep(1, NROW(re_turns)), de_sign)
design_inv <- MASS::ginv(de_sign)
coeff_fit <- drop(design_inv %*% re_turns)
# Calculate t-values of AR(5) coefficients
residual_s <- drop(re_turns - drop(de_sign %*% coeff_fit))
var_resid <- sum(residual_s^2)/(n_rows - NROW(coeff_fit))
co_var <- var_resid*MASS::ginv(crossprod(de_sign))
coeff_fitd <- sqrt(diag(co_var))
coeff_tvals <- drop(coeff_fit)/coeff_fitd
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Yule-Walker Equations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To lighten the notation we can assume that the time series $r_i$ has zero mean $\mathbb{E}[r_i] = 0$ and unit variance $\mathbb{E}[r_i^2] = 1$.  ($\mathbb{E}$ is the expectation operator.)
      \vskip1ex
      Then the \emph{autocorrelations} of $r_i$ are equal to: $\rho_k = \mathbb{E}[r_i r_{i-k}]$.
      \vskip1ex
      If we multiply the \emph{autoregressive} process \emph{AR(p)}: $r_i = \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i$, by $r_{i-k}$ and take the expectations, then we obtain the Yule-Walker equations:
      \begin{displaymath}
        \begin{pmatrix}
          \rho_1 \\
          \rho_2 \\
          \rho_3 \\
          \vdots \\
          \rho_p
        \end{pmatrix} =
        \begin{pmatrix}
          1 & \rho_1 & \dots & \rho_{p-1} \\
          \rho_1 & 1 & \dots & \rho_{p-2} \\
          \rho_2 & \rho_1 & \dots & \rho_{p-3} \\
          \vdots & \vdots & \ddots & \vdots \\
          \rho_{p-1} & \rho_{p-2} & \dots & 1
        \end{pmatrix}
        \begin{pmatrix}
          \varphi_1 \\
          \varphi_2 \\
          \varphi_3 \\
          \vdots \\
          \varphi_p
        \end{pmatrix}
      \end{displaymath}
      The Yule-Walker equations relate the \emph{autocorrelation coefficients} $\rho_i$ with the coefficients of the \emph{AR(p)} process $\varphi_i$.
      \vskip1ex
      The Yule-Walker equations can be solved for the \emph{AR(p)} coefficients $\varphi_i$ using matrix inversion.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compute autocorrelation coefficients
ac_f <- acf(ari_ma, lag=10, plot=FALSE)
ac_f <- drop(ac_f$acf)
acf1 <- ac_f[-NROW(ac_f)]
# Define Yule-Walker matrix
yule_walker <- sapply(2:9, function(lagg) {
  c(acf1[lagg:1], acf1[2:(NROW(acf1)-lagg+1)])
})  # end sapply
yule_walker <- cbind(acf1, yule_walker, rev(acf1))
# Generalized inverse of Yule-Walker matrix
yule_walker_inv <- MASS::ginv(yule_walker)
# Solve Yule-Walker equations
coeff_yw <- drop(yule_walker_inv %*% ac_f[-1])
round(coeff_yw, 5)
coeff_fit
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated using the function \texttt{filter()} with the argument \texttt{method="recursive"}.
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled \texttt{C++} function \texttt{stats:::C\_rfilter()}.
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e2
co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
set.seed(1121); in_nov <- rnorm(n_rows)
# Simulate AR process using filter()
ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
ari_ma <- as.numeric(ari_ma)
# Simulate AR process using C_rfilter()
arima_fast <- .Call(stats:::C_rfilter, in_nov, co_eff,
  double(n_rows + n_coeff))
all.equal(ari_ma, arima_fast[-(1:n_coeff)],
  check.attributes=FALSE)
      @
      The one step ahead \emph{forecast} $f_i$ is equal to the \emph{convolution} of the time series $r_i$ with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ar_forecast.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Forecast AR(3) process using loop in R
forecast_s <- numeric(NROW(ari_ma)+1)
forecast_s[1] <- 0
forecast_s[2] <- co_eff[1]*ari_ma[1]
forecast_s[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1]
for (it in 4:NROW(forecast_s)) {
  forecast_s[it] <- ari_ma[(it-1):(it-3)] %*% co_eff
}  # end for
# Plot with legend
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(ari_ma, main="Forecasting Using AR(3) Model",
  xlab="", ylab="", type="l")
lines(forecast_s, col="orange", lwd=3)
legend(x="topright", legend=c("series", "forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Forecasting of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one step ahead \emph{forecast} $f_i$ is equal to the \emph{convolution} of the time series $r_i$ with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      The above \emph{convolution} can be quickly calculated by using the function \texttt{filter()} with the argument \texttt{method="convolution"}.
      \vskip1ex
      The convolution can be calculated even faster by directly calling the compiled \texttt{C++} function \texttt{stats:::C\_cfilter()}.
      \vskip1ex
      The forecasts can also be calculated using the design matrix multiplied by the \emph{AR(p)} coefficients.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Forecast using filter()
filter_fast <- filter(x=ari_ma, sides=1,
  filter=co_eff, method="convolution")
filter_fast <- as.numeric(filter_fast)
# Compare excluding warmup period
all.equal(forecast_s[-(1:n_coeff)], filter_fast[-(1:(n_coeff-1))],
          check.attributes=FALSE)
# Filter using C_cfilter() compiled C++ function directly
filter_fast <- .Call(stats:::C_cfilter, ari_ma, filter=co_eff,
                     sides=1, circular=FALSE)
# Compare excluding warmup period
all.equal(forecast_s[-(1:n_coeff)], filter_fast[-(1:(n_coeff-1))],
          check.attributes=FALSE)
# Define predictor matrix for forecasting
predic_tor <- sapply(0:(n_coeff-1), function(lagg) {
  rutils::lag_it(ari_ma, lagg=lagg)
})  # end sapply
# Forecast using predictor matrix
filter_fast <- c(0, drop(predic_tor %*% co_eff))
# Compare with loop in R
all.equal(forecast_s, filter_fast, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Using \texttt{predict.Arima()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The forecasts of the \emph{AR(p)} process can also be calculated using the function \texttt{predict()}.
      \vskip1ex
      The function \texttt{predict()} is a \emph{generic function} for forecasting based on a given model.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} is \emph{dispatched} by \texttt{R} for calculating predictions from \emph{ARIMA} models produced by the function \texttt{stats::arima()}.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} returns a prediction object which is a \texttt{list} containing the predicted value and its standard error.
      \vskip1ex
      The function \texttt{stats::arima()} calibrates (fits) an \emph{ARIMA} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Fit ARIMA model using arima()
arima_fit <- arima(ari_ma, order=c(3,0,0), include.mean=FALSE)
arima_fit$coef
co_eff
# One-step-ahead forecast using predict.Arima()
pre_dict <- predict(arima_fit, n.ahead=1)
# Or directly call predict.Arima()
# pre_dict <- predict.Arima(arima_fit, n.ahead=1)
# Inspect the prediction object
class(pre_dict)
names(pre_dict)
class(pre_dict$pred)
unlist(pre_dict)
# One-step-ahead forecast using matrix algebra
fore_cast <- drop(ari_ma[n_rows:(n_rows-2)] %*% arima_fit$coef)
# Compare one-step-ahead forecasts
all.equal(pre_dict$pred[[1]], fore_cast)
# Get information about predict.Arima()
?stats:::predict.Arima
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Forecasting Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{forecasting residuals} $\varepsilon_i$ are equal to the differences between the actual values $r_i$ minus their \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$.
      \vskip1ex
      Accurate forecasting of an \emph{AR(p)} process requires knowing its coefficients.
      \vskip1ex
      If the coefficients of the \emph{AR(p)} process are known exactly, then its \emph{in-sample residuals} $\varepsilon_i$ are equal to its \emph{innovations} $\xi_i$: $\varepsilon_i = r_i - f_i = \xi_i$.
      \vskip1ex
      In practice, the \emph{AR(p)} coefficients are not known, so they must be fitted to the empirical time series.
      \vskip1ex
      If the \emph{AR(p)} coefficients are fitted to the empirical time series, then its \emph{residuals} are \emph{not} equal to its \emph{innovations}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the in-sample forecasting residuals
residual_s <- (ari_ma - forecast_s[-NROW(forecast_s)])
# Compare residuals with innovations
all.equal(in_nov, residual_s, check.attributes=FALSE)
plot(residual_s, t="l", lwd=3, xlab="", ylab="",
     main="ARIMA Forecast Errors")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting and Forecasting Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice, the \emph{AR(p)} coefficients are not known, so they must be fitted to the empirical time series first, before forecasting.
      \vskip1ex
      Forecasting using an autoregressive model is performed by first fitting an \emph{AR(p)} model to past data, and calculating its coefficients.
      \vskip1ex
      The fitted coefficients are then applied to calculating the \emph{out-of-sample} forecasts.
      \vskip1ex
      The model fitting procedure depends on two unknown \emph{meta-parameters}: the order $p$ of the \emph{AR(p)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define AR process parameters
n_rows <- 1e3
co_eff <- c(0.5, 0.0, 0.0); n_coeff <- NROW(co_eff)
set.seed(1121); in_nov <- rnorm(n_rows)
# Simulate AR process using C_rfilter()
ari_ma <- .Call(stats:::C_rfilter, in_nov, co_eff,
  double(n_rows + n_coeff))[-(1:n_coeff)]
# Define order of the AR(p) forecasting model
or_der <- 5
# Define predictor matrix for forecasting
de_sign <- sapply(1:or_der, rutils::lag_it, in_put=ari_ma)
de_sign <- cbind(rep(1, n_rows), de_sign)
colnames(de_sign) <- paste0("pred_", 1:NCOL(de_sign))
# Add response equal to series
de_sign <- cbind(ari_ma, de_sign)
colnames(de_sign)[1] <- "response"
# Specify length of look-back interval
look_back <- 100
# Invert the predictor matrix
rang_e <- (n_rows-look_back):(n_rows-1)
design_inv <- MASS::ginv(de_sign[rang_e, -1])
# Calculate fitted coefficients
coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
# Calculate forecast
drop(de_sign[n_rows, -1] %*% coeff_fit)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Autoregressive Forecasting Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the simulation of a model on historical data to test its forecasting accuracy.
      \vskip1ex
      The autoregressive forecasting model can be \emph{backtested} by calculating forecasts over a \emph{rolling look-back interval}.
      \vskip1ex
      The coefficients of the \emph{AR(p)} process are fitted to past data, and then applied to calculating out-of-sample forecasts.
      \vskip1ex
      The \emph{backtesting} procedure allows determining the optimal \emph{meta-parameters} of the model fitting procedure: the order $p$ of the \emph{AR(p)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Perform rolling forecasting
forecast_s <- sapply(look_back:n_rows, function(now) {
  # Calculate look-back range
  star_t <- max(1, now-look_back)
  rang_e <- star_t:(now-1)
  # Invert the predictor matrix
  design_inv <- MASS::ginv(de_sign[rang_e, -1])
  # Calculate fitted coefficients
  coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
  # Calculate forecast
  drop(de_sign[now, -1] %*% coeff_fit)
})  # end sapply
# Add warmup period
forecast_s <- c(numeric(look_back-1), forecast_s)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Accuracy of the Autoregressive Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of a forecasting model can be measured using the \emph{mean squared error} and the \emph{correlation}.
      \vskip1ex
      The mean squared error (\emph{MSE}) of a forecasting model is the average of the squared forecasting residuals $\varepsilon_i$, equal to the differences between the actual values $r_i$ minus the \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$:
      \begin{displaymath}
        \operatorname{MSE} = \frac{1}{n} \sum_{i=1}^n (r_i - f_i)^2
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Mean squared error
mean((ari_ma - forecast_s)^2)
# Correlation
cor(forecast_s, ari_ma)
# Plot forecasting series with legend
x11(width=6, height=5)
par(mar=c(3, 3, 2, 0), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(ari_ma[(n_rows-look_back):n_rows], xlab="", ylab="", type="l", lwd=2,
  main="Rolling Forecasting Using AR(5) Model")
lines(forecast_s[(n_rows-look_back):n_rows], col="orange", lwd=2)
legend(x="topright", legend=c("series", "forecasts"),
       col=c("black", "orange"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_forecast_resid.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Function for the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{meta-parameters} of the \emph{backtesting} function are the order $p$ of the \emph{AR(p)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtesting function
back_test <- function(se_ries, or_der=5, look_back=100) {
  n_rows <- NROW(se_ries)
  # Define predictor matrix for forecasting
  de_sign <- sapply(1:or_der, rutils::lag_it, in_put=se_ries)
  de_sign <- cbind(rep(1, n_rows), de_sign)
  # Add response equal to series
  de_sign <- cbind(se_ries, de_sign)
  # Perform rolling forecasting
  forecast_s <- sapply(look_back:n_rows, function(now) {
    # Calculate look-back range
    star_t <- max(1, now-look_back)
    rang_e <- star_t:(now-1)
    # Invert the predictor matrix
    design_inv <- MASS::ginv(de_sign[rang_e, -1])
    # Calculate fitted coefficients
    coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
    # Calculate forecast
    drop(de_sign[now, -1] %*% coeff_fit)
  })  # end sapply
  # Add warmup period
  forecast_s <- c(numeric(look_back-1), forecast_s)
  c(mse=mean((se_ries - forecast_s)^2), cor=cor(forecast_s, se_ries))
}  # end back_test
# Apply the backtesting function
back_test(ari_ma, or_der=5, look_back=100)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Optimal Parameters of the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{backtesting} function can be used to find the optimal \emph{meta-parameters} of the autoregressive forecasting model.
      \vskip1ex
      The accuracy of the forecasting model depends on the order $p$ of the \emph{AR(p)} model and on the length of the look-back interval (\texttt{look\_back}).
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      <<echo=TRUE,eval=FALSE>>=
look_backs <- seq(20, 200, 20)
back_tests <- sapply(look_backs, back_test,
              se_ries=ari_ma, or_der=or_der)
back_tests <- t(back_tests)
rownames(back_tests) <- look_backs
# Plot forecasting series with legend
plot(x=look_backs, y=back_tests[, 1],
  xlab="look-back", ylab="MSE", type="l", lwd=2,
  main="MSE of AR(5) Forecasting Model")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_forecast_mse.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Ornstein-Uhlenbeck Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Ornstein-Uhlenbeck} process, the returns $r_i$ are equal to the difference between the equilibrium price $\mu$ minus the latest price $p_{i-1}$, times the mean reversion parameter $\theta$, plus random \emph{innovations}:
      \begin{align*}
        r_i &= p_i - p_{i-1} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i \\
        p_i &= p_{i-1} + r_i
      \end{align*}
      Where $\sigma$ is the return volatility, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process for prices can be written as an \emph{AR(1)} process plus a drift:
      \begin{displaymath}
        p_i = \theta \, \mu + (1 - \theta ) \, p_{i-1} + \sigma \, \xi_i
      \end{displaymath}
      The \emph{Ornstein-Uhlenbeck} process cannot be simulated using the function \texttt{filter()} because of the drift term, so it must be simulated using explicit loops, either in \texttt{R} or in \texttt{C++}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define Ornstein-Uhlenbeck parameters
eq_price <- 1.0; sigma_r <- 0.02
the_ta <- 0.01; n_rows <- 1000
drif_t <- the_ta*eq_price
theta_1 <- 1-the_ta
# Simulate Ornstein-Uhlenbeck process
in_nov <- rnorm(n_rows)
re_turns <- numeric(n_rows)
price_s <- numeric(n_rows)
price_s[1] <- sigma_r*in_nov[1]
for (i in 2:n_rows) {
  re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + sigma_r*in_nov[i]
  price_s[i] <- price_s[i-1] + re_turns[i]
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Solution of the Ornstein-Uhlenbeck Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ornstein-Uhlenbeck} process in continuous time is:
      \begin{displaymath}
        \mathrm{d} p_t = \theta \, (\mu - p_t) \, \mathrm{d} t + \sigma \, \mathrm{d} W_t
      \end{displaymath}
      Where $W_t$ is a \emph{Wiener process}, with $\mathrm{d} W_t$ following the standard normal distribution $\phi(0, \sqrt{\mathrm{d}t})$.
      \vskip1ex
      The solution of the \emph{Ornstein-Uhlenbeck} process is given by:
      \begin{displaymath}
        p_t = p_0 e^{-\theta t} + \mu (1 - e^{-\theta t}) + \sigma \int_{0}^{t} {e^{\theta (s - t)} \mathrm{d} W_s}
      \end{displaymath}
      \vskip1ex
      The mean and variance are given by:\\
      $\mathbb{E}[p_t] = p_0 e^{-\theta t} + \mu (1 - e^{-\theta t}) \rightarrow \mu$ \\
      $\mathbb{E}[(p_t - \mathbb{E}[p_t])^2] = \frac{\sigma^2}{2 \theta} (1 - e^{-\theta t}) \rightarrow \frac{\sigma^2}{2 \theta}$
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process is mean reverting to a non-zero equilibrium price $\mu$.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process needs a \emph{warmup period} before it reaches equilibrium.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ou_proc.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot(price_s, type="l",
     xlab="time", ylab="prices",
     main="Ornstein-Uhlenbeck Process")
legend("topright",
       title=paste(c(paste0("sigma_r = ", sigma_r),
                     paste0("eq_price = ", eq_price),
                     paste0("the_ta = ", the_ta)),
                   collapse="\n"),
       legend="", cex=0.8, inset=0.1, bg="white", bty="n")
abline(h=eq_price, col='red', lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ornstein-Uhlenbeck Process Returns Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Ornstein-Uhlenbeck} process, the returns are negatively correlated to the lagged prices.
      <<echo=TRUE,eval=FALSE>>=
re_turns <- rutils::diff_it(price_s)
lag_prices <- rutils::lag_it(price_s)
for_mula <- re_turns ~ lag_prices
l_m <- lm(for_mula)
summary(l_m)
# Plot regression
plot(for_mula, main="OU Returns Versus Lagged Prices")
abline(l_m, lwd=2, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ou_scatter.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calibrating the Ornstein-Uhlenbeck Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility parameter of the Ornstein-Uhlenbeck process can be estimated directly from the standard deviation of the returns.
      \vskip1ex
      The $\theta$ and $\mu$ parameters can be estimated from the linear regression of the returns versus the lagged prices.
      \vskip1ex
      Calculating regression parameters directly from formulas has the advantage of much faster calculations.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate volatility parameter
c(volatility=sigma_r, estimate=sd(re_turns))
# Extract OU parameters from regression
co_eff <- summary(l_m)$coefficients
# Calculate regression alpha and beta directly
be_ta <- cov(re_turns, lag_prices)/var(lag_prices)
al_pha <- (mean(re_turns) - be_ta*mean(lag_prices))
cbind(direct=c(alpha=al_pha, beta=be_ta), lm=co_eff[, 1])
all.equal(c(alpha=al_pha, beta=be_ta), co_eff[, 1],
          check.attributes=FALSE)
# Calculate regression standard errors directly
beta_s <- c(alpha=al_pha, beta=be_ta)
fit_ted <- (al_pha + be_ta*lag_prices)
residual_s <- (re_turns - fit_ted)
prices_squared <- sum((lag_prices - mean(lag_prices))^2)
beta_sd <- sqrt(sum(residual_s^2)/prices_squared/(n_rows-2))
alpha_sd <- sqrt(sum(residual_s^2)/(n_rows-2)*(1/n_rows + mean(lag_prices)^2/prices_squared))
cbind(direct=c(alpha_sd=alpha_sd, beta_sd=beta_sd), lm=co_eff[, 2])
all.equal(c(alpha_sd=alpha_sd, beta_sd=beta_sd), co_eff[, 2],
          check.attributes=FALSE)
# Compare mean reversion parameter theta
c(theta=(-the_ta), round(co_eff[2, ], 3))
# Compare equilibrium price mu
c(eq_price=eq_price, estimate=-co_eff[1, 1]/co_eff[2, 1])
# Compare actual and estimated parameters
co_eff <- cbind(c(the_ta*eq_price, -the_ta), co_eff[, 1:2])
rownames(co_eff) <- c("drift", "theta")
colnames(co_eff)[1] <- "actual"
round(co_eff, 4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Schwartz Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ornstein-Uhlenbeck} prices can be negative, while actual prices are usually not negative.
      \vskip1ex
      So the \emph{Ornstein-Uhlenbeck} process is better suited for simulating the logarithm of prices, which can be negative.
      \vskip1ex
      The \emph{Schwartz} process is similar to the \emph{Ornstein-Uhlenbeck} process, but it avoids negative prices by compounding the percentage returns $r_i$ instead of summing them:
      \begin{align*}
        r_i &= \log{p_i} - \log{p_{i-1}} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i \\
        p_i &= p_{i-1} * \exp(r_i)
      \end{align*}
      Where the parameter $\theta$ is the strength of mean reversion, $\sigma$ is the volatility, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      <<echo=TRUE,eval=FALSE>>=
# Simulate Schwartz process
re_turns <- numeric(n_rows)
price_s <- numeric(n_rows)
price_s[1] <- eq_price
set.seed(1121)  # Reset random numbers
for (i in 2:n_rows) {
  re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + sigma_r*rnorm(1)
  price_s[i] <- price_s[i-1]*exp(re_turns[i])
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/ou_schwartz.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
plot(price_s, type="l", xlab="time", ylab="prices",
     main="Schwartz Process")
legend("topright",
       title=paste(c(paste0("sigma_r = ", sigma_r),
                     paste0("eq_price = ", eq_price),
                     paste0("the_ta = ", the_ta)),
                   collapse="\n"),
       legend="", cex=0.8, inset=0.12, bg="white", bty="n")
abline(h=eq_price, col='red', lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  Study all the lecture slides in \texttt{FRE7241\_Lecture\_5.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_5.R}
\end{block}
\begin{block}{Recommended}
  \begin{itemize}[]
    \item Read about \emph{optimization methods}:\\
    \emph{Bolker Optimization Methods.pdf}\\
    \emph{Yollin Optimization.pdf}\\
    \emph{Boudt DEoptim Large Portfolio Optimization.pdf}\\
    \item Read about \emph{PCA} in:\\
    \emph{pca-handout.pdf}\\
    \emph{pcaTutorial.pdf}\\
  \end{itemize}
\end{block}

\end{frame}


\end{document}
