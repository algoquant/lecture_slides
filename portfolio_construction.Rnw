% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='tiny', fig.width=4, fig.height=4)
options(digits=3)
options(width=80, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Portfolio Construction]{Portfolio Construction}
\subtitle{FRE6871 \& FRE7241, Spring 2021}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Asset Allocation and Portfolio Construction}


%%%%%%%%%%%%%%%
\subsection{draft: Asset Allocation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset allocation means dividing an investment portfolio among different asset classes, such as large company stocks, small company stocks, international stocks, bonds, commodities, cash, etc.
      \vskip1ex
      The goal of asset allocation is to diversify the sources of returns and to reduce risk, depending on the investor's risk tolerance, investment goals, and investment time horizon.
      \vskip1ex
      For example, an investor who needs to fund college for her children might put some of her investments into government bonds that mature when her children will need to pay for college.
      \vskip1ex
      1,600 years ago \href{http://finance.yahoo.com/news/naive-diversification-vs-optimization-163550886.html}{rabbi Isaac bar Aha} proposed a simple heuristic method (rule of thumb) for asset allocation: "put a third in land, a third in merchandise, and a third in cash".
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# Use ETF returns from package rutils
library(rutils)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# Initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names),
                  NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# Create portfolio object
portf_init <- portfolio.spec(
  assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # Initial portfolio
  type="weight_sum",  # Constraint sum weights
  min_sum=0.9, max_sum=1.1)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # Maximize mean return
  name="mean")
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # Minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Construction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolio construction means determining the amounts to be invested different assets, such as specific stocks, bonds, commodities, etc.
      \vskip1ex
      Portfolio optimization is one approach to portfolio construction.
      \vskip1ex
      Heuristic Methods for Portfolio Construction
      \vskip1ex
      \href{https://www.london.edu/faculty-and-research/faculty/profiles/d/demiguel-v}{Victor DeMiguel} and others have demonstrated that optimized portfolios perform poorly out-of-sample, and that simple heuristic methods can perform better than portfolio optimization.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# Use ETF returns from package rutils
library(rutils)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# Initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names),
                  NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# Create portfolio object
portf_init <- portfolio.spec(
  assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # Initial portfolio
  type="weight_sum",  # Constraint sum weights
  min_sum=0.9, max_sum=1.1)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # Maximize mean return
  name="mean")
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # Minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Portfolio Efficient Frontier}


%%%%%%%%%%%%%%%
\subsection{Vector and Matrix Calculus}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
    \begin{columns}[T]
    \column{0.5\textwidth}
      Let $\mathbf{v}$ and $\mathbf{w}$ be vectors, with $\mathbf{v} = \left\{ v_i \right\}_{i=1}^{i=n}$, and let $\mathbbm{1}$ be the unit vector, with $\mathbbm{1} = \left\{ 1 \right\}_{i=1}^{i=n}$.
      \vskip1ex
      Then the inner product of $\mathbf{v}$ and $\mathbf{w}$ can be written as $\mathbf{v}^T \mathbf{w} = \mathbf{w}^T \mathbf{v} = {\sum_{i=1}^n {v_i w_i}}$.
      \vskip1ex
      We can then express the sum of the elements of $\mathbf{v}$ as the inner product: $\mathbf{v}^T \mathbbm{1} = \mathbbm{1}^T \mathbf{v} = {\sum_{i=1}^n v_i}$.
      \vskip1ex
      And the sum of squares of $\mathbf{v}$ as the inner product: $\mathbf{v}^T \mathbf{v} = {\sum_{i=1}^n v_i^2}$.
      \vskip1ex
      Let $\mathbb{A}$ be a matrix, with $\mathbb{A} = \left\{ A_{ij} \right\}_{{i,j}=1}^{{i,j}=n}$.
      \vskip1ex
      Then the inner product of matrix $\mathbb{A}$ with vectors $\mathbf{v}$ and $\mathbf{w}$ can be written as:
      \begin{displaymath}
        \mathbf{v}^T \mathbb{A} \, \mathbf{w} = \mathbf{w}^T \mathbb{A}^T \mathbf{v} = {\sum_{{i,j}=1}^n {A_{ij} v_i w_j}}
      \end{displaymath}
    \column{0.5\textwidth}
      The derivative of a scalar variable with respect to a vector variable is a vector, for example:
      \begin{align*}
        \frac{d (\mathbf{v}^T \mathbbm{1})}{d \mathbf{v}} = d_v[\mathbf{v}^T \mathbbm{1}] = d_v[\mathbbm{1}^T \mathbf{v}] = \mathbbm{1}^T\\
        d_v[\mathbf{v}^T \mathbf{w}] = d_v[\mathbf{w}^T \mathbf{v}] = \mathbf{w}^T\\
        d_v[\mathbf{v}^T \mathbb{A} \, \mathbf{w}] = \mathbf{w}^T \mathbb{A}^T\\
        d_v[\mathbf{v}^T \mathbb{A} \, \mathbf{v}] = \mathbf{v}^T \mathbb{A} + \mathbf{v}^T \mathbb{A}^T
      \end{align*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Weight Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolio optimization requires constraints on the portfolio weights to prevent excessive leverage (size of positions relative to capital).
      \vskip1ex
      Portfolio-level constraints limit the combined size of the weights.
      \vskip1ex
      For example, under \emph{linear} constraints the sum of the weights is equal to \texttt{1}: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, so that the weights are constrained to a \emph{hyperplane}.
      \vskip1ex
      The disadvantage of \emph{linear} constraints is that they allow highly leveraged portfolios, with very large positive and negative weights.
      \vskip1ex
      Under \emph{quadratic} constraints the sum of the \emph{squared} weights is equal to \texttt{1}: $\mathbf{w}^T \mathbf{w} = {\sum_{i=1}^n w_i^2} = 1$, so that the weights are constrained to a \emph{hypersphere}.
      \vskip1ex
      Box constraints limit the individual weights, for example: $0 \leq w_i \leq 1$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Linear constraint
weight_s <- weight_s/sum(weight_s)
# Quadratic constraint
weight_s <- weight_s/sqrt(sum(weight_s^2))
# Box constraints
weight_s[weight_s > 1] <- 1
weight_s[weight_s < 0] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum Return Portfolio Using Linear Programming}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the maximum return portfolio are obtained by maximizing the portfolio returns:
      \begin{displaymath}
        w_{max} = \operatorname*{arg\,max}_{w} [ \, \mathbf{r}^T \mathbf{w} \, ] = \operatorname*{arg\,max}_{w} [ \, \sum_{i=1}^n w_i r_i \, ]
      \end{displaymath}
      Where $\mathbf{r}$ is the vector of returns, and $\mathbf{w}$ is the vector of portfolio weights, constrained by:
      \begin{align*}
        \mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1\\
        0 \leq w_i \leq 1
      \end{align*}
      The weights of the maximum return portfolio can be calculated using linear programming (\emph{LP}), which is the optimization of linear objective functions subject to linear constraints.
      \vskip1ex
      The function \texttt{Rglpk\_solve\_LP()} from package \emph{Rglpk} solves linear programming problems by calling the \emph{GNU Linear Programming Kit} library.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(quantmod)
library(Rglpk)
# Vector of symbol names
sym_bols <- c("VTI", "IEF", "DBC")
n_weights <- NROW(sym_bols)
# Calculate mean returns
re_turns <- rutils::etf_env$re_turns[, sym_bols]
re_turns <- zoo::na.locf(re_turns, na.rm=FALSE)
re_turns <- na.omit(re_turns)
mean_rets <- colMeans(re_turns)
# Specify weight constraints
constraint_s <- matrix(c(rep(1, n_weights), 1, 1, 0),
                       nc=n_weights, byrow=TRUE)
direction_s <- c("==", "<=")
rh_s <- c(1, 0)
# Specify weight bounds (-1, 1) (default is c(0, Inf))
bound_s <-
  list(lower=list(ind=1:n_weights, val=rep(-1, n_weights)),
       upper=list(ind=1:n_weights, val=rep(1, n_weights)))
# Perform optimization
op_tim <- Rglpk::Rglpk_solve_LP(
  obj=mean_rets,
  mat=constraint_s,
  dir=direction_s,
  rhs=rh_s,
  bounds=bound_s,
  max=TRUE)
unlist(op_tim[1:2])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Minimum Variance} Portfolio Under \protect\emph{Linear} Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio variance is equal to: $\mathbf{w}^T \mathbb{C} \, \mathbf{w}$, where $\mathbb{C}$ is the covariance matrix of returns.
      \vskip1ex
      If the portfolio weights $\mathbf{w}$ are subject to \emph{linear} constraints: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, then the weights that minimize the portfolio variance can be found by minimizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - \, \lambda \, (\mathbf{w}^T \mathbbm{1} - 1)
      \end{displaymath}
      Where $\lambda$ is a \emph{Lagrange multiplier}.
      \vskip1ex
      The derivative of a scalar variable with respect to a vector variable is a vector, for example:
      \begin{align*}
        d_w[\mathbf{w}^T \mathbbm{1}] = d_w[\mathbbm{1}^T \mathbf{w}] = \mathbbm{1}^T\\
        d_w[\mathbf{w}^T \mathbf{r}] = d_w[\mathbf{r}^T \mathbf{w}] = \mathbf{r}^T\\
        d_w[\mathbf{w}^T \mathbb{C} \, \mathbf{w}] = \mathbf{w}^T \mathbb{C} + \mathbf{w}^T \mathbb{C}^T
      \end{align*}
      Where $\mathbbm{1}$ is the unit vector, and $\mathbf{w}^T \mathbbm{1} = \mathbbm{1}^T \mathbf{w} = \sum_{i=1}^n {x_i}$
    \column{0.5\textwidth}
      The derivative of the \emph{Lagrangian} $\mathcal{L}$ with respect to $\mathbf{w}$ is given by:
      \begin{displaymath}
        d_w \mathcal{L} = 2 \mathbf{w}^T \mathbb{C} - \lambda \mathbbm{1}^T
      \end{displaymath}
      By setting the derivative to zero we find $\mathbf{w}$ equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{1}{2} \lambda \, \mathbb{C}^{-1} \mathbbm{1}
      \end{displaymath}
      By multiplying the above from the left by $\mathbbm{1}^T$, and using $\mathbf{w}^T \mathbbm{1} = 1$, we find $\lambda$ to be equal to:
      \begin{displaymath}
        \lambda = \frac{2}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      And finally the portfolio weights are then equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mathbbm{1}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      If the portfolio weights are subject to \emph{quadratic} constraints: $\mathbf{w}^T \mathbf{w} = 1$ then the minimum variance weights are equal to the highest order \emph{principal component} (with the smallest eigenvalue) of the covariance matrix $\mathbb{C}$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance of the \protect\emph{Minimum Variance} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the \emph{minimum variance} portfolio under the constraint $\mathbf{w}^T \mathbbm{1} = 1$ can be calculated using the inverse of the covariance matrix:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mathbbm{1}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      The variance of the \emph{minimum variance} portfolio is equal to:
      \begin{displaymath}
        \sigma^2 = \frac{\mathbbm{1}^T \mathbb{C}^{-1} \mathbb{C} \, \mathbb{C}^{-1} \mathbbm{1}}{(\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1})^2} = \frac{1}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices.
      \vskip1ex
      The \texttt{\%*\%} operator performs \emph{inner} (\emph{scalar}) multiplication of vectors and matrices.
      \vskip1ex
      \emph{Inner} multiplication multiplies the rows of one matrix with the columns of another matrix, so that each pair produces a single number:
      \vskip1ex
      The function \texttt{drop()} removes any dimensions of length \emph{one}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate covariance matrix of returns and its inverse
cov_mat <- cov(re_turns)
cov_inv <- solve(a=cov_mat)
u_nit <- rep(1, NCOL(cov_mat))
# Minimum variance weights with constraint
# weight_s <- solve(a=cov_mat, b=u_nit)
weight_s <- cov_inv %*% u_nit
weight_s <- weight_s / drop(t(u_nit) %*% weight_s)
# Minimum variance
t(weight_s) %*% cov_mat %*% weight_s
1/(t(u_nit) %*% cov_inv %*% u_nit)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Portfolios}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A portfolio which has the smallest variance, given a target return, is an \emph{efficient portfolio}.
      \vskip1ex
      The \emph{efficient portfolio} weights have two constraints: the sum of portfolio weights $\mathbf{w}$ is equal to \texttt{1}: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, and the mean portfolio return is equal to the target return $r_t$: $\mathbf{w}^T \mathbf{r} = {\sum_{i=1}^n w_i r_i} = r_t$.
      \vskip1ex
      The weights that minimize the portfolio variance under these constraints can be found by minimizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - \, \lambda_1 \, (\mathbf{w}^T \mathbbm{1} - 1) - \, \lambda_2 \, (\mathbf{w}^T \mathbf{r} - r_t)
      \end{displaymath}
      Where $\lambda_1$ and $\lambda_2$ are the \emph{Lagrange multipliers}.
      \vskip1ex
      The derivative of the \emph{Lagrangian} $\mathcal{L}$ with respect to $\mathbf{w}$ is given by:
      \begin{displaymath}
        d_w \mathcal{L} = 2 \mathbf{w}^T \mathbb{C} - \lambda_1 \mathbbm{1}^T - \lambda_2 \mathbf{r}^T
      \end{displaymath}
      By setting the derivative to zero we obtain the \emph{efficient portfolio} weights $\mathbf{w}$:
      \begin{displaymath}
        \mathbf{w} = \frac{1}{2} (\lambda_1 \, \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbb{C}^{-1} \mathbf{r})
      \end{displaymath}
    \column{0.5\textwidth}
      By multiplying the above from the left first by $\mathbbm{1}^T$, and then by $\mathbf{r}^T$, we obtain a system of two equations for $\lambda_1$ and $\lambda_2$:
      \begin{align*}
        2 \mathbbm{1}^T \mathbf{w} = \lambda_1 \, \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} = 2\\
        2 \mathbf{r}^T \mathbf{w} = \lambda_1 \, \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r} = 2 r_t
      \end{align*}
      The above can be written in matrix notation as:
      \begin{displaymath}
        \begin{bmatrix}
          \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} & \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} \\
          \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} & \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}
        \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix} =
        \begin{bmatrix}
          2 \\
          2 r_t
        \end{bmatrix}
      \end{displaymath}
      Or:
      \begin{displaymath}
        \begin{bmatrix}
          a & b \\
          b & c
        \end{bmatrix}
        \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix} =
        \mathbb{F} \lambda =
        2 \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} =
        2 u
      \end{displaymath}
      With $a = \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$, $b = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r}$, $c = \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}$,
      $\lambda = \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix}$,
      $u = \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix}$,
        and
      $\mathbb{F} = u^T \mathbb{C}^{-1} u = \begin{bmatrix}
          a & b \\
          b & c
        \end{bmatrix}$.
      \vskip1ex
      The \emph{Lagrange multipliers} can be solved as:
      \begin{displaymath}
        \lambda = 2 \mathbb{F}^{-1} u
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Portfolio} Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolio} weights $\mathbf{w}$ can now be solved as:
      \begin{align*}
        \mathbf{w} = \frac{1}{2} (\lambda_1 \, \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbb{C}^{-1} \mathbf{r}) = \\
        \frac{1}{2}
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \lambda =
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \mathbb{F}^{-1} \, u = \\
        \frac{1}{a c-b^2}
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \begin{bmatrix}
          c & -b \\
          -b & a
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} = \\
        \frac{(c - b r_t)  \, \mathbb{C}^{-1} \mathbbm{1} + (a r_t - b)  \, \mathbb{C}^{-1} \mathbf{r}}{a c-b^2}
      \end{align*}
      With $a = \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$, $b = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r}$, $c = \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}$.
      \vskip1ex
      The above formula shows that a convex sum of two \emph{efficient portfolio} weights: $w = \alpha w_1 + (1-\alpha) w_2$ \\
      Are also the weights of an \emph{efficient portfolio}, with target return equal to: $r_t = \alpha r_1 + (1-\alpha) r_2$
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate vector of mean returns
mean_rets <- colMeans(re_turns)
# Specify the target return
tar_get <- 1.5*mean(re_turns)
# Products of inverse with mean returns and unit vector
f_mat <- matrix(c(
  t(u_nit) %*% cov_inv %*% u_nit,
  t(u_nit) %*% cov_inv %*% mean_rets,
  t(mean_rets) %*% cov_inv %*% u_nit,
  t(mean_rets) %*% cov_inv %*% mean_rets), nc=2)
# Solve for the Lagrange multipliers
mult_s <- solve(a=f_mat, b=c(2, 2*tar_get))
# Calculate weights
weight_s <- drop(0.5*cov_inv %*% cbind(u_nit, mean_rets) %*% mult_s)
# Calculate constraints
all.equal(1, sum(weight_s))
all.equal(tar_get, sum(mean_rets*weight_s))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance of the \protect\emph{Efficient Portfolios}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolio} variance is equal to:
      \begin{align*}
        \sigma^2 = \mathbf{w}^T \mathbb{C} \, \mathbf{w} = \frac{1}{4} \lambda^T \mathbb{F} \, \lambda = u^T \mathbb{F}^{-1} \, u =\\
        \frac{1}{a c-b^2}
        {\begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix}}^T
        \begin{bmatrix}
          c & -b \\
          -b & a
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} =\\
        \frac{a r_t^2 - 2b r_t + c}{a c-b^2}
      \end{align*}
      The above formula shows that the variance of the \emph{efficient portfolios} is a \emph{parabola} with respect to the target return $r_t$.
      \vskip1ex
      The vertex of the \emph{parabola} is at $r_t = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} / \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$ and $\sigma^2 = 1 / \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio return and standard deviation
portf_rets <- drop(re_turns %*% weight_s)
c(return=mean(portf_rets), sd=sd(portf_rets))
all.equal(mean(portf_rets), tar_get)
# Calculate portfolio variance
uu <- c(1, tar_get)
f_inv <- solve(f_mat)
all.equal(var(portf_rets), drop(t(uu) %*% f_inv %*% uu))
# Calculate vertex of variance parabola
weight_s <- drop(cov_inv %*% u_nit /
  drop(t(u_nit) %*% cov_inv %*% u_nit))
portf_rets <- drop(re_turns %*% weight_s)
v_rets <- drop(t(u_nit) %*% cov_inv %*% mean_rets /
  t(u_nit) %*% cov_inv %*% u_nit)
all.equal(mean(portf_rets), v_rets)
var_min <- drop(1/t(u_nit) %*% cov_inv %*% u_nit)
all.equal(var(portf_rets), var_min)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Frontier}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient frontier} is the plot of the \emph{efficient portfolio} standard deviations with respect to the target return $r_t$, which is a \emph{hyperbola}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate efficient frontier
target_s <- v_rets*(1+seq(from=-1, to=1, by=0.1))
eff_front <- sapply(target_s, function(tar_get) {
  uu <- c(1, tar_get)
  sqrt(drop(t(uu) %*% f_inv %*% uu))
})  # end sapply
# Plot efficient frontier
x11(width=6, height=5)
plot(x=eff_front, y=target_s, t="l", col="blue", lwd=2,
     main="Efficient Frontier and Minimum Variance Portfolio",
     xlab="standard deviation", ylab="return")
points(x=sqrt(var_min), y=v_rets, col="green", lwd=6)
text(x=sqrt(var_min), y=v_rets, labels="minimum \nvariance",
     pos=4, cex=0.8)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Tangent Line} and the \protect\emph{Risk-free} Rate}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{tangent} line can be drawn at every point on the \protect\emph{efficient frontier}.
      \vskip1ex
      The slope $\beta$ of the \emph{tangent} line can be calculated by differentiating the variance $\sigma^2$ by the target return $r_t$:
      \begin{align*}
        \frac{d \sigma^2}{d r_t} = 2 \sigma \frac{d \sigma}{d r_t} = \frac{2 a r_t - 2 b}{a c-b^2} \\
        \frac{d \sigma}{d r_t} = \frac{a r_t - b}{\sigma \, (a c-b^2)} \\
        \beta = \frac{\sigma \, (a c-b^2)}{a r_t - b}
      \end{align*}
      The \emph{tangent} line connects the \emph{tangent} point on the \protect\emph{efficient frontier} with a \emph{risk-free} rate $r_f$.
    \column{0.5\textwidth}
      The \emph{risk-free} rate $r_f$ can be calculated as the intercept of the tangent line:
      \begin{align*}
        r_f = r_t - \sigma \, \beta = r_t - \frac{\sigma^2 \, (a c-b^2)}{a r_t - b} = \\
        r_t - \frac{a r_t^2 - 2b r_t + c}{a c-b^2} \frac{a c-b^2}{a r_t - b} = \\
        r_t - \frac{a r_t^2 - 2b r_t + c}{a r_t - b} = \frac{b r_t - c}{a r_t - b}
      \end{align*}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio standard deviation
std_dev <- sqrt(drop(t(uu) %*% f_inv %*% uu))
# Calculate the slope of the tangent line
slop_e <- (std_dev*det(f_mat))/(f_mat[1, 1]*tar_get-f_mat[1, 2])
# Calculate the risk-free rate as intercept of the tangent line
risk_free <- tar_get - slop_e*std_dev
# Calculate the risk-free rate from target return
risk_free <- (tar_get*f_mat[1, 2]-f_mat[2, 2]) /
  (tar_get*f_mat[1, 1]-f_mat[1, 2])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Tangent Line} on the \protect\emph{Efficient Frontier}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolios} are also called \emph{tangency portfolios}, since they are the tangent points on the \emph{efficient frontier}.
      \vskip1ex
      The \emph{tangency portfolio} is the \emph{market portfolio} corresponding to the given \emph{risk-free} rate.
      \vskip1ex
      The \emph{tangent line} at the \emph{market portfolio} is known as the \emph{Capital Market Line} (CML).
      <<echo=TRUE,eval=FALSE>>=
# Plot efficient frontier
plot(x=eff_front, y=target_s, t="l", col="blue", lwd=2,
     xlim=c(0.0, max(eff_front)),
     main="Efficient Frontier and Tangency Portfolio",
     xlab="standard deviation", ylab="return")
# Plot minimum variance
points(x=sqrt(var_min), y=v_rets, col="green", lwd=6)
text(x=sqrt(var_min), y=v_rets, labels="minimum \nvariance",
     pos=4, cex=0.8)
# Plot tangent point
points(x=std_dev, y=tar_get, col="red", lwd=6)
text(x=std_dev, y=tar_get, labels="tangency\nportfolio", pos=2, cex=0.8)
# Plot risk-free point
points(x=0, y=risk_free, col="red", lwd=6)
text(x=0, y=risk_free, labels="risk-free", pos=4, cex=0.8)
# Plot tangent line
abline(a=risk_free, b=slop_e, lwd=2, col="green")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_tangent2.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum \protect\emph{Sharpe} Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratio is defined as the ratio of excess returns divided by the portfolio standard deviation:
      \begin{displaymath}
        SR = \frac{\mathbf{w}^T \mu}{\sigma}
      \end{displaymath}
      Where $\mu = \mathbf{r} - r_f$ is the vector of excess returns (in excess of the risk-free rate $r_f$), $\mathbf{w}$ is the vector of portfolio weights, and $\sigma = \sqrt{\mathbf{w}^T \mathbb{C} \, \mathbf{w}}$, where $\mathbb{C}$ is the covariance matrix of returns.
      \vskip1ex
      We can calculate the maximum \emph{Sharpe} portfolio weights by setting the derivative of the \emph{Sharpe} ratio with respect to the weights, to zero:
      \begin{displaymath}
        d_w {SR} = \frac{1}{\sigma} (\mu^T - \frac{(\mathbf{w}^T \mu) (\mathbf{w}^T \mathbb{C})}{\sigma^2}) = 0
      \end{displaymath}
      We then get:
      \begin{displaymath}
        (\mathbf{w}^T \mathbb{C} \, \mathbf{w}) \, \mu = (\mathbf{w}^T \mu) \, \mathbb{C} \mathbf{w}
      \end{displaymath}
      We can multiply the above equation by $\mathbb{C}^{-1}$ to get:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbf{w}^T \mathbb{C} \, \mathbf{w}}{\mathbf{w}^T \mu} \, \mathbb{C}^{-1} \mu
      \end{displaymath}
    \column{0.5\textwidth}
      We can finally rescale the weights so that they satisfy the constraint $\mathbf{w}^T \mathbbm{1} = 1$:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      These are the weights of the maximum \emph{Sharpe} portfolio, with the vector of excess returns equal to $\mu$, and the covariance matrix equal to $\mathbb{C}$.
      \vskip1ex
      The maximum \emph{Sharpe} portfolio is an \emph{efficient portfolio}, and so its mean return is equal to some target return $r_t$: $\mathbf{w}^T \mathbf{r} = {\sum_{i=1}^n w_i r_i} = r_t$.
      \vskip1ex
      The mean portfolio return can be written as:
      \begin{align*}
        \mathbf{r}^T \mathbf{w} = \frac{\mathbf{r}^T \mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu} =
        \frac{\mathbf{r}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)}{\mathbbm{1}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)} = \\
        r_t = \frac{\mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} \, r_f - \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} \, r_f - \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{align*}
      The above formula calculates the target return $r_t$ from the risk-free rate $r_f$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Returns and Variance of Maximum \protect\emph{Sharpe} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the maximum \emph{Sharpe} portfolio are equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      Where $\mu$ is the vector of excess returns, and $\mathbb{C}$ is the covariance matrix.
      \vskip1ex
      The excess returns of the maximum \emph{Sharpe} portfolio are equal to:
      \begin{displaymath}
        R = \mathbf{w}^T \mu = \frac{\mu^T \mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      The variance of the maximum \emph{Sharpe} portfolio is equal to:
      \begin{displaymath}
        \sigma^2 = \frac{\mu^T \mathbb{C}^{-1} \mathbb{C} \, \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2} = \frac{\mu^T \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2}
      \end{displaymath}
      The \emph{Sharpe} ratio is equal to:
      \begin{displaymath}
        SR = \sqrt{\mu^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate excess re_turns
risk_free <- 0.03/252
ex_cess <- re_turns - risk_free
# Calculate covariance and inverse matrix
cov_mat <- cov(re_turns)
u_nit <- rep(1, NCOL(cov_mat))
cov_inv <- solve(a=cov_mat)
# Calculate mean excess returns
ex_cess <- sapply(ex_cess, mean)
# Weights of maximum Sharpe portfolio
# weight_s <- solve(a=cov_mat, b=re_turns)
weight_s <- cov_inv %*% ex_cess
weight_s <- weight_s/drop(t(u_nit) %*% weight_s)
# Sharpe ratios
sqrt(252)*sum(weight_s * ex_cess) /
  sqrt(drop(weight_s %*% cov_mat %*% weight_s))
sapply(re_turns - risk_free,
  function(x) sqrt(252)*mean(x)/sd(x))
weights_maxsharpe <- weight_s
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Portfolios Under Zero Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the correlations of returns are equal to zero, then the covariance matrix is diagonal:
      \begin{displaymath}
        \mathbb{C} = \begin{pmatrix}
          \sigma^2_1 & 0 & \cdots & 0 \\
          0 & \sigma^2_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^2_n
        \end{pmatrix}
      \end{displaymath}
      Where $\sigma^2_i$ is the variance of returns of asset \texttt{i}.
      \vskip1ex
      The inverse of $\mathbb{C}$ is then simply:
      \begin{displaymath}
        \mathbb{C}^{-1} = \begin{pmatrix}
          \sigma^{-2}_1 & 0 & \cdots & 0 \\
          0 & \sigma^{-2}_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^{-2}_n
        \end{pmatrix}
      \end{displaymath}
    \column{0.5\textwidth}
      The \emph{minimum variance} portfolio weights are proportional to the inverse of the individual variances:
      \begin{displaymath}
        w_i = \frac{1}{\sigma^2_i \sum_{i=1}^n \sigma^{-2}_i}
      \end{displaymath}
      The maximum \emph{Sharpe} portfolio weights are proportional to the ratio of excess returns divided by the individual variances:
      \begin{displaymath}
        w_i = \frac{\mu_i}{\sigma^2_i \sum_{i=1}^n \mu_i \sigma^{-2}_i}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Optimization Using Principal Components}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      First apply Principal Component Analysis and then perform portfolio optimization using the principal components.
      \vskip1ex
      If the correlations of returns are equal to zero, then the covariance matrix is diagonal:
      \begin{displaymath}
        \mathbb{C} = \begin{pmatrix}
          \sigma^2_1 & 0 & \cdots & 0 \\
          0 & \sigma^2_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^2_n
        \end{pmatrix}
      \end{displaymath}
      Where $\sigma^2_i$ is the variance of returns of asset \texttt{i}.
      \vskip1ex
      The inverse of $\mathbb{C}$ is then simply:
      \begin{displaymath}
        \mathbb{C}^{-1} = \begin{pmatrix}
          \sigma^{-2}_1 & 0 & \cdots & 0 \\
          0 & \sigma^{-2}_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^{-2}_n
        \end{pmatrix}
      \end{displaymath}
    \column{0.5\textwidth}
      The \emph{minimum variance} portfolio weights are proportional to the inverse of the individual variances:
      \begin{displaymath}
        w_i = \frac{1}{\sigma^2_i \sum_{i=1}^n \sigma^{-2}_i}
      \end{displaymath}
      The maximum \emph{Sharpe} portfolio weights are proportional to the ratio of excess returns divided by the individual variances:
      \begin{displaymath}
        w_i = \frac{\mu_i}{\sigma^2_i \sum_{i=1}^n \mu_i \sigma^{-2}_i}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum \protect\emph{Sharpe} and \protect\emph{Minimum Variance} Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The maximum \emph{Sharpe} and \emph{Minimum Variance} portfolios are both \emph{efficient portfolios}, with the lowest risk (standard deviation) for the given level of return.
      <<echo=TRUE,eval=FALSE>>=
library(quantmod)
# Calculate minimum variance weights
weight_s <- cov_inv %*% u_nit
weights_minvar <- weight_s/drop(t(u_nit) %*% weight_s)
# Calculate optimal portfolio returns
optim_rets <- xts(
  x=cbind(exp(cumsum(re_turns %*% weights_maxsharpe)),
          exp(cumsum(re_turns %*% weights_minvar))),
  order.by=index(re_turns))
colnames(optim_rets) <- c("maxsharpe", "minvar")
# Plot optimal portfolio returns, with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "green")
x11(width=6, height=5)
chart_Series(optim_rets, theme=plot_theme,
  name="Maximum Sharpe and
  Minimum Variance portfolios")
legend("top", legend=colnames(optim_rets), cex=0.8,
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/maxsharpe_minvar.png}\\
      The \emph{Capital Market Line} represents delevered and levered portfolios, consisting of the \emph{market portfolio} combined with the \emph{risk-free} rate.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Frontier} and \protect\emph{Capital Market Line}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The maximum \emph{Sharpe} portfolio weights depend on the value of the risk-free rate $r_f$,
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} (\mathbf{r} - r_f)}{\mathbbm{1}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)}
      \end{displaymath}
      The \emph{Efficient Frontier} is the set of \emph{efficient portfolios}, that have the lowest risk (standard deviation) for the given level of return.
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are \emph{efficient portfolios}, and they lie on the \emph{Efficient Frontier}, forming a tangent line from the risk-free rate to the \emph{Efficient Frontier}, known as the \emph{Capital Market Line} (CML).
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are considered to be the \emph{market portfolios}, corresponding to different values of the risk-free rate $r_f$.
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are also called \emph{tangency} portfolios, since they are the tangent point on the \emph{Efficient Frontier}.
      \vskip1ex
      The \emph{Capital Market Line} is the line drawn from the \emph{risk-free} rate to the \emph{market portfolio} on the \emph{Efficient Frontier}.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{Efficient Frontier} and Maximum \protect\emph{Sharpe} Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
x11(wid_th <- 6, hei_ght <- 6)
# Calculate minimum variance weights
weight_s <- cov_inv %*% u_nit
weight_s <- weight_s / drop(t(u_nit) %*% weight_s)
# Minimum standard deviation and return
std_dev <- sqrt(252*drop(weight_s %*% cov_mat %*% weight_s))
min_ret <- 252*sum(weight_s * mean_rets)
# Calculate maximum Sharpe portfolios
risk_free <- (min_ret * seq(-10, 10, by=0.1)^3)/252
eff_front <- sapply(risk_free, function(risk_free) {
  weight_s <- cov_inv %*% (mean_rets - risk_free)
  weight_s <- weight_s/drop(t(u_nit) %*% weight_s)
  # Portfolio return and standard deviation
  c(return=252*sum(weight_s * mean_rets),
    stddev=sqrt(252*drop(weight_s %*% cov_mat %*% weight_s)))
})  # end sapply
eff_front <- cbind(252*risk_free, t(eff_front))
colnames(eff_front)[1] <- "risk-free"
eff_front <- eff_front[is.finite(eff_front[, "stddev"]), ]
eff_front <- eff_front[order(eff_front[, "return"]), ]
# Plot maximum Sharpe portfolios
plot(x=eff_front[, "stddev"],
     y=eff_front[, "return"], t="l",
     xlim=c(0.0*std_dev, 3.0*std_dev),
     ylim=c(0.0*min_ret, 2.0*min_ret),
     main="Efficient Frontier and Capital Market Line",
     xlab="standard deviation", ylab="return")
points(x=eff_front[, "stddev"], y=eff_front[, "return"],
       col="red", lwd=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting the \protect\emph{Capital Market Line}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot minimum variance portfolio
points(x=std_dev, y=min_ret, col="green", lwd=6)
text(std_dev, min_ret, labels="minimum \nvariance",
     pos=4, cex=0.8)
# Draw Capital Market Line
sor_ted <- sort(eff_front[, 1])
risk_free <-
  sor_ted[findInterval(x=0.5*min_ret, vec=sor_ted)]
points(x=0, y=risk_free, col="blue", lwd=6)
text(x=0, y=risk_free, labels="risk-free",
     pos=4, cex=0.8)
in_dex <- match(risk_free, eff_front[, 1])
points(x=eff_front[in_dex, "stddev"],
       y=eff_front[in_dex, "return"],
       col="blue", lwd=6)
text(x=eff_front[in_dex, "stddev"],
     y=eff_front[in_dex, "return"],
     labels="market portfolio",
     pos=2, cex=0.8)
sharp_e <- (eff_front[in_dex, "return"]-risk_free)/
  eff_front[in_dex, "stddev"]
abline(a=risk_free, b=sharp_e, col="blue", lwd=2)
text(x=0.7*eff_front[in_dex, "stddev"],
     y=0.7*eff_front[in_dex, "return"]+0.01,
     labels="Capital Market Line", pos=2, cex=0.8,
     srt=45*atan(sharp_e*hei_ght/wid_th)/(0.25*pi))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}\\
      \vspace{-1em}
      The \emph{Capital Market Line} represents delevered and levered portfolios, consisting of the \emph{market portfolio} combined with the \emph{risk-free} rate.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Random Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate random portfolios
n_portf <- 1000
ret_sd <- sapply(1:n_portf, function(in_dex) {
  weight_s <- runif(n_weights-1, min=-0.25, max=1.0)
  weight_s <- c(weight_s, 1-sum(weight_s))
  # Portfolio return and standard deviation
  c(return=252*sum(weight_s * mean_rets),
    stddev=sqrt(252*drop(weight_s %*% cov_mat %*% weight_s)))
})  # end sapply
# Plot scatterplot of random portfolios
x11(wid_th <- 6, hei_ght <- 6)
plot(x=ret_sd["stddev", ], y=ret_sd["return", ],
     main="Efficient Frontier and Random Portfolios",
     xlim=c(0.5*std_dev, 0.8*max(ret_sd["stddev", ])),
     xlab="standard deviation", ylab="return")
# Plot maximum Sharpe portfolios
lines(x=eff_front[, "stddev"],
     y=eff_front[, "return"], lwd=2)
points(x=eff_front[, "stddev"], y=eff_front[, "return"],
       col="red", lwd=3)
# Plot minimum variance portfolio
points(x=std_dev, y=min_ret, col="green", lwd=6)
text(std_dev, min_ret, labels="minimum\nvariance",
     pos=2, cex=0.8)
# Plot market portfolio
points(x=eff_front[in_dex, "stddev"],
       y=eff_front[in_dex, "return"], col="green", lwd=6)
text(x=eff_front[in_dex, "stddev"],
     y=eff_front[in_dex, "return"],
     labels="market\nportfolio",
     pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_random.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
points(x=sqrt(252*diag(cov_mat)),
       y=252*mean_rets, col="blue", lwd=6)
text(x=sqrt(252*diag(cov_mat)), y=252*mean_rets,
     labels=names(mean_rets),
     col="blue", pos=1, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Plotting Random Portfolios Without Using Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Vector of symbol names
sym_bols <- c("VTI", "IEF", "DBC")
n_weights <- NROW(sym_bols)
# Calculate random portfolios
n_portf <- 1000
ret_sd <- sapply(1:n_portf, function(in_dex) {
  weight_s <- runif(n_weights, min=0, max=10)
  weight_s <- weight_s/sum(weight_s)
  portf_rets <- etf_env$re_turns[, sym_bols] %*% weight_s
  100*c(ret=mean(portf_rets), sd=sd(portf_rets))
})  # end sapply
# Plot scatterplot of random portfolios
x11(width=6, height=5)
plot(x=ret_sd[2, ], y=ret_sd[1, ], xlim=c(0, max(ret_sd[2, ])),
     main="Random portfolios",
     ylim=c(min(0, min(ret_sd[1, ])), max(ret_sd[1, ])),
     xlab=rownames(ret_sd)[2], ylab=rownames(ret_sd)[1])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_random.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
points(x=sqrt(252*diag(cov_mat)),
       y=252*mean_rets, col="blue", lwd=6)
text(x=sqrt(252*diag(cov_mat)), y=252*mean_rets,
     labels=names(mean_rets),
     col="blue", pos=1, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Efficient Frontier for Two-asset Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The covariance matrix for two assets is equal to:
      \begin{displaymath}
        \mathbb{C} = \begin{pmatrix}
          \sigma^2_1 & \sigma_{12} \\
          \sigma_{12} & \sigma^2_2
        \end{pmatrix}
      \end{displaymath}
      Where $\sigma_{12}$ is the covariance of returns between the two assets,
      The excess returns of a two-asset portfolio are equal to:
      \begin{displaymath}
        R = w \mu_1 + (1 - w) \mu_2
      \end{displaymath}
      Solving for the weight $\mathbf{w}$:
      \begin{displaymath}
        w = (R - \mu_2) / (\mu_1 - \mu_2)
      \end{displaymath}
      \vskip1ex
      The variance of the maximum \emph{Sharpe} portfolio is equal to:
      \begin{displaymath}
        \sigma^2 = \frac{\mu^T \mathbb{C}^{-1} \mathbb{C} \, \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2} = \frac{\mu^T \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2}
      \end{displaymath}
    \column{0.5\textwidth}
      If the correlations of returns are equal to zero, then
      The \emph{minimum variance} portfolio weights are proportional to the inverse of the individual variances:
      \begin{displaymath}
        w_i = \frac{1}{\sigma^2_i \sum_{i=1}^n \sigma^{-2}_i}
      \end{displaymath}
      The maximum \emph{Sharpe} portfolio weights are proportional to the ratio of excess returns divided by the individual variances:
      \begin{displaymath}
        w_i = \frac{\mu_i}{\sigma^2_i \sum_{i=1}^n \mu_i \sigma^{-2}_i}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Efficient Frontier for Two-asset Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<corr_two_assets,echo=TRUE,eval=FALSE>>=
risk_free <- 0.03
re_turns <- c(asset1=0.05, asset2=0.06)
std_devs <- c(asset1=0.4, asset2=0.5)
cor_rel <- 0.6
cov_mat <- matrix(c(1, cor_rel, cor_rel, 1), nc=2)
cov_mat <- t(t(std_devs*cov_mat)*std_devs)
weight_s <- seq(from=-1, to=2, length.out=31)
weight_s <- cbind(weight_s, 1-weight_s)
portf_rets <- weight_s %*% re_turns
portf_sd <-
  sqrt(rowSums(weight_s * (weight_s %*% cov_mat)))
sharpe_ratios <- (portf_rets-risk_free)/portf_sd
in_dex <- which.max(sharpe_ratios)
max_Sharpe <- max(sharpe_ratios)
# Plot efficient frontier
x11(wid_th <- 6, hei_ght <- 5)
par(mar=c(3,3,2,1)+0.1, oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(portf_sd, portf_rets, t="l",
 main=paste0("Efficient frontier and CML for two assets\ncorrelation = ", 100*cor_rel, "%"),
 xlab="standard deviation", ylab="return",
 lwd=2, col="orange",
 xlim=c(0, max(portf_sd)),
 ylim=c(0.02, max(portf_rets)))
# Add Market Portfolio (maximum Sharpe ratio portfolio)
points(portf_sd[in_dex], portf_rets[in_dex],
       col="blue", lwd=3)
text(x=portf_sd[in_dex], y=portf_rets[in_dex],
     labels=paste(c("market portfolio\n",
       structure(c(weight_s[in_dex], 1-weight_s[in_dex]),
               names=names(re_turns))), collapse=" "),
     pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/cml_two_assets.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
points(std_devs, re_turns, col="green", lwd=3)
text(std_devs, re_turns, labels=names(re_turns), pos=4, cex=0.8)
# Add point at risk-free rate and draw Capital Market Line
points(x=0, y=risk_free, col="blue", lwd=3)
text(0, risk_free, labels="risk-free\nrate", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, lwd=2, col="blue")
range_s <- par("usr")
text(portf_sd[in_dex]/2, (portf_rets[in_dex]+risk_free)/2,
     labels="Capital Market Line", cex=0.8, , pos=3,
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Efficient Frontier of Stock and Bond Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:3)),eval=FALSE>>=
# Plot portfolios in x11() window
x11(wid_th <- 6, hei_ght <- 5)
par(oma=c(0, 0, 0, 0), mar=c(3,3,2,1)+0.1, mgp=c(2, 1, 0), cex.lab=1.0, cex.axis=1.0, cex.main=1.0, cex.sub=1.0)
# Vector of symbol names
sym_bols <- c("VTI", "IEF")
# Matrix of portfolio weights
weight_s <- seq(from=-1, to=2, length.out=31)
weight_s <- cbind(weight_s, 1-weight_s)
# Calculate portfolio returns and volatilities
re_turns <- rutils::etf_env$re_turns[, sym_bols]
ret_sd <- re_turns %*% t(weight_s)
ret_sd <- cbind(252*colMeans(ret_sd),
  sqrt(252)*matrixStats::colSds(ret_sd))
colnames(ret_sd) <- c("returns", "stddev")
risk_free <- 0.06
ret_sd <- cbind(ret_sd,
  (ret_sd[, "returns"]-risk_free)/ret_sd[, "stddev"])
colnames(ret_sd)[3] <- "Sharpe"
in_dex <- which.max(ret_sd[, "Sharpe"])
max_Sharpe <- ret_sd[in_dex, "Sharpe"]
plot(x=ret_sd[, "stddev"], y=ret_sd[, "returns"],
     main="Stock and Bond portfolios", t="l",
     xlim=c(0, 0.7*max(ret_sd[, "stddev"])), ylim=c(0, max(ret_sd[, "returns"])),
     xlab="standard deviation", ylab="return")
# Add blue point for market portfolio
points(x=ret_sd[in_dex, "stddev"], y=ret_sd[in_dex, "returns"], col="blue", lwd=6)
text(x=ret_sd[in_dex, "stddev"], y=ret_sd[in_dex, "returns"],
     labels=paste(c("market portfolio\n", structure(c(weight_s[in_dex, 1], weight_s[in_dex, 2]), names=sym_bols)), collapse=" "),
     pos=3, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_stocks_bonds.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
mean_rets <- 252*sapply(re_turns, mean)
std_devs <- sqrt(252)*sapply(re_turns, sd)
points(std_devs, mean_rets, col="green", lwd=6)
text(std_devs, mean_rets, labels=names(re_turns), pos=2, cex=0.8)
# Add point at risk-free rate and draw Capital Market Line
points(x=0, y=risk_free, col="blue", lwd=6)
text(0, risk_free, labels="risk-free", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, col="blue", lwd=2)
range_s <- par("usr")
text(max(ret_sd[, "stddev"])/3, 0.75*max(ret_sd[, "returns"]),
     labels="Capital Market Line", cex=0.8, , pos=3,
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Market Portfolio for Stocks and Bonds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
# Plot portfolios in x11() window
x11(wid_th <- 6, hei_ght <- 5)
# Calculate cumulative returns of VTI and IEF
optim_rets <- lapply(re_turns,
  function(re_turns) exp(cumsum(re_turns)))
optim_rets <- rutils::do_call(cbind, optim_rets)
# Calculate market portfolio returns
optim_rets <- cbind(exp(cumsum(re_turns %*%
    c(weight_s[in_dex], 1-weight_s[in_dex]))),
  optim_rets)
colnames(optim_rets)[1] <- "market"
# Plot market portfolio with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green")
chart_Series(optim_rets, theme=plot_theme,
             name="Market portfolio for stocks and bonds")
legend("top", legend=colnames(optim_rets),
       cex=0.8, inset=0.1, bg="white", lty=1,
       lwd=6, col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/market_stocks_bonds.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Portfolio Optimization}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk (\protect\emph{CVaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} (\emph{CVaR}) is equal to the average of the \emph{VaR} for confidence levels less than a given confidence level $\alpha$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the Expected Shortfall (\emph{ES}), or the Expected Tail Loss (\emph{ETL}).
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data, and returns a list with a vector of loss values and a vector of corresponding densities.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=4)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
# VTI percentage returns
re_turns <- rutils::diff_it(log(quantmod::Cl(rutils::etf_env$VTI)))
conf_level <- 0.1
va_r <- quantile(re_turns, conf_level)
c_var <- mean(re_turns[re_turns < va_r])
# Or
sort_ed <- sort(as.numeric(re_turns))
in_dex <- round(conf_level*NROW(re_turns))
va_r <- sort_ed[in_dex]
c_var <- mean(sort_ed[1:in_dex])
# Plot histogram of VTI returns
min_var <- (-0.05)
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=100, xlim=c(min_var, 0.01),
  ylab="frequency", freq=FALSE, main="VTI Returns Histogram")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_var.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot density of losses
densi_ty <- density(re_turns, adjust=1.5)
lines(densi_ty, lwd=3, col="blue")
# Add line for VaR
abline(v=va_r, col="red", lwd=3)
y_max <- max(densi_ty$y)
text(x=va_r, y=2*y_max/3, labels="VaR", lwd=2, pos=2)
# Add shading for CVaR
rang_e <- (densi_ty$x < va_r) & (densi_ty$x > min_var)
polygon(
  c(min_var, densi_ty$x[rang_e], va_r),
  c(0, densi_ty$y[rang_e], 0),
  col=rgb(1, 0, 0,0.5), border=NA)
text(x=1.5*va_r, y=y_max/7, labels="CVaR", lwd=2, pos=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CVaR} Portfolio Weights Using Linear Programming}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the minimum \emph{CVaR} portfolio can be calculated using linear programming (\emph{LP}), which is the optimization of linear objective functions subject to linear constraints,
      \begin{displaymath}
        w_{min} = \operatorname*{arg\,max}_{w} [ \, \sum_{i=1}^n w_i b_i \, ]
      \end{displaymath}
      Where $b_i$ is the negative objective vector, and $\mathbf{w}$ is the vector of returns weights, constrained by:
      \begin{align*}
        \mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1\\
        0 \leq w_i \leq 1
      \end{align*}
      The function \texttt{Rglpk\_solve\_LP()} from package \emph{Rglpk} solves linear programming problems by calling the \emph{GNU Linear Programming Kit} library.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(rutils)  # Load rutils
library(Rglpk)
# Vector of symbol names and returns
sym_bols <- c("VTI", "IEF", "DBC")
n_weights <- NROW(sym_bols)
re_turns <- rutils::etf_env$re_turns[((NROW(re_turns)-6):NROW(re_turns)), sym_bols]
mean_rets <- colMeans(re_turns)
conf_level <- 0.05
r_min <- 0 ; w_min <- 0 ; w_max <- 1
weight_sum <- 1
n_cols <- NCOL(re_turns) # number of assets
n_rows <- NROW(re_turns) # number of rows
# Creat objective vector
obj_vector <- c(numeric(n_cols), rep(-1/(conf_level*n_rows), n_rows), -1)
# Specify weight constraints
constraint_s <- rbind(
  cbind(rbind(1, mean_rets),
        matrix(data=0, nrow=2, ncol=(n_rows+1))),
  cbind(coredata(re_turns), diag(n_rows), 1))
rh_s <- c(weight_sum, r_min, rep(0, n_rows))
direction_s <- c("==", ">=", rep(">=", n_rows))
# Specify weight bounds
bound_s <- list(
  lower=list(ind=1:n_cols, val=rep(w_min, n_cols)),
  upper=list(ind=1:n_cols, val=rep(w_max, n_cols)))
# Perform optimization
op_tim <- Rglpk_solve_LP(obj=obj_vector, mat=constraint_s, dir=direction_s, rhs=rh_s, types=rep("C", NROW(obj_vector)), max=T, bounds=bound_s)
op_tim$solution
constraint_s %*% op_tim$solution
obj_vector %*% op_tim$solution
as.numeric(op_tim$solution[1:n_cols])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Sharpe} Ratio Objective Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{optimize()} performs \emph{one-dimensional} optimization over a single independent variable.
      \vskip1ex
      \texttt{optimize()} searches for the minimum of the objective function with respect to its first argument, in the specified interval.
      \vspace{-1em}
        <<echo=(-(1:3)),eval=FALSE>>=
# Calculate daily percentage re_turns
sym_bols <- c("VTI", "IEF", "DBC")
re_turns <- rutils::etf_env$re_turns[, sym_bols]
# Create initial vector of portfolio weights
weight_s <- rep(1, NROW(sym_bols))
names(weight_s) <- sym_bols
# Objective equal to minus Sharpe ratio
objec_tive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  if (sd(portf_rets) == 0)
    return(0)
  else
    return(-mean(portf_rets)/sd(portf_rets))
}  # end objec_tive
# Objective for equal weight portfolio
objec_tive(weight_s, re_turns=re_turns)
op_tim <- unlist(optimize(
  f=function(weight)
    objec_tive(c(1, 1, weight), re_turns=re_turns),
  interval=c(-4, 1)))
# Vectorize objective function with respect to third weight
vec_object <- function(weights) sapply(weights,
  function(weight) objec_tive(c(1, 1, weight),
    re_turns=re_turns))
# Or
vec_object <- Vectorize(FUN=function(weight)
    objec_tive(c(1, 1, weight), re_turns=re_turns),
  vectorize.args="weight")  # end Vectorize
vec_object(1)
vec_object(1:3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_obj_one_dim.png}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 1), mgp=c(2, 1, 0), mar=c(3, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Plot objective function with respect to third weight
curve(expr=vec_object,
      type="l", xlim=c(-4.0, 1.0),
      xlab=paste("weight of", names(weight_s[3])),
      ylab="", lwd=2)
title(main="Objective Function", line=-1)  # Add title
points(x=op_tim[1], y=op_tim[2], col="green", lwd=6)
text(x=op_tim[1], y=op_tim[2],
     labels="minimum objective", pos=4, cex=0.8)

### below is simplified code for plotting objective function
# Create vector of DBC weights
weight_s <- seq(from=-4, to=1, by=0.1)
obj_val <- sapply(weight_s,
  function(weight) objec_tive(c(1, 1, weight)))
plot(x=weight_s, y=obj_val, t="l",
      xlab="weight of DBC", ylab="", lwd=2)
title(main="Objective Function", line=-1)  # Add title
points(x=op_tim[1], y=op_tim[2], col="green", lwd=6)
text(x=op_tim[1], y=op_tim[2],
     labels="minimum objective", pos=4, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Perspective Plot of Portfolio Objective Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{persp()} plots a 3d perspective surface plot of a function specified over a grid of argument values.
      \vskip1ex
      The function \texttt{outer()} calculates the values of a function over a grid spanned by two variables, and returns a matrix of function values.
      \vskip1ex
      The package \emph{rgl} allows creating \emph{interactive} 3d scatterplots and surface plots including perspective plots, based on the \emph{OpenGL} framework.
      <<portf_persp,echo=TRUE,eval=FALSE,fig.width=10,fig.height=10,fig.show='hide'>>=
# Vectorize function with respect to all weights
vec_object <- Vectorize(
  FUN=function(w1, w2, w3) objec_tive(c(w1, w2, w3)),
  vectorize.args=c("w2", "w3"))  # end Vectorize
# Calculate objective on 2-d (w2 x w3) parameter grid
w2 <- seq(-3, 7, length=50)
w3 <- seq(-5, 5, length=50)
grid_object <- outer(w2, w3, FUN=vec_object, w1=1)
rownames(grid_object) <- round(w2, 2)
colnames(grid_object) <- round(w3, 2)
# Perspective plot of objective function
persp(w2, w3, -grid_object,
      theta=45, phi=30, shade=0.5,
      col=rainbow(50), border="green",
      main="objective function")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_persp.png}
    \vspace{-3em}
      <<echo=TRUE,eval=FALSE,fig.width=10,fig.height=10>>=
# Interactive perspective plot of objective function
library(rgl)
rgl::persp3d(z=-grid_object, zlab="objective",
        col="green", main="objective function")
rgl::persp3d(
  x=function(w2, w3) {-vec_object(w1=1, w2, w3)},
  xlim=c(-3, 7), ylim=c(-5, 5),
  col="green", axes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Multi-dimensional Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Vector of initial portfolio weights equal to 1
weight_s <- rep(1, n_weights)
names(weight_s) <- sym_bols
# Objective function equal to standard deviation of returns
objec_tive <- function(weight_s) {
  portf_rets <- re_turns %*% weight_s
  sd(portf_rets)/sum(weight_s)
}  # end objec_tive
# objec_tive() for equal weight portfolio
objec_tive(weight_s)
objec_tive(2*weight_s)
# Perform portfolio optimization
op_tim <- optim(par=weight_s,
                fn=objec_tive,
                method="L-BFGS-B",
                upper=rep(10, n_weights),
                lower=rep(-10, n_weights))
# Rescale the optimal weights
weight_s <- op_tim$par/sum(op_tim$par)
# Minimum variance portfolio returns
optim_rets <- xts(x=re_turns %*% weight_s,
                  order.by=index(re_turns))
chart_Series(x=exp(cumsum(optim_rets)), name="minvar portfolio")
# Add green point for minimum variance portfolio
optim_sd <- 100*sd(optim_rets)
optim_ret <- 100*mean(optim_rets)
points(x=optim_sd, y=optim_ret, col="green", lwd=6)
text(x=optim_sd, y=optim_ret, labels="minvar", pos=2, cex=0.8)


# Objective function equal to minus Sharpe ratio
risk_free <- 0.03
objec_tive <- function(weight_s) {
  portf_rets <- 100*etf_env$re_turns[, names(weight_s)] %*% weight_s / sum(weight_s)
  -mean(portf_rets-risk_free)/sd(portf_rets)
}  # end objec_tive
# Perform portfolio optimization
op_tim <- optim(par=weight_s,
                   fn=objec_tive,
                   method="L-BFGS-B",
                   upper=rep(10, n_weights),
                   lower=rep(-10, n_weights))
# Maximum Sharpe ratio portfolio returns
weight_s <- op_tim$par/sum(op_tim$par)
optim_rets <- xts(x=re_turns %*% weight_s,
                  order.by=index(re_turns))
chart_Series(x=exp(cumsum(optim_rets)), name="maxSR portfolio")
optim_sd <- 100*sd(optim_rets)
optim_ret <- 100*mean(optim_rets)
points(x=optim_sd, y=optim_ret,
       col="blue", lwd=3)
text(x=optim_sd, y=optim_ret,
     labels="maxSR", pos=2, cex=0.8)
max_Sharpe <- (optim_ret-risk_free)/optim_sd
# Plot individual assets
mean_rets <- 100*sapply(re_turns, mean)
std_devs <- 100*sapply(re_turns, sd)
points(std_devs, mean_rets, col="green", lwd=3)
text(std_devs, mean_rets, labels=names(mean_rets), pos=2, cex=0.8)
# Add point at risk-free rate and draw Capital Market Line
points(x=0, y=risk_free)
text(0, risk_free, labels="risk-free", pos=4, cex=0.8)
abline(a=risk_free, b=max_Sharpe, col="blue")
range_s <- par("usr")
text(optim_sd/3, (optim_ret+risk_free)/2.5,
     labels="Capital Market Line", cex=0.8, , pos=3,
     srt=45*atan(max_Sharpe*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   hei_ght/wid_th)/(0.25*pi))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_random.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multi-dimensional Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{optim()} performs \emph{multi-dimensional} optimization.
      \vskip1ex
      The argument \texttt{par} are the initial parameter values.
      \vskip1ex
      The argument \texttt{fn} is the objective function to be minimized.
      \vskip1ex
      The argument of the objective function which is to be optimized, must be a vector argument.
      \vskip1ex
      \texttt{optim()} accepts additional parameters bound to the dots \texttt{"..."} argument, and passes them to the \texttt{fn} objective function.
      \vskip1ex
      The arguments \texttt{lower} and \texttt{upper} specify the search range for the variables of the objective function \texttt{fn}.
      \vskip1ex
      \texttt{method="L-BFGS-B"} specifies the quasi-Newton optimization method.
      \vskip1ex
      \texttt{optim()} returns a list containing the location of the minimum and the objective function value.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Optimization to find weights with maximum Sharpe ratio
op_tim <- optim(par=weight_s,
                   fn=objec_tive,
                   re_turns=re_turns,
                   method="L-BFGS-B",
                   upper=c(1.1, 10, 10),
                   lower=c(0.9, -10, -10))
# Optimal parameters
op_tim$par
op_tim$par <- op_tim$par/sum(op_tim$par)
# Optimal Sharpe ratio
-objec_tive(op_tim$par)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Portfolio Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimized portfolio has both long and short positions, and outperforms its individual component assets.
      \vskip1ex
      \vspace{-1em}
      <<optim_portf_basic,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
x11(width=6, height=5)
par(oma=c(1, 1, 1, 0), mgp=c(2, 1, 0), mar=c(2, 1, 2, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Plot in two vertical panels
layout(matrix(c(1,2), 2),
       widths=c(1,1), heights=c(1,3))
# barplot of optimal portfolio weights
barplot(op_tim$par, col=c("red", "green", "blue"),
        main="Optimized portfolio weights")
# Calculate cumulative returns of VTI, IEF, DBC
cum_rets <- lapply(re_turns,
  function(re_turns) exp(cumsum(re_turns)))
cum_rets <- rutils::do_call(cbind, cum_rets)
# Calculate optimal portfolio returns with VTI, IEF, DBC
optim_rets <- cbind(
  exp(cumsum(re_turns %*% op_tim$par)),
  cum_rets)
colnames(optim_rets)[1] <- "optim_rets"
# Plot optimal returns with VTI, IEF, DBC
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green", "blue")
chart_Series(optim_rets, theme=plot_theme,
             name="Optimized portfolio performance")
legend("top", legend=colnames(optim_rets), cex=0.8,
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
# Or plot non-compounded (simple) cumulative returns
PerformanceAnalytics::chart.CumReturns(
  cbind(re_turns %*% op_tim$par, re_turns),
  lwd=2, ylab="", legend.loc="topleft", main="")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/optim_portf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Mean-Variance Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The mean-variance objective function is designed to maximize portfolio returns and minimize their variance:
      \begin{displaymath}
        O(x) = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - q \, \mathbf{w}^T \mathbf{r}
      \end{displaymath}
      Where $\mathbb{C}$ is the covariance matrix of returns, $\mathbf{r}$ is the vector of returns, $\mathbf{w}$ is the vector of  portfolio weights, and $q$ is the risk tolerance factor.
      \vskip1ex
      The mean-variance optimal portfolio is defined as
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
        \mathbf{w}^T \mathbb{C} \, \mathbf{w}
      \end{displaymath}
      Where the sum of portfolio weights $\mathbf{w}$ is constrained to equal \texttt{1}: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$.
      \vskip1ex
      Legacy stuff below:\\
      A Linear Regression model with $p$ explanatory variables $\{x_j\}$, is defined by the formula:
      \begin{displaymath}
        z_i = \alpha + \sum_{j=1}^{k} {\beta_j x_{i,j}} + \varepsilon_i
      \end{displaymath}
      Or in vector notation:
      \begin{displaymath}
        z = \alpha + \beta x + \varepsilon
      \end{displaymath}
      The response variable $z$ and the $p$ explanatory variables $\{x_j\}$ each contain \texttt{n} observations.
      \vskip1ex
      The response variable $z$ is a vector of length \texttt{n}, and the explanatory variable \texttt{x} is a $(n,p)$-dimensional matrix.
      \vskip1ex
      $\alpha$ and $\beta$ are the unknown regression coefficients, with $\alpha$ a scalar and $\beta$ a vector of length $p$.
      \vskip1ex
      $\varepsilon_i$ are the residuals, assumed to be normally distributed, independent, and stationary, with $\varepsilon$ a vector of length $p$.
    \column{0.5\textwidth}
      The OLS estimate for $\alpha$ is given by:
      \begin{align*}
        \alpha = z^T \mathbbm{1} - \beta x^T \mathbbm{1}
      \end{align*}
      If the variables are de-meaned, then the OLS estimate for $\beta$ is given by equating the RSS derivative to zero:
      \begin{flalign*}
        & RSS_\beta = - 2 (z - \beta x)^T x = 0\\
        & x^T z - \beta x^T x = 0\\
        & \beta = (x^T x)^{-1} x^T z
      \end{flalign*}
      The matrix $x^T x$ is the covariance matrix of the matrix \texttt{x}.
      \vskip1ex
      The covariance matrix $x^T x$ is invertible if the columns of \texttt{x} are linearly independent.
      \vskip1ex
      The matrix $(x^T x)^{-1} x^T$ is known as the \emph{Moore-Penrose pseudo-inverse} of the matrix \texttt{x}.
      \vskip1ex
      In the special case when the inverse matrix $x^{-1}$ does exist, then the \emph{pseudo-inverse} matrix simplifies to the inverse: $(x^T x)^{-1} x^T = x^{-1} (x^T)^{-1} x^T = x^{-1}$
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{quadprog} for Quadratic Programming}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Quadratic programming (\emph{QP}) is the optimization of quadratic objective functions subject to linear constraints.
      \vskip1ex
      Let $O(x)$ be an objective function that is quadratic with respect to a vector variable \texttt{x}:
      \begin{displaymath}
        O(x) = \frac{1}{2} x^T \mathbb{Q} x - d^T x
      \end{displaymath}
      Where $\mathbb{Q}$ is a \emph{positive definite} matrix ($x^T \mathbb{Q} x > 0$), and $d$ is a vector.
      \vskip1ex
      An example of a \emph{positive definite} matrix is the covariance matrix of linearly independent variables.
      \vskip1ex
      Let the linear constraints on the variable \texttt{x} be specified as:
      \begin{displaymath}
        \mathbb{A} x \geq b
      \end{displaymath}
      Where $\mathbb{A}$ is a matrix, and $b$ is a vector.
      \vskip1ex
      The function \texttt{solve.QP()} from package \emph{quadprog} performs optimization of quadratic objective functions subject to linear constraints.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:6)),eval=FALSE>>=
risk_free <- 0.03
re_turns <- c(asset1=0.05, asset2=0.06)
std_devs <- c(asset1=0.4, asset2=0.5)
cor_rel <- 0.6
cov_mat <- matrix(c(1, cor_rel, cor_rel, 1), nc=2)
cov_mat <- t(t(std_devs*cov_mat)*std_devs)
library(quadprog)
# Minimum variance weights without constraints
op_tim <- solve.QP(Dmat=2*cov_mat,
                    dvec=rep(0, 2),
                    Amat=matrix(0, nr=2, nc=1),
                    bvec=0)
# Minimum variance weights sum equal to 1
op_tim <- solve.QP(Dmat=2*cov_mat,
                    dvec=rep(0, 2),
                    Amat=matrix(1, nr=2, nc=1),
                    bvec=1)
# Optimal value of objective function
t(op_tim$solution) %*% cov_mat %*% op_tim$solution
## Perform simple optimization for reference
# Objective function for simple optimization
objec_tive <- function(x) {
  x <- c(x, 1-x)
  t(x) %*% cov_mat %*% x
}  # end objec_tive
unlist(optimize(f=objec_tive, interval=c(-1, 2)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using Package \protect\emph{quadprog}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The objective function is designed to minimize portfolio variance and maximize its returns:
      \begin{displaymath}
        O(x) = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - \mathbf{w}^T \mathbf{r}
      \end{displaymath}
      Where $\mathbb{C}$ is the covariance matrix of returns, $\mathbf{r}$ is the vector of returns, and $\mathbf{w}$ is the vector of  portfolio weights.
      \vskip1ex
      The portfolio weights $\mathbf{w}$ are constrained as:
      \begin{align*}
        \mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1\\
        0 \leq w_i \leq 1
      \end{align*}
      The function \texttt{solve.QP()} has the arguments:
      \vskip1ex
      \texttt{Dmat} and \texttt{dvec} are the matrix and vector defining the quadratic objective function.
      \vskip1ex
      \texttt{Amat} and \texttt{bvec} are the matrix and vector defining the constraints.
      \vskip1ex
      \texttt{meq} specifies the number of equality constraints
      (the first \texttt{meq} constraints are equalities, and the rest are inequalities).
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate daily percentage re_turns
sym_bols <- c("VTI", "IEF", "DBC")
re_turns <- rutils::etf_env$re_turns[, sym_bols]
# Calculate the covariance matrix
cov_mat <- cov(re_turns)
# Minimum variance weights, with sum equal to 1
op_tim <- quadprog::solve.QP(Dmat=2*cov_mat,
                    dvec=numeric(3),
                    Amat=matrix(1, nr=3, nc=1),
                    bvec=1)
# Minimum variance, maximum returns
op_tim <- quadprog::solve.QP(Dmat=2*cov_mat,
                    dvec=apply(0.1*re_turns, 2, mean),
                    Amat=matrix(1, nr=3, nc=1),
                    bvec=1)
# Minimum variance positive weights, sum equal to 1
a_mat <- cbind(matrix(1, nr=3, nc=1),
               diag(3), -diag(3))
b_vec <- c(1, rep(0, 3), rep(-1, 3))
op_tim <- quadprog::solve.QP(Dmat=2*cov_mat,
                    dvec=numeric(3),
                    Amat=a_mat,
                    bvec=b_vec,
                    meq=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{DEoptim} for Global Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{DEoptim()} from package \emph{DEoptim} performs \emph{global} optimization using the \emph{Differential Evolution} algorithm.
      \vskip1ex
      \emph{Differential Evolution} is a genetic algorithm which evolves a population of solutions over several generations,\\
      \hskip1em\url{http://www1.icsi.berkeley.edu/~storn/code.html}
      \vskip1ex
      The first generation of solutions is selected randomly.
      \vskip1ex
      Each new generation is obtained by combining solutions from the previous generation,       \vskip1ex
      The best solutions are selected for creating the next generation.
      \vskip1ex
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization.
      \vskip1ex
      \emph{Gradient} optimization methods are more efficient than \emph{Differential Evolution} for smooth objective functions with no local minima.
    \column{0.5\textwidth}
        <<echo=TRUE,eval=FALSE>>=
# Rastrigin function with vector argument for optimization
rastri_gin <- function(vec_tor, pa_ram=25){
  sum(vec_tor^2 - pa_ram*cos(vec_tor))
}  # end rastri_gin
vec_tor <- c(pi/6, pi/6)
rastri_gin(vec_tor=vec_tor)
library(DEoptim)
## Optimize rastri_gin using DEoptim
op_tim <-  DEoptim(rastri_gin,
  upper=c(6, 6), lower=c(-6, -6),
  DEoptim.control(trace=FALSE, itermax=50))
# Optimal parameters and value
op_tim$optim$bestmem
rastri_gin(op_tim$optim$bestmem)
summary(op_tim)
plot(op_tim)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using Package \protect\emph{Deoptim}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate daily percentage re_turns
re_turns <- rutils::etf_env$re_turns[, sym_bols]
# Objective equal to minus Sharpe ratio
objec_tive <- function(weight_s, re_turns) {
  portf_rets <- re_turns %*% weight_s
  if (sd(portf_rets) == 0)
    return(0)
  else
    return(-mean(portf_rets)/sd(portf_rets))
}  # end objec_tive
# Perform optimization using DEoptim
op_tim <- DEoptim::DEoptim(fn=objec_tive,
  upper=rep(10, NCOL(re_turns)),
  lower=rep(-10, NCOL(re_turns)),
  re_turns=re_turns,
  control=list(trace=FALSE, itermax=100, parallelType=1))
weight_s <- op_tim$optim$bestmem/sum(abs(op_tim$optim$bestmem))
names(weight_s) <- colnames(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{Shrinkage}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The technique of \emph{shrinkage} (\emph{regularization}) is designed to reduce the number of parameters in a model, for example in portfolio optimization.
      \vskip1ex
      The \emph{shrinkage} technique adds a penalty term to the objective function.
      \vskip1ex
      The \emph{elastic net} regularization is a combination of \emph{ridge} regularization and \emph{Lasso} regularization:
      \begin{align*}
        w_{max} = \operatorname*{arg\,max}_{w} [ \, \mathbf{w}^T \mathbf{r} - \\
        \lambda ( (1-\alpha) \sum_{i=1}^n w_i^2 + \alpha \sum_{i=1}^n|w_i| ) \, ]
      \end{align*}
      The portfolio weights $\mathbf{w}$ are shrunk to zero as the parameters $\lambda$ and $\alpha$ increase.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Objective with shrinkage penalty
objec_tive <- function(weight_s, re_turns, lamb_da, al_pha) {
  portf_rets <- re_turns %*% weight_s
  if (sd(portf_rets) == 0)
    return(0)
  else {
    penal_ty <- lamb_da*((1-al_pha)*sum(weight_s^2) +
        al_pha*sum(abs(weight_s)))
    return(-mean(portf_rets)/sd(portf_rets) + penal_ty)
  }
}  # end objec_tive
# Objective for equal weight portfolio
weight_s <- rep(1, NROW(sym_bols))
names(weight_s) <- sym_bols
lamb_da <- 0.5 ; al_pha <- 0.5
objec_tive(weight_s, re_turns=re_turns,
  lamb_da=lamb_da, al_pha=al_pha)
# Perform optimization using DEoptim
op_tim <- DEoptim::DEoptim(fn=objec_tive,
  upper=rep(10, NCOL(re_turns)),
  lower=rep(-10, NCOL(re_turns)),
  re_turns=re_turns,
  lamb_da=lamb_da,
  al_pha=al_pha,
  control=list(trace=FALSE, itermax=100, parallelType=1))
weight_s <- op_tim$optim$bestmem/sum(abs(op_tim$optim$bestmem))
names(weight_s) <- colnames(re_turns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Optimization Packages in \texttt{R}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The following \texttt{R} packages provide functions for portfolio optimization:
      \begin{itemize}
        \item package \emph{PortfolioAnalytics}: relies on packages \emph{xts}, \emph{ROI}, and \emph{DEoptim},
        \item package \emph{parma}: relies on packages \emph{xts}, \emph{Rglpk}, and \emph{quadprog},
        \item package \emph{fPortfolio} from the \emph{Rmetrics} suite: relies on packages \emph{tseries}, \emph{Rglpk}, and \emph{quadprog},
      \end{itemize}
      These portfolio optimization packages call generic optimization functions written in compiled \texttt{C++}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1))>>=
load(file="C:/Develop/lecture_slides/data/zoo_data.RData")
ts_stx <- as.ts(zoo_stx)
class(ts_stx)
tail(ts_stx[, 1:4])
library(xts)
st_ox <- as.xts(zoo_stx)
class(st_ox)
tail(st_ox[, 1:4])
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{PortfolioAnalytics} for Portfolio Optimization}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PortfolioAnalytics}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      The package \emph{PortfolioAnalytics} contains functions and data sets for portfolio optimization.
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package.
      \vskip1ex
    \column{0.6\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)  # load package "PortfolioAnalytics"
# get documentation for package "PortfolioAnalytics"
packageDescription("PortfolioAnalytics")  # get short description

help(package="PortfolioAnalytics")  # load help page

data(package="PortfolioAnalytics")  # list all datasets in "PortfolioAnalytics"

ls("package:PortfolioAnalytics")  # list all objects in "PortfolioAnalytics"

detach("package:PortfolioAnalytics")  # remove PortfolioAnalytics from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Definition}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolios are defined by a named vector of asset weights, and portfolio constraints and objectives.
      \vskip1ex
      \texttt{portfolio.spec} creates a portfolio object that contains asset weights, constraints, and objectives.
      \vskip1ex
      \texttt{add.constraint} adds or updates constraints on of the portfolio object.
      \vskip1ex
      \texttt{add.objective} adds or updates risk/return objectives of the portfolio object.
      <<echo=TRUE,eval=FALSE>>=
library(PortfolioAnalytics)
# Use ETF returns from package rutils
library(rutils)
portf_names <- c("VTI", "IEF", "DBC", "XLF",
        "VNQ", "XLP", "XLV", "XLU", "XLB", "XLE")
# Initial portfolio to equal weights
portf_init <- rep(1/NROW(portf_names), NROW(portf_names))
# named vector
names(portf_init) <- portf_names
# Create portfolio object
portf_init <- portfolio.spec(assets=portf_init)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_init,  # Initial portfolio
  type="weight_sum",  # Constraint sum weights
  min_sum=0.9, max_sum=1.1)
# Add constraints
portf_maxSR <- add.constraint(
  portfolio=portf_maxSR,
  type="long_only")  # box constraint min=0, max=1
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="return",  # Maximize mean return
  name="mean")
# Add objectives
portf_maxSR <- add.objective(
  portfolio=portf_maxSR,
  type="risk",  # Minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_basic,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
# Perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=etf_env$re_turns[, portf_names],  # Specify returns
  portfolio=portf_maxSR,  # Specify portfolio
  optimize_method="DEoptim", # Use DEoptim
  maxSR=TRUE,  # Maximize Sharpe
  trace=TRUE, traceDE=0)
# Plot optimization
chart.RiskReward(maxSR_DEOpt,
  risk.col="stddev",
  return.col="mean")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
maxSR_DEOpt$weights
maxSR_DEOpt$objective_measures$mean[1]
maxSR_DEOpt$objective_measures$StdDev[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_basic-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Scatterplot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_scatter,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# Plot optimization
chart.RiskReward(maxSR_DEOpt,
  risk.col="StdDev",
  return.col="mean")

# Plot risk/ret points in portfolio scatterplot
risk_ret_points <- function(rets=etf_env$re_turns,
        risk=c("sd", "ETL"), sym_bols=c("VTI", "IEF")) {
  risk <- match.arg(risk)  # Match to arg list
  if (risk=="ETL") {
    stopifnot(
      "package:PerformanceAnalytics" %in% search() ||
      require("PerformanceAnalytics", quietly=TRUE))
  }  # end if
  risk <- match.fun(risk)  # Match to function
  risk_ret <- t(sapply(rets[, sym_bols],
     function(x_ts)
       c(ret=mean(x_ts), risk=abs(risk(x_ts)))))
  points(x=risk_ret[, "risk"], y=risk_ret[, "ret"],
         col="red", lwd=3)
  text(x=risk_ret[, "risk"], y=risk_ret[, "ret"],
       labels=rownames(risk_ret), col="red",
       lwd=2, pos=4)
}  # end risk_ret_points

risk_ret_points()
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_scatter-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Sharpe Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_portf_SR_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
plot_portf <- function(portfolio,
            rets_data=etf_env$re_turns) {
  weight_s <- portfolio$weights
  portf_names <- names(weight_s)
  # Calculate xts of portfolio
  portf_max <- xts(
    rets_data[, portf_names] %*% weight_s,
    order.by=index(rets_data))
  colnames(portf_max) <-
    deparse(substitute(portfolio))
  graph_params <- par(oma=c(1, 0, 1, 0),
    mgp=c(2, 1, 0), mar=c(2, 1, 2, 1),
    cex.lab=0.8, cex.axis=1.0,
    cex.main=0.8, cex.sub=0.5)
  layout(matrix(c(1,2), 2),
    widths=c(1,1), heights=c(1,3))
  barplot(weight_s, names.arg=portf_names,
          las=3, ylab="", xlab="Symbol", main="")
  title(main=paste("Loadings",
                colnames(portf_max)), line=-1)
  chart.CumReturns(
    cbind(portf_max, rets_data[, c("IEF", "VTI")]),
    lwd=2, ylab="", legend.loc="topleft", main="")
  title(main=paste0(colnames(portf_max),
                    ", IEF, VTI"), line=-1)
  par(graph_params)  # restore original parameters
  invisible(portf_max)
}  # end plot_portf
maxSR_DEOpt_xts <- plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_portf_SR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Leverage Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The leverage constraint applies to the sum of absolute weights.
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add leverage constraint abs(weight_sum)
portf_maxSRN <- add.constraint(
  portfolio=portf_init, type="leverage",
  min_sum=0.9, max_sum=1.1)
# Add box constraint long/short
portf_maxSRN <- add.constraint(
  portfolio=portf_maxSRN,
  type="box", min=-0.2, max=0.2)

# Add objectives
portf_maxSRN <- add.objective(
  portfolio=portf_maxSRN,
  type="return",  # Maximize mean return
  name="mean")
# Add objectives
portf_maxSRN <- add.objective(
  portfolio=portf_maxSRN,
  type="risk",  # Minimize StdDev
  name="StdDev")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Leverage Constraint Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_leverage,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
# Perform optimization of weights
maxSRN_DEOpt <- optimize.portfolio(
  R=etf_env$re_turns[, portf_names],  # Specify returns
  portfolio=portf_maxSRN,  # Specify portfolio
  optimize_method="DEoptim", # Use DEoptim
  maxSR=TRUE,  # Maximize Sharpe
  trace=TRUE, traceDE=0)
# Plot optimization
chart.RiskReward(maxSRN_DEOpt,
  risk.col="StdDev",
  return.col="mean",
  xlim=c(
    maxSR_DEOpt$objective_measures$StdDev[[1]]-0.001,
    0.016))
  points(x=maxSR_DEOpt$objective_measures$StdDev[[1]],
         y=maxSR_DEOpt$objective_measures$mean[1],
         col="green", lwd=3)
  text(x=maxSR_DEOpt$objective_measures$StdDev[[1]],
         y=maxSR_DEOpt$objective_measures$mean[1],
       labels="maxSR", col="green",
       lwd=2, pos=4)
# Plot risk/ret points in portfolio scatterplot
risk_ret_points()
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_leverage-1}
    \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE,tidy=TRUE>>=
library(PortfolioAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
maxSRN_DEOpt$weights
maxSRN_DEOpt$objective_measures$mean[1]
maxSRN_DEOpt$objective_measures$StdDev[[1]]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized Leverage Constraint Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_portf_SRN_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
maxSRN_DEOpt_xts <-
  plot_portf(portfolio=maxSRN_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_portf_SRN_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe Portfolios \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns.
      <<optim_SR_SRN_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, maxSRN_DEOpt_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, maxSRN_DEOpt$weights)
c(maxSR_DEOpt$objective_measures$mean,
maxSRN_DEOpt$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
maxSRN_DEOpt$objective_measures$StdDev[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_SRN_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{STARR Portfolio Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The objective constraint applies to risk or return.
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add constraints
portf_maxSTARR <- add.constraint(
  portfolio=portf_init,  # Initial portfolio
  type="weight_sum",  # Constraint sum weights
  min_sum=0.9, max_sum=1.1)
# Add constraints
portf_maxSTARR <- add.constraint(
  portfolio=portf_maxSTARR,
  type="long_only")  # box constraint min=0, max=1
# Add objectives
portf_maxSTARR <- add.objective(
  portfolio=portf_maxSTARR,
  type="return",  # Maximize mean return
  name="mean")
# Add objectives
portf_maxSTARR <- add.objective(
  portfolio=portf_maxSTARR,
  type="risk",  # Minimize Expected Shortfall
  name="ES")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{STARR Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_STARR,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
# Perform optimization of weights
maxSTARR_DEOpt <- optimize.portfolio(
  R=etf_env$re_turns[, portf_names],  # Specify returns
  portfolio=portf_maxSTARR,  # Specify portfolio
  optimize_method="DEoptim", # Use DEoptim
  maxSTARR=TRUE,  # Maximize STARR
  trace=TRUE, traceDE=0)

# Plot optimization
chart.RiskReward(maxSTARR_DEOpt,
  risk.col="ES",
  return.col="mean")
# Plot risk/ret points in portfolio scatterplot
risk_ret_points(risk="ETL")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
maxSTARR_DEOpt$weights
maxSTARR_DEOpt$objective_measures$mean[1]
maxSTARR_DEOpt$objective_measures$ES[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_STARR-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized STARR Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_STARR_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
maxSTARR_DEOpt_xts <-
  plot_portf(portfolio=maxSTARR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_STARR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe STARR \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns.
      <<optim_SR_STARR_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, maxSTARR_DEOpt_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, maxSTARR_DEOpt$weights)
c(maxSR_DEOpt$objective_measures$mean,
maxSTARR_DEOpt$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
maxSTARR_DEOpt$objective_measures$ES[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_STARR_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Efficient Frontier and Capital Market Line}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Efficient Frontier} is the set of \emph{efficient portfolios}, that have the lowest risk (standard deviation) for the given level of return.
      \vskip1ex
      The Capital Market Line (CML) is the line drawn from the risk-free asset to the tangent point on the Efficient Frontier.
      \vskip1ex
      The tangent point on the \emph{Efficient Frontier} is the \emph{Market Portfolio}.
      <<optim_eff_front,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PortfolioAnalytics)
# Plot the efficient frontier
chart.EfficientFrontier(maxSR_DEOpt,
                match.col="StdDev",
                n.portfolios=15, type="l")
points(x=maxSRN_DEOpt$objective_measures$StdDev[[1]],
         y=maxSRN_DEOpt$objective_measures$mean[1],
         col="green", lwd=3)
text(x=maxSRN_DEOpt$objective_measures$StdDev[[1]],
         y=maxSRN_DEOpt$objective_measures$mean[1],
       labels="maxSRN", col="green",
       lwd=2, pos=4)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_eff_front-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{minES Portfolio Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The objective constraint applies to risk or return.
      \vspace{-1em}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(PortfolioAnalytics)
# Add constraints
portf_minES <- add.constraint(
  portfolio=portf_init,  # Initial portfolio
  type="weight_sum",  # Constraint sum weights
  min_sum=0.9, max_sum=1.1)
# Add constraints
portf_minES <- add.constraint(
  portfolio=portf_minES,
  type="long_only")  # box constraint min=0, max=1
# Add objectives
portf_minES <- add.objective(
  portfolio=portf_minES,
  type="risk",  # Minimize ES
  name="ES")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{minES Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_minES,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
# Perform optimization of weights
minES_ROI <- optimize.portfolio(
  R=etf_env$re_turns[, portf_names],  # Specify returns
  portfolio=portf_minES,  # Specify portfolio
  optimize_method="ROI", # Use ROI
  trace=TRUE, traceDE=0)

# Plot optimization
chart.RiskReward(maxSTARR_DEOpt,
  risk.col="ES",
  return.col="mean")
  points(x=minES_ROI$objective_measures$ES[[1]],
         y=mean(minES_ROI_xts),
         col="green", lwd=3)
  text(x=minES_ROI$objective_measures$ES[[1]],
         y=mean(minES_ROI_xts),
       labels="minES", col="green",
       lwd=2, pos=4)
# Plot risk/ret points in portfolio scatterplot
risk_ret_points(risk="ETL")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE,tidy=TRUE>>=
options(width=50)
library(PortfolioAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
minES_ROI$weights
minES_ROI$objective_measures$ES[[1]]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_minES-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimized minES Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_minES_vis,echo=(-(1:1)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
library(PortfolioAnalytics)
minES_ROI_xts <-
  plot_portf(portfolio=minES_ROI)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_minES_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sharpe minES \texttt{CumReturns} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart.CumReturns()} plots the cumulative returns of a time series of returns.
      <<optim_SR_minES_vis,echo=(-(1:2)),eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
chart.CumReturns(
  cbind(maxSR_DEOpt_xts, minES_ROI_xts),
  lwd=2, ylab="",
  legend.loc="topleft", main="")
      @
    \vspace{-2em}
      <<echo=(-(1:3)),eval=FALSE>>=
options(width=50)
library(PerformanceAnalytics)
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
rbind(maxSR_DEOpt$weights, minES_ROI$weights)
c(maxSR_DEOpt$objective_measures$mean,
minES_ROI$objective_measures$mean)
c(maxSR_DEOpt$objective_measures$StdDev[[1]],
minES_ROI$objective_measures$ES[[1]])
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_minES_vis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_1h,echo=(-(1:3)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
options(width=50)
# Perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=etf_env$re_turns["/2011", portf_names],
  portfolio=portf_maxSR,  # Specify portfolio
  optimize_method="DEoptim", # Use DEoptim
  maxSR=TRUE,  # Maximize Sharpe
  trace=TRUE, traceDE=0)
weights_1h <- maxSR_DEOpt$weights

# Plot optimization
maxSR_DEOpt_xts <-
  plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_1h-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolios (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<optim_SR_2h,echo=(-(1:3)),eval=FALSE,fig.width=7,fig.height=8,fig.show='hide'>>=
load(file="C:/Develop/lecture_slides/data/portf_optim.RData")
library(PortfolioAnalytics)
options(width=50)
# Perform optimization of weights
maxSR_DEOpt <- optimize.portfolio(
  R=etf_env$re_turns["2011/", portf_names],
  portfolio=portf_maxSR,  # Specify portfolio
  optimize_method="DEoptim", # Use DEoptim
  maxSR=TRUE,  # Maximize Sharpe
  trace=TRUE, traceDE=0)
weights_2h <- maxSR_DEOpt$weights

# Plot optimization
maxSR_DEOpt_xts <-
  plot_portf(portfolio=maxSR_DEOpt)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_SR_2h-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
options(width=50)
weights_1h
weights_2h
weights_1h - weights_2h
      @
    \vspace{-2em}
      <<optim_weights,echo=(-(1:2)),eval=FALSE,fig.height=5,fig.show='hide'>>=
par(oma=c(1,0,1,0), mgp=c(2,1,0), mar=c(2,1,2,1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
barplot(weights_1h,
        names.arg=names(weights_1h),
        las=3, ylab="", xlab="",
        main="Portfolio Weights First Half")
barplot(weights_2h,
        names.arg=names(weights_2h),
        las=3, ylab="", xlab="",
        main="Portfolio Weights Second Half")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_weights-1}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Credit Portfolio Models}


%%%%%%%%%%%%%%%
\subsection{Simulating Single-period Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a portfolio of credit assets (bonds or loans) over a single period of time.
      \vskip1ex
      At the end of the period, some of the assets default, while the rest don't.
      \vskip1ex
      The default probabilities are equal to $p_i$.
      \vskip1ex
      Individual defaults can be simulated by comparing the probabilities $p_i$ with the uniform random numbers $u_i$.
      \vskip1ex
      Default occurs if $u_i$ is less than the default probability $p_i$:
      \begin{displaymath}
        u_i < p_i
      \end{displaymath}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop.
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{for()} loops.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate random default probabilities
set.seed(1121)
n_assets <- 100
def_probs <- runif(n_assets, max=0.2)
mean(def_probs)
# Simulate number of defaults
uni_form <- runif(n_assets)
sum(uni_form < def_probs)
# Simulate average number of defaults using for() loop (inefficient way)
n_simu <- 1000
set.seed(1121)
de_faults <- numeric(n_simu)
for (i in 1:n_simu) {  # Perform loop
  uni_form <- runif(n_assets)
  de_faults[i] <- sum(uni_form < def_probs)
}  # end for
# Calculate average number of defaults
mean(de_faults)
# Simulate using vectorized functions  (efficient way)
set.seed(1121)
uni_form <- matrix(runif(n_simu*n_assets), ncol=n_simu)
de_faults <- colSums(uni_form < def_probs)
mean(de_faults)
# Plot the distribution of defaults
x11(width=6, height=5)
plot(density(de_faults), main="Distribution of Defaults", 
     xlab="number of defaults", ylab="frequqncy")
abline(v=mean(de_faults), lwd=3, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Values and Default Thresholds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Defaults can also be simulated using normally distributed variables $a_i$ called \emph{asset values}, instead of the uniformly distributed variables $u_i$.
      \vskip1ex
      The asset values $a_i$ are the \emph{quantiles} corresponding to the uniform variables $u_i$: $a_i = \Phi^{-1}(u_i)$ (where $\Phi()$ is the cumulative standard normal distribution).
      \vskip1ex
      Similarly, the default probabilities $p_i$ are also transformed into \emph{default thresholds} $t_i$, which are the \emph{quantiles}: $t_i = \Phi^{-1}(p_i)$.
      \vskip1ex
      Before, default occurred if $u_i$ was less than the default probability $p_i$: $u_i < p_i$.
      \vskip1ex
      Now, default occurs if the \emph{asset value} $a_i$ is less than the \emph{default threshold} $t_i$: $a_i < t_i$.
      \vskip1ex
      The asset values $a_i$ are mathematical variables which can be negative, so they are not actual company asset values.
      <<echo=TRUE,eval=FALSE>>=
# Calculate default thresholds and asset values
def_thresh <- qnorm(def_probs)
asset_s <- qnorm(uni_form)
# Simulate defaults
de_faults <- colSums(asset_s < def_thresh)
mean(de_faults)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_def_threshold.png}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot Standard Normal distribution
x11(width=6, height=5)
x_lim <- 4; def_thresh <- qnorm(0.025)
curve(expr=dnorm(x), type="l", xlim=c(-x_lim, x_lim),
      xlab="asset value", ylab="", lwd=3,
      col="blue", main="Distribution of Asset Values")
abline(v=def_thresh, col="red", lwd=3)
text(x=def_thresh-0.1, y=0.15, labels="default threshold",
       lwd=2, srt=90, pos=3)
# Plot polygon area
x_var <- seq(-x_lim, x_lim, length=100)
y_var <- dnorm(x_var)
are_a <- ((x_var >= (-x_lim)) & (x_var <= def_thresh))
polygon(c(x_lim, x_var[are_a], def_thresh),
        c(-1, y_var[are_a], -1), col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model of Correlated Asset Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      So far, the asset values are independent from each other, but in reality default events are correlated.
      \vskip1ex
      The \emph{Vasicek} model introduces correlation between the asset values $a_i$.
      \vskip1ex
      Under the \emph{Vasicek} single factor model, the asset value $a_i$ is equal to the sum of a \emph{systematic} factor $s$, plus an \emph{idiosyncratic} factor $z_i$:
      \begin{displaymath}
        a_i = \sqrt{\rho} \, s + \sqrt{1-\rho} \, z_i
      \end{displaymath}
      Where $\rho$ is the correlation between asset values.
      \vskip1ex
      The variables $s$, $z_i$, and $a_i$ all follow the Standard Normal distribution $\phi(0, 1)$.
      \vskip1ex
      The \emph{Vasicek} model resembles the \emph{CAPM} model, with the asset value equal to the sum of a \emph{systematic} factor plus an \emph{idiosyncratic} factor.
      \vskip1ex
      The Bank for International Settlements (BIS) uses the \emph{Vasicek} model as part of its regulatory capital requirements for bank credit risk:\\
      \tiny{
\hskip1em\url{http://bis2information.org/content/Vasicek_model}\\
\hskip1em\url{https://www.bis.org/bcbs/basel3.htm}\\
\hskip1em\url{https://www.bis.org/bcbs/irbriskweight.pdf}
      }
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define correlation parameters
rh_o <- 0.2
rho_sqrt <- sqrt(rh_o) ; rho_sqrtm <- sqrt(1-rh_o)
n_assets <- 5 ; n_simu <- 10000
# Calculate vector of systematic and idiosyncratic factors
system_atic <- rnorm(n_simu)
idio_syncratic <- rnorm(n_simu*n_assets)
# Simulate asset values using vectorized functions (efficient way)
asset_s <- rho_sqrt*system_atic + rho_sqrtm*idio_syncratic
dim(asset_s) <- c(n_simu, n_assets)
# Asset values are standard normally distributed
apply(asset_s, MARGIN=2, function(x) c(mean=mean(x), sd=sd(x)))
# Calculate correlations between asset values
cor(asset_s)
# Simulate asset values using for() loop (inefficient way)
# Allocate matrix of assets
asset_s <- matrix(nr=n_simu, nc=n_assets)
# Simulate asset values using for() loop
for (i in 1:n_simu) {  # Perform loop
  asset_s[i, ] <- rho_sqrt*system_atic[i] + rho_sqrtm*rnorm(n_assets)
}  # end for
cor(asset_s)
# benchmark the speed of the two methods
library(microbenchmark)
summary(microbenchmark(
  for_loop={for (i in 1:n_simu) {
    rho_sqrt*system_atic[i] + rho_sqrtm*rnorm(n_assets)}},
  vector_ized={rho_sqrt*system_atic + rho_sqrtm*rnorm(n_simu*n_assets)},
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model of Correlated Defaults}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Vasicek} model, default occurs if the \emph{asset value} $a_i$ is less than the \emph{default threshold} $t_i$:
      \begin{align*}
        a_i = \sqrt{\rho} s + \sqrt{1-\rho} z_i \\
        a_i < t_i
      \end{align*}
      The \emph{systematic} factor $s$ may be considered to represent the state of the macro economy, with positive values representing an economic expansion, and negative values representing an economic recession.
      \vskip1ex
      When the value of the \emph{systematic} factor $s$ is positive, then the asset values will all tend to be bigger as well, which will produce fewer defaults.
      \vskip1ex
      But when the \emph{systematic} factor is negative, then the asset values will tend to be smaller, which will produce more defaults.
      \vskip1ex
      This way the \emph{Vasicek} model introduces a correlation among defaults.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate random default probabilities
n_assets <- 5
def_probs <- runif(n_assets, max=0.2)
mean(def_probs)
# Calculate default thresholds
def_thresh <- qnorm(def_probs)
# Calculate number of defaults using vectorized functions (efficient way)
# Calculate vector of number of defaults
rowMeans(t(asset_s) < def_thresh)
def_probs
# Calculate number of defaults using for() loop (inefficient way)
# allocate matrix of de_faults
de_faults <- matrix(nr=n_simu, nc=n_assets)
# Simulate asset values using for() loop
for (i in 1:n_simu) {  # Perform loop
  de_faults[i, ] <- (asset_s[i, ] < def_thresh)
}  # end for
colSums(de_faults) / n_simu
def_probs
# Calculate correlations between defaults
cor(de_faults)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Correlation and Default Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Default correlation is defined as the correlation between the \texttt{Boolean} vectors of default events.
      \vskip1ex
      The \emph{Vasicek} model introduces correlation among default events, through the correlation of \emph{asset values}.
      \vskip1ex
      If \emph{asset values} have a positive correlation, then the defaults among credits are clustered together, and if one credit defaults then the other credits are more likely to default as well.
      \vskip1ex
      Empirical studies have found that the asset correlation $\rho$ can vary between \texttt{5\%} to \texttt{20\%}, depending on the default risk.
      \vskip1ex
      Credits with higher default risk tend to also have higher asset correlation, since they are more  sensitive to the economic conditions.
      \vskip1ex
      Default correlations are usually much lower than the corresponding asset correlations.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define default probabilities
n_assets <- 2
def_prob <- 0.2
def_thresh <- qnorm(def_prob)
# Define correlation parameters
rh_o <- 0.2
rho_sqrt <- sqrt(rh_o) ; rho_sqrtm <- sqrt(1-rh_o)
# Calculate vector of systematic factors
n_simu <- 1000
system_atic <- rnorm(n_simu)
# Simulate asset values using vectorized functions
asset_s <- rho_sqrt*system_atic + rho_sqrtm*rnorm(n_simu*n_assets)
dim(asset_s) <- c(n_simu, n_assets)
# Calculate number of defaults using vectorized functions
de_faults <- t(t(asset_s) < def_thresh)
# Calculate correlations between defaults
cor(de_faults)
# Calculate average number of defaults and compare to def_prob
colSums(de_faults) / n_simu
def_prob
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cumulative Defaults Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If all the default probabilities are the same $p_i=p$, then the default threshold is equal to $t=\Phi^{-1}(p)$, and the conditional default probability $p(s)$, given the systematic factor $s$, is equal to:
      \begin{displaymath}
        p(s) = \Phi(\frac{t - \sqrt{\rho} s}{\sqrt{1-\rho}})
      \end{displaymath}
      The cumulative probability $P(x)$ for the percentage \texttt{x} of portfolio defaults (the portfolio cumulative default distribution) is equal to:
      \begin{displaymath}
        P(x) = \Phi(\frac{{\sqrt{1-\rho}} \, \Phi^{-1}(x) - t}{\sqrt{\rho}})
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define cumulative default probability function
def_cumdistr <- function(x, def_thresh=(-2), rh_o=0.2)
  pnorm((sqrt(1-rh_o)*qnorm(x) - def_thresh)/sqrt(rh_o))
def_cumdistr(x=0.2, def_thresh=qnorm(def_prob), rh_o=rh_o)
# Plot cumulative default probability function
def_prob <- 0.4; def_thresh <- qnorm(def_prob)
curve(expr=def_cumdistr(x, def_thresh=def_thresh, rh_o=0.05),
      xlim=c(0, 0.999), lwd=3,
      xlab="percent default", ylab="probability",
      col="green", main="Cumulative Default Probabilities")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_cum_def.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot default distribution with higher correlation
curve(expr=def_cumdistr(x, def_thresh=def_thresh, rh_o=0.2),
      xlim=c(0, 0.999), add=TRUE, lwd=3,
      col="blue", main="")
# Add legend
legend(x="topleft",
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       bty="n", lwd=6, lty=1, col=c("blue", "green"))
# Add unconditional default probability
abline(v=def_prob, col="red", lwd=3)
text(x=def_prob, y=0.0,
       labels="default probability",
       lwd=2, srt=90, pos=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Defaults Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability density $f(x)$ of portfolio defaults is equal to the derivative of the cumulative default distribution:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{\sqrt{\rho}} \exp(-\frac{1}{2 \rho} ({\sqrt{1-\rho}} \, \Phi^{-1}(x) - t)^2 + \\ \frac{1}{2} {\Phi^{-1}(x)^2)}
      \end{multline*}
      <<echo=TRUE,eval=FALSE>>=
# Define default probability density function
def_distr <- function(x, def_thresh=(-2), rh_o=0.2)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x) -
  def_thresh)^2/(2*rh_o) + qnorm(x)^2/2)
# Define parameters
rh_o <- 0.2 ; rho_sqrt <- sqrt(rh_o) ; rho_sqrtm <- sqrt(1-rh_o)
def_prob <- 0.3; def_thresh <- qnorm(def_prob)
def_distr(0.03, def_thresh=def_thresh, rh_o=rh_o)
# Plot probability distribution of defaults
curve(expr=def_distr(x, def_thresh=def_thresh, rh_o=0.1),
      xlim=c(0, 1.0), lwd=3,
      xlab="percentage of defaults", ylab="density",
      col="green", main="Distribution of Defaults")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_distr_def.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot default distribution with higher correlation
curve(expr=def_distr(x, def_thresh=def_thresh, rh_o=0.3),
      xlab="default percentage", ylab="",
      add=TRUE, lwd=3, col="blue", main="")
# Add legend
legend(x="topright",
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.05, cex=0.8, bg="white",
       bty="n", lwd=6, lty=1, col=c("blue", "green"))
# Add unconditional default probability
abline(v=def_prob, col="red", lwd=3)
text(x=def_prob, y=2,
       labels="default probability",
       lwd=2, srt=90, pos=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Defaults Under Extreme Correlations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the correlation $\rho$ is close to \emph{zero}, then the asset values $a_i$ are independent from each other, and defaults are also independent, so that the percentage of portfolio defaults is very close to the default probability $p$.
      \vskip1ex
      In that case, the probability density of portfolio defaults is very narrow and is centered on the default probability $p$.
      \vskip1ex
      If the correlation $\rho$ is close to \emph{one}, then the asset values $a_i$ are almost the same, and defaults occur at the same time, so that the percentage of portfolio defaults is either \emph{zero} or \emph{one}.
      \vskip1ex
      In that case, the probability density of portfolio defaults becomes \emph{bimodal}, with two peaks around  \emph{zero} and \emph{one}.
      <<echo=TRUE,eval=FALSE>>=
# Plot default distribution with low correlation
curve(expr=def_distr(x, def_thresh=def_thresh, rh_o=0.01),
      xlab="default percentage", ylab="", lwd=2,
      col="green", main="Distribution of Defaults")
# Plot default distribution with high correlation
curve(expr=def_distr(x, def_thresh=def_thresh, rh_o=0.99),
      xlab="percentage of defaults", ylab="density",
      add=TRUE, lwd=2, n=10001, col="blue", main="")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_high_corr.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Add legend
legend(x="top",
       legend=c("high correlation", "low correlation"),
       title=NULL, inset=0.1, cex=0.8, bg="white",
       bty="n", lwd=6, lty=1, col=c("blue", "green"))
# Add unconditional default probability
abline(v=0.1, col="red", lwd=2)
text(x=0.1, y=10, lwd=2, pos=4,
       labels="default probability")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Numerical Integration of Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{integrate()} performs numerical integration of a function of a single variable, i.e. it calculates a definite integral over an integration interval.
      \vskip1ex
      Additional parameters can be passed to the integrated function through the dots \texttt{"..."} argument of the function \texttt{integrate()}.
      \vskip1ex
      The function \texttt{integrate()} accepts the integration limits \texttt{-Inf} and \texttt{Inf} equal to minus and plus infinity.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Get help for integrate()
?integrate
# Calculate slowly converging integral
func_tion <- function(x) {1/((x+1)*sqrt(x))}
integrate(func_tion, lower=0, upper=10)
integrate(func_tion, lower=0, upper=Inf)
# Integrate function with parameter lamb_da
func_tion <- function(x, lamb_da=1) {
  exp(-x*lamb_da)
}  # end func_tion
integrate(func_tion, lower=0, upper=Inf)
integrate(func_tion, lower=0, upper=Inf, lamb_da=2)
# Cumulative probability over normal distribution
pnorm(-2)
integrate(dnorm, low=2, up=Inf)
str(dnorm)
pnorm(-1)
integrate(dnorm, low=2, up=Inf, mean=1)
# Expected value over normal distribution
integrate(function(x) x*dnorm(x), low=2, up=Inf)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Loss Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The expected loss (\emph{EL}) of a credit portfolio is equal to the sum of default probabilities ($p_i$) multiplied by the loss given default (\emph{LGD}, also known as the \emph{loss severity} - equal to $1$ minus the \emph{recovery rate}):
      \begin{displaymath}
        EL = \sum_{i=1}^{n} p_i LGD_i
      \end{displaymath}
      If the \emph{LGD} amounts are all the same, then the \emph{portfolio loss distribution} can be obtained from the \emph{default distribution}, adjusted for the \emph{LGD}:
      \begin{multline*}
        \hspace{-1.7em}f(x) = \frac{\sqrt{1-\rho}}{LGD \sqrt{\rho}} \exp(-\frac{1}{2 \rho} ({\sqrt{1-\rho}} \Phi^{-1}(\frac{x}{LGD}) - t)^2 + \\ \frac{1}{2} {\Phi^{-1}(\frac{x}{LGD}))^2}
      \end{multline*}
      <<echo=TRUE,eval=FALSE>>=
# Vasicek model parameters
rh_o <- 0.1; l_gd <- 0.4
def_prob <- 0.05; def_thresh <- qnorm(def_prob)
# Define Vasicek loss distribution function
loss_distr <- function(x, def_thresh=(-2), rh_o=0.2, l_gd=0.4)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*qnorm(x/l_gd) - def_thresh)^2/(2*rh_o) + qnorm(x/l_gd)^2/2)/l_gd
integrate(loss_distr, low=0, up=l_gd, def_thresh=(-2), rh_o=rh_o, l_gd=l_gd)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_loss_distr.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot probability distribution of losses
x11(width=6, height=5)
curve(expr=loss_distr(x, def_thresh=def_thresh, rh_o=rh_o),
      type="l", xlim=c(0, 0.06),
      xlab="loss percentage", ylab="density", lwd=3,
      col="blue", main="Portfolio Loss Density")
# Add line for expected loss
abline(v=l_gd*def_prob, col="red", lwd=3)
text(x=l_gd*def_prob-0.001, y=35, labels="expected loss", lwd=3, pos=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Collateralized Debt Obligations (\protect\emph{CDOs})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Collateralized Debt Obligations (cash \emph{CDOs}) are securities (bonds) collateralized by other debt assets.
      \vskip1ex
      The \emph{CDO} assets can be debt instruments like bonds, loans, and mortgages.
      \vskip1ex
      The \emph{CDO} liabilities are \emph{CDO} tranches, which receive cashflows from the \emph{CDO} assets, and are exposed to their defaults.
      \vskip1ex
      \emph{CDO} tranches have an attachment point (subordination, i.e. the percentage of asset default losses at which the tranche starts absorbing those losses), and a detachment point when the tranche is wiped out (suffers \texttt{100\%} losses).
      \vskip1ex
      The \emph{equity tranche} is the most junior tranche, and is the first to absorb default losses.
      \vskip1ex
      The \emph{mezzanine tranches} are senior to the \emph{equity tranche} and absorb losses ony after the \emph{equity tranche} is wiped out.
      \vskip1ex
      The \emph{senior tranche} is the most senior tranche, and is the last to absorb losses.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/CDO2.jpg}
      \vskip1ex
      \vskip1ex
      \includegraphics[width=0.45\paperwidth]{figure/CDO.jpg}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CDO} Tranche Losses}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Single-tranche (synthetic) \emph{CDOs} are credit default swaps which reference credit portfolios.
      \vskip1ex
      The expected loss \emph{EL} on a \emph{CDO} tranche is:
      \begin{displaymath}
        EL = \frac{1}{d - a} \int_{a}^{d} {(x-a) \, f(x) \, \mathrm{d}x} + \int_{d}^{LGD} {f(x) \, \mathrm{d}x}
      \end{displaymath}
      Where $f(x)$ is the density of portfolio losses, and \emph{a} and \emph{d} are the tranche attachment (subordination) and detachment points.
      \vskip1ex
      The difference $(d-a)$ is the tranche \emph{thickness}, so that $EL$ is the expected loss as a percentage of the tranche notional.
      \vskip1ex
      A single-tranche \emph{CDO} can be thought of as a short option spread on the asset defaults, struck at the attachment and detachment points.
      <<echo=TRUE,eval=FALSE>>=
# Define cumulative default probability function
cum_loss <- function(x, def_thresh=(-2), rh_o=0.2, l_gd=0.4)
  pnorm((sqrt(1-rh_o)*qnorm(x/l_gd) - def_thresh)/sqrt(rh_o))
# Define Vasicek loss distribution function
# (vectorized version with error handling for x)
loss_distr <- function(x, def_thresh=-2, rh_o=0.1, l_gd=0.4) {
  q_norm <- ifelse(x/l_gd < 0.999, qnorm(x/l_gd), 3.1)
  sqrt((1-rh_o)/rh_o)*exp(-(sqrt(1-rh_o)*q_norm - def_thresh)^2/(2*rh_o) + q_norm^2/2)/l_gd
}  # end loss_distr
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/cdo_tranche.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
def_prob <- 0.2; def_thresh <- qnorm(def_prob)
rh_o <- 0.1; l_gd <- 0.4
at_tach <- 0.15; de_tach <- 0.2
# Expected tranche loss is sum of two terms
tranche_loss <-
  # Loss between at_tach and de_tach
  integrate(function(x, at_tach) (x-at_tach)*loss_distr(x,
      def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd),
      low=at_tach, up=de_tach, at_tach=at_tach)$value / (de_tach-at_tach) +
  # Loss in excess of de_tach
        (1-cum_loss(x=de_tach, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd))
# Plot probability distribution of losses
curve(expr=loss_distr(x, def_thresh=def_thresh, rh_o=rh_o),
      type="l", xlim=c(0, 3*l_gd*def_prob),
      xlab="loss percentage", ylab="density", lwd=3,
      col="orange", main="CDO Tranche Losses")
# Add line for expected loss
abline(v=l_gd*def_prob, col="red", lwd=3)
text(x=l_gd*def_prob-0.001, y=4, labels="expected loss",
       lwd=2, srt=90, pos=3)
# Add lines for attach and detach
abline(v=at_tach, col="blue", lwd=3)
text(x=at_tach-0.001, y=4, labels="attach",
       lwd=2, srt=90, pos=3)
abline(v=de_tach, col="green", lwd=3)
text(x=de_tach-0.001, y=4, labels="detach",
       lwd=2, srt=90, pos=3)
# Add shading for CDO tranche
var_s <- seq(at_tach, de_tach, length=100)
densi_ty <- sapply(var_s, loss_distr,
  def_thresh=def_thresh, rh_o=rh_o)
# Draw shaded polygon
polygon(c(at_tach, var_s, de_tach), density=20,
  c(-1, densi_ty, -1), col="red", border=NA)
text(x=0.5*(at_tach+de_tach), y=0, labels="CDO tranche", cex=0.9, lwd=2, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Value at Risk (\emph{VaR}) measures extreme portfolio loss (but not the worst possible loss), defined as the \emph{quantile} of the loss distribution, corresponding to a given confidence level $\alpha$.
      \vskip1ex
      A loss exceeding the \emph{EL} is called the Unexpected Loss (\emph{UL}), and can be calculated from the \emph{portfolio loss distribution}.
      <<echo=TRUE,eval=FALSE>>=
# Add lines for unexpected loss
abline(v=0.04, col="blue", lwd=3)
arrows(x0=0.02, y0=35, x1=0.04, y1=35, code=3, lwd=3, cex=0.5)
text(x=0.03, y=36, labels="unexpected loss", lwd=2, pos=3)
# Add lines for VaR
abline(v=0.055, col="red", lwd=3)
arrows(x0=0.0, y0=25, x1=0.055, y1=25, code=3, lwd=3, cex=0.5)
text(x=0.03, y=26, labels="VaR", lwd=2, pos=3)
text(x=0.055-0.001, y=10, labels="VaR", lwd=2, srt=90, pos=3)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_distr_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} (\emph{CVaR}) is equal to the average of the \emph{VaR} for confidence levels less than a given confidence level $\alpha$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^{\alpha} {\mathrm{VaR}(p) \, \mathrm{d}p} = \frac{1}{\alpha} \int_{\mathrm{VaR}}^{LGD} {x \, f(x) \, \mathrm{d}x}
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the Expected Shortfall (\emph{ES}), or Expected Tail Loss (\emph{ETL}).
      <<echo=TRUE,eval=FALSE>>=
va_r <- 0.04; min_var <- 4*l_gd*def_prob
# Calculate CVaR
c_var <- integrate(function(x, ...) x*loss_distr(x, ...),
  low=va_r, up=l_gd, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)$value
c_var <- c_var/integrate(loss_distr, low=va_r, up=l_gd, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)$value
# Plot probability distribution of losses
curve(expr=loss_distr(x, def_thresh=def_thresh, rh_o=rh_o),
      type="l", xlim=c(0, 0.06),
      xlab="loss percentage", ylab="density", lwd=3,
      col="orange", main="Conditional Value at Risk")
# Add line for expected loss
abline(v=l_gd*def_prob, col="red", lwd=3)
text(x=l_gd*def_prob-0.001, y=10, labels="expected loss", lwd=2, srt=90, pos=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_distr_cvar.png}
      <<echo=TRUE,eval=FALSE>>=
# Add lines for VaR
abline(v=va_r, col="red", lwd=3)
text(x=va_r-0.001, y=10, labels="VaR",
       lwd=2, srt=90, pos=3)
# Add shading for CVaR
var_s <- seq(va_r, min_var, length=100)
densi_ty <- sapply(var_s, loss_distr,
  def_thresh=def_thresh, rh_o=rh_o)
# Draw shaded polygon
polygon(c(va_r, var_s, min_var), density=20,
  c(-1, densi_ty, -1), col="red", border=NA)
text(x=va_r+0.005, y=0, labels="CVaR", lwd=2, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Value at Risk (\emph{VaR}) measures extreme portfolio loss (but not the worst possible loss), defined as the \emph{quantile} of the loss distribution, corresponding to a given confidence level $\alpha$.
      \vskip1ex
      The \emph{quantile} of the loss distribution (the \emph{VaR}), for a given a confidence level $\alpha$, is given by the inverse of the cumulative loss distribution:
      \begin{displaymath}
        VaR(\alpha) = LGD \cdot \Phi(\frac{{\sqrt{\rho}} \Phi^{-1}(\alpha) + t}{\sqrt{1-\rho}})
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# VaR (quantile of the loss distribution)
var_func <- function(x, def_thresh=qnorm(0.1), rh_o=0.1, l_gd=0.4)
  l_gd*pnorm((sqrt(rh_o)*qnorm(x) + def_thresh)/sqrt(1-rh_o))
var_func(x=0.99, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)
# Plot VaR
curve(expr=var_func(x, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd),
      type="l", xlim=c(0, 0.999), xlab="confidence level", ylab="VaR", lwd=3,
      col="orange", main="VaR versus Confidence Level")
# Add line for expected loss
abline(h=l_gd*def_prob, col="red", lwd=3)
text(x=0.2, y=l_gd*def_prob, labels="expected loss", lwd=2, pos=3)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk and Confidence Levels}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confidence levels of \emph{VaR} values can also be calculated by integrating over the tail of the loss density function.
      <<echo=TRUE,eval=FALSE>>=
# Integrate loss_distr() over full range
integrate(loss_distr, low=0.0, up=l_gd,
          def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)
# Calculate expected losses using loss_distr()
integrate(function(x, ...) x*loss_distr(x, ...),
          low=0.0, up=l_gd,
          def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)
# Calculate confidence levels corresponding to VaR values
var_s <- seq(0.07, 0.12, 0.001)
level_s <- sapply(var_s, function(va_r, ...) {
  integrate(loss_distr, low=va_r, up=l_gd, ...)
}, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)  # end sapply
level_s <- cbind(as.numeric(t(level_s)[, 1]), var_s)
colnames(level_s) <- c("level_s", "VaRs")
# Calculate 95% confidence level VaR value
level_s[match(TRUE, level_s[, "level_s"] < 0.05), "VaRs"]
plot(x=1-level_s[, "level_s"],
     y=level_s[, "VaRs"], lwd=2,
     xlab="confidence level", ylab="VaRs",
     t="l", main="VaR Values and Confidence Levels")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_var_conf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} values can be calculated by integrating over the tail of the loss density function.
      <<echo=TRUE,eval=FALSE>>=
# Calculate CVaR values
cvar_s <- sapply(var_s, function(va_r, ...) {
  integrate(function(x, ...) x*loss_distr(x, ...),
            low=va_r, up=l_gd, ...)
}, def_thresh=def_thresh, rh_o=rh_o, l_gd=l_gd)  # end sapply
level_s <- cbind(level_s, as.numeric(t(cvar_s)[, 1]))
colnames(level_s)[3] <- "CVaRs"
# Divide CVaR by confidence level
level_s[, "CVaRs"] <- level_s[, "CVaRs"]/level_s[, "level_s"]
# Calculate 95% confidence level CVaR value
level_s[match(TRUE,
  level_s[, "level_s"] < 0.05), "CVaRs"]
# Plot CVaRs
plot(x=1-level_s[, "level_s"],
     y=level_s[, "CVaRs"],
     t="l", col="red", lwd=2,
     ylim=range(level_s[, c("VaRs", "CVaRs")]),
     xlab="confidence level", ylab="CVaRs",
     main="CVaR Values and Confidence Levels")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_cvar_levels.png}
      <<echo=TRUE,eval=FALSE>>=
# Add VaRs
lines(x=1-level_s[, "level_s"], y=level_s[, "VaRs"], lwd=2)
# Add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"),
       title="default probability = 5%
correlation = 10%
loss given default = 40%",
       inset=0.1, cex=0.8, bg="white", bty="n",
       lwd=6, lty=1, col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Portfolio Losses Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the default probabilities $p_i$ are not all the same, then there's no formula for the \emph{portfolio loss distribution} under the Vasicek Model.
      \vskip1ex
      In that case the portfolio losses and \emph{VaR} must be simulated.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
n_assets <- 300; n_simu <- 1000; l_gd <- 0.4
# Define correlation parameters
rh_o <- 0.2; rho_sqrt <- sqrt(rh_o); rho_sqrtm <- sqrt(1-rh_o)
# Calculate default probabilities and thresholds
set.seed(1121)
def_probs <- runif(n_assets, max=0.2)
def_thresh <- qnorm(def_probs)
# Simulate losses under Vasicek model
system_atic <- rnorm(n_simu)
asset_s <- matrix(rnorm(n_simu*n_assets), ncol=n_simu)
asset_s <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_s))
loss_es <- l_gd*colSums(asset_s < def_thresh)/n_assets
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VaR} and \protect\emph{CVaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data, and returns a list with a vector of loss values and a vector of corresponding densities.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate VaR from confidence level
conf_level <- 0.95
va_r <- quantile(loss_es, conf_level)
# Calculate the CVaR as the mean losses in excess of VaR
c_var <- mean(loss_es[loss_es > va_r])
# Plot the density of portfolio losses
x11(width=6, height=5)
densi_ty <- density(loss_es)
plot(densi_ty, xlab="loss percentage", ylab="density", 
     lwd=3, col="blue", main="Portfolio Loss Distribution")
# Add vertical line for expected loss
expected_loss <- l_gd*mean(def_probs)
abline(v=expected_loss, col="red", lwd=3)
x_max <- max(densi_ty$x); y_max <- max(densi_ty$y)
text(x=expected_loss, y=(6*y_max/7), labels="expected loss", 
     lwd=2, pos=4)
# Add vertical line for VaR
abline(v=va_r, col="red", lwd=3)
text(x=va_r, y=4*y_max/5, labels="VaR", lwd=2, pos=4)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_loss_distr_simu.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Draw shaded polygon for CVaR
in_dex <- (densi_ty$x > va_r)
x_var <- c(min(densi_ty$x[in_dex]), densi_ty$x[in_dex], max(densi_ty$x))
polygon(x_var, c(-1, densi_ty$y[in_dex], -1), col="red", border=NA, density=10)
# Add text for CVaR
text(x=5*va_r/4, y=(y_max/7), labels="CVaR", lwd=2, pos=4)
# Add text with data
text(x_max, y_max, labels=paste0(
       "Expected Loss = ", format(100*expected_loss, digits=3), "%", "\n",
       "Loss severity = ", format(100*l_gd, digits=3), "%", "\n",
       "Correlation = ", format(100*rh_o, digits=3), "%", "\n",
       "VaR = ", format(100*va_r, digits=3), "%", "\n",
       "CVaR = ", format(100*c_var, digits=3), "%"), 
     adj=c(1, 1), cex=0.8, lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VaR} can be calculated from the simulated portfolio losses using the function \texttt{quantile()}.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_var_simu.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VaRs from confidence levels
level_s <- seq(0.93, 0.99, 0.01)
var_s <- quantile(loss_es, probs=level_s)
plot(x=level_s, y=var_s, t="l", lwd=2,
     xlab="confidence level", ylab="VaRs",
     main="Simulated VaR and Confidence Levels")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating \protect\emph{CVaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{CVaR} can be calculated from the frequency of tail losses in excess of the \emph{VaR}.
      \vskip1ex
      The function \texttt{table()} calculates the frequency distribution of categorical data.
      <<echo=TRUE,eval=FALSE>>=
# Calculate CVaRs
cvar_s <- sapply(var_s, function(va_r) {
  mean(loss_es[loss_es >= va_r])
})  # end sapply
cvar_s <- cbind(cvar_s, var_s)
# Alternative CVaR calculation using frequency table
# first calculate frequency table of loss_es
# ta_ble <- table(loss_es)/n_simu
# Calculate CVaRs from frequency table
# Cvar_s <- sapply(var_s, function(va_r) {
#   tai_l <- ta_ble[names(ta_ble) > va_r]
#   tai_l %*% as.numeric(names(tai_l)) / sum(tai_l)
# })  # end sapply
# Plot CVaRs
plot(x=level_s, y=cvar_s[, "cvar_s"],
     t="l", col="red", lwd=2,
     ylim=range(cvar_s),
     xlab="confidence level", ylab="CVaRs",
     main="Simulated CVaR and Confidence Levels")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_cvar_simu.png}
      <<echo=TRUE,eval=FALSE>>=
# Add VaRs
lines(x=level_s, y=cvar_s[, "var_s"], lwd=2)
# Add legend
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for Simulating \protect\emph{VaR} Under the Vasicek Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      The function \texttt{calc\_var()} simulates default losses under the \emph{Vasicek} model, for a vector of confidence levels, and calculates a vector of \emph{VaR} and \emph{CVaR} values.
      <<echo=TRUE,eval=FALSE>>=
calc_var <- function(def_thresh, # Default thresholds
                     l_gd=0.6, # loss given default
                     rho_sqrt, rho_sqrtm, # asset correlation
                     n_simu=1000, # number of simulations
                     level_s=seq(0.93, 0.99, 0.01) # Confidence levels
                     ) {
  # Define model parameters
  n_assets <- NROW(def_thresh)
  # Simulate losses under Vasicek model
  system_atic <- rnorm(n_simu)
  asset_s <- matrix(rnorm(n_simu*n_assets), ncol=n_simu)
  asset_s <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_s))
  loss_es <- l_gd*colSums(asset_s < def_thresh)/n_assets
  # Calculate VaRs and CVaRs
  var_s <- quantile(loss_es, probs=level_s)
  cvar_s <- sapply(var_s, function(va_r) {
    mean(loss_es[loss_es >= va_r])
  })  # end sapply
  names(var_s) <- level_s
  names(cvar_s) <- level_s
  c(var_s, cvar_s)
}  # end calc_var
      @
    \column{0.3\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The values of \emph{VaR} and \emph{CVaR} produced by the function \texttt{calc\_var()} are subject to uncertainty because they're calculated from a simulation.
      \vskip1ex
      We can calculate the standard errors of \emph{VaR} and \emph{CVaR} by running the function \texttt{calc\_var()} many times and repeating the simulation in a loop.
      \vskip1ex
      This bootstrap will only capture the uncertainty due to the finite number of trials in the simulation, but not due to the uncertainty of model parameters.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
n_assets <- 300; n_simu <- 1000; l_gd <- 0.4
rh_o <- 0.2; rho_sqrt <- sqrt(rh_o); rho_sqrtm <- sqrt(1-rh_o)
# Calculate default probabilities and thresholds
set.seed(1121)
def_probs <- runif(n_assets, max=0.2)
def_thresh <- qnorm(def_probs)
# Define number of bootstrap simulations
n_boot <- 500
# Perform bootstrap of calc_var
set.seed(1121)
boot_data <- sapply(rep(l_gd, n_boot), calc_var,
  def_thresh=def_thresh,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end sapply
boot_data <- t(boot_data)
# Calculate vectors of standard errors of VaR and CVaR from boot_data data
std_error_var <- apply(boot_data[, 1:7], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_data[, 8:14], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
# Scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} at High Confidence Levels}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of \emph{VaR} and \emph{CVaR} are inversely proportional to square root of the number of loss events in the simulation that exceed the \emph{VaR}.
      \vskip1ex
      So the greater the number of loss events, the smaller the standard errors, and vice versa.
      \vskip1ex
      But as the confidence level increases, the \emph{VaR} also increases, and the number of loss events decreases, causing larger standard errors.
      \vskip1ex
      So the as the confidence level increases, the standard errors of \emph{VaR} and \emph{CVaR} also increase.
      \vskip1ex
      The \emph{scaled} (relative) standard errors of \emph{VaR} and \emph{CVaR} also increase with the confidence level, making them much less reliable at very high confidence levels.
      \vskip1ex
      The standard error of \emph{CVaR} is even greater than that of \emph{VaR}.
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_cvar_stderror.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the standard errors of VaRs and CVaRs
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(x=colnames(std_error_cvar), y=std_error_cvar[2, ], 
  t="l", col="red", lwd=2,
  ylim=range(c(std_error_var[2, ], std_error_cvar[2, ])),
  xlab="confidence level", ylab="standard error",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var[2, ], lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of \protect\emph{VaR} Using Parallel Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{scaled} standard errors of \emph{VaR} and \emph{CVaR} increase with the confidence level, making them much less reliable at very high confidence levels.
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
n_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
# Perform bootstrap of calc_var for Windows
clusterSetRNGStream(clus_ter, 1121)
boot_data <- parLapply(clus_ter, rep(l_gd, n_boot),
  fun=calc_var, def_thresh=def_thresh,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end parLapply
# Bootstrap under Mac-OSX or Linux
boot_data <- mclapply(rep(l_gd, n_boot),
  FUN=calc_var, def_thresh=def_thresh,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end mclapply
boot_data <- rutils::do_call(rbind, boot_data)
stopCluster(clus_ter)  # Stop R processes over cluster
# Calculate vectors of standard errors of VaR and CVaR from boot_data data
std_error_var <- apply(boot_data[, 1:7], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_data[, 8:14], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
# Scale the standard errors of VaRs and CVaRs
std_error_var_scaled <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar_scaled <- std_error_cvar[2, ]/std_error_cvar[1, ]
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_cvar_stderror_parallel.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the standard errors of VaRs and CVaRs
x11(width=6, height=5)
plot(x=colnames(std_error_cvar),
  y=std_error_cvar_scaled, t="l", col="red", lwd=2,
  ylim=range(c(std_error_var_scaled, std_error_cvar_scaled)),
  xlab="confidence level", ylab="standard error",
  main="Scaled standard errors of CVaR and VaR")
lines(x=colnames(std_error_var), y=std_error_var_scaled, lwd=2)
legend(x="topleft", legend=c("CVaRs", "VaRs"), bty="n",
       title=NULL, inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vasicek Model With Uncertain Default Probabilities}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      The previous bootstrap only captured the uncertainty due to the finite simulation trials, but not due to the uncertainty of model parameters, such as the default probabilities and correlations.
      \vskip1ex
      The below function \texttt{calc\_var()} can simulate the \emph{Vasicek} model with uncertain default probabilities.
      <<echo=TRUE,eval=FALSE>>=
calc_var <- function(def_probs, # Default probabilities
                     l_gd=0.6, # loss given default
                     rho_sqrt, rho_sqrtm, # asset correlation
                     n_simu=1000, # number of simulations
                     level_s=seq(0.93, 0.99, 0.01) # Confidence levels
                     ) {
  # Calculate random default thresholds
  def_thresh <- qnorm(runif(1, min=0.5, max=1.5)*def_probs)
  # Simulate losses under Vasicek model
  n_assets <- NROW(def_probs)
  system_atic <- rnorm(n_simu)
  asset_s <- matrix(rnorm(n_simu*n_assets), ncol=n_simu)
  asset_s <- t(rho_sqrt*system_atic + t(rho_sqrtm*asset_s))
  loss_es <- l_gd*colSums(asset_s < def_thresh)/n_assets
  # Calculate VaRs and CVaRs
  var_s <- quantile(loss_es, probs=level_s)
  cvar_s <- sapply(var_s, function(va_r) {
    mean(loss_es[loss_es >= va_r])
  })  # end sapply
  names(var_s) <- level_s
  names(cvar_s) <- level_s
  c(var_s, cvar_s)
}  # end calc_var
      @
    \column{0.3\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors Due to Uncertain Default Probabilities}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The greatest contribution to the standard errors of \emph{VaR} and \emph{CVaR} is from the uncertainty of model parameters, such as the default probabilities, correlations, and loss severities.
      \vskip1ex
      For example, a \texttt{50\%} uncertainty in the default probabilities can produce a \texttt{20\%} uncertainty of the \emph{VaR}.
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # load package parallel
n_cores <- detectCores() - 1  # number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
# Perform bootstrap of calc_var for Windows
clusterSetRNGStream(clus_ter, 1121)
boot_data <- parLapply(clus_ter, rep(l_gd, n_boot),
  fun=calc_var, def_probs=def_probs,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end parLapply
# Bootstrap under Mac-OSX or Linux
boot_data <- mclapply(rep(l_gd, n_boot),
  FUN=calc_var, def_probs=def_probs,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end mclapply
boot_data <- rutils::do_call(rbind, boot_data)
stopCluster(clus_ter)  # Stop R processes over cluster
# Calculate vectors of standard errors of VaR and CVaR from boot_data data
std_error_var_param <- apply(boot_data[, 1:7], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar_param <- apply(boot_data[, 8:14], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_var_stderror_default_uncertainty.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the standard errors of VaRs under uncertain default probabilities
x11(width=6, height=5)
plot(x=colnames(std_error_var),
  y=std_error_var[2, ], t="l", lwd=3,
  ylim=range(c(std_error_var[2, ], std_error_var_param[2, ])),
  xlab="confidence level", ylab="standard error",
  main="Standard Errors of VaR 
  with Uncertain Default Probabilities")
lines(x=colnames(std_error_var), y=std_error_var_param[2, ],
      col="red", lwd=3)
legend(x=0.95, y=0.02, bty="n",
       legend=c("VaR Fixed Def Probs", "VaR Random Def Probs"), 
       title=NULL, inset=0.05, cex=1.0, bg="white",
       lwd=6, lty=1, col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Relative Errors Due to Uncertain Default Probabilities}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{scaled} (relative) standard errors of \emph{VaR} and \emph{CVaR} under uncertain default probabilities decrease with higher confidence level, because the standard errors are less dependent on the confidence level and don't increase as fast as the \emph{VaR} does.
      <<echo=TRUE,eval=FALSE>>=
# Scale the standard errors of VaRs and CVaRs
std_error_var_scaled <- std_error_var_param[2, ]/
  std_error_var_param[1, ]
std_error_cvar_scaled <- std_error_cvar_param[2, ]/
  std_error_cvar_param[1, ]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vasicek_stderror_default_uncertainty.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the standard errors of VaRs and CVaRs
x11(width=6, height=5)
plot(x=colnames(std_error_cvar_param),
  y=std_error_cvar_scaled, t="l", col="red", lwd=3,
  ylim=range(c(std_error_var_scaled, std_error_cvar_scaled)),
  xlab="confidence level", ylab="standard error",
  main="Relative Standard Errors of VaR and CVaR
  with Uncertain Default Probabilities")
lines(x=names(std_error_var_scaled), y=std_error_var_scaled, lwd=3)
legend(x="topright", legend=c("CVaR", "VaR"), bty="n",
       title=NULL, inset=0.05, cex=1.0, bg="white",
       lwd=6, lty=1, col=c("red", "black"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Model Risk of Credit Portfolio Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Credit portfolio models are subject to very significant \emph{model risk} due to the uncertainties of model parameters, such as the default probabilities, correlations, and loss severities.
      \vskip1ex
      Model risk is the risk of incorrect model predictions due to incorrect model specification, and due to incorrect model parameters.
      \vskip1ex
      Jon Danielsson at the London School of Economics (LSE) has studied the model risk of \emph{VaR} and \emph{CVaR} in:
\href{https://www.systemicrisk.ac.uk/publications/discussion-papers/why-risk-so-hard-measure}{Why Risk is So Hard to Measure}, and in 
\href{https://www.federalreserve.gov/econres/feds/model-risk-of-risk-models.htm}{Model Risk of Risk Models}.
      \vskip1ex
      Jon Danielsson has pointed out that there's not enough historical data to be able to accurately calculate the credit model parameters.
      \vskip1ex
      Jon Danielsson and Chen Zhou have demonstrated that accurately estimating \emph{CVaR} at \texttt{5\%} confidence 
\href{http://www.bloomberg.com/view/articles/2016-05-23/big-banks-risk-does-not-compute}{would require decades of price history}, something that simply doesn't exist for many assets.
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Simulating the Vasicek Model Using Importance Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The simulation estimates of \emph{VaR} and \emph{CVaR} have large standard errors because default events are rare, so the number of events which contribute to their estimates is therefore small.
      \vskip1ex
      The \emph{variance} of an estimate produced by simulation decreases with the number of events which contribute to the estimate: $\sigma^2 \propto \frac{1}{n}$.
      \vskip1ex
      \emph{Importance sampling} with probability tilting can be applied to reduce the standard errors of \emph{VaR} and \emph{CVaR}.
      \vskip1ex
      The exponential probability tilting can be applied to the \emph{systematic} factor $s$:
      \begin{displaymath}
        \Phi(s, \lambda) = \exp(s \lambda - \lambda^2/2) \cdot \Phi(s, \lambda = 0)
      \end{displaymath}
      Where $\lambda$ is the tilt parameter.
      \vskip1ex
      The simulation outputs are then multiplied by the weights to compensate for the probability tilting:
      \begin{displaymath}
        w_x = \exp(-x \lambda + \lambda^2/2)
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
    % wippp
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
n_assets <- 300; n_simu <- 1000; l_gd <- 0.4
# Define correlation parameters
rh_o <- 0.2; rho_sqrt <- sqrt(rh_o); rho_sqrtm <- sqrt(1-rh_o)
# Calculate default probabilities and thresholds
set.seed(1121)
def_probs <- runif(n_assets, max=0.2)
def_thresh <- qnorm(def_probs)
# Calculate vector of systematic factors
system_atic <- rnorm(n_simu)
# Calculate vector of idiosyncratic factors
idio_syncratic <-
  matrix(rnorm(n_simu*n_assets), ncol=n_simu)
# Simulate losses under Vasicek model
asset_s <-
  t(rho_sqrt*system_atic + t(rho_sqrtm*idio_syncratic))
loss_es <-
  l_gd*colSums(asset_s < def_thresh)/n_assets
# Calculate VaRs
level_s <- seq(0.93, 0.99, 0.01)
var_s <- quantile(loss_es, probs=level_s)

# Importance sampling losses
lamb_da <- 3
asset_s <-
  t(rho_sqrt*system_atic + t(rho_sqrtm*idio_syncratic))

cond_thresh <- outer(rho_sqrtm*def_thresh, -rho_sqrt*system_atic, FUN="+")

cond_probs <- pnorm(cond_thresh)

tilt_probs <- lamb_da*cond_probs/(1 + cond_probs*(lamb_da - 1))

weight_s <- (1 + tilt_probs*(lamb_da - 1))/lamb_da

tilt_thresh <- qnorm(tilt_probs)

loss_es <-
  l_gd*colSums(weight_s*(asset_s < tilt_thresh))/n_assets

var_s <- quantile(loss_es, probs=level_s)


foo <- (uni_form < def_probs)

def_thresh <- qnorm(def_probs)





asset_s <- t(rho_sqrt*(system_atic - lamb_da) +
      t(rho_sqrtm*idio_syncratic))
loss_es <-
  l_gd*colSums(asset_s < def_thresh)/n_assets
# Calculate VaRs
var_s <- quantile(loss_es, probs=level_s)


asset_s <- t(rho_sqrt*(system_atic - lamb_da) +
      t(rho_sqrtm*idio_syncratic))
loss_es <-
  l_gd*colSums(asset_s < def_thresh)/n_assets
# Calculate VaRs
var_s <- quantile(loss_es, probs=level_s)



plot(x=level_s, y=var_s, t="l", lwd=2,
     xlab="confidence level", ylab="VaRs",
     main="Simulated VaR and Confidence Levels")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Standard Errors of \protect\emph{VaR} Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The values of \emph{VaR} and \emph{CVaR} produced by the function \texttt{calc\_var()} are subject to uncertainty because they're calculated from a simulation.
      \vskip1ex
      We can calculate the standard errors of \emph{VaR} and \emph{CVaR} by running the function \texttt{calc\_var()} many times and repeating the simulation in a loop.
      \vskip1ex
      This bootstrap will only capture the uncertainty due to the finite number of trials in the simulation, but not due to the uncertainty of model parameters.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define model parameters
n_assets <- 300; n_simu <- 1000; l_gd <- 0.4
rh_o <- 0.2; rho_sqrt <- sqrt(rh_o); rho_sqrtm <- sqrt(1-rh_o)
# Calculate default probabilities and thresholds
set.seed(1121)
def_probs <- runif(n_assets, max=0.2)
def_thresh <- qnorm(def_probs)
# Define number of bootstrap simulations
n_boot <- 500
# Perform bootstrap of calc_var
set.seed(1121)
boot_data <- sapply(rep(l_gd, n_boot),
  calc_var,
  def_thresh=def_thresh,
  rho_sqrt=rho_sqrt, rho_sqrtm=rho_sqrtm,
  n_simu=n_simu, level_s=level_s)  # end sapply
boot_data <- t(boot_data)
# Calculate vectors of standard errors of VaR and CVaR from boot_data data
std_error_var <- apply(boot_data[, 1:7], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
std_error_cvar <- apply(boot_data[, 8:14], MARGIN=2,
    function(x) c(mean=mean(x), sd=sd(x)))
# Scale the standard errors of VaRs and CVaRs
std_error_var[2, ] <- std_error_var[2, ]/std_error_var[1, ]
std_error_cvar[2, ] <- std_error_cvar[2, ]/std_error_cvar[1, ]
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
