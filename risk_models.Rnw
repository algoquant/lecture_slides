% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
% \usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Risk Analysis and Model Construction]{Risk Analysis and Model Construction}
\subtitle{FRE6871 \& FRE7241, Spring 2023}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Evaluating Manager Skill}


%%%%%%%%%%%%%%%
\subsection{Tests for Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free rates.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Test if IEF can time VTI
retp <- na.omit(rutils::etfenv$returns[, c("IEF", "VTI")])
retvti <- retp$VTI
desv <- cbind(retp, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(desv)[3:4] <- c("merton", "treynor")
# Merton-Henriksson test
regmod <- lm(IEF ~ VTI + merton, data=desv); summary(regmod)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/timing_skill_ief_vti.png}
      <<echo=TRUE,eval=FALSE>>=
# Treynor-Mazuy test
regmod <- lm(IEF ~ VTI + treynor, data=desv); summary(regmod)
# Plot residual scatterplot
x11(width=6, height=5)
resids <- (desv$IEF - regmod$coeff["VTI"]*retvti)
plot.default(x=retvti, y=resids, xlab="VTI", ylab="IEF")
title(main="Treynor-Mazuy Market Timing Test\n for IEF vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fitv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fitv, pch=16, col="red")
text(x=0.0, y=0.8*max(resids), paste("Treynor test t-value =", tvalue))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Identifying Managers With Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      % Adapt from RFinance\2017.Rmd and from file scratch.R.
      \vskip1ex
      Consider a binary investment (gamble) with the probability of winning equal to $p$, the winning amount (gain) equal to $a$, and the loss equal to $b$.
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount $a$, or loses an amount $b$.
      \vskip1ex
      Assuming that an investor makes decisions exclusively on the basis of the expected value of future wealth, then they would choose to invest all their wealth on the gamble if its expected value is positive, and choose not to invest at all if its expected value is negative.
    \column{0.5\textwidth}
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gamblev <- data.frame(win=c("p", "a"), lose=c("q = 1 - p", "-b"))
rownames(gamblev) <- c("probability", "payout")
# print(xtable(gamblev), comment=FALSE, size="tiny")
print(xtable(gamblev), comment=FALSE)
      @
      The expected value of the gamble is equal to: $m = p \, a - q \, b$.
      \vskip1ex
      The variance of the gamble is equal to: $var=p \, q \, (a + b)^2$.
      \vskip1ex
      Without loss of generality we can assume that $p=q = \frac{1}{2}$,\\
      $m = 0.5 \, (b - a)$,\\
      $var=0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{m}{sqrt(var)} = \frac{(b - a)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Investor Risk Preferences and Portfolio Selection}


%%%%%%%%%%%%%%%
\subsection{Single Period Binary Gamble}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a single investment (gamble) with a binary outcome: \\
      The investor makes no up-front payments, and either wins an amount $a$ (with probability $p$), or loses an amount $b$ (with probability $q = 1-p$).
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gamblev <- data.frame(win=c("p", "a", "1 + a"), lose=c("q = 1 - p", "-b", "1 - b"))
rownames(gamblev) <- c("probability", "payout", "terminal wealth")
# print(xtable(gamblev), comment=FALSE, size="tiny")
print(xtable(gamblev), comment=FALSE)
      @
      The initial wealth is equal to $1$ dollar, and the terminal wealth after the gamble is either $1 + a$ (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      \vskip1ex
      The amounts $a$ and $b$ are expressed as percentages of the wealth risked in the gamble, and the ratio $a / b$ is called the \emph{betting odds}.
      \vskip1ex
      The expected return on the gamble is called the \emph{edge} and is equal to: $\mu = p \, a - q \, b$, and the variance of returns is equal to: $\sigma^2 = p \, q \, (a + b)^2$.
    \column{0.5\textwidth}
      If the investor chooses to risk only a fraction $k_f$ of wealth, then the return on the gamble is either $k_f a$ (with probability $p$), or $- k_f b$ (with probability $q = 1-p$).
      \vskip1ex
      The fraction $k_f$ can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      And the expected return on the gamble is equal to: $p \, k_f a - q \, k_f b = k_f \, \mu$.
      \vskip1ex
      If an investor makes decisions exclusively based on the expected return $\mu$, then they would either invest all their wealth ($k_f = 1$) on the gamble if $\mu > 0$, or choose not to invest at all ($k_f = 0$) if $\mu < 0$.
      \vskip1ex
      Without loss of generality we can assume that $p = q = \frac{1}{2}$.
      \vskip1ex
      And then $\mu = 0.5 \, (a - b)$, and $\sigma^2 = 0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{\mu}{\sigma} = \frac{(a - b)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Utility and Fractional Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{expected utility} hypothesis states that investors try to maximize the expected \emph{utility} of wealth, not the expected wealth.
      \vskip1ex
      In 1738 Daniel Bernoulli introduced the concept of \emph{logarithmic utility} in his work \emph{"Specimen Theoriae Novae de Mensura Sortis"} (New Theory of the Measurement of Risk).
      \vskip1ex
      The \emph{logarithmic utility} function is defined as the logarithm of wealth: $u(w) = \log(w)$.
      \vskip1ex
      Under \emph{logarithmic utility} investor preferences depend on the percentage change of wealth, instead of the absolute change of wealth: $\mathrm{d} u(w) = \frac{\mathrm{d}w}{w}$.
      \vskip1ex
      An investor with \emph{logarithmic utility} invests only a fraction $k_f$ of their wealth in a gamble, depending on the risk-return of the gamble.
      \vskip1ex
      If the initial wealth is equal to $1$, then the expected value of \emph{logarithmic utility} for the binary gamble is equal to: $u(k_f) = p \, \log(1 + k_f a) + q \, \log(1 - k_f b)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/util_log.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define logarithmic utility
utilfun <- function(frac, p=0.3, a=20, b=1) {
  p*log(1+frac*a) + (1-p)*log(1-frac*b)
}  # end utilfun
# Plot utility
curve(expr=utilfun, xlim=c(0, 1),
      ylim=c(-0.5, 0.4), xlab="betting fraction",
      ylab="utility", main="", lwd=2)
title(main="Logarithmic Utility", line=0.5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Fractional Betting Under Logarithmic Utility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The betting fraction that maximizes the \emph{utility} can be found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u(k_f)}{\mathrm{d} k_f} = \frac{p \, a}{1 + k_f a} - \frac{q \, b}{1 - k_f b} = 0
      \end{displaymath}
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a} = \frac{p \, a - q \, b}{b \, a} = \frac{\mu}{b \, a}
      \end{displaymath}
      The optimal $k_f$ is called the \emph{Kelly fraction}, and it depends on the parameters of the gamble.
      \vskip1ex
      The \emph{Kelly fraction} can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      If we assume that $b=1$, then the betting odds are equal to $a$ and the \emph{Kelly fraction} is: $k_f = \frac{p (a + 1) - 1}{a}$
      \vskip1ex
      The \emph{Kelly fraction} is then equal to the expected payout divided by the betting odds.
      \vskip1ex
      If the expected payout of the gamble is not positive, then an investor with logarithmic utility should not allocate any capital to the gamble.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_fraction.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define and plot Kelly fraction
kelly_frac <- function(a, p=0.5, b=1) {
  p/b - (1-p)/a
}  # end kelly_frac
curve(expr=kelly_frac, xlim=c(0, 5),
      ylim=c(-2, 1), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="max Kelly fraction=0.5")
title(main="Kelly fraction", line=-0.8)
      @
\end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kelly criterion} states that investors should bet the optimal \emph{Kelly fraction} of their capital in a gamble.
      \vskip1ex
      Investors with concave utility functions (for example logarithmic utility) are sensitive to the risk of ruin (losing all their capital).
      \vskip1ex
      Applying the \emph{Kelly criterion} and betting only a fraction of their capital reduces the risk of ruin (but it doesn't eliminate the risk if prices drop suddenly).
      \vskip1ex
      The loss amount $b$ determines the risk of ruin, with larger values of $b$ increasing the risk of ruin.
      \vskip1ex
      Therefore investors will choose a smaller betting fraction $k_f$ for larger values of $b$.
      \vskip1ex
      This means that even for huge odds in their favor, investors may not choose to invest all their capital, because of the risk of ruin.
      \vskip1ex
      For example, if the betting odds are very large $a \to \infty$, then the \emph{Kelly fraction}: $k_f = \frac{p}{b}$.
    \column{0.5\textwidth}
    \vspace{-1em}
    \includegraphics[width=0.45\paperwidth]{figure/kelly_fraction_max.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5),
      ylim=c(-1, 1.5), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $r_t$ be the random return on the gamble in period $i$, and let $w_i = (1 + k_f r_t)$ be the random wealth increment.
      \vskip1ex
      Then the terminal wealth after $n$ rounds is equal to the compounded wealth increments: $w_n = \prod_{i=1}^n w_i = \prod_{i=1}^n (1 + k_f r_t)$.
      \vskip1ex
      And the utility is equal to the sum of the individual utilities:
      \begin{displaymath}
        u_n = \log(w_n) = \sum_{i=1}^n \log(w_i) = \sum_{i=1}^n \log(1 + k_f r_t) = \sum_{i=1}^n u_i
      \end{displaymath}
      The individual utilities are all maximized by the same \emph{Kelly fraction} $k_f$, so the \emph{Kelly fraction} for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Wealth of Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In multiperiod betting the investor participates in $n$ rounds of gambles, and in each round they risk a fixed fraction $k_f$ of their current outstanding wealth.
      \vskip1ex
      In each round the wealth is multiplied by either $(1 + k_f a)$ (win) or $(1 - k_f b)$ (loss), so that the current outstanding wealth changes over time.
      \vskip1ex
      The terminal wealth after $n$ rounds with $m$ wins is equal to: $w(k_f) = (1 + k_f a)^m (1 - k_f b)^{n-m}$.
      \vskip1ex
      If the number of rounds $n$ is very large, then the number of wins is almost always equal to $m = n \, p$, and the terminal wealth is equal to: $w(k_f) = (1 + k_f a)^{np} (1 - k_f b)^{nq}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_multi.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Wealth of multiperiod binary betting
wealthv <- function(f, a=0.8, b=0.1, n=1e3, i=150) {
  (1+f*a)^i * (1-f*b)^(n-i)
}  # end wealth
curve(expr=wealthv, xlim=c(0, 1),
      xlab="betting fraction",
      ylab="wealth", main="", lwd=2)
title(main="Wealth of Multiperiod Betting", line=0.1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.9\textwidth}
      The betting fraction $k_f$ that maximizes the terminal wealth is found by setting the derivative of $w(k_f)$ to zero:
      \begin{flalign*}
        & \frac{\mathrm{d} w(k_f)}{\mathrm{d} k_f} = n p a (1 + k_f a)^{np-1} (1 - k_f b)^{nq} - n q b (1 + k_f a)^{np} (1 - k_f b)^{nq-1} & \\
        & = (\frac{n p a}{1 + k_f a} - \frac{n q b}{1 - k_f b}) (1 + k_f a)^{np} (1 - k_f b)^{nq} = 0 &
      \end{flalign*}
      We can then solve for the optimal betting fraction $k_f$:
      \begin{flalign*}
        \frac{p a}{1 + k_f a} - \frac{q b}{1 - k_f b} = 0 \\
        p a (1 - k_f b) - q b (1 + k_f a) = 0 \\
        p a - q b - k_f a b = 0 \\
        k_f = \frac{p a - q b}{a b} = \frac{p}{b} - \frac{q}{a}
      \end{flalign*}
      The above is just the \emph{Kelly fraction} $k_f$ that maximizes the utility.
      \vskip1ex
      So the \emph{Kelly fraction} $k_f$ that maximizes the utility also maximizes the terminal wealth.
    \column{0.1\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.9\textwidth}
      The terminal wealth after $n$ repeated gambles with $m$ wins is equal to: $(1 + k_f a)^m (1 - k_f b)^{n-m}$.
      \vskip1ex
      And the expected value of the wealth is equal to:
      \begin{displaymath}
        w(k_f) = \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}}}
      \end{displaymath}
      We can then find the fraction $k_f$ which maximizes the expected wealth $w(k_f)$:
      \begin{flalign*}
        \frac{\mathrm{d} w(k_f)}{\mathrm{d} k_f} = \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}} (\frac{a m}{1 + k_f a} - \frac{b (n-m)}{1 - k_f b}) = \\
        \frac{a}{1 + k_f a} \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {m} - \\
        \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}} (\frac{a m}{1 + k_f a} - \frac{b (n-m)}{1 - k_f b}) \\
      \end{flalign*}
      If the investor chooses to risk only a fraction $k_f$ of wealth, then the wealth after the gamble is either $1 + k_f a$ (with probability $p$), or $1 - k_f b$ (with probability $q = 1-p$).
      \vskip1ex
      (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      initial wealth is equal to $1$, and the
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(k_f) &= \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} \log((1 + k_f a)^m (1 - k_f b)^{n-m}) &\\
        &= \log(1 + k_f a) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} m} + &\\
        & \log(1 - k_f b) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} (n-m)} &\\
        &= n \, p \, \log(1 + k_f a) + n \, q \, \log(1 - k_f b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $k_f$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.1\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Utility of Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(k_f) &= \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} \log((1 + k_f a)^m (1 - k_f b)^{n-m}) &\\
        &= \log(1 + k_f a) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} m} + &\\
        & \log(1 - k_f b) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} (n-m)} &\\
        &= n \, p \, \log(1 + k_f a) + n \, q \, \log(1 - k_f b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $k_f$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.3\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Margin}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $r_t$ be the percentage returns on a \emph{risky asset}, so that the asset price $p_t$ at time $t$ is given by:
      \begin{displaymath}
        p_t = p_0 \prod_{i=1}^t {(1 + r_t)}
      \end{displaymath}
      The initial investor wealth at time $t=0$ is equal to $1$ dollar, and they also borrow on margin $m$ dollars to invest in the \emph{risky asset}.
      \vskip1ex
      The investor's \emph{wealth} at time $t$ is equal to (the margin borrowing rate is assumed to be zero):
      \begin{displaymath}
        w_t = 1 + m \, \frac{p_t - p_0}{p_0}
      \end{displaymath}
      The \emph{leverage} $k_f$ is equal to the \emph{margin debt} $m$ divided by the total wealth $w_t$: $k_f = m / w_t$.
      \vskip1ex
      If the asset price drops then the \emph{leverage} increases, because the \emph{margin debt} is fixed while the wealth drops.
      \vskip1ex
      If the asset price drops enough so that the wealth reaches zero, then the investment is liquidated and the investor is ruined.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/simu_prices.png}
      <<echo=(-(1:5)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
set.seed(1121)  # Reset random number generator
# Simulate asset prices
calc_pricev <- function(x) cumprod(1 + rnorm(1e3, sd=0.01))
price_paths <- sapply(1:3, calc_pricev)
plot(price_paths[, 1], type="l", lwd=3,
     main="Simulated Asset Prices",
     ylim=range(price_paths),
     lty="solid", xlab="time", ylab="price")
lines(price_paths[, 2], col="blue", lwd=3)
lines(price_paths[, 3], col="orange", lwd=3)
abline(h=0.5, col="red", lwd=3)
text(x=200, y=0.5, pos=3, labels="liquidation threshold")
      @
  \end{columns}
\end{block}


\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In order to avoid ruin, the investor may choose to maintain a fixed \emph{leverage ratio} equal to $k_f$, so that the amount invested in the \emph{risky asset} is proportional to the \emph{wealth}: $k_f \, w_t$.
      \vskip1ex
      This requires buying the \emph{risky asset} when its price increases, and selling it when it drops.
      \vskip1ex
      The return on the \emph{risky asset} in a single period is equal to: $k_f \, w_t \, r_t$, so the \emph{terminal wealth} at time $t$ is equal to the compounded returns:
      \begin{displaymath}
        w_t = (1 + k_f \, r_1) \ldots (1 + k_f \, r_t) = \prod_{i=1}^t {(1 + k_f \, r_t)}
      \end{displaymath}
      The utility of the \emph{terminal wealth} is equal to the sum of the utilities of single periods:
      \begin{flalign*}
        & \mathbbm{E}[\log{w_t}] = \mathbbm{E}[\log((1 + k_f \, r_1) \ldots (1 + k_f \, r_t))] &\\
        & = \sum_{i=1}^t {\mathbbm{E}[\log{(1 + k_f \, r_t)}]} = t \, \mathbbm{E}[\log{(1 + k_f \, r)}]
      \end{flalign*}
      The last equality holds because all the utilities of single periods are the same.
    \column{0.5\textwidth}
      Let the returns over a short time period be equal to $r$, with probability distribution $p(r)$.
      \vskip1ex
      The mean return $\bar{r}$, and variance $\sigma^2$ are:
      \begin{displaymath}
        \bar{r} = \int {r \, p(r) \, \mathrm{d}r} \; ; \quad
        \sigma^2 = \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      Since the returns are over a short time period, we have: $r \ll 1$ and $\bar{r} \ll \sigma$, so that we can replace $r - \bar{r}$ with $r$ as follows:
      \begin{displaymath}
        \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r} \approx \int {r^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Leveraged Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      So the utility of the \emph{terminal wealth} $u_t$ is equal to the utility of a single period times the number of periods:
      \begin{displaymath}
        u_t = \mathbbm{E}[\log{w_t}] = t \, \mathbbm{E}[\log{(1 + k_f \, r)}] = t \, u_r
      \end{displaymath}
      The utility of the asset returns $u_r$ is equal to:
      \begin{displaymath}
        u_r = \mathbbm{E}[\log{(1 + k_f \, r)}] = \int {\log(1 + k_f \, r) \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      The leverage $k_f$ is limited so that $(1 + k_f \, r) > 0$ for all return values $r$.
      \vskip1ex
      If the mean returns are positive, then at first the utility increases with leverage, but only up to a point.
      \vskip1ex
      With higher leverage, the negative utility of time periods with negative returns becomes significant, forcing the aggregate utility to drop.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
retp <- rutils::etfenv$returns$VTI
retp <- na.omit(retp)
c(mean=mean(retp), stdev=sd(retp))
range(retp)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/util_rets.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define vectorized logarithmic utility function
utilfun <- function(kellyfrac, retp) {
  sapply(kellyfrac, function(x) sum(log(1 + x*retp)))
}  # end utilfun
utilfun(1, retp)
utilfun(c(1, 4), retp)
# Plot the logarithmic utility
curve(expr=utilfun(x, retp=retp),
      xlim=c(0.1, 5), xlab="leverage", ylab="utility",
      main="Utility of Asset Returns", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Criterion for Optimal Leverage of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logarithmic utility}  $u_r$ can be expanded in the moments of the return distribution:
      \begin{flalign*}
        & u_r = \mathbbm{E}[\log{(1 + k_f \, r)}] = \int {\log(1 + k_f \, r) \, p(r) \, \mathrm{d}r} & \\
        & = \int {(k_f \, r - \frac{(k_f \, r)^2}{2} + \frac{(k_f \, r)^3}{3} - \frac{(k_f \, r)^4}{4}) \, p(r) \, \mathrm{d}r} & \\
        & = k_f \bar{r} - \frac{k_f^2 \sigma^2}{2} + \frac{k_f^3 \sigma^3 \varsigma}{3} - \frac{k_f^4 \sigma^4 \kappa}{4}
      \end{flalign*}
      Where $\varsigma = \int {\frac{r^3}{\sigma^3} \, p(r) \, \mathrm{d}r}$ is the \emph{skewness}, and $\kappa = \int {\frac{r^4}{\sigma^4} \, p(r) \, \mathrm{d}r}$ is the \emph{kurtosis}.
      \vskip1ex
      The \emph{Kelly leverage} which maximizes the \emph{utility} is found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u_r}{\mathrm{d}k_f} = \bar{r} - k_f \sigma^2 + k_f^2 \sigma^3 \varsigma - k_f^3 \sigma^4 \kappa = 0
      \end{displaymath}
      % wippp
      This shows that the logarithmic utility has positive odd derivatives and negative even derivatives.
    \column{0.5\textwidth}
      Assuming that the third and fourth moments $\sigma^4 \varsigma$ and $\sigma^4 \kappa$ are small and can be neglected, we get:
      \begin{displaymath}
        k_f = \frac{\bar{r}}{\sigma^2} = \frac{S_r}{\sigma} \; ; \quad u_r = \frac{1}{2} \frac{{\bar{r}}^2}{\sigma^2} = \frac{1}{2} S_r^2
      \end{displaymath}
      The \emph{Kelly leverage} is \emph{approximately} equal to the \emph{Sharpe ratio} divided by the \emph{standard deviation}.
      \vskip1ex
      The optimal utility $u_r$ is \emph{approximately} equal to half the \emph{Sharpe ratio} $S_r$ squared.
      \vskip1ex
      The \emph{standard deviation} and \emph{Sharpe ratio} are calculated over the same time interval as the returns (not annualized).
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly leverage
mean(retp)/var(retp)
PerformanceAnalytics::KellyRatio(R=retp, method="full")
# Kelly leverage
unlist(optimize(
  f=function(x) -utilfun(x, retp),
  interval=c(1, 4)))
      @
    \vspace{-1em}
%    \vspace{-1em}
%    \includegraphics[width=0.45\paperwidth]{figure/kelly_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy Wealth Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth of a Kelly Strategy with a fixed leverage ratio $k_f$ is equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_t)}
      \end{displaymath}
      The \emph{Kelly fraction} $k_f$ provides the optimal leverage to maximize the utility of wealth, by balancing the benefit of leveraging higher positive returns, with the risk of ruin due to excessive leverage.
      \vskip1ex
      If the mean asset returns are positive, then a higher leverage ratio provides higher returns.
      \vskip1ex
      But if the leverage is too high, then the losses in periods with negative returns wipe out most of the wealth, so then it's slow to recover.
      <<echo=(-(1:2)),eval=FALSE>>=
# Calculate the VTI returns
retp <- rutils::etfenv$returns$VTI
retp <- na.omit(retp)
# Calculate the wealth paths
kelly_ratio <- drop(mean(retp)/var(retp))
kelly_wealthv <- cumprod(1 + kelly_ratio*retp)
hyper_kelly <- cumprod(1 + (kelly_ratio+2)*retp)
sub_kelly <- cumprod(1 + (kelly_ratio-2)*retp)
kelly_paths <- cbind(kelly_wealth, hyper_kelly, sub_kelly)
colnames(kelly_paths) <- c("kelly", "hyper-kelly", "sub-kelly")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth paths
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "orange", "blue")
quantmod::chart_Series(kelly_paths, theme=plot_theme, name="Wealth Paths")
legend("topleft", legend=colnames(kelly_paths),
       inset=0.1, bg="white", lty=1, lwd=6, y.intersp=0.5, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy With Margin Account}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{margin debt} $m_t$ is equal to the dollar amount borrowed to purchase the \emph{risky asset}.
      \vskip1ex
      The wealth $w_t$ at time $t$ is equal to the initial wealth $w_0 = 1$ plus the dollar amount of the \emph{risky asset} $a_t$, minus the \emph{margin debt} $m_t$: $w_t = 1 + a_t - m_t$.
      \vskip1ex
      The dollar amount of the \emph{risky asset} $a_t$ is equal to the wealth $w_t$ times the \emph{leverage} $k_f$: $a_t = k_f w_t$.
      \vskip1ex
      So the \emph{margin debt} $m_t$ is proportional to the wealth $w_t$: $m_t = (k_f - 1) w_t + 1$.
      \vskip1ex
      The wealth changes from $w_{t-1}$ to: $w_t = w_{t-1} (1 + k_f \, r_t)$, while the dollar amount of the \emph{risky asset} changes from $a_{t-1} = k_f w_{t-1}$ to: $a_t = k_f w_{t-1} (1 + r_t)$, so that the leverage changes from $k_f$ to:
      \begin{displaymath}
        \frac{k_f w_{t-1} (1 + r_t)}{w_{t-1} (1 + k_f \, r_t)} = \frac{k_f (1 + r_t)}{1 + k_f \, r_t}
      \end{displaymath}
    \column{0.5\textwidth}
      In order to maintain a fixed \emph{leverage ratio} equal to $k_f$, the investor must actively trade the \emph{risky asset}, and the \emph{margin debt} $m_t$ changes over time.
      \vskip1ex
      The change in margin in a single time period is equal to:
      \begin{displaymath}
        \Delta m_t = (k_f - 1) \Delta w_t = k_f (k_f - 1) w_{t-1} r_t
      \end{displaymath}
      The dollar amount of the \emph{risky asset} traded is equal to the change in \emph{margin}.
      \vskip1ex
      Therefore the investor must borrow on margin and buy the \emph{risky asset} when its price increases, and sell it when it drops.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy With Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} is the percentage difference between the \emph{offer} minus the \emph{bid} price, divided by the \emph{mid} price.
      \vskip1ex
      The \emph{bid-offer spread} for liquid stocks can be assumed to be about \texttt{10} basis points (bps).
      \vskip1ex
      The \emph{transaction costs} $c^r$ due to the \emph{bid-offer spread} are equal to half the \emph{bid-offer spread} $\delta$ times the absolute value of the traded dollar amount of the \emph{risky asset}:
      \begin{displaymath}
        c^r = \frac{\delta}{2} \left| \Delta m_t \right|
      \end{displaymath}
      If the transaction costs are much less than the change in wealth $c^r \ll \left| \Delta w_t \right|$, then we can write approximately:
      \begin{displaymath}
        c^r = \frac{\delta}{2} k_f (k_f - 1) w_{t-1} \left| r_t \right|
      \end{displaymath}
    \column{0.5\textwidth}
      The wealth of the Kelly Strategy after accounting for the \emph{bid-offer spread} is then equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_t - \frac{\delta}{2} k_f (k_f - 1) \left| r_t \right|)}
      \end{displaymath}
      The effect of the \emph{bid-offer spread} is to reduce the effective asset returns by an amount proportional to the \emph{bid-offer spread}.
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate the wealth paths
kelly_ratio <- drop(mean(retp)/var(retp))
wealthv <- cumprod(1 + kelly_ratio*retp)
wealth_trans <- cumprod(1 + kelly_ratio*retp - 
  0.5*bid_offer*kelly_ratio*(kelly_ratio-1)*abs(retp))
# Calculate the compounded wealth from returns
wealthv <- cbind(wealthv, wealth_trans)
colnames(wealthv) <- c("Kelly", "Including bid-offer")
# Plot compounded wealth
dygraphs::dygraph(wealthv, main="Kelly Strategy With Transaction Costs") %>%
  dyOptions(colors=c("green", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Half-Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In reality investors don't know the probability of winning or the odds of the gamble, so they can't accurately Calculate the optimal \emph{Kelly fraction}.
      \vskip1ex
      The \emph{Kelly fraction}: $k_f = \frac{\bar{r}}{\sigma^2}$ is especially sensitive to the uncertainty of the expected returns $\bar{r}$.
      \vskip1ex
      If the expected returns are over-estimated, then it can produce an inflated value of the \emph{Kelly fraction}, leading to ruin.
      \vskip1ex
      The risk of applying too much leverage (over-betting) is much greater than the risk of applying too little leverage (under-betting).
      \vskip1ex
      Too much leverage (over-betting) not only reduces returns, but it increases the risk of ruin.
      \vskip1ex
      So in practice many investors apply only half the theoretical \emph{Kelly fraction} (the Half-Kelly), to reduce the risk of ruin.
    \column{0.5\textwidth}
      Perform bootstrap simulation to obtain the standard error of the \emph{Kelly fraction}.
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5),
      ylim=c(-1, 1.5), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Risk aversion is the investor preference to avoid losses more than to seek similar percentage gains in wealth.
      \vskip1ex
      For example, for a risk averse investor, a $10\%$ loss of wealth is more important than a $10\%$ gain.
      \vskip1ex
      Risk aversion is associated with the \emph{diminishing marginal utility} of the percentage change in wealth $\Delta w$.
      \vskip1ex
      This manifests itself as a concave utility function, with a negative second derivative $u''(w) < 0$.
      \vskip1ex
      For example, the \emph{logarithmic utility} function is concave.
      \vskip1ex
      The Arrow-Pratt coefficient of relative risk aversion is proportional to the convexity $u''(w)$ of the utility, and is defined as: $\eta = - \frac{w \, u''(w)}{u'(w)}$.
      \vskip1ex
      The relative risk aversion of \emph{logarithmic utility} is equal to one: $\eta = 1$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/util_log2.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot logarithmic utility function
curve(expr=log, lwd=3, col="blue", xlim=c(0.5, 5),
      xlab="wealth", ylab="utility",
      main="Logarithmic Utility")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Relative Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      It's not a given that all investors have a risk aversion coefficient equal to $1$, and other \emph{utility functions} are possible.
      \vskip1ex
      The Constant Relative Risk Aversion (\emph{CRRA}) utility function is a generalization of logarithmic utility:
      \begin{displaymath}
        u(w) = \frac{w^{1 - \eta} - 1}{1 - \eta}
      \end{displaymath}
      Where $\eta$ is the risk aversion parameter.
      \vskip1ex
      The relative risk aversion of the \emph{CRRA} utility function is constant and equal to $\eta$.
      \vskip1ex
      When the risk aversion parameter is equal to one $\eta = 1$, then the \emph{CRRA} utility function is equal to the logarithmic utility.
      \vskip1ex
      In practice, the risk aversion parameter $\eta$ is not known, and must be estimated through empirical studies.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/util_crra.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define CRRA utility
cr_ra <- function(w, ra) {
  (w^(1-ra) - 1)/(1-ra)
}  # end cr_ra
# Plot utility functions
curve(expr=cr_ra(x, ra=0.7), xlim=c(0.5, 5), lwd=3,
      xlab="wealth", ylab="utility", main="", col="blue")
curve(expr=log, add=TRUE, lwd=3)
curve(expr=cr_ra(x, ra=1.3), add=TRUE, lwd=3, col="red")
# Add title and legend
title(main="CRRA Utility", line=0.5)
legend(x="topleft", legend=c("risk seeking", "logarithmic", "risk averse"),
       title="Risk Aversion", inset=0.05, cex=0.8, bg="white", y.intersp=0.5, 
       lwd=6, lty=1, bty="n", col=c("blue", "black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: CRRA Optimal Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      It's not a given that all investors have a risk aversion coefficient equal to $1$, and other \emph{utility functions} are possible.
      \vskip1ex
      The Constant Relative Risk Aversion (\emph{CRRA}) utility function is a generalization of logarithmic utility:
      \begin{displaymath}
        u(w) = \frac{w^{1 - \eta} - 1}{1 - \eta}
      \end{displaymath}
      Where $\eta$ is the risk aversion parameter.
      \vskip1ex
      The relative risk aversion of the \emph{CRRA} utility function is constant and equal to $\eta$.
      \vskip1ex
      When the risk aversion parameter is equal to one $\eta = 1$, then the \emph{CRRA} utility function is equal to the logarithmic utility.
      \vskip1ex
      In practice, the risk aversion parameter $\eta$ is not known, and must be estimated through empirical studies.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/util_crra.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define CRRA utility
cr_ra <- function(w, ra) {
  (w^(1-ra) - 1)/(1-ra)
}  # end cr_ra
# Plot utility functions
curve(expr=cr_ra(x, ra=0.7), xlim=c(0.5, 5), lwd=3,
      xlab="wealth", ylab="utility", main="", col="blue")
curve(expr=log, add=TRUE, lwd=3)
curve(expr=cr_ra(x, ra=1.3), add=TRUE, lwd=3, col="red")
# Add title and legend
title(main="CRRA Utility", line=0.5)
legend(x="topleft", legend=c("risk seeking", "logarithmic", "risk averse"),
       title="Risk Aversion", inset=0.05, cex=0.8, bg="white", y.intersp=0.5, 
       lwd=6, lty=1, bty="n", col=c("blue", "black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{draft: CRRA Strategy Wealth Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth of a Kelly Strategy with a fixed leverage ratio $k_f$ is equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_t)}
      \end{displaymath}
      The \emph{Kelly fraction} $k_f$ provides the optimal leverage to maximize the utility of wealth, by balancing the benefit of leveraging higher positive returns, with the risk of ruin due to excessive leverage.
      \vskip1ex
      If the mean asset returns are positive, then a higher leverage ratio provides higher returns.
      \vskip1ex
      But if the leverage is too high, then the losses in periods with negative returns wipe out most of the wealth, so then it's slow to recover.
      <<echo=(-(1:2)),eval=FALSE>>=
# Calculate the VTI returns
retp <- rutils::etfenv$returns$VTI
retp <- na.omit(retp)
# Calculate the wealth paths
kelly_ratio <- drop(mean(retp)/var(retp))
kelly_wealthv <- cumprod(1 + kelly_ratio*retp)
hyper_kelly <- cumprod(1 + (kelly_ratio+2)*retp)
sub_kelly <- cumprod(1 + (kelly_ratio-2)*retp)
kelly_paths <- cbind(kelly_wealth, hyper_kelly, sub_kelly)
colnames(kelly_paths) <- c("kelly", "hyper-kelly", "sub-kelly")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth paths
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "orange", "blue")
quantmod::chart_Series(kelly_paths, theme=plot_theme,
             name="Wealth Paths")
legend("topleft", legend=colnames(kelly_paths),
       inset=0.1, bg="white", lty=1, lwd=6, y.intersp=0.5, 
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Utility of Lottery Tickets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Lottery tickets are equivalent to binary gambles with a very small probability of winning $p$, but a very large winning amount $a$, and a small loss amount $b$ equal to the ticket price.
      \vskip1ex
      The expected payout $\mu = p \, a - q \, b$ of most lottery tickets is negative.
      \vskip1ex
      So under \emph{logarithmic utility}, the Kelly fraction $k_f$ for most lottery tickets is also negative, meaning that investors should not be expected to buy these lottery tickets.
      \vskip1ex
      But in reality many people do buy lottery tickets with negative expected payouts, which means that their utility functions are not logarithmic.
      \vskip1ex
      The demand for lottery tickets can be explained by assuming a strong demand for positive \emph{skewness}, which exceeds the demand for a positive payout.
      \vskip1ex
      People buy lottery tickets because they want a small chance of a very large payout, even if the average payout is negative.
    \column{0.5\textwidth}
      Without loss of generality we can assume that the lottery ticket price is one dollar $b = 1$, that it pays out $a$ dollars, and that the expected payout is equal to zero: $\mu = p \, a - q \, b = 0$.
      \vskip1ex
      Then the probabilities of winning and losing are equal to:
      $p = \frac{1}{a + 1}$ and $q = \frac{a}{a + 1}$.
      \vskip1ex
      The variance is equal to: $\sigma^2 = p \, q \, (a + 1)^2 = a$.
      \vskip1ex
      And the \emph{skewness} is equal to:
      $\varsigma = \frac{1}{\sigma^3} (\frac{a^3}{a + 1} - \frac{a}{a + 1}) = \frac{a - 1}{\sqrt{a}}$.
      \vskip1ex
      So the positive \emph{skewness} of a lottery ticket increases as the square root of the \emph{betting odds} $a$, and it can become very large for large \emph{betting odds}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor \protect\emph{Risk Aversion}, \protect\emph{Prudence} and \protect\emph{Temperance}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investor risk and return preferences depend on the signs of the derivatives of their \emph{utility} function.
      \vskip1ex
      Investors with \emph{logarithmic utility} have positive \emph{odd} derivatives ($u'(w) > 0$ and $u'''(w) > 0$) and negative \emph{even} derivatives ($u''(w) < 0$ and $u''''(w) < 0$), which is typical for most other investors as well.
      \vskip1ex
      \emph{Risk averse} investors have a negative second derivative of utility $u''(w) < 0$.
      \vskip1ex
      The demand for lottery tickets shows that investors' utility typically has a positive third derivative $u'''(w) > 0$.
      \vskip1ex
      Positive \emph{odd} derivatives imply a preference for larger \emph{odd moments} of the change in the wealth distribution (mean, skewness).
      \vskip1ex
      Negative \emph{even} derivatives imply a preference for smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      The preference for smaller \emph{variance} is called \emph{risk aversion}, for larger \emph{skewness} is called \emph{prudence}, and for smaller \emph{kurtosis} is called \emph{temperance}.
    \column{0.5\textwidth}
      The expected change of the \emph{utility} of wealth $\mathbb{E}[\Delta u(w)]$ can be expanded in the moments of the wealth distribution $\Delta w$:
      \begin{flalign*}
        \mathbb{E}[\Delta u(w)] &= u'(w) \mathbb{E}[\Delta w] + \frac{u''(w)}{2} \sigma^2 &\\
        & + \frac{u'''(w)}{3!} \mu3 + \frac{u''''(w)}{4!} \mu3
      \end{flalign*}
      Where $\mathbb{E}[\Delta w]$ is the expected change of wealth, $\sigma^2 = \int {{\Delta w}^2 \, p(w) \, \mathrm{d}w}$ is the \emph{variance} of The change in wealth, and $\mu3 = \int {{\Delta w}^3 \, p(w) \, \mathrm{d}w} = \sigma^3 \varsigma$ and $\mu4 = \int {{\Delta w}^4 \, p(w) \, \mathrm{d}w} = \sigma^4 \kappa$ are the third and fourth moments, proportional to the \emph{skewness} $\varsigma$ and the \emph{kurtosis} $\kappa$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Preferences and Empirical Return Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The investor preference for higher \emph{returns} and for lower \emph{volatility} is expressed by maximizing the \emph{Sharpe ratio}.
      \vskip1ex
      The third and fourth moments of asset returns are usually much smaller than the \emph{variance}, so they typically have a smaller effect on the investor risk and return preferences.
      \vskip1ex
      Nevertheless, there is evidence that investors also have significant preferences for positive \emph{skewness} and lower \emph{kurtosis}.
      \vskip1ex
      But stock returns typically have negative \emph{skewness} and excess \emph{kurtosis}, the opposite of what investors prefer.
      \vskip1ex
      Many investors may prefer positive \emph{skewness}, even at the expense of lower \emph{returns}, similar to the buyers of lottery tickets.
      \vskip1ex
      A paper by Amaya asks if the
      \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1898735}{\emph{Realized Skewness Predicts the Cross-Section of Equity Returns?}}
      \vskip1ex
      But higher moments are hard to estimate accurately from low frequency (daily) returns, which makes empirical investigations more difficult.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
retp <- rutils::etfenv$returns$VTI
retp <- na.omit(retp)
# Calculate the higher moments of VTI returns
c(mean=sum(retp),
  variance=sum(retp^2),
  mom3=sum(retp^3),
  mom4=sum(retp^4))/NROW(retp)
# Calculate the higher moments of minutely SPY returns
spy <- HighFreq::SPY[, 4]
spy <- na.omit(spy)
spy <- rutils::diffit(log(spy))
c(mean=sum(spy),
  variance=sum(spy^2),
  mom3=sum(spy^3),
  mom4=sum(spy^4))/NROW(spy)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The utility $u$ of the stock and bond portfolio with weights $stocku, bondu$ is equal to:
      \begin{displaymath}
        u = \sum_{i=1}^n {\log(1 + stocku \, r^s_i + bondu \, r^b_i)}
      \end{displaymath}
      Where $r^s_i, r^b_i$ are the stock and bond returns.
      <<echo=TRUE,eval=FALSE>>=
retp <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
# Logarithmic utility of stock and bond portfolio
utilfun <- function(stocku, bondu) {
  -sum(log(1 + stocku*retp$VTI + bondu*retp$IEF))
}  # end utilfun
# Create matrix of utility values
stocku <- seq(from=3, to=7, by=0.2)
bondu <- seq(from=12, to=20, by=0.2)
utilm <- sapply(bondu, function(y) sapply(stocku,
  function(x) utilfun(x, y)))
# Set rgl options and load package rgl
options(rgl.useNULL=TRUE)
library(rgl)
# Draw 3d surface plot of utility
rgl::persp3d(stocku, bondu, utilm, col="green",
        xlab="stocks", ylab="bonds", zlab="utility")
# Render the surface plot
rgl::rglwidget(elementId="plot3drgl")
# Save the surface plot to png file
rgl::rgl.snapshot("utility_surface.png")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/utility_surface.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly optimal stock and bond portfolio weights $stocku, bondu$ can be calculated by maximizing the utility $u$.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
weightv <- sapply(retp, function(x) mean(x)/var(x))
# Kelly weight for stocks
unlist(optimize(f=function(x) utilfun(x, bondu=0), interval=c(1, 4)))
# Kelly weight for bonds
unlist(optimize(f=function(x) utilfun(x, stocku=0), interval=c(1, 14)))
# Vectorized utility of stock and bond portfolio
utility_vec <- function(weights) {
  utilfun(weightv[1], weightv[2])
}  # end utility_vec
# Optimize with respect to vector argument
optiml <- optim(fn=utility_vec, par=c(3, 10),
                method="L-BFGS-B",
                upper=c(8, 20), lower=c(2, 5))
# Exact Kelly weights
optiml$par
      @
    \column{0.5\textwidth}
      The Kelly optimal weights can be calculated approximately by first calculating the individual stock and bond weights, and then multiplying them by the Kelly weight of the combined portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
retsport <- (retp %*% weightv)
drop(mean(retsport)/var(retsport))*weightv
# Exact Kelly weights
optiml$par
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice, the Kelly optimal weights under logarithmic utility are too aggressive and they require very active trading, so half-Kelly or even quarter-Kelly weights are used instead.
      <<echo=TRUE,eval=FALSE>>=
# Quarter-Kelly sub-optimal weights
weightv <- optiml$par/4
# Plot Kelly optimal portfolio
retp <- cbind(retp, weightv[1]*retp$VTI + weightv[2]*retp$IEF)
colnames(retp)[3] <- "Kelly_sub_optimal"
# Calculate the compounded wealth from returns
wealthv <- cumprod(1 + retp)
# Plot compounded wealth
dygraphs::dygraph(wealthv, main="Stock and Bond Portfolio") %>%
  dyOptions(colors=c("green", "blue", "green")) %>%
  dySeries("Kelly_sub_optimal", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_stocks_bonds.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly weights $k_f$ are calculated daily over a rolling look-back interval:
      \begin{displaymath}
        k_f = \frac{\bar{r_t}}{\sigma^2_t}
      \end{displaymath}
      \vskip1ex
      The distribution of the Kelly weights depends on the rolling returns $\bar{r_t}$ and variance $\sigma^2_t$.
      <<echo=TRUE,eval=FALSE>>=
retp <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
# Calculate the rolling returns and variance
look_back <- 200
var_rolling <- HighFreq::roll_var(retp, look_back)
weightv <- HighFreq::roll_sum(retp, look_back)/look_back
weightv <- weightv/var_rolling
weightv[1, ] <- 1/NCOL(weights)
weightv <- zoo::na.locf(weights)
sum(is.na(weights))
range(weights)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_distr.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the weights
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot(density(retp$IEF), t="l", lwd=3, col="red",
     xlab="weights", ylab="density",
     ylim=c(0, max(density(retp$VTI)$y)),
     main="Kelly Weight Distributions")
lines(density(retp$VTI), t="l", col="blue", lwd=3)
legend("topright", legend=c("VTI", "IEF"),
       inset=0.1, bg="white", lty=1, lwd=6, y.intersp=0.5, 
       col=c("blue", "red"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy For Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling Kelly strategy, the leverage of the risky asset $k_f$ changes over time.
      \vskip1ex
      The leverage is equal to the updated weight from the previous period.
      <<echo=TRUE,eval=FALSE>>=
# Scale and lag the Kelly weights
weightv <- lapply(weightv, function(x) 10*x/sum(abs(range(x))))
weightv <- do.call(cbind, weightv)
weightv <- rutils::lagit(weights)
# Calculate the compounded Kelly wealth and VTI
wealthv <- cbind(cumprod(1 + weightv$VTI*retp$VTI), cumprod(1 + retp$VTI))
colnames(wealthv) <- c("Kelly Strategy", "VTI")
dygraphs::dygraph(wealthv, main="VTI Strategy Using Rolling Kelly Weight") %>%
  dyAxis("y", label="Kelly Strategy", independentTicks=TRUE) %>%
  dyAxis("y2", label="VTI", independentTicks=TRUE) %>%
  dySeries(name="Kelly Strategy", axis="y", label="Kelly Strategy", strokeWidth=1, col="red") %>%
  dySeries(name="VTI", axis="y2", label="VTI", strokeWidth=1, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_roll_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy With Transaction Costs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{margin debt} $m_t$ is proportional to the wealth $w_t$: $m_t = (k_f - 1) w_t + 1$.
      \vskip1ex
      The dollar amount of the \emph{risky asset} traded is equal to the change in \emph{margin}, equal to: $\Delta m_t = \Delta [(k_f - 1) w_t]$.
      \vskip1ex
      If the transaction costs are large, then they will reduce the wealth and reduce the dollar amount of the \emph{risky asset} held by the investor.
      \vskip1ex
      The transaction costs depend on the change in wealth, and the wealth is decreased by the transaction costs.
      \vskip1ex
      So the transaction costs in each time period must be calculated recursively in a loop from the wealth in the past period.
      \vskip1ex
      If the transaction costs are much less than the change in wealth $c^r \ll \left| \Delta w_t \right|$, then they can be calculated approximately as the absolute value of the change in \emph{margin} $m_t^{nc}$ for a wealth path with no transaction costs:
      \begin{displaymath}
        c^r = \frac{\delta}{2} \left| \Delta m_t^{nc} \right|
      \end{displaymath}
    \column{0.5\textwidth}
      The transaction costs as a percentage of wealth are equal to: $c_t/w_t^{nc}$, where $w_t^{nc}$ is the wealth assuming no transaction costs.
      \vskip1ex
      The wealth of the Kelly Strategy after accounting for the \emph{bid-offer spread} is then equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_t - \frac{\delta}{2} \frac{\left| \Delta m_i^{nc} \right|}{w_i^{nc}})}
      \end{displaymath}
      The effect of the \emph{bid-offer spread} is to reduce the effective asset returns by an amount proportional to the \emph{bid-offer spread}.
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate the compounded Kelly wealth and margin
wealthv <- cumprod(1 + weightv$VTI*retp$VTI)
marginv <- (retp$VTI - 1)*wealthv + 1
# Calculate the transaction costs
costs <- bid_offer*drop(rutils::diffit(marginv))/2
wealth_diff <- drop(rutils::diffit(wealthv))
costs_rel <- ifelse(wealth_diff>0, costs/wealth_diff, 0)
range(costs_rel)
hist(costs_rel, breaks=10000, xlim=c(-0.02, 0.02))
# Scale and lag the transaction costs
costs <- rutils::lagit(abs(costs)/wealthv)
# ReCalculate the compounded Kelly wealth
wealth_trans <- cumprod(1 + retp$VTI*retp$VTI - costs)
# Plot compounded wealth
wealthv <- cbind(wealthv, wealth_trans)
colnames(wealthv) <- c("Kelly", "Including bid-offer")
dygraphs::dygraph(wealthv, main="Kelly Strategy With Transaction Costs") %>%
  dyOptions(colors=c("green", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy For Stocks and Bonds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling Kelly strategy, the leverage of the risky asset $k_f$ changes over time.
      \vskip1ex
      The leverage is equal to the updated weight from the previous period.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the compounded wealth from returns
wealthv <- cumprod(1 + rowSums(weightv*retp))
wealthv <- xts::xts(wealthv, zoo::index(retp))
quantmod::chart_Series(wealthv, name="Rolling Kelly Strategy For VTI and IEF")
# Calculate the compounded Kelly wealth and VTI
wealthv <- cbind(wealthv, cumprod(1 + 0.6*retp$IEF + 0.4*retp$VTI))
colnames(wealthv) <- c("Kelly Strategy", "VTI plus IEF")
dygraphs::dygraph(wealthv, main="Rolling Kelly Strategy For VTI and IEF") %>%
  dyAxis("y", label="Kelly Strategy", independentTicks=TRUE) %>%
  dyAxis("y2", label="VTI plus IEF", independentTicks=TRUE) %>%
  dySeries(name="Kelly Strategy", axis="y", label="Kelly Strategy", strokeWidth=1, col="red") %>%
  dySeries(name="VTI plus IEF", axis="y2", label="VTI plus IEF", strokeWidth=1, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/kelly_roll_vti_ief.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Risk and Performance Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PerformanceAnalytics} for Risk and Performance Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package 
      \href{https://cran.r-project.org/web/packages/PerformanceAnalytics/index.html}{\emph{PerformanceAnalytics}} 
      contains functions for calculating risk and performance statistics, such as the variance, skewness, kurtosis, beta, alpha, etc.
      \vskip1ex
      The function \texttt{data()} loads external data or listv data sets in a package.
      \vskip1ex
      \texttt{managers} is an \emph{xts} time series containing monthly percentage returns of six asset managers (HAM1 through HAM6), the EDHEC Long-Short Equity hedge fund index, the \texttt{S\&P 500}, and US Treasury 10-year bond and 3-month bill total returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package PerformanceAnalytics
library(PerformanceAnalytics)
# Get documentation for package PerformanceAnalytics
# Get short description
packageDescription("PerformanceAnalytics")
# Load help page
help(package="PerformanceAnalytics")
# List all objects in PerformanceAnalytics
ls("package:PerformanceAnalytics")
# List all datasets in PerformanceAnalytics
data(package="PerformanceAnalytics")
# Remove PerformanceAnalytics from search path
detach("package:PerformanceAnalytics")
      @
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
perf_data <- unclass(data(
    package="PerformanceAnalytics"))$results[, -(1:2)]
apply(perf_data, 1, paste, collapse=" - ")
# Load "managers" data set
data(managers)
class(managers)
dim(managers)
head(managers, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plots of Cumulative Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.CumReturns()} from package \emph{PerformanceAnalytics} plots the cumulative returns of a time series of returns.
      <<echo=TRUE,eval=FALSE>>=
# Load package "PerformanceAnalytics"
library(PerformanceAnalytics)
# Calculate ETF returns
retp <- rutils::etfenv$returns[, c("VTI", "DBC", "IEF")]
retp <- na.omit(retp)
# Plot cumulative ETF returns
x11(width=6, height=5)
chart.CumReturns(retp, lwd=2, ylab="",
  legend.loc="topleft", main="ETF Cumulative Returns")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_cum_returns.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Histogram()} from package \emph{PerformanceAnalytics} plots the histogram (frequency distribution) and the density of returns.
      <<echo=TRUE,eval=FALSE>>=
retp <- na.omit(rutils::etfenv$returns$VTI)
chart.Histogram(retp, xlim=c(-0.04, 0.04),
  colorset = c("lightgray", "red", "blue"), lwd=3,
  main=paste("Distribution of", colnames(retp), "Returns"),
  methods = c("add.density", "add.normal"))
legend("topright", inset=0.05, bty="n", y.intersp=0.4,
       leg=c("VTI Density", "Normal"),
       lwd=6, lty=1, col=c("red", "blue"))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/returns_histogram.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Boxplot()} from package \emph{PerformanceAnalytics} plots a box-and-whisker plot for a distribution of returns.
      \vskip1ex
      The function \texttt{chart.Boxplot()} is a wrapper and calls the function \texttt{graphics::boxplot()} to plot the box plots.
      \vskip1ex
      A \emph{box plot} (box-and-whisker plot) is a graphical display of a distribution of data: \\
      The \emph{box} represents the upper and lower quartiles, \\
      The vertical lines (whiskers) represent values beyond the quartiles, \\
      Open circles represent values beyond the nominal range (outliers).
      <<echo=TRUE,eval=FALSE>>=
retp <- rutils::etfenv$returns[,
  c("VTI", "IEF", "IVW", "VYM", "IWB", "DBC", "VXX")]
x11(width=6, height=5)
chart.Boxplot(names=FALSE, retp)
par(cex.lab=0.8, cex.axis=0.8)
axis(side=2, at=(1:NCOL(retp))/7.5-0.05,labels=colnames(retp))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_box_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation Estimator of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a nonparametric measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate normally distributed data
nrows <- 1000
datav <- rnorm(nrows)
sd(datav)
mad(datav)
median(abs(datav - median(datav)))
median(abs(datav - median(datav)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
bootd <- sapply(1:10000, function(x) {
  samplev <- datav[sample.int(nrows, replace=TRUE)]
  c(sd=sd(samplev), mad=mad(samplev))
})  # end sapply
bootd <- t(bootd)
# Analyze bootstrapped variance
head(bootd)
sum(is.na(bootd))
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
ncores <- detectCores() - 1  # Number of cores
cluster <- makeCluster(ncores)  # Initialize compute cluster
bootd <- parLapply(cluster, 1:10000,
  function(x, datav) {
    samplev <- datav[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, datav=datav)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
bootd <- mclapply(1:10000, function(x) {
    samplev <- datav[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, mc.cores=ncores)  # end mclapply
stopCluster(cluster)  # Stop R processes over cluster
bootd <- rutils::do_call(rbind, bootd)
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
retp <- na.omit(rutils::etfenv$returns$VTI)
nrows <- NROW(retp)
sd(retp)
mad(retp)
# Bootstrap of sd and mad estimators
bootd <- sapply(1:10000, function(x) {
  samplev <- retp[sample.int(nrows, replace=TRUE)]
  c(sd=sd(samplev), mad=mad(samplev))
})  # end sapply
bootd <- t(bootd)
# Means and standard errors from bootstrap
100*apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
ncores <- detectCores() - 1  # Number of cores
cluster <- makeCluster(ncores)  # Initialize compute cluster
clusterExport(cluster, c("nrows", "returns"))
bootd <- parLapply(cluster, 1:10000,
  function(x) {
    samplev <- retp[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  })  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
bootd <- mclapply(1:10000, function(x) {
    samplev <- retp[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, mc.cores=ncores)  # end mclapply
stopCluster(cluster)  # Stop R processes over cluster
bootd <- rutils::do_call(rbind, bootd)
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Downside Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some investors argue that positive returns don't represent risk, only those returns less than the target rate of return $r_t$.
      \vskip1ex
      The \emph{Downside Deviation} (semi-deviation) $\sigma_{d}$ is equal to the standard deviation of returns less than the target rate of return $r_t$:
      \begin{displaymath}
        \sigma_{d} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ([r_i-r_t]_{-})^2}
      \end{displaymath}
      The function \texttt{DownsideDeviation()} from package \emph{PerformanceAnalytics} calculates the downside deviation, for either the full time series (\texttt{method="full"}) or only for the subseries less than the target rate of return $r_t$ (\texttt{method="subset"}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
# Define target rate of return of 50 bps
targetr <- 0.005
# Calculate the full downside returns
returns_sub <- (retp - targetr)
returns_sub <- ifelse(returns_sub < 0, returns_sub, 0)
nrows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/nrows),
  drop(DownsideDeviation(retp, MAR=targetr, method="full")))
# Calculate the subset downside returns
returns_sub <- (retp - targetr)
returns_sub <- returns_sub[returns_sub < 0]
nrows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/nrows),
  drop(DownsideDeviation(retp, MAR=targetr, method="subset")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{drawdown} is the drop in prices from their historical peak, and is equal to the difference between the prices minus the cumulative maximum of the prices.
      \vskip1ex
      \emph{Drawdown risk} determines the risk of liquidation due to stop loss limits.
      <<echo=TRUE,eval=FALSE>>=
# Calculate time series of VTI drawdowns
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
drawdns <- (closep - cummax(closep))
# Extract the date index from the time series closep 
datev <- zoo::index(closep)
# Calculate the maximum drawdown date and depth
indexmin <- which.min(drawdns)
datemin <- datev[indexmin]
maxdd <- drawdns[datemin]
# Calculate the drawdown start and end dates
startd <- max(datev[(datev < datemin) & (drawdns == 0)])
endd <- min(datev[(datev > datemin) & (drawdns == 0)])
# dygraph plot of VTI drawdowns
datav <- cbind(closep, drawdns)
colnamev <- c("VTI", "Drawdowns")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="VTI Drawdowns") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], 
   valueRange=(1.2*range(drawdns)+0.1), independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red") %>%
  dyEvent(startd, "start drawdown", col="blue") %>%
  dyEvent(datemin, "max drawdown", col="red") %>%
  dyEvent(endd, "end drawdown", col="green")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/drawdown_plot.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot VTI drawdowns using package quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
x11(width=6, height=5)
quantmod::chart_Series(x=closep, name="VTI Drawdowns", theme=plot_theme)
xval <- match(startd, datev)
yval <- max(closep)
abline(v=xval, col="blue")
text(x=xval, y=0.95*yval, "start drawdown", col="blue", cex=0.9)
xval <- match(datemin, datev)
abline(v=xval, col="red")
text(x=xval, y=0.9*yval, "max drawdown", col="red", cex=0.9)
xval <- match(endd, datev)
abline(v=xval, col="green")
text(x=xval, y=0.85*yval, "end drawdown", col="green", cex=0.9)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk Using \texttt{PerformanceAnalytics::table.Drawdowns()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Drawdowns()} from package \emph{PerformanceAnalytics} calculates a data frame of drawdowns.
      <<echo=TRUE,eval=FALSE>>=
library(xtable)
library(PerformanceAnalytics)
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
retp <- rutils::diffit(closep)
# Calculate table of VTI drawdowns
tablev <- PerformanceAnalytics::table.Drawdowns(retp, geometric=FALSE)
# Convert dates to strings
tablev <- cbind(sapply(tablev[, 1:3], as.character), tablev[, 4:7])
# Print table of VTI drawdowns
print(xtable(tablev), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
      <<echo=FALSE,eval=TRUE,size="tiny",results='asis'>>=
library(xtable)
library(PerformanceAnalytics)
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
retp <- rutils::diffit(closep)
# Calculate table of VTI drawdowns
tablev <- PerformanceAnalytics::table.Drawdowns(retp, geometric=FALSE)
# Convert dates to strings
tablev <- cbind(sapply(tablev[, 1:3], as.character), tablev[, 4:7])
# Print table of VTI drawdowns
print(xtable(tablev), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{PerformanceSummary} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{charts.PerformanceSummary()} from package \emph{PerformanceAnalytics} plots three charts: cumulative returns, return bars, and drawdowns, for time series of returns.
      <<echo=(-(1:1)),eval=FALSE>>=
# Load "managers" data set
data(managers)
charts.PerformanceSummary(ham1,
  main="", lwd=2, ylog=TRUE)
      @
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.45\paperwidth]{figure/performance_summary-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Loss Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns has a long left tail of negative returns representing the risk of loss.
      \vskip1ex
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$.
      \vskip1ex
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retp <- na.omit(rutils::etfenv$returns$VTI)
confl <- 0.1
varisk <- quantile(retp, confl)
cvar <- mean(retp[retp <= varisk])
# Plot histogram of VTI returns
x11(width=6, height=5)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
histp <- hist(retp, col="lightgrey",
  xlab="returns", ylab="frequency", breaks=100,
  xlim=c(-0.05, 0.01), freq=FALSE, main="VTI Returns Histogram")
# Calculate density
densv <- density(retp, adjust=1.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_var.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot density
lines(densv, lwd=3, col="blue")
# Plot line for VaR
abline(v=varisk, col="red", lwd=3)
text(x=varisk, y=25, labels="VaR", lwd=2, pos=2)
# Plot polygon shading for CVaR
text(x=1.5*varisk, y=10, labels="CVaR", lwd=2, pos=2)
varmax <- -0.06
rangev <- (densv$x < varisk) &  (densv$x > varmax)
polygon(c(varmax, densv$x[rangev], varisk),
  c(0, densv$y[rangev], 0), col=rgb(1, 0, 0,0.5), border=NA)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk (\protect\emph{VaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$:
      \begin{displaymath}
        \alpha = \int_{-\infty}^{\mathrm{VaR}(\alpha)} \operatorname{f}(r) \, \mathrm{d}r
      \end{displaymath}
      Where $\operatorname{f}(r)$ is the probability density (distribution) of returns.
      \vskip1ex
      At a high confidence level, the value of $\mathrm{VaR}$ is subject to estimation error, and various numerical methods are used to approximate it.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      A simpler but less accurate way of calculating the quantile is by sorting and selecting the data closest to the quantile.
      \vskip1ex
      The function \texttt{VaR()} from package \emph{PerformanceAnalytics} calculates the \emph{Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retp <- na.omit(rutils::etfenv$returns$VTI)
nrows <- NROW(retp)
confl <- 0.05
# Calculate VaR approximately by sorting
sortv <- sort(as.numeric(retp))
cutoff <- round(confl*nrows)
varisk <- sortv[cutoff]
# Calculate VaR as quantile
varisk <- quantile(retp, probs=confl)
# PerformanceAnalytics VaR
PerformanceAnalytics::VaR(retp, p=(1-confl), method="historical")
all.equal(unname(varisk),
  as.numeric(PerformanceAnalytics::VaR(retp,
  p=(1-confl), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk (\protect\emph{CVaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the \emph{Expected Shortfall} (\emph{ES}), or the Expected Tail Loss (\emph{ETL}).
      \vskip1ex
      The function \texttt{ETL()} from package \emph{PerformanceAnalytics} calculates the \emph{Conditional Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VaR as quantile
varisk <- quantile(retp, confl)
# Calculate CVaR as expected loss
cvar <- mean(retp[retp <= varisk])
# PerformanceAnalytics VaR
PerformanceAnalytics::ETL(retp, p=(1-confl), method="historical")
all.equal(unname(cvar),
  as.numeric(PerformanceAnalytics::ETL(retp,
    p=(1-confl), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Return Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Stats()} from package \emph{PerformanceAnalytics} calculates a data frame of risk and return statistics of the return distributions.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the risk-return statistics
riskstats <-
  PerformanceAnalytics::table.Stats(rutils::etfenv$returns)
class(riskstats)
# Transpose the data frame
riskstats <- as.data.frame(t(riskstats))
# Add Name column
riskstats$Name <- rownames(riskstats)
# Add Sharpe ratio column
riskstats$"Arithmetic Mean" <- 
  sapply(rutils::etfenv$returns, mean, na.rm=TRUE)
riskstats$Sharpe <- 
  sqrt(252)*riskstats$"Arithmetic Mean"/riskstats$Stdev
# Sort on Sharpe ratio
riskstats <- riskstats[order(riskstats$Sharpe, decreasing=TRUE), ]
      @
    \column{0.5\textwidth}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Copy from rutils to save time
riskstats <- rutils::etfenv$riskstats
# Add Sharpe ratio column
# riskstats$Sharpe <- riskstats$"Arithmetic Mean"/riskstats$Stdev
# Sort on Sharpe ratio
riskstats <- riskstats[order(riskstats$Sharpe, decreasing=TRUE), ]
# Print data frame
knitr::kable(riskstats[, c("Sharpe", "Skewness", "Kurtosis")])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Risk and Return Preferences}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investors typically prefer larger \emph{odd moments} of the return distribution (mean, skewness), and smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      But positive skewness is often associated with lower returns, which can be observed in the \emph{VIX} volatility ETFs, \emph{VXX} and \emph{SVXY}.
      \vskip1ex
      The \emph{VXX} ETF is long the \emph{VIX} index (effectively long an option), so it has positive skewness and small kurtosis, but negative returns (it's short market risk).
      \vskip1ex
      Since the \emph{VXX} is effectively long an option, it pays option premiums so it has negative returns most of the time, with isolated periods of positive returns when markets drop.
      \vskip1ex
      The \emph{SVXY} ETF is short the \emph{VIX} index, so it has negative skewness and large kurtosis, but positive returns (it's long market risk).
      \vskip1ex
      Since the \emph{SVXY} is effectively short an option, it earns option premiums so it has positive returns most of the time, but it suffers sharp losses when markets drop.
    \column{0.5\textwidth}
    \vspace{1em}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Print data frame
knitr::kable(riskstats[c("VXX", "SVXY"), c("Sharpe", "Skewness", "Kurtosis")])
      @
      \includegraphics[width=0.45\paperwidth]{figure/vix_vxx_svxy.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of VXX versus SVXY
pricev <- na.omit(rutils::etfenv$prices[, c("VXX", "SVXY")])
pricev <- pricev["2017/"]
colnamev <- c("VXX", "SVXY")
colnames(pricev) <- colnamev
dygraphs::dygraph(pricev, main="Prices of VXX and SVXY") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="green") %>%
  dyLegend(show="always", width=300) %>% dyLegend(show="always", width=300) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness and Return Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Similarly to the \emph{VXX} and \emph{SVXY}, for most other ETFs positive skewness is often associated with lower returns.
      \vskip1ex
      Some of the exceptions are bond ETFs (like \emph{IEF}), which have both non-negative skewness and positive returns.
      \vskip1ex
      Another exception are commodity ETFs (like \emph{USO} oil), which have both negative skewness and negative returns.
      <<echo=TRUE,eval=FALSE>>=
# Remove VIX volatility ETF data
riskstats <- riskstats[-match(c("VXX", "SVXY"), riskstats$Name), ]
# Plot scatterplot of Sharpe vs Skewness
plot(Sharpe ~ Skewness, data=riskstats,
     ylim=1.1*range(riskstats$Sharpe),
     main="Sharpe vs Skewness")
# Add labels
text(x=riskstats$Skewness, y=riskstats$Sharpe,
          labels=riskstats$Name, pos=3, cex=0.8)
# Plot scatterplot of Kurtosis vs Skewness
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
plot(Kurtosis ~ Skewness, data=riskstats,
     ylim=c(1, max(riskstats$Kurtosis)),
     main="Kurtosis vs Skewness")
# Add labels
text(x=riskstats$Skewness, y=riskstats$Kurtosis,
          labels=riskstats$Name, pos=1, cex=0.5)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/etf_skew_sharp.png}
      % \includegraphics[width=0.45\paperwidth]{figure/etf_skew_kurtosis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Skewness and Return Tradeoff for ETFs and Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The ETFs or stocks can be sorted on their skewness to create high\_skew and low\_skew cohorts.
      \vskip1ex
      But the high\_skew cohort has better returns than the low\_skew cohort - contrary to the thesis that assets with positive skewness produce lower returns than those with a negative skewness.
      \vskip1ex
      The low and high volatility cohorts have very similar returns, contrary to expectations.  So do the low and high kurtosis cohorts.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
### Below is for ETFs
# Sort on Sharpe ratio
riskstats <- riskstats[order(riskstats$Skewness, decreasing=TRUE), ]
# Select high skew and low skew ETFs
cutoff <- (NROW(riskstats) %/% 2)
high_skew <- riskstats$Name[1:cutoff]
low_skew <- riskstats$Name[(cutoff+1):NROW(riskstats)]
# Calculate returns and log prices
retp <- rutils::etfenv$returns
retp <- zoo::na.locf(retp, na.rm=FALSE)
retp[is.na(retp)] <- 0
sum(is.na(retp))
high_skew <- rowMeans(retp[, high_skew])
low_skew <- rowMeans(retp[, low_skew])
wealthv <- cbind(high_skew, low_skew)
wealthv <- xts::xts(wealthv, zoo::index(retp))
wealthv <- cumsum(wealthv)
# dygraph plot of high skew and low skew ETFs
colnamev <- colnames(wealthv)
dygraphs::dygraph(wealthv, main="Log Wealth of Low and High Skew ETFs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="green") %>%
  dyLegend(show="always", width=300)

### Below is for S&P500 constituent stocks
# calc_mom() calculates the moments of returns
calc_mom <- function(retp, moment=3) {
  retp <- na.omit(retp)
  sum(((retp - mean(retp))/sd(retp))^moment)/NROW(retp)
}  # end calc_mom
# Calculate skew and kurtosis of VTI returns
calc_mom(retp, moment=3)
calc_mom(retp, moment=4)
# Load the S&P500 constituent stock returns
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
dim(retp)
sum(is.na(retp))
# retp <- retp["2000/"]
skews <- sapply(retp, calc_mom, moment=3)
# skews <- sapply(retp, calc_mom, moment=4)
# skews <- sapply(retp, sd, na.rm=TRUE)
skews <- sort(skews)
namesv <- names(skews)
nrows <- NROW(namesv)
# Select high skew and low skew ETFs
cutoff <- NROW(riskstats %/% 2)
low_skew <- namesv[1:cutoff]
high_skew <- namesv[(cutoff+1):nrows]

# low_skew <- namesv[1:50]
# Calculate returns and log prices
low_skew <- rowMeans(retp[, low_skew], na.rm=TRUE)
low_skew[1] <- 0
high_skew <- rowMeans(retp[, high_skew], na.rm=TRUE)
high_skew[1] <- 0
wealthv <- cbind(high_skew, low_skew)
wealthv <- xts::xts(wealthv, zoo::index(retp))
wealthv <- cumsum(wealthv)
# dygraph plot of high skew and low skew ETFs
colnamev <- colnames(wealthv)
dygraphs::dygraph(wealthv, main="Log Wealth of Low and High Skew Stocks") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="green") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Return Measures}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe ratio} $\mathrm{S_r}$ is equal to the excess returns (in excess of the risk-free rate $r_f$) divided by the standard deviation $\sigma$ of the returns:
      \begin{displaymath}
        \mathrm{S_r} = \frac{E[r-r_f]}{\sigma}
      \end{displaymath}
      The \emph{Sortino ratio} $\mathrm{{So}_r}$ is equal to the excess returns divided by the \emph{downside deviation} $\sigma_{d}$ (standard deviation of returns that are less than a target rate of return $r_t$):
      \begin{displaymath}
        \mathrm{{So}_r} = \frac{E[r-r_t]}{\sigma_{d}}
      \end{displaymath}
      The \emph{Calmar ratio} $\mathrm{C_r}$ is equal to the excess returns divided by the \emph{maximum drawdown} $\mathrm{DD}$ of the returns:
      \begin{displaymath}
        \mathrm{C_r} = \frac{E[r-r_f]}{\mathrm{DD}}
      \end{displaymath}
      The \emph{Dowd ratio} $\mathrm{D_r}$ is equal to the excess returns divided by the \emph{Value at Risk} ($\mathrm{VaR}$) of the returns:
      \begin{displaymath}
        \mathrm{D_r} = \frac{E[r-r_f]}{\mathrm{VaR}}
      \end{displaymath}
      The \emph{Conditional Dowd ratio} $\mathrm{{Dc}_r}$ is equal to the excess returns divided by the \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) of the returns:
      \begin{displaymath}
        \mathrm{{Dc}_r} = \frac{E[r-r_f]}{\mathrm{CVaR}}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
retp <- rutils::etfenv$returns[, c("VTI", "IEF")]
retp <- na.omit(retp)
# Calculate the Sharpe ratio
confl <- 0.05
PerformanceAnalytics::SharpeRatio(retp, p=(1-confl), 
  method="historical")
# Calculate the Sortino ratio
PerformanceAnalytics::SortinoRatio(retp)
# Calculate the Calmar ratio
PerformanceAnalytics::CalmarRatio(retp)
# Calculate the Dowd ratio
PerformanceAnalytics::SharpeRatio(retp, FUN="VaR", 
  p=(1-confl), method="historical")
# Calculate the Dowd ratio from scratch
varisk <- sapply(retp, quantile, probs=confl)
-sapply(retp, mean)/varisk
# Calculate the Conditional Dowd ratio
PerformanceAnalytics::SharpeRatio(retp, FUN="ES", 
  p=(1-confl), method="historical")
# Calculate the Conditional Dowd ratio from scratch
cvar <- sapply(retp, function(x) {
  mean(x[x < quantile(x, confl)])
})
-sapply(retp, mean)/cvar
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk of Aggregated Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stock returns aggregated over longer holding periods are closer to normally distributed, and their skewness, kurtosis, and tail risks are significantly lower than for daily returns.
      \vskip1ex
      Stocks become less risky over longer holding periods, so investors may choose to own a higher percentage of stocks, provided they hold them for a longer period of time.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI daily percentage returns
retp <- na.omit(rutils::etfenv$returns$VTI)
nrows <- NROW(retp)
# Bootstrap aggregated annual VTI returns
holdp <- 252
reta <- sqrt(holdp)*sapply(1:nrows, function(x) {
    mean(retp[sample.int(nrows, size=holdp, replace=TRUE)])
})  # end sapply
# Calculate mean, standard deviation, skewness, and kurtosis
datav <- cbind(retp, reta)
colnames(datav) <- c("VTI", "Agg")
sapply(datav, function(x) {
  # Standardize the returns
  meanv <- mean(x); stdev <- sd(x); x <- (x - meanv)/stdev
  c(mean=meanv, stdev=stdev, skew=mean(x^3), kurt=mean(x^4))
})  # end sapply
# Calculate the Sharpe and Dowd ratios
confl <- 0.02
ratiom <- sapply(datav, function(x) {
  stdev <- sd(x)
  varisk <- unname(quantile(x, probs=confl))
  cvar <- mean(x[x < varisk])
  mean(x)/c(Sharpe=stdev, Dowd=-varisk, DowdC=-cvar)
})  # end sapply
# Annualize the daily risk
ratiom[, 1] <- sqrt(252)*ratiom[, 1]
ratiom
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/risk_aggregated.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the densities of returns
plot(density(retp), t="l", lwd=3, col="blue",
     xlab="returns", ylab="density", xlim=c(-0.04, 0.04),
     main="Distribution of Aggregated Stock Returns")
lines(density(reta), t="l", col="red", lwd=3)
curve(expr=dnorm(x, mean=mean(reta), sd=sd(reta)), col="green", lwd=3, add=TRUE)
legend("topright", legend=c("VTI Daily", "Aggregated", "Normal"), y.intersp=0.4,
       inset=-0.1, bg="white", lty=1, lwd=6, col=c("blue", "red", "green"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Feature Engineering}


%%%%%%%%%%%%%%%
\subsection{draft: Feature Engineering}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Feature engineering derives predictive data elements (features) from a large input data set.
      \vskip1ex
      Feature engineering reduces the size of the input data set to a smaller set of features with the highest predictive power.
      \vskip1ex
      The predictive features are then used as inputs into machine learning models.
      \vskip1ex
      \emph{Out-of-sample} features only depend on past data, while \emph{in-sample} features depend both on past and future data.
      \vskip1ex
      A \emph{trailing} data filter is an example of an \emph{out-of-sample} feature.
      \vskip1ex
      A \emph{centered} data filter is an example of an \emph{in-sample} feature.
      \vskip1ex
      Out-of-sample features are used in forecasting and scrubbing real-time (live) data.
      \vskip1ex
      In-sample features are used in data labeling and scrubbing historical data.
      \vskip1ex
      \emph{Principal Component Analysis} (\emph{PCA}) is a \emph{dimension reduction} technique used in multivariate feature engineering.
      \vskip1ex
      Feature engineering can be developed using \emph{domain knowledge} and analytical techniques.
      \vskip1ex
      Some features indicate trend, for example the moving average asset returns.
      \vskip1ex
      Other features indicate turning points when prices are too too rich or too cheap.
      \vskip1ex
      Features indicating turning points are often quasi-stationary time series which oscillate around zero and correspond to the extreme tops and bottoms of prices.
      \vskip1ex
      the process of using of the data to create features that make machine learning algorithms work. 
      If feature engineering is done correctly, it increases the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process.
      \vskip1ex
      The \emph{data table} brackets \texttt{"[]"} operator can accept three arguments: \texttt{[i, j, by]}
      \begin{itemize}
        \item \texttt{i}: the row index to select,
        \item \texttt{j}: a list of columns or functions on columns,
        \item \texttt{by}: the columns of factors to aggregate over.
      \end{itemize}
      The \emph{data table} columns can be \emph{aggregated} over categories (factors) defined by one or more columns passed to the \texttt{"by"} argument.
      \vskip1ex
      The \texttt{"keyby"} argument is similar to \texttt{"by"}, but it sorts the output according to the categories used to group by.
      \vskip1ex
      Multiple \emph{data table} columns can be referenced by passing a list of names.
      \vskip1ex
      The dot \texttt{.()} operator is equivalent to the list function \texttt{list()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Number of flights from each airport
dtable[, .N, by=origin]
# Same, but add names to output
dtable[, .(flights=.N), by=.(airport=origin)]
# Number of AA flights from each airport
dtable[carrier=="AA", .(flights=.N),
           by=.(airport=origin)]
# Number of flights from each airport and airline
dtable[, .(flights=.N),
           by=.(airport=origin, airline=carrier)]
# Average aircraft_delay
dtable[, mean(aircraft_delay)]
# Average aircraft_delay from JFK
dtable[origin=="JFK", mean(aircraft_delay)]
# Average aircraft_delay from each airport
dtable[, .(delay=mean(aircraft_delay)),
           by=.(airport=origin)]
# Average and max delays from each airport and month
dtable[, .(mean_delay=mean(aircraft_delay), max_delay=max(aircraft_delay)),
           by=.(airport=origin, month=month)]
# Average and max delays from each airport and month
dtable[, .(mean_delay=mean(aircraft_delay), max_delay=max(aircraft_delay)),
           keyby=.(airport=origin, month=month)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Convolution Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a trailing linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_t$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_t = \varphi_1 r_{t-1} + \varphi_2 r_{t-2} + \ldots + \varphi_p r_{t-p}
      \end{displaymath}
      Where $f_t$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
      \vskip1ex
      \texttt{filter()} with \texttt{method="convolution"} calls the function \texttt{stats:::C\_cfilter()} to calculate the \emph{convolution}.
      \vskip1ex
      Convolution filtering can be performed even faster by directly calling the compiled function \texttt{stats:::C\_cfilter()}.
      \vskip1ex
      The function \texttt{HighFreq::roll\_conv()} calculates the \emph{weighted} trailing sum (convolution) even faster than \texttt{stats:::C\_cfilter()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract log VTI prices
ohlc <- log(rutils::etfenv$VTI)
closep <- quantmod::Cl(ohlc)
colnames(closep) <- "VTI"
nrows <- NROW(closep)
# Inspect the R code of the function filter()
filter
# Calculate EWMA weights
look_back <- 21
weightv <- exp(-0.1*1:look_back)
weightv <- weightv/sum(weights)
# Calculate convolution using filter()
pricef <- filter(closep, filter=weightv, method="convolution", sides=1)
# filter() returns time series of class "ts"
class(pricef)
# Get information about C_cfilter()
getAnywhere(C_cfilter)
# Filter using C_cfilter() over past values (sides=1).
priceff <- .Call(stats:::C_cfilter, closep, filter=weightv,
                     sides=1, circular=FALSE)
all.equal(as.numeric(pricef), priceff, check.attributes=FALSE)
# Calculate EWMA prices using HighFreq::roll_conv()
pricecpp <- HighFreq::roll_conv(closep, weightv=weightv)
all.equal(priceff[-(1:look_back)], as.numeric(pricecpp)[-(1:look_back)])
# Benchmark speed of trailing calculations
library(microbenchmark)
summary(microbenchmark(
  filter=filter(closep, filter=weightv, method="convolution", sides=1),
  priceff=.Call(stats:::C_cfilter, closep, filter=weightv, sides=1, circular=FALSE),
  rcpp=HighFreq::roll_conv(closep, weightv=weightv)
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} with \texttt{method="recursive"} calls the function \texttt{stats:::C\_rfilter()} to calculate the \emph{recursive filter} as follows:
      \begin{displaymath}
        r_t = \varphi_1 r_{t-1} + \varphi_2 r_{t-2} + \ldots + \varphi_p r_{t-p} + \xi_t
      \end{displaymath}
      Where $r_t$ is the filtered output vector, $\varphi_i$ are the filter coefficients, and $\xi_t$ are standard normal \emph{innovations}.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      The function \texttt{HighFreq::sim\_arima()} is very fast because it's written using the \texttt{C++} \emph{Armadillo} numerical library.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
nrows <- NROW(closep)
# Calculate ARIMA coefficients and innovations
coeff <- matrix(weights)/4
ncoeff <- NROW(coeff)
innov <- matrix(rnorm(nrows))
arimav <- filter(x=innov, filter=coeff, method="recursive")
# Get information about C_rfilter()
getAnywhere(C_rfilter)
# Filter using C_rfilter() compiled C++ function directly
arimafast <- .Call(stats:::C_rfilter, innov, coeff,
                    double(ncoeff + nrows))
all.equal(as.numeric(arimav), arimafast[-(1:ncoeff)], 
          check.attributes=FALSE)
# Filter using C++ code
arimacpp <- HighFreq::sim_ar(coeff, innov)
all.equal(arimafast[-(1:ncoeff)], drop(arimacpp))
# Benchmark speed of the three methods
summary(microbenchmark(
  filter=filter(x=innov, filter=coeff, method="recursive"),
  priceff=.Call(stats:::C_rfilter, innov, coeff, double(ncoeff + nrows)),
  Rcpp=HighFreq::sim_ar(coeff, innov)
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Data Smoothing and The Bias-Variance Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering through an averaging filter produces data \emph{smoothing}.
      \vskip1ex
      Smoothing real-time data with a trailing filter reduces its \emph{variance} but it increases its \emph{bias} because it introduces a time lag.
      \vskip1ex
      Smoothing historical data with a centered filter reduces its \emph{variance} but it introduces \emph{data snooping}.
      \vskip1ex
      In engineering, smoothing is called a \emph{low-pass filter}, since it eliminates high frequency signals, and it passes through low frequency signals.
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing EWMA prices using HighFreq::roll_conv()
look_back <- 21
weightv <- exp(-0.1*1:look_back)
weightv <- weightv/sum(weights)
pricef <- HighFreq::roll_conv(closep, weightv=weightv)
# Combine prices with smoothed prices
pricev <- cbind(closep, pricef)
colnames(pricev)[2] <- "VTI Smooth"
# Calculate standard deviations of returns
sapply(rutils::diffit(pricev), sd)
# Plot dygraph
dygraphs::dygraph(pricev["2009"], main="VTI Prices and Trailing Smoothed Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_smooth.png}
      <<echo=TRUE,eval=FALSE>>=
# Center the smoothed prices
pricef <- rutils::lagit(pricef, -(look_back %/% 2), pad_zeros=FALSE)
# Combine prices with smoothed prices
pricev <- cbind(closep, pricef)
colnames(pricev)[2] <- "VTI Smooth"
# Plot dygraph
dygraphs::dygraph(pricev["2009"], main="VTI Prices and Centered Smoothed Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Plotting Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<VTI_filter,eval=FALSE,fig.width=6,fig.height=5,fig.show='hide'>>=
library(rutils)  # Load package rutils
library(ggplot2)  # Load ggplot2
library(gridExtra)  # Load gridExtra
# Coerce to zoo and merge the time series
pricef <- cbind(closep, pricef)
colnames(pricef) <- c("VTI", "VTI filtered")
# Plot ggplot2
autoplot(pricef["2008/2010"],
    main="Filtered VTI", facets=NULL) +  # end autoplot
xlab("") + ylab("") +
theme(  # Modify plot theme
    legend.position=c(0.1, 0.5),
    plot.title=element_text(vjust=-2.0),
    plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
    plot.background=element_blank(),
    axis.text.y=element_blank()
    )  # end theme
# end ggplot2
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ggplot_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Smoothed Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Smoothing a time series of prices produces autocorrelations of their returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI log returns
retp <- rutils::diffit(closef)
# Open plot window
x11(width=6, height=7)
# Set plot parameters
par(oma=c(1, 1, 0, 1), mar=c(1, 1, 1, 1), mgp=c(0, 0.5, 0),
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Set two plot panels
par(mfrow=c(2,1))
# Plot ACF of VTI returns
rutils::plot_acf(retp[, 1], lag=10, xlab="")
title(main="ACF of VTI Returns", line=-1)
# Plot ACF of smoothed VTI returns
rutils::plot_acf(retp[, 2], lag=10, xlab="")
title(main="ACF of Smoothed VTI Returns", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_acf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{RSI} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Relative Strength Index} (\emph{RSI}) is defined as the weighted average of prices over a trailing interval:
      \begin{displaymath}
        p^{RSI}_t = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) p_{t-j}
      \end{displaymath}
      The decay parameter $\lambda$ determines the rate of decay of the \emph{RSI} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa,
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Get close prices and calculate close-to-close returns
# closep <- quantmod::Cl(rutils::etfenv$VTI)
closep <- quantmod::Cl(HighFreq::SPY)
colnames(closep) <- rutils::get_name(colnames(closep))
retspy <- TTR::ROC(closep)
retspy[1] <- 0
# Calculate the RSI indicator
r_si <- TTR::RSI(closep, 2)
# Calculate the long (up) and short (dn) signals
sig_up <- ifelse(r_si < 10, 1, 0)
sig_dn <- ifelse(r_si > 90, -1, 0)
# Lag signals by one period
sig_up <- rutils::lagit(sig_up, 1)
sig_dn <- rutils::lagit(sig_dn, 1)
# Replace NA signals with zero position
sig_up[is.na(sig_up)] <- 0
sig_dn[is.na(sig_dn)] <- 0
# Combine up and down signals into one
sig_nals <- sig_up + sig_dn
# Calculate cumulative returns
eq_up <- exp(cumsum(sig_up*retspy))
eq_dn <- exp(cumsum(-1*sig_dn*retspy))
eq_all <- exp(cumsum(sig_nals*retspy))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/rsi_indic.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot daily cumulative returns in panels
endd <- endpoints(retspy, on="days")
plot.zoo(cbind(eq_all, eq_up, eq_dn)[endd], lwd=c(2, 2, 2),
  ylab=c("Total","Long","Short"), col=c("red","green","blue"),
  main=paste("RSI(2) strategy for", colnames(closep), "from",
             format(start(retspy), "%B %Y"), "to",
             format(end(retspy), "%B %Y")))
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{EWMA Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a trailing interval:
      \begin{displaymath}
        p^{EWMA}_t = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j p_{t-j}
      \end{displaymath}
      The decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      \vskip1ex
      The function \texttt{HighFreq::roll\_wsum()} calculates the convolution of a time series with a vector of weights.
      <<echo=TRUE,eval=FALSE>>=
# Extract log VTI prices
ohlc <- rutils::etfenv$VTI
datev <- zoo::index(ohlc)
closep <- log(quantmod::Cl(ohlc))
colnames(closep) <- "VTI"
nrows <- NROW(closep)
# Calculate EWMA weights
look_back <- 111
lambda <- 0.9
weightv <- lambda^(1:look_back)
weightv <- weightv/sum(weights)
# Calculate EWMA prices as the convolution
ewmacpp <- HighFreq::roll_sumw(closep, weightv=weightv)
pricev <- cbind(closep, ewmacpp)
colnames(pricev) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ewma_feature.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main="VTI EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
# Standard plot of  EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
colorv <- c("blue", "red")
plot_theme$col$line.col <- colors
quantmod::chart_Series(pricev["2008/2009"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
legend("topleft", legend=colnames(pricev), y.intersp=0.4,
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive EWMA Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{EWMA} prices can be calculated recursively as follows:
      \begin{displaymath}
        p^{EWMA}_t = \lambda p^{EWMA}_{t-1} + (1-\lambda) p_t
      \end{displaymath}
      The decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      \vskip1ex
      The recursive \emph{EWMA} prices are slightly different from those calculated as a convolution, because the convolution uses a fixed look-back interval.
      \vskip1ex
      The compiled \texttt{C++} function \texttt{stats:::C\_rfilter()} calculates the EWMA prices recursively.
      \vskip1ex
      The function \texttt{HighFreq::run\_mean()} calculates the EWMA prices recursively using the \texttt{C++} \emph{Armadillo} numerical library.
      <<echo=TRUE,eval=FALSE>>=
# Calculate EWMA prices recursively using C++ code
ewmar <- .Call(stats:::C_rfilter, closep, lambda, c(as.numeric(closep[1])/(1-lambda), double(NROW(closep))))[-1]
# Or R code
# ewmar <- filter(closep, filter=lambda, init=as.numeric(closep[1, 1])/(1-lambda), method="recursive")
ewmar <- (1-lambda)*ewmar
# Calculate EWMA prices recursively using RcppArmadillo C++
ewmacpp <- HighFreq::run_mean(closep, lambda=lambda)
all.equal(drop(ewmacpp), ewmar)
# Compare the speed of C++ code with RcppArmadillo C++
library(microbenchmark)
summary(microbenchmark(
  filtercpp=HighFreq::run_mean(closep, lambda=lambda),
  rfilter=.Call(stats:::C_rfilter, closep, lambda, c(as.numeric(closep[1])/(1-lambda), double(NROW(closep)))),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ewma_recursive.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
pricev <- cbind(closep, ewmacpp)
colnames(pricev) <- c("VTI", "VTI EWMA")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main="Recursive VTI EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
# Standard plot of  EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
colorv <- c("blue", "red")
plot_theme$col$line.col <- colors
quantmod::chart_Series(pricev["2008/2009"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
legend("topleft", legend=colnames(pricev),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volume-Weighted Average Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes:
      \begin{displaymath}
        p^{VWAP}_t = \frac{\sum_{j=0}^{n} v_{t-j} p_{t-j}}{\sum_{j=0}^{n} v_{t-j}}
      \end{displaymath}
      The \emph{VWAP} applies more weight to prices with higher trading volumes, which allows it to react more quickly to recent market volatility.
      \vskip1ex
      The drawback of the \emph{VWAP} indicator is that it applies large weights to prices far in the past.
      \vskip1ex
      The \emph{VWAP} is often used as a technical indicator in trend following strategies.
      <<echo=TRUE,eval=FALSE>>=
# Calculate log OHLC prices and volumes
volumv <- quantmod::Vo(ohlc)
colnames(volumv) <- "Volume"
nrows <- NROW(closep)
# Calculate the VWAP prices
look_back <- 21
vwap <- HighFreq::roll_sum(closep, look_back=look_back, weightv=volumv)
colnames(vwap) <- "VWAP"
pricev <- cbind(closep, vwap)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_feature.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colorv <- c("blue", "red")
dygraphs::dygraph(pricev["2008/2009"], main="VTI VWAP Prices") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=300)
# Plot VWAP prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(pricev["2008/2009"], theme=plot_theme,
             lwd=2, name="VTI VWAP Prices")
legend("bottomright", legend=colnames(pricev),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive VWAP Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VWAP} prices $p^{VWAP}$ can also be calculated recursively as the ratio of the mean volume weighted prices $\bar{vp}$ divided by the mean trading volumes $\bar{v}$:
      \begin{flalign*}
        & \bar{v}_t = \lambda \bar{v}_{t-1} + (1-\lambda) v_t \\
        & \bar{vp}_t = \lambda \bar{vp}_{t-1} + (1-\lambda) v_t p_t \\
        & p^{VWAP} = \frac{\bar{vp}}{\bar{v}}
      \end{flalign*}
      The recursive \emph{VWAP} prices are slightly different from those calculated as a convolution, because the convolution uses a fixed look-back interval.
      \vskip1ex
      The advantage of the recursive \emph{VWAP} indicator is that it gradually "forgets" about large trading volumes far in the past.
      \vskip1ex
      The recursive formula is also much faster to calculate because it doesn't require a buffer of past data.
      \vskip1ex
      The compiled \texttt{C++} function \texttt{stats:::C\_rfilter()} calculates the trailing weighted values recursively.
      \vskip1ex
      The function \texttt{HighFreq::run\_mean()} also calculates the trailing weighted values recursively.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vwap_recursive.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VWAP prices recursively using C++ code
lambda <- 0.9
volumer <- .Call(stats:::C_rfilter, volumv, lambda, c(as.numeric(volumv[1])/(1-lambda), double(NROW(volumv))))[-1]
pricer <- .Call(stats:::C_rfilter, volumv*closep, lambda, c(as.numeric(volumv[1]*closep[1])/(1-lambda), double(NROW(closep))))[-1]
vwapr <- pricer/volumer
# Calculate VWAP prices recursively using RcppArmadillo C++
vwapcpp <- HighFreq::run_mean(closep, lambda=lambda, weightv=volumv)
all.equal(vwapr, drop(vwapcpp))
# Dygraphs plot the VWAP prices
pricev <- xts(cbind(vwap, vwapr), zoo::index(ohlc))
colnames(pricev) <- c("VWAP trailing", "VWAP recursive")
dygraphs::dygraph(pricev["2008/2009"], main="VWAP Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Smooth Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are calculated by filtering prices through a \emph{differencing} filter.
      \vskip1ex
      The simplest \emph{differencing} filter is the filter with coefficients $(1, -1)$: $r_t = p_t - p_{t-1}$.
      \vskip1ex
      Differencing is a \emph{high-pass filter}, since it eliminates low frequency signals, and it passes through high frequency signals.
      \vskip1ex
      An alternative measure of returns is the difference between two moving averages of prices:
      $r_t = p^{fast}_t - p^{slow}_t$
      \vskip1ex
      The difference between moving averages is a \emph{mid-pass filter}, since it eliminates both low and high frequency signals, and it passes through medium frequency signals.
      <<echo=TRUE,eval=FALSE>>=
# Calculate two EWMA prices
look_back <- 21
lambda <- 0.1
weightv <- exp(-lambda*1:look_back)
weightv <- weightv/sum(weights)
ewmaf <- HighFreq::roll_conv(closep, weightv=weightv)
lambda <- 0.05
weightv <- exp(-lambda*1:look_back)
weightv <- weightv/sum(weights)
ewmas <- HighFreq::roll_conv(closep, weightv=weightv)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_returns.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI prices
ewmadiff <- (ewmaf - ewmas)
pricev <- cbind(closep, ewmadiff)
symbol <- "VTI"
colnames(pricev) <- c(symbol, paste(symbol, "Returns"))
# Plot dygraph of VTI Returns
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main=paste(symbol, "EWMA Returns")) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fractional Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The lag operator $L$ applies a lag (time shift) to a time series: $L(p_t) = p_{t-1}$.
      \vskip1ex
      The simple returns can then be expressed as equal to the returns operator $(1 - L)$ applied to the prices: $r_t = (1 - L) p_t$.
      \vskip1ex
      The simple returns can be generalized to the fractional returns by raising the returns operator to some power $\delta < 1$:
      \begin{multline*}
        r_t = (1 - L)^\delta p_t = \\
        p_t - \delta L p_t + \frac{\delta(\delta-1)}{2!} L^2 p_t - \frac{\delta(\delta-1)(\delta-2)}{3!} L^3 p_t + \cdots = \\
        p_t - \delta p_{t-1} + \frac{\delta(\delta-1)}{2!} p_{t-2} - \frac{\delta(\delta-1)(\delta-2)}{3!} p_{t-3} + \cdots
      \end{multline*}
      The fractional returns provide a tradeoff between simple returns (which are range-bound but with no memory) and prices (which have memory but are not range-bound). 
      <<echo=TRUE,eval=FALSE>>=
# Calculate the fractional weights
deltav <- 0.1
weightv <- (deltav - 0:(look_back-2)) / 1:(look_back-1)
weightv <- (-1)^(1:(look_back-1))*cumprod(weights)
weightv <- c(1, weightv)
weightv <- (weightv - mean(weights))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracret.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the fractional VTI returns
retf <- HighFreq::roll_conv(closep, weightv=weightv)
pricev <- cbind(closep, retf)
symbol <- "VTI"
colnames(pricev) <- c(symbol, paste(symbol, "Returns"))
# Plot dygraph of VTI Returns
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008-08/2009-08"], main=paste(symbol, "Fractional Returns")) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_t = {\sum_{i=1}^t r_i}$.
      \vskip1ex
      Integrated processes typically have a \emph{unit root} (they have unlimited range), even if their underlying difference process does not have a \emph{unit root} (has limited range).
      \vskip1ex
      Asset returns don't have a \emph{unit root} (they have limited range) while prices have a \emph{unit root} (they have unlimited range).
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} \emph{ADF} test is designed to test the \emph{null hypothesis} that a time series has a \emph{unit root}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Perform ADF test for prices
tseries::adf.test(closep)
# Perform ADF test for returns
tseries::adf.test(retp)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Fractional Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The fractional returns for exponent values close to zero $\delta \approx 0$ resemble the asset price, while for values close to one $\delta \approx 1$ they resemble the standard returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate fractional VTI returns
deltav <- 0.1*c(1, 3, 5, 7, 9)
retfrac <- lapply(deltav, function(deltav) {
  weightv <- (deltav - 0:(look_back-2)) / 1:(look_back-1)
  weightv <- c(1, (-1)^(1:(look_back-1))*cumprod(weights))
  weightv <- (weightv - mean(weights))
  HighFreq::roll_conv(closep, weightv=weightv)
})  # end lapply
retfrac <- do.call(cbind, retfrac)
retfrac <- cbind(closep, retfrac)
colnames(retfrac) <- c("VTI", paste0("frac_", deltav))
# Calculate ADF test statistics
adfstats <- sapply(retfrac, function(x)
  suppressWarnings(tseries::adf.test(x)$statistic)
)  # end sapply
names(adfstats) <- colnames(retfrac)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracrets.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of fractional VTI returns
colorv <- colorRampPalette(c("blue", "red"))(NCOL(retfrac))
colnamev <- colnames(retfrac)
dyplot <- dygraphs::dygraph(retfrac["2008-08/2009-08"], main="Fractional Returns") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col=colorv[1])
for (i in 2:NROW(colnamev))
  dyplot <- dyplot %>%
  dyAxis("y2", label=colnamev[i], independentTicks=TRUE) %>%
  dySeries(name=colnamev[i], axis="y2", label=colnamev[i], strokeWidth=2, col=colorv[i])
dyplot <- dyplot %>% dyLegend(width=300)
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Volume Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing \emph{volume z-score} is equal to the volume $v_t$ minus the trailing average volumes $\bar{v_t}$ divided by the volatility of the volumes $\sigma_t$:
      \begin{displaymath}
        z_t = \frac{v_t - \bar{v_t}}{\sigma_t}
      \end{displaymath}
      Trading volumes are typically higher when prices drop and they are also positively correlated with the return volatility.
      \vskip1ex
      The volume z-scores represent the first derivative (slope) of the volumes, since the volume level is subtracted.
      \vskip1ex
      The volume z-scores are positively skewed because returns are negatively skewed.
      <<echo=TRUE,eval=FALSE>>=
# Calculate volume z-scores
volumv <- quantmod::Vo(ohlc)
look_back <- 21
volumean <- HighFreq::roll_mean(volumv, look_back=look_back)
volumsd <- sqrt(HighFreq::roll_var(rutils::diffit(volumv), look_back=look_back))
volumsd[1] <- 0
volumz <- ifelse(volumsd > 0, (volumv - volumean)/volumsd, 0)
# Plot histogram of volume z-scores
hist(volumz, breaks=1e2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volume_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of volume z-scores of VTI prices
pricev <- cbind(closep, volumz)
colnames(pricev) <- c("VTI", "Z-Scores")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main="VTI Volume Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{true range} is the difference between low and high prices is a proxy for the spot volatility in a bar of data.
      \vskip1ex
      The \emph{volatility z-score} is equal to the spot volatility $v_t$ minus the trailing average volatility $\bar{v_t}$ divided by the standard deviation of the volatility $\sigma_t$:
      \begin{displaymath}
        z_t = \frac{v_t - \bar{v_t}}{\sigma_t}
      \end{displaymath}
      Volatility is typically higher when prices drop and it's also positively correlated with the trading volumes.
      \vskip1ex
      The volatility z-scores represent the first derivative (slope) of the volatilities, since the volatility level is subtracted.
      \vskip1ex
      The volatility z-scores are positively skewed because returns are negatively skewed.
      <<echo=TRUE,eval=FALSE>>=
# Calculate volatility (true range) z-scores
volat <- log(quantmod::Hi(ohlc) - quantmod::Lo(ohlc))
look_back <- 21
volatm <- HighFreq::roll_mean(volat, look_back=look_back)
volat <- (volat - volatm)
volatsd <- sqrt(HighFreq::roll_var(rutils::diffit(volat), look_back=look_back))
volatsd[1] <- 0
volatz <- ifelse(volatsd > 0, volat/volatsd, 0)
# Plot histogram of the volatility z-scores
hist(volatz, breaks=1e2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volat_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot scatterplot of volume and volatility z-scores
plot(as.numeric(volatz), as.numeric(volumz),
     xlab="volatility z-score", ylab="volume z-score")
regmod <- lm(volatz ~ volumz)
abline(regmod, col="red", lwd=3)
# Plot dygraph of VTI volatility z-scores
pricev <- cbind(closep, volatz)
colnames(pricev) <- c("VTI", "Z-Scores")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main="VTI Volatility Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing Volatility Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{volatility z-score} can also be defined as the difference between the fast $v^{f}_t$ minus the slow $v^{s}_t$ trailing volatilities, divided by the standard deviation of the volatility $\sigma_t$:
      \begin{flalign*}
        & z_t = \frac{v^{f}_t - v^{s}_t}{\sigma_t} \\
      \end{flalign*}
      The function \texttt{HighFreq::run\_var()} calculates the trailing variance of a \emph{time series} of returns, by recursively weighting the past variance estimates $\sigma^2_{t-1}$, with the squared differences of the returns minus the trailing means $(r_t - \bar{r}_t)^2$, using the weight decay factor $\lambda$:
      \begin{flalign*}
        & \bar{r}_t = \lambda \bar{r}_{t-1} + (1-\lambda) r_t \\
        & \sigma^2_t = \lambda \sigma^2_{t-1} + (1-\lambda) (r_t - \bar{r}_t)^2
      \end{flalign*}
      The parameter $\lambda$ determines the rate of decay of the weight of past values.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the recursive trailing VTI volatility
lambdaf <- 0.8
lambdas <- 0.81
volatf <- sqrt(HighFreq::run_var(retp, lambda=lambdaf))
volats <- sqrt(HighFreq::run_var(retp, lambda=lambdas))
# Calculate the recursive trailing z-scores of VTI volatility
volatd <- volatf - volats
volatsd <- sqrt(HighFreq::run_var(rutils::diffit(volatd), lambda=lambdaf))
volatsd[1] <- 0
volatz <- ifelse(volatsd > 0, volatd/volatsd, 0)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volat_zscores_rec.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of the volatility z-scores
hist(volatz, breaks=1e2)
# Plot scatterplot of volume and volatility z-scores
plot(as.numeric(volatz), as.numeric(volumz),
     xlab="volatility z-score", ylab="volume z-score")
regmod <- lm(volatz ~ volumz)
abline(regmod, col="red", lwd=3)
# Plot dygraph of VTI volatility z-scores
pricev <- cbind(closep, volatz)
colnames(pricev) <- c("VTI", "Z-Scores")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2008/2009"], main="VTI Online Volatility Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Centered Price Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An extreme local price is a price which differs significantly from neighboring prices.
      \vskip1ex
      Extreme prices can be identified in-sample using the centered \emph{price z-score} equal to the price difference with neighboring prices divided by the volatility of returns $\sigma_t$:
      \begin{displaymath}
        z_t = \frac{p_t - 0.5 (p_{t-k} - p_{t+k})}{\sigma_t}
      \end{displaymath}
      Where $p_{t-k}$ and $p_{t+k}$ are the lagged and advanced prices.
      \vskip1ex
      The lag parameter $k$ is the interval for calculating the volatility of returns $\sigma_t$.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the centered volatility
look_back <- 21
half_back <- look_back %/% 2
volat <- HighFreq::roll_var(closep, look_back=look_back)
volat <- sqrt(volat)
volat <- rutils::lagit(volat, lagg=(-half_back))
# Calculate the z-scores of prices
pricez <- (closep - 
  0.5*(rutils::lagit(closep, half_back, pad_zeros=FALSE) + 
  rutils::lagit(closep, -half_back, pad_zeros=FALSE)))
pricez <- ifelse(volat > 0, pricez/volat, 0)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_price_zscores_centered.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of z-scores of VTI prices
pricev <- cbind(closep, pricez)
colnames(pricev) <- c("VTI", "Z-Scores")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2009"], main="VTI Centered Price Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Labeling the Tops and Bottoms of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The local tops and bottoms of prices can be labeled approximately in-sample using the z-scores of prices and threshold values.
      \vskip1ex
      The local tops of prices represent \emph{overbought} conditions, while the bottoms represent \emph{oversold} conditions.
      \vskip1ex
      The labeled data can be used as a response or target variable in machine learning classifier models.
      \vskip1ex
      But it's not feasible to classify the prices out-of-sample exactly according to their in-sample labels.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the thresholds for labeling tops and bottoms
confl <- c(0.2, 0.8)
threshv <- quantile(pricez, confl)
# Calculate the vectors of tops and bottoms
tops <- zoo::coredata(pricez > threshv[2])
bottoms <- zoo::coredata(pricez < threshv[1])
# Simulate in-sample VTI strategy
posv <- rep(NA_integer_, nrows)
posv[1] <- 0
posv[tops] <- (-1)
posv[bottoms] <- 1
posv <- zoo::na.locf(posv)
posv <- rutils::lagit(posv)
pnls <- retp*posv
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_topsbottoms_insample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
wealthv <- cbind(retp, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd],
  main="Price Tops and Bottoms Strategy In-sample") %>%
  dyAxis("y", label="VTI", independentTicks=TRUE) %>%
  dyAxis("y2", label="Strategy", independentTicks=TRUE) %>%
  dySeries(name="VTI", axis="y", label="VTI", strokeWidth=2, col="blue") %>%
  dySeries(name="Strategy", axis="y2", label="Strategy", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing Price Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing price z-score is equal to the difference between the current price $p_t$ minus the trailing average price $\bar{p}_{t-k}$, divided by the volatility of the price $\sigma_t$:
      \begin{displaymath}
        z_t = \frac{p_t - \bar{p}_{t-k}}{\sigma_t}
      \end{displaymath}
      The lag parameter $k$ is the look-back interval for calculating the volatility of returns $\sigma_t$.
      \vskip1ex
      The trailing price z-scores represent the first derivative (slope) of the prices, since the price level is subtracted.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the trailing VTI volatility
volat <- HighFreq::roll_var(closep, look_back=look_back)
volat <- sqrt(volat)
# Calculate the trailing z-scores of VTI prices
pricez <- (closep - rutils::lagit(closep, look_back, pad_zeros=FALSE))
pricez <- ifelse(volat > 0, pricez/volat, 0)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_price_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of the trailing z-scores of VTI prices
pricev <- cbind(closep, pricez)
colnames(pricev) <- c("VTI", "Z-Scores")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2009"], 
  main="VTI Trailing Price Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Trailing Price Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The recursive trailing price z-score is equal to the difference between the current price $p_t$ minus the trailing average price $\bar{p}$, divided by the volatility of the price $\sigma_t$:
      \begin{flalign*}
        & \bar{p}_t = \lambda \bar{p}_{t-1} + (1-\lambda) p_t \\
        & \sigma^2_t = \lambda \sigma^2_{t-1} + (1-\lambda) (p_t - \bar{p}_t)^2 \\
        & z_t = \frac{p_t - \bar{p}_t}{\sigma_t}
      \end{flalign*}
      The parameter $\lambda$ determines the rate of decay of the weight of past prices.
      If $\lambda$ is close to \texttt{1} then the decay is weak and past prices have a greater weight, and the trailing mean values have a stronger dependence on past prices.  This is equivalent to a long look-back interval.
      And vice versa if $\lambda$ is close to \texttt{0}.
      \vskip1ex
      The functions \texttt{HighFreq::run\_mean()} and \texttt{HighFreq::run\_var()} calculate the trailing mean and variance by recursively updating the past estimates with the new values, using the weight decay factor $\lambda$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_price_zscores_rec.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the recursive trailing VTI volatility
lambda <- 0.9
volat <- HighFreq::run_var(closep, lambda=lambda)
volat <- sqrt(volat)
# Calculate the recursive trailing z-scores of VTI prices
pricer <- (closep - HighFreq::run_mean(closep, lambda=lambda))
pricer <- ifelse(volat > 0, pricer/volat, 0)
# Plot dygraph of the trailing z-scores of VTI prices
pricev <- xts::xts(cbind(pricez, pricer), datev)
colnames(pricev) <- c("Z-Scores", "Recursive")
colnamev <- colnames(pricev)
dygraphs::dygraph(pricev["2009"], main="VTI Online Trailing Price Z-Scores") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>% 
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing Regression Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      We can define the trailing \emph{z-score} $z_t$ of the stock price $p_t$ as the \emph{standardized residual} of the linear regression with respect to a predictor variable (for example the time $t_i$):
      \begin{flalign*}
        & z_t = \frac{p_t - p^{fit}_t}{\sigma_t} \\
        & p^{fit}_t = \alpha_t + \beta_t t_i
      \end{flalign*}
      Where $p^{fit}_t$ are the fitted values, $\alpha_t$ and $\beta_t$ are the \emph{regression coefficients}, and $\sigma_t$ is the standard deviation of the residuals.
      \vskip1ex
      The regression z-scores represent the second derivative (curvature) of the stock prices, since the price level and slope are subtracted.
      \vskip1ex
      The regression z-scores can be used as a rich or cheap indicator, either relative to past prices, or relative to prices in a stock pair.
      \vskip1ex
      The regression residuals must be calculated in a loop, so it's much faster to calculate them using functions written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{HighFreq::roll\_reg()} calculates trailing regressions and their residuals.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_regr_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing price regression z-scores
datev <- matrix(zoo::index(closep))
look_back <- 21
# Create a default list of regression parameters
controlv <- HighFreq::param_reg()
regs <- HighFreq::roll_reg(respv=closep, predm=datev, 
   look_back=look_back, controlv=controlv)
regs[1:look_back, ] <- 0
# Plot dygraph of z-scores of VTI prices
datav <- cbind(closep, regs[, NCOL(regs)])
colnames(datav) <- c("VTI", "Z-Scores")
colnamev <- colnames(datav)
dygraphs::dygraph(datav["2009"], main="VTI Regression Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Trailing Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing regressions of the stock price $p_t$ with respect to the predictor (explanatory) variables $X_t$ are defined by:
      \begin{displaymath}
        p_t = \beta_t X_t + \epsilon_t
      \end{displaymath}
      The trailing regression coefficients $\beta_t$ and the residuals $\epsilon_t$ can be calculated as:
      \begin{flalign*}
        & \beta_t = \operatorname{cov}^{-1}_{X t} \operatorname{cov}_t \\
        & \epsilon_t = r_t - \beta_t p_t
      \end{flalign*}
      Where $\operatorname{cov}_t$ is the covariance matrix between the response $p_t$ and the predictor $X_t$ variables, and $\operatorname{cov}_{X t}$ is the covariance matrix between the predictors. 
      \vskip1ex
      The covariance matrices are updated using the following recursive (online) formulas:
      \begin{flalign*}
        & \operatorname{cov}_t = \lambda \operatorname{cov}_{t-1} + (1-\lambda) p^T_t X_t \\
        & \operatorname{cov}_{X t} = \lambda \operatorname{cov}_{X (t-1)} + (1-\lambda) X^T_t X_t
      \end{flalign*}
      The function \texttt{HighFreq::run\_reg()} recursively calculates trailing regressions and their residuals.
      <<echo=TRUE,eval=FALSE>>=
# Calculate recursive trailing price regression versus time
lambda <- 0.9
# Create a list of regression parameters
controlv <- HighFreq::param_reg(residscale="standardize")
regs <- HighFreq::run_reg(closep, matrix(datev), lambda=lambda, controlv=controlv)
colnames(regs) <- c("alpha", "beta", "zscores")
tail(regs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_regr_betas_rec.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of regression betas
datav <- cbind(closep, 252*regs[, "beta"])
colnames(datav) <- c("VTI", "Slope")
colnamev <- colnames(datav)
dygraphs::dygraph(datav["2009"], main="VTI Online Regression Slope") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Trailing Regression Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The recursive trailing \emph{z-score} $z_t$ of the stock price $p_t$ is equal to the standardized residual $\epsilon_t$:
      \begin{flalign*}
        & \epsilon_t = \lambda \epsilon_{t-1} + (1-\lambda) (p_t - \beta_t p_t) \\
        & \bar{\epsilon}_t = \lambda \bar{\epsilon}_{t-1} + (1-\lambda) \epsilon_t \\
        & \varsigma^2_t = \lambda \varsigma^2_{t-1} + (1-\lambda) (\epsilon_t - \bar{\epsilon}_t)^2 \\
        & z_t = \frac{\epsilon_t}{\varsigma_t}
      \end{flalign*}
      Where $\varsigma^2_t$ is the variance of the residuals $\epsilon_t$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_regr_zscores_rec.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of z-scores of VTI prices
datav <- cbind(closep, regs[, "zscores"])
colnames(datav) <- c("VTI", "Z-Scores")
colnamev <- colnames(datav)
dygraphs::dygraph(datav["2009"], main="VTI Online Regression Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Hampel Filter}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a nonparametric measure of dispersion (variability):
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(p_t - \operatorname{median}(\mathbf{p})))
      \end{displaymath}
      The \emph{Hampel filter} is effective in detecting outliers in the data because it uses the nonparametric \emph{MAD} dispersion measure.
      \vskip1ex
      The \emph{Hampel z-score} is equal to the deviation from the median divided by the \emph{MAD}:
      \begin{displaymath}
        z_i = \frac{p_t - \operatorname{median}(\mathbf{p})}{\operatorname{MAD}}
      \end{displaymath}
      A time series of \emph{z-scores} over past data can be calculated using a trailing look-back window.
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
# Define look-back window
look_back <- 11
# Calculate time series of trailing medians
medianv <- roll::roll_median(closep, width=look_back)
# medianv <- TTR::runMedian(closep, n=look_back)
# Calculate time series of MAD
madv <- HighFreq::roll_var(closep, look_back=look_back, method="nonparametric")
# madv <- TTR::runMAD(closep, n=look_back)
# Calculate time series of z-scores
zscores <- (closep - medianv)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hampel_zscores.png}\\
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
x11(width=6, height=5)
# Plot the prices and medians
dygraphs::dygraph(cbind(closep, medianv), main="VTI median") %>%
  dyOptions(colors=c("black", "red")) %>%
  dyLegend(show="always", width=300)
# Plot histogram of z-scores
histp <- hist(zscores, col="lightgrey",
  xlab="z-scores", breaks=50, xlim=c(-4, 4),
  ylab="frequency", freq=FALSE, main="Hampel Z-Scores histogram")
lines(density(zscores, adjust=1.5), lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{One-sided and Two-sided Data Filters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filters calculated over past data are referred to as \emph{one-sided} filters, and they are appropriate for filtering real-time data.
      \vskip1ex
      Filters calculated over both past and future data are called \emph{two-sided} (centered) filters, and they are appropriate for filtering historical data.
      \vskip1ex
      The function \texttt{HighFreq::roll\_var()} with parameter \texttt{method="nonparametric"} calculates the trailing \emph{MAD} using a look-back interval over past data.
      \vskip1ex
      The functions \texttt{TTR::runMedian()} and \texttt{TTR::runMAD()} calculate the trailing medians and \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      If the trailing medians and \emph{MAD} are advanced (shifted backward) in time, then they are calculated over both past and future data (centered).
      \vskip1ex
      The function \texttt{rutils::lag\_it()} with a negative \texttt{lagg} parameter value advances (shifts back) future data points to the present.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate one-sided Hampel z-scores
medianv <- roll::roll_median(closep, width=look_back)
# medianv <- TTR::runMedian(closep, n=look_back)
madv <- HighFreq::roll_var(closep, look_back=look_back, method="nonparametric")
# madv <- TTR::runMAD(closep, n=look_back)
zscores <- (closep - medianv)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
# Calculate two-sided Hampel z-scores
half_back <- look_back %/% 2
medianv <- rutils::lagit(medianv, lagg=(-half_back))
madv <- rutils::lagit(madv, lagg=(-half_back))
zscores <- (closep - medianv)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Modeling Volatility and Skewness}


%%%%%%%%%%%%%%%
\subsection{Calculating the Trailing Variance of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The variance of asset returns exhibits \emph{heteroskedasticity}, i.e. it changes over time.
      \vskip1ex
      The trailing variance of returns is given by:
      \begin{flalign*}
        \sigma^2_t &= \frac{1}{k-1} \sum_{j=0}^{k-1} (r_{t-j}-\bar{r_t})^2 \\
        \bar{r_t} &= \frac{1}{k}{\sum_{j=0}^{k-1} r_{t-j}}
      \end{flalign*}
      Where \texttt{k} is the \emph{look-back interval} equal to the number of data points
      for performing aggregations over the past.
      \vskip1ex
      It's also possible to calculate the trailing variance in \texttt{R} using vectorized functions, without using an \texttt{apply()} loop.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retp <- na.omit(rutils::etfenv$returns$VTI)
nrows <- NROW(retp)
# Define end points
endd <- 1:NROW(retp)
# Start points are multi-period lag of endd
look_back <- 11
startp <- c(rep_len(0, look_back-1), endd[1:(nrows-look_back+1)])
# Calculate trailing variance in sapply() loop - takes long
varv <- sapply(1:nrows, function(it) {
  retp <- retp[startp[it]:endd[it]]
  sum((retp - mean(retp))^2)/look_back
})  # end sapply
# Use only vectorized functions
retc <- cumsum(retp)
retc <- (retc - c(rep_len(0, look_back), retc[1:(nrows-look_back)]))
retc2 <- cumsum(retp^2)
retc2 <- (retc2 - c(rep_len(0, look_back), retc2[1:(nrows-look_back)]))
var2 <- (retc2 - retc^2/look_back)/look_back
all.equal(varv[-(1:look_back)], as.numeric(var2)[-(1:look_back)])
# Or using package rutils
retc <- rutils::roll_sum(retp, look_back=look_back)
retc2 <- rutils::roll_sum(retp^2, look_back=look_back)
var2 <- (retc2 - retc^2/look_back)/look_back
# Coerce variance into xts
tail(varv)
class(varv)
varv <- xts(varv, order.by=zoo::index(retp))
colnames(varv) <- "VTI.variance"
head(varv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Trailing Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package
      \href{https://cran.r-project.org/web/packages/roll/index.html}{\color{blue}{\emph{roll}}}
      contains functions for calculating \emph{weighted} trailing aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for the \emph{weighted} trailing sum,
        \item \texttt{roll\_var()} for the \emph{weighted} trailing variance,
        \item \texttt{roll\_scale()} for the trailing scaling and centering of time series,
        \item \texttt{roll\_pcr()} for the trailing principal component regressions of time series.
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages 
      \href{https://cran.r-project.org/web/packages/Rcpp/index.html}{\color{blue}{\emph{Rcpp}}},
      \href{https://cran.r-project.org/web/packages/RcppArmadillo/index.html}{\color{blue}{\emph{RcppArmadillo}}},
      and 
      \href{https://cran.r-project.org/web/packages/RcppParallel/index.html}{\color{blue}{\emph{RcppParallel}}}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing VTI variance using package HighFreq
varv <- roll::roll_var(retp, width=look_back)
colnames(varv) <- "Variance"
head(varv)
sum(is.na(varv))
varv[1:(look_back-1)] <- 0
# Benchmark calculation of trailing variance
library(microbenchmark)
summary(microbenchmark(
  sapply=sapply(1:nrows, function(it) {
    var(retp[startp[it]:endd[it]])
  }),
  roll=roll::roll_var(retp, width=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing \protect\emph{EWMA} Realized Volatility Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time-varying volatility can be more accurately estimated using an \emph{Exponentially Weighted Moving Average} (\emph{EWMA}) variance estimator.
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      \begin{displaymath}
        \sigma^2_t = \lambda \sigma^2_{t-1} + (1-\lambda) r^2_t = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j r^2_{t-j}
      \end{displaymath}
      $\sigma^2_t$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period \texttt{i} and the past \emph{realized} variance.
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa.
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} calculates the convolution of a vector or a time series with a filter of coefficients (weights).
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} is very fast because it's compiled \texttt{C++} code.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_vti_ewma.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate EWMA VTI variance using compiled C++ function
look_back <- 51
weightv <- exp(-0.1*1:look_back)
weightv <- weightv/sum(weights)
varv <- .Call(stats:::C_cfilter, retp^2, filter=weightv, sides=1, circular=FALSE)
varv[1:(look_back-1)] <- varv[look_back]
# Plot EWMA volatility
varv <- xts:::xts(sqrt(varv), order.by=zoo::index(retp))
dygraphs::dygraph(varv, main="VTI EWMA Volatility") %>%
  dyOptions(colors="blue") %>% dyLegend(show="always", width=300)
quantmod::chart_Series(xtsv, name="VTI EWMA Volatility")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating \protect\emph{EWMA} Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the \emph{time series} has non-zero \emph{expected} mean, then the trailing \emph{EWMA} variance is a vector given by the estimator:
      \begin{flalign*}
        \sigma^2_t &= \frac{1}{k-1} \sum_{j=0}^{k-1} {w_j (r_{t-j}-\bar{r_t})^2} \\
        \bar{r_t} &= \frac{1}{k}{\sum_{j=0}^{k-1} {w_j r_{t-j}}}
      \end{flalign*}
      Where $w_j$ is the vector of exponentially decaying weights:
      \begin{displaymath}
        w_j = \frac{\lambda^j}{\sum_{j=0}^{k-1} \lambda^j}
      \end{displaymath}
      The function \texttt{roll\_var()} from package \emph{roll} calculates the trailing \emph{EWMA} variance.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing VTI variance using package roll
library(roll)  # Load roll
varv <- roll::roll_var(retp, weights=rev(weights), width=look_back)
colnames(varv) <- "VTI.variance"
class(varv)
head(varv)
sum(is.na(varv))
varv[1:(look_back-1)] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing Realized Volatility Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{HighFreq::run\_var()} calculates the trailing variance of a \emph{time series} of returns, by recursively weighting the past variance estimates $\sigma^2_{t-1}$, with the squared differences of the returns minus the trailing means $(r_t - \bar{r}_t)^2$, using the weight decay factor $\lambda$:
      \begin{flalign*}
        & \bar{r}_t = \lambda \bar{r}_{t-1} + (1-\lambda) r_t \\
        & \sigma^2_t = \lambda \sigma^2_{t-1} + (1-\lambda) (r_t - \bar{r}_t)^2
      \end{flalign*}
      Where $\sigma^2_t$ is the trailing variance at time $t$, and $r_t$ are the returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_vti_rec.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate realized variance recursively
lambda <- 0.9
volat <- HighFreq::run_var(retp, lambda=lambda)
volat <- sqrt(volat)
# Plot EWMA volatility
volat <- xts:::xts(volat, order.by=datev)
dygraphs::dygraph(volat, main="VTI Realized Volatility") %>%
  dyOptions(colors="blue") %>% dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Daily Volatility From Intraday Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{close-to-close} volatility $\sigma$ depends on the \emph{Close} prices $C_i$ from \emph{OHLC} data:
      \begin{flalign*}
        \sigma^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (r_i - \bar{r})^2 \\
        \bar{r} &= \frac{1}{n}{\sum_{i=0}^{n} r_i} \quad r_i = \log(\frac{C_i}{C_{i-1}})
      \end{flalign*}
      But intraday time series of prices (for example \texttt{HighFreq::SPY} prices), can have large overnight jumps which inflate the volatility estimates.
      \vskip1ex
      So the overnight returns must be divided by the overnight time interval (in seconds), which produces per second returns.
      \vskip1ex
      The per second returns can be multiplied by \texttt{60} to scale them back up to per minute returns.
      \vskip1ex
      The function \texttt{zoo::index()} extracts the time index of a time series.
      \vskip1ex
      The function \texttt{xts::.index()} extracts the time index expressed in the number of seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely SPY returns (unit per minute) single day
# Minutely SPY volatility (unit per minute)
retspy <- rutils::diffit(log(SPY["2012-02-13", 4]))
sd(retspy)
# SPY returns multiple days (includes overnight jumps)
retspy <- rutils::diffit(log(SPY[, 4]))
sd(retspy)
# Table of time intervals - 60 second is most frequent
indeks <- rutils::diffit(xts::.index(SPY))
table(indeks)
# SPY returns divided by the overnight time intervals (unit per second)
retspy <- retspy/indeks
retspy[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retspy)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Range Volatility Estimators of \protect\emph{OHLC} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Range estimators of return volatility utilize the \texttt{high} and \texttt{low} prices, and therefore have lower standard errors than the standard \emph{close-to-close} estimator.
      \vskip1ex
      The \emph{Garman-Klass} estimator uses the \emph{low-to-high} price range, but it underestimates volatility because it doesn't account for \emph{close-to-open} price jumps:
      \begin{displaymath}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)\log(\frac{C_i}{O_i})^2)
      \end{displaymath}
      The \emph{Yang-Zhang} estimator accounts for \emph{close-to-open} price jumps and has the lowest standard error among unbiased estimators:
      \begin{multline*}
        \hspace{-1em}\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{O_i}{C_{i-1}})-\bar{r}_{co})^2 + \\
        0.134(\log(\frac{C_i}{O_i})-\bar{r}_{oc})^2 + \\
        \frac{0.866}{n} \sum_{i=1}^{n} (\log(\frac{H_i}{O_i})\log(\frac{H_i}{C_i}) + \log(\frac{L_i}{O_i})\log(\frac{L_i}{C_i}))
      \end{multline*}
    \column{0.5\textwidth}
      The \emph{Yang-Zhang} (\emph{YZ}) and \emph{Garman-Klass-Yang-Zhang} (\emph{GKYZ}) estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator.
      \vskip1ex
      But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the \emph{YZ} and \emph{GKYZ} range estimators.
      \vskip1ex
      Therefore in practice the \emph{YZ} and \emph{GKYZ} range estimators underestimate the volatility, and their standard errors are reduced less than by the theoretical amount, for the same reason.
      \vskip1ex
      The \emph{Garman-Klass-Yang-Zhang} estimator is another very efficient and unbiased estimator, and also accounts for \emph{close-to-open} price jumps:
      \begin{multline*}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} ((\log(\frac{O_i}{C_{i-1}})-\bar{r})^2 + \\
        0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)(\log(\frac{C_i}{O_i})^2))
      \end{multline*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Trailing Range Variance Using \protect\emph{HighFreq}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{HighFreq::calc\_var\_ohlc()} calculates the \emph{variance} of returns using several different range volatility estimators.
      \vskip1ex
      If the logarithms of the \emph{OHLC} prices are passed into \texttt{HighFreq::calc\_var\_ohlc()} then it calculates the variance of percentage returns, and if simple \emph{OHLC} prices are passed then it calculates the variance of dollar returns. 
      \vskip1ex
      The function \texttt{HighFreq::roll\_var\_ohlc()} calculates the \emph{trailing} variance of returns using several different range volatility estimators.
      \vskip1ex
      The functions \texttt{HighFreq::calc\_var\_ohlc()} and \texttt{HighFreq::roll\_var\_ohlc()} are very fast because they are written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{TTR::volatility()} calculates the range volatility, but it's significantly slower than \texttt{HighFreq::calc\_var\_ohlc()}.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
spy <- HighFreq::SPY["2008/2009"]
# Calculate daily SPY volatility using package HighFreq
sqrt(6.5*60*HighFreq::calcvar_ohlc(log(spy), 
  method="yang_zhang"))
# Calculate daily SPY volatility from minutely prices using package TTR
sqrt((6.5*60)*mean(na.omit(
  TTR::volatility(spy, N=1, calc="yang.zhang"))^2))
# Calculate trailing SPY variance using package HighFreq
varv <- HighFreq::roll_var_ohlc(log(spy), method="yang_zhang", 
  look_back=look_back)
# Plot range volatility
varv <- xts:::xts(sqrt(varv), order.by=zoo::index(spy))
dygraphs::dygraph(varv["2009-02"], main="SPY Trailing Range Volatility") %>%
  dyOptions(colors="blue") %>% dyLegend(show="always", width=300)
# Benchmark the speed of HighFreq vs TTR
library(microbenchmark)
summary(microbenchmark(
  ttr=TTR::volatility(rutils::etfenv$VTI, N=1, calc="yang.zhang"),
  highfreq=HighFreq::calcvar_ohlc(log(rutils::etfenv$VTI), method="yang_zhang"),
  times=2))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{VXX Prices and the Trailing Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VXX} ETF invests in \emph{VIX} futures, so its price is tied to the level of the \emph{VIX} index, with higher \emph{VXX} prices corresponding to higher levels of the \emph{VIX} index. 
      \vskip1ex
      The trailing volatility of past returns moves in sympathy with the implied volatility and \emph{VXX} prices, but with a lag.
      \vskip1ex
      But \emph{VXX} prices exhibit a very strong downward trend which makes them hard to compare with the trailing volatility.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VXX log prices
vxx <- na.omit(rutils::etfenv$prices$VXX)
datev <- zoo::index(vxx)
look_back <- 41
vxx <- log(vxx)
# Calculate trailing VTI volatility
closep <- get("VTI", rutils::etfenv)[datev]
closep <- log(closep)
volat <- sqrt(HighFreq::roll_var_ohlc(ohlc=closep, look_back=look_back, scalev=FALSE))
volat[1:look_back] <- volat[look_back+1]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vxx_volat.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of VXX and VTI volatility
datav <- cbind(vxx, volat)
colnames(datav)[2] <- "VTI Volatility"
colnamev <- colnames(datav)
captiont <- "VXX and VTI Volatility"
dygraphs::dygraph(datav[, 1:2], main=captiont) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Cointegration of VXX Prices and the trailing Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing volatility of past returns moves in sympathy with the implied volatility and \emph{VXX} prices, but with a lag.
      \vskip1ex
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Greater values of $\alpha$ produce a stronger feedback between the realized returns and variance, causing larger variance spikes and higher kurtosis.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VXX log prices
vxx <- na.omit(rutils::etfenv$prices$VXX)
datev <- zoo::index(vxx)
look_back <- 41
vxx <- log(vxx)
vxx <- (vxx - HighFreq::roll_mean(vxx, look_back=look_back))
vxx[1:look_back] <- vxx[look_back+1]
# Calculate trailing VTI volatility
closep <- get("VTI", rutils::etfenv)[datev]
closep <- log(closep)
volat <- sqrt(HighFreq::roll_var_ohlc(ohlc=closep, look_back=look_back, scalev=FALSE))
volat[1:look_back] <- volat[look_back+1]
# Calculate regression coefficients of XLB ~ XLE
betav <- drop(cov(vxx, volat)/var(volat))
alpha <- drop(mean(vxx) - betav*mean(volat))
# Calculate regression residuals
fitv <- (alpha + betav*volat)
residuals <- (vxx - fitv)
# Perform ADF test on residuals
tseries::adf.test(residuals, k=1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_hist.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of VXX and VTI volatility
datav <- cbind(vxx, volat)
colnamev <- colnames(datav)
captiont <- "VXX and VTI Volatility"
dygraphs::dygraph(datav[, 1:2], main=captiont) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red") %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variance calculated over non-overlapping intervals has very statistically significant autocorrelations.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 3, 1, 1), oma=c(0, 0, 0, 0))
# Calculate VTI percentage returns
retp <- na.omit(rutils::etfenv$returns$VTI)
# Calculate trailing VTI variance using package roll
look_back <- 21
varv <- HighFreq::roll_var(retp, look_back=look_back)
colnames(varv) <- "Variance"
# Number of look_backs that fit over returns
nrows <- NROW(retp)
nagg <- nrows %/% look_back
# Define end points with beginning stub
endd <- c(0, nrows-look_back*nagg + (0:nagg)*look_back)
nrows <- NROW(endd)
# Subset variance to end points
varv <- varv[endd]
# Plot autocorrelation function
rutils::plot_acf(varv, lag=10, main="ACF of Variance")
# Plot partial autocorrelation
pacf(varv, lag=10, main="PACF of Variance", ylab=NA)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_var.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/pacf_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{ARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ARCH(1,1)} is a volatility model defined by two coupled equations:
      \begin{flalign*}
        \sigma^2_t &= \omega + \beta \sigma^2_{t-1} + \xi^2_t
      \end{flalign*}
      Where $\sigma^2_t$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance $(r_t - \bar{r}_t)^2$ and the past variance $\sigma^2_{t-1}$, and $\xi_t$ are standard normal \emph{innovations}.
      \vskip1ex
      The return process $r_t$ follows a normal distribution with a time-dependent variance $\sigma^2_t$.
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance.
      \vskip1ex
      The long-term expected value of the variance is proportional to the parameter $\omega$:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      So the sum of $\alpha$ plus $\beta$ should be less than $1$, otherwise the volatility is explosive.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define GARCH parameters
alpha <- 0.3; betav <- 0.5;
omega <- 1e-4*(1-alpha-betav)
nrows <- 1000
# Calculate matrix of standard normal innovations
set.seed(1121)  # Reset random numbers
innov <- rnorm(nrows)
retp <- numeric(nrows)
varv <- numeric(nrows)
varv[1] <- omega/(1-alpha-betav)
retp[1] <- sqrt(varv[1])*innov[1]
# Simulate GARCH model
for (i in 2:nrows) {
  retp[i] <- sqrt(varv[i-1])*innov[i]
  varv[i] <- omega + alpha*retp[i]^2 + betav*varv[i-1]
}  # end for
# Simulate the GARCH process using Rcpp
garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha,  
  beta=betav, innov=matrix(innov))
all.equal(garch_data, cbind(retp, varv), check.attributes=FALSE)
      @
      The \emph{GARCH} process must be simulated using an explicit loop, so it's better to perform it in \texttt{C++} instead of \texttt{R}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{GARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} is a volatility model defined by two coupled equations:
      \begin{flalign*}
        r_t &= \sigma_{t-1} \xi_t \\
        \sigma^2_t &= \omega + \beta \sigma^2_{t-1} + \alpha r_t^2
      \end{flalign*}
      Where $\sigma^2_t$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance $r_t^2$ and the past variance $\sigma^2_{t-1}$, and $\xi_t$ are standard normal \emph{innovations}.
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance.
      \vskip1ex
      The return process $r_t$ follows a normal distribution, \emph{conditional} on the variance in the previous period $\sigma^2_{t-1}$.
      \vskip1ex
      But the \emph{unconditional} distribution of returns is \emph{not} normal, since their standard deviation is time-dependent, so they are \emph{leptokurtic} (fat tailed).
      \vskip1ex
      The long-term expected value of the variance is proportional to the parameter $\omega$:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      So the sum of $\alpha$ plus $\beta$ should be less than $1$, otherwise the volatility is explosive.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define GARCH parameters
alpha <- 0.3; betav <- 0.5;
omega <- 1e-4*(1-alpha-betav)
nrows <- 1000
# Calculate matrix of standard normal innovations
set.seed(1121)  # Reset random numbers
innov <- rnorm(nrows)
retp <- numeric(nrows)
varv <- numeric(nrows)
varv[1] <- omega/(1-alpha-betav)
retp[1] <- sqrt(varv[1])*innov[1]
# Simulate GARCH model
for (i in 2:nrows) {
  retp[i] <- sqrt(varv[i-1])*innov[i]
  varv[i] <- omega + alpha*retp[i]^2 + betav*varv[i-1]
}  # end for
# Simulate the GARCH process using Rcpp
garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha,  
  beta=betav, innov=matrix(innov))
all.equal(garch_data, cbind(retp, varv), check.attributes=FALSE)
      @
      The \emph{GARCH} process must be simulated using an explicit loop, so it's better to perform it in \texttt{C++} instead of \texttt{R}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} volatility model produces volatility clustering - periods of high volatility followed by a quick decay.
      \vskip1ex
      But the decay of the volatility in the \emph{GARCH} model is faster than what is observed in practice.
      \vskip1ex
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Larger values of $\alpha$ produce a stronger feedback between the realized returns and variance, which produce larger variance spikes, which produce larger kurtosis.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window on Mac
dev.new(width=6, height=5, noRStudioGD=TRUE)
# Set plot parameters to reduce whitespace around plot
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
# Plot GARCH cumulative returns
plot(cumsum(retp), t="l", col="blue", xlab="", ylab="",
  main="GARCH Cumulative Returns")
quartz.save("figure/garch_returns.png", type="png", 
  width=6, height=5)
# Plot GARCH volatility
plot(sqrt(varv), t="l", col="blue", xlab="", ylab="",
  main="GARCH Volatility")
quartz.save("figure/garch_volat.png", type="png", 
  width=6, height=5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_returns.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/garch_volat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Returns Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} volatility model produces \emph{leptokurtic} returns with fat tails in their the distribution.
      \vskip1ex
      Student's \emph{t-distribution} has fat tails, so it fits asset returns much better than the normal distribution.
      \vskip1ex
      Student's \emph{t-distribution} with \texttt{3} degrees of freedom is often used to represent asset returns.
      \vskip1ex
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution into a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate kurtosis of GARCH returns
mean(((retp-mean(retp))/sd(retp))^4)
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(retp)
# Fit t-distribution into GARCH returns
fitobj <- MASS::fitdistr(retp, densfun="t", df=2)
locv <- fitobj$estimate[1]
scalev <- fitobj$estimate[2]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_hist.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of GARCH returns
histp <- hist(retp, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.03, 0.03),
  ylab="frequency", freq=FALSE, main="GARCH Returns Histogram")
lines(density(retp, adjust=1.5), lwd=2, col="blue")
curve(expr=dt((x-locv)/scalev, df=2)/scalev,
  type="l", xlab="", ylab="", lwd=2,
  col="red", add=TRUE)
legend("topright", inset=-0, bty="n", y.intersp=0.4,
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
quartz.save("figure/garch_hist.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package 
      \href{https://cran.r-project.org/web/packages/fGarch/index.html}{\emph{fGarch}}
      contains functions for applying \emph{GARCH} models.
      \vskip1ex
      The function \texttt{fGarch::garchSpec()} specifies a \emph{GARCH} model.
      \vskip1ex
      The function \texttt{fGarch::garchSim()} simulates a \emph{GARCH} model, but it uses its own random innovations, so its output is not reproducible.
      <<echo=TRUE,eval=FALSE>>=
# Specify GARCH model
garch_spec <- fGarch::garchSpec(model=list(ar=c(0, 0), omega=omega, 
  alpha=alpha, beta=betav))
# Simulate GARCH model
garch_sim <- fGarch::garchSim(spec=garch_spec, n=nrows)
retp <- as.numeric(garch_sim)
# Calculate kurtosis of GARCH returns
moments::moment(retp, order=4) /
  moments::moment(retp, order=2)^2
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(retp)
# Plot histogram of GARCH returns
histp <- hist(retp, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH Returns Histogram")
lines(density(retp, adjust=1.5), lwd=3, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_fGarch_hist.png}
      <<echo=TRUE,eval=FALSE>>=
# Fit t-distribution into GARCH returns
fitobj <- MASS::fitdistr(retp, densfun="t", df=2, lower=c(-1, 1e-7))
locv <- fitobj$estimate[1]
scalev <- fitobj$estimate[2]
curve(expr=dt((x-locv)/scalev, df=2)/scalev,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05, bty="n", y.intersp=0.4,
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Returns Kurtosis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The expected value of the variance $\sigma^2$ of \emph{GARCH} returns is proportional to the parameter $\omega$:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      The expected value of the kurtosis $\kappa$ of \emph{GARCH} returns is equal to:
      \begin{flalign*}
        \kappa = 3 + \frac{6 \alpha^2}{1 - 2 \alpha^2 - (\alpha + \beta)^2}
      \end{flalign*}
      The excess kurtosis $\kappa - 3$ is proportional to $\alpha^2$ because larger values of the parameter $\alpha$ produce larger variance spikes which produce larger kurtosis.
      \vskip1ex
      The distribution of kurtosis is highly positively skewed, especially for short returns samples, so most kurtosis values will be significantly below their expected value. 
      <<echo=TRUE,eval=FALSE>>=
# Calculate variance of GARCH returns
var(retp)
# Calculate expected value of variance
omega/(1-alpha-betav)
# Calculate kurtosis of GARCH returns
mean(((retp-mean(retp))/sd(retp))^4)
# Calculate expected value of kurtosis
3 + 6*alpha^2/(1-2*alpha^2-(alpha+betav)^2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_kurtosis.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the distribution of GARCH kurtosis
kurt <- sapply(1:1e4, function(x) {
  garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha, 
    beta=betav, innov=matrix(rnorm(nrows)))
  retp <- garch_data[, 1]
  c(var(retp), mean(((retp-mean(retp))/sd(retp))^4))
})  # end sapply
kurt <- t(kurt)
apply(kurt, 2, mean)
# Plot the distribution of GARCH kurtosis
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
histp <- hist(kurt[, 2], breaks=500, col="lightgrey",
  xlim=c(2, 8), xlab="returns", ylab="frequency", freq=FALSE,
  main="Distribution of GARCH Kurtosis")
lines(density(kurt[, 2], adjust=1.5), lwd=3, col="blue")
abline(v=(3 + 6*alpha^2/(1-2*alpha^2-(alpha+betav)^2)), lwd=3, col="red")
text(x=7.0, y=0.4, "Expected Kurtosis")
quartz.save("figure/garch_kurtosis.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance Estimation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} model can be used to estimate the trailing variance of empirical (historical) returns.
      \vskip1ex
      If the time series of returns $r_t$ is given, then it can be used in the \emph{GARCH(1,1)} formula to estimate the trailing variance $\sigma^2_t$: 
      \begin{displaymath}
        \sigma^2_t = \omega + \beta \sigma^2_{t-1} + \alpha r_t^2
      \end{displaymath}
      If the returns from the \emph{GARCH(1,1)} simulation are used in the above formula, then it produces the simulated \emph{GARCH(1,1)} variance.
      \vskip1ex
      But to estimate the trailing variance of historical returns, the parameters $\omega$, $\alpha$, and $\beta$ must be estimated through model calibration.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate the GARCH process using Rcpp
garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha,  
  beta=betav, innov=matrix(innov))
# Extract the returns
retp <- garch_data[, 1]
# Estimate the trailing variance from the returns
varv <- numeric(nrows)
varv[1] <- omega/(1-alpha-betav)
for (i in 2:nrows) {
  varv[i] <- omega + alpha*retp[i]^2 +
    betav*varv[i-1]
}  # end for
all.equal(garch_data[, 2], varv, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Calibration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GARCH} models can be calibrated from the returns using the \emph{maximum-likelihood} method.
      \vskip1ex
      But it's a complex optimization procedure which requires a large amount of data for accurate results.
      \vskip1ex
      The function \texttt{fGarch::garchFit()} calibrates a \emph{GARCH} model on a time series of returns.
      \vskip1ex
      The function \texttt{garchFit()} returns an \texttt{S4} object of class \emph{fGARCH}, with multiple slots containing the \emph{GARCH} model outputs and diagnostic information.
      <<echo=TRUE,eval=FALSE>>=
library(fGarch)
# Fit returns into GARCH
garch_fit <- fGarch::garchFit(data=retp)
# Fitted GARCH parameters
garch_fit@fit$coef
# Actual GARCH parameters
c(mu=mean(retp), omega=omega,alpha=alpha, beta=betav)
# Plot GARCH fitted volatility
plot(sqrt(garch_fit@fit$series$h), t="l",
  col="blue", xlab="", ylab="",
  main="GARCH Fitted Volatility")
quartz.save("figure/garch_fGarch_fitted.png", 
  type="png", width=6, height=5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_fGarch_fitted.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/garch_volat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{GARCH(1,1)} volatility model, the returns follow the process: $r_t = \sigma_{t-1} \xi_t$.  (We can assume that the returns have been centered.)
      \vskip1ex
      So the \emph{conditional} distribution of returns is normal with standard deviation equal to $\sigma_{t-1}$:
      \begin{displaymath}
        \phi(r_t, \sigma_{t-1}) = \frac{e^{-r^2_t/2\sigma^2_{t-1}}}{\sqrt{2 \pi} \sigma_{t-1}}
      \end{displaymath}
      The \emph{log-likelihood} function $\mathcal{L}(\omega, \alpha, \beta | r_t)$ for the normally distributed returns is therefore equal to:
      \begin{displaymath}
        \mathcal{L}(\omega, \alpha, \beta | r_t) = - \sum_{t=1}^n (\frac{r^2_t}{\sigma^2_{t-1}} + \log(\sigma^2_{t-1}))
      \end{displaymath}
      The \emph{log-likelihood} depends on the \emph{GARCH(1,1)} parameters $\omega$, $\alpha$, and $\beta$ because the trailing variance $\sigma^2_t$ depends on the \emph{GARCH(1,1)} parameters:
      \begin{displaymath}
        \sigma^2_t = \omega + \beta \sigma^2_{t-1} + \alpha r^2_t
      \end{displaymath}
      The \emph{GARCH} process must be simulated using an explicit loop, so it's better to perform it in \texttt{C++} instead of \texttt{R}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define likelihood function
likefun <- function(omega, alpha, betav) {
  # Estimate the trailing variance from the returns
  varv <- numeric(nrows)
  varv[1] <- omega/(1-alpha-betav)
  for (i in 2:nrows) {
    varv[i] <- omega + alpha*retp[i]^2 + betav*varv[i-1]
  }  # end for
  varv <- ifelse(varv > 0, varv, 0.000001)
  # Lag the variance
  varv <- rutils::lagit(varv, pad_zeros=FALSE)
  # Calculate the likelihood
  -sum(retp^2/varv + log(varv))
}  # end likefun
# Calculate the likelihood in R
likefun(omega, alpha, betav)
# Calculate the likelihood in Rcpp
HighFreq::lik_garch(omega=omega, alpha=alpha,
  beta=betav, returns=matrix(retp))
# Benchmark speed of likelihood calculations
library(microbenchmark)
summary(microbenchmark(
  Rcode=likefun(omega, alpha, betav),
  Rcpp=HighFreq::lik_garch(omega=omega, alpha=alpha, beta=betav, returns=matrix(retp))
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} \emph{log-likelihood} function depends on three parameters $\mathcal{L}(\omega, \alpha, \beta | r_t)$.
      \vskip1ex
      The more parameters the harder it is to find their optimal values using optimization.
      \vskip1ex
      We can simplify the optimization task by assuming that the expected variance is equal to the realized variance:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta} = \frac{1}{n-1} \sum_{t=1}^n (r_t-\bar{r})^2
      \end{displaymath}
      This way the \emph{log-likelihood} becomes a function of only two parameters, say $\alpha$ and $\beta$. 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
# Calculate the variance of returns
retp <- garch_data[, 1, drop=FALSE]
varv <- var(retp)
retp <- (retp - mean(retp))
# Calculate likelihood as function of alpha and betav parameters
likefun <- function(alpha, betav) {
  omega <- variance*(1 - alpha - betav)
  -HighFreq::lik_garch(omega=omega, alpha=alpha, beta=betav, returns=retp)
}  # end likefun
# Calculate matrix of likelihood values
alphas <- seq(from=0.15, to=0.35, len=50)
betav <- seq(from=0.35, to=0.5, len=50)
likmat <- sapply(alphas, function(alpha) sapply(betav,
  function(betav) likefun(alpha, betav)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Perspective Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The perspective plot shows that the \emph{log-likelihood} is much more sensitive to the $\beta$ parameter than to $\alpha$. 
      \vskip1ex
      The function \texttt{rgl::persp3d()} plots an \emph{interactive} 3d surface plot of a \emph{vectorized} function or a matrix.
      \vskip1ex
      The optimal values of $\alpha$ and $\beta$ can be found approximately using a grid search on the \emph{log-likelihood} matrix. 
      <<eval=FALSE,echo=TRUE>>=
# Set rgl options and load package rgl
options(rgl.useNULL=TRUE); library(rgl)
# Draw and render 3d surface plot of likelihood function
ncols <- 100
color <- rainbow(ncols, start=2/6, end=4/6)
zcols <- cut(likmat, ncols)
rgl::persp3d(alphas, betav, likmat, col=color[zcols],
        xlab="alpha", ylab="beta", zlab="likelihood")
rgl::rglwidget(elementId="plot3drgl", width=700, height=700)
# Perform grid search
coord <- which(likmat == min(likmat), arr.ind=TRUE)
c(alphas[coord[2]], betav[coord[1]])
likmat[coord]
likefun(alphas[coord[2]], betav[coord[1]])
# Optimal and actual parameters
options(scipen=2)  # Use fixed not scientific notation
cbind(actual=c(alpha=alpha, beta=betav, omega=omega),
  optimal=c(alphas[coord[2]], betav[coord[1]], variance*(1 - sum(alphas[coord[2]], betav[coord[1]]))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garchlik.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The flat shape of the \emph{GARCH} likelihood function makes it difficult for steepest descent optimizers to find the best parameters.
      \vskip1ex
      The function \texttt{DEoptim()} from package \emph{DEoptim} performs \emph{global} optimization using the \emph{Differential Evolution} algorithm.
      \vskip1ex
      \emph{Differential Evolution} is a genetic algorithm which evolves a population of solutions over several generations:\\
      \hskip1em\url{https://link.springer.com/content/pdf/10.1023/A:1008202821328.pdf}
      \vskip1ex
      The first generation of solutions is selected randomly.
      \vskip1ex
      Each new generation is obtained by combining the best solutions from the previous generation.
      \vskip1ex
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization.
      \vskip1ex
      \emph{Gradient} optimization methods are more efficient than \emph{Differential Evolution} for smooth objective functions with no local minima.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
# Define vectorized likelihood function
likefun <- function(x, retp) {
  alpha <- x[1]; betav <- x[2]; omega <- x[3]
  -HighFreq::lik_garch(omega=omega, alpha=alpha, beta=betav, returns=retp)
}  # end likefun
# Initial parameters
initp <- c(alpha=0.2, beta=0.4, omega=varv/0.2)
# Find max likelihood parameters using steepest descent optimizer
fitobj <- optim(par=initp,
  fn=likefun, # Log-likelihood function
  method="L-BFGS-B", # Quasi-Newton method
  returns=retp, 
  upper=c(0.35, 0.55, varv), # Upper constraint
  lower=c(0.15, 0.35, varv/100)) # Lower constraint
# Optimal and actual parameters
cbind(actual=c(alpha=alpha, beta=betav, omega=omega),
      optimal=c(fitobj$par["alpha"], fitobj$par["beta"], fitobj$par["omega"]))
# Find max likelihood parameters using DEoptim
optiml <- DEoptim::DEoptim(fn=likefun,
  upper=c(0.35, 0.55, varv), # Upper constraint
  lower=c(0.15, 0.35, varv/100), # Lower constraint
  returns=retp, 
  control=list(trace=FALSE, itermax=1000, parallelType=1))
# Optimal and actual parameters
cbind(actual=c(alpha=alpha, beta=betav, omega=omega),
      optimal=c(optiml$optim$bestmem[1], optiml$optim$bestmem[2], optiml$optim$bestmem[3]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance of Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} model can be used to estimate the trailing variance of empirical (historical) returns.
      \vskip1ex
      If the time series of returns $r_t$ is given, then it can be used in the \emph{GARCH(1,1)} formula to estimate the trailing variance $\sigma^2_t$: 
      \begin{displaymath}
        \sigma^2_t = \omega + \beta \sigma^2_{t-1} + \alpha r_t^2
      \end{displaymath}
      The \emph{GARCH} formula can be viewed as a generalization of the \emph{EWMA} trailing variance.
      <<eval=FALSE,echo=TRUE>>=
# Calculate VTI returns
retp <- na.omit(rutils::etfenv$returns$VTI)
# Find max likelihood parameters using DEoptim
optiml <- DEoptim::DEoptim(fn=likefun,
  upper=c(0.4, 0.9, varv), # Upper constraint
  lower=c(0.1, 0.5, varv/100), # Lower constraint
  returns=retp, 
  control=list(trace=FALSE, itermax=1000, parallelType=1))
# Optimal parameters
par_am <- unname(optiml$optim$bestmem)
alpha <- par_am[1]; betav <- par_am[2]; omega <- par_am[3]
c(alpha, betav, omega)
# Equilibrium GARCH variance
omega/(1-alpha-betav)
drop(var(retp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_volat_vti.png}
      <<eval=FALSE,echo=TRUE>>=
# Estimate the GARCH volatility of VTI returns
nrows <- NROW(retp)
varv <- numeric(nrows)
varv[1] <- omega/(1-alpha-betav)
for (i in 2:nrows) {
  varv[i] <- omega + alpha*retp[i]^2 + betav*varv[i-1]
}  # end for
# Estimate the GARCH volatility using Rcpp
garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha, 
  beta=betav, innov=retp, is_random=FALSE)
all.equal(garch_data[, 2], varv, check.attributes=FALSE)
# Plot dygraph of the estimated GARCH volatility
dygraphs::dygraph(xts::xts(sqrt(varv), zoo::index(retp)), 
  main="Estimated GARCH Volatility of VTI") %>%
  dyOptions(colors="blue") %>% dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance Forecasts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one-step-ahead forecast of the squared returns is equal to their expected value: $r^2_{t+1} = \mathbb{E}[(\sigma_t \xi_t)^2] = \sigma^2_t$, since $\mathbb{E}[\xi^2_t] = 1$.
      \vskip1ex
      So the variance forecasts depend on the variance in the previous period: 
      $\sigma^2_{t+1} = \mathbb{E}[\omega + \alpha r^2_{t+1} + \beta \sigma^2_t] = \omega + (\alpha + \beta) \sigma^2_t$ 
      \vskip1ex
      The variance forecasts gradually settles to the equilibrium value $\sigma^2$, such that the forecast is equal to itself: $\sigma^2 = \omega + (\alpha + \beta) \sigma^2$.
      \vskip1ex
      This gives: $\sigma^2 = \frac{\omega}{1 - \alpha - \beta}$, which is the long-term expected value of the variance.
      \vskip1ex
      So the variance forecasts decay exponentially to their equilibrium value $\sigma^2$ at the decay rate equal to $(\alpha + \beta)$: 
      \begin{displaymath}
        \sigma^2_{t+1} - \sigma^2 = (\alpha + \beta) (\sigma^2_t - \sigma^2)
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Simulate GARCH model
garch_data <- HighFreq::sim_garch(omega=omega, alpha=alpha, 
  beta=betav, innov=matrix(innov))
varv <- garch_data[, 2]
# Calculate the equilibrium variance
vareq <- omega/(1-alpha-betav)
# Calculate the variance forecasts
varf <- numeric(10)
varf[1] <- vareq + (alpha + betav)*(xts::last(varv) - vareq)
for (i in 2:10) {
  varf[i] <- vareq + (alpha + betav)*(varf[i-1] - vareq)
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_forecast.png}
      <<echo=TRUE,eval=FALSE>>=
# Open plot window on Mac
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
# Plot GARCH variance forecasts
plot(tail(varv, 30), t="l", col="blue", xlab="", ylab="",
  xlim=c(1, 40), ylim=c(0, max(tail(varv, 30))), 
  main="GARCH Variance Forecasts")
text(x=15, y=0.5*vareq, "realized variance")
lines(x=30:40, y=c(xts::last(varv), varf), col="red", lwd=3)
text(x=35, y=0.6*vareq, "variance forecasts")
abline(h=vareq, lwd=3, col="red")
text(x=10, y=1.1*vareq, "Equilibrium variance")
quartz.save("figure/garch_forecast.png", type="png", 
  width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: old stuff about Estimating Volatility of Intraday Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{close-to-close} estimator depends on \emph{Close} prices specified over the aggregation intervals:
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{C_i}{C_{i-1}})-\bar{r})^2
      \end{displaymath}
      \vspace{-1em}
      \begin{displaymath}
        \bar{r} = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{C_i}{C_{i-1}})
      \end{displaymath}
      Volatility estimates for intraday time series depend both on the units of returns (per second, minute, day, etc.), and on the aggregation interval (secondly, minutely, daily, etc.)
      \vskip1ex
      A minutely time interval is equal to \texttt{60} seconds, a daily time interval is equal to \texttt{24*60*60 = 86,400} seconds.
      \vskip1ex
      For example, it's possible to measure returns in minutely intervals in units per second.
      \vskip1ex
      The estimated volatility is directly proportional to the measurement units.
      \vskip1ex
      For example, the volatility estimated from per minute returns is \texttt{60} times the volatility estimated from per second returns.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely SPY returns (unit per minute) single day
retspy <- rutils::diffit(log(SPY["2012-02-13", 4]))
# Minutely SPY volatility (unit per minute)
sd(retspy)
# Divide minutely SPY returns by time intervals (unit per second)
retspy <- retspy/rutils::diffit(xts::.index(SPY["2012-02-13"]))
retspy[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retspy)
# SPY returns multiple days
retspy <- rutils::diffit(log(SPY[, 4]))
# Minutely SPY volatility (includes overnight jumps)
sd(retspy)
# Table of intervals - 60 second is most frequent
indeks <- rutils::diffit(xts::.index(SPY))
table(indeks)
# hist(indeks)
# SPY returns with overnight scaling (unit per second)
retspy <- retspy/indeks
retspy[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retspy)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Comparing Range Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The range volatility estimators have much lower variability (standard errors) than the standard \emph{Close-to-Close} estimator.
      \vskip1ex
      Is the above correct?  Because the plot shows otherwise.
      \vskip1ex
      The range volatility estimators follow the standard \emph{Close-to-Close} estimator, except in intervals of high intra-period volatility.
      \vskip1ex
      During the May 6, 2010 \emph{flash crash}, range volatility spiked more than the \emph{Close-to-Close} volatility.
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
ohlc <- log(rutils::etfenv$VTI)
# Calculate variance
varcl <- HighFreq::run_variance(ohlc=ohlc,
        method="close")
var_yang_zhang <- HighFreq::run_variance(ohlc=ohlc)
stdev <- 24*60*60*sqrt(252*cbind(varcl, var_yang_zhang))
colnames(stdev) <- c("close stdev", "Yang-Zhang")
# Plot the time series of volatility
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
quantmod::chart_Series(stdev["2011-07/2011-12"],
  theme=plot_theme, name="Standard Deviations: Close and YZ")
legend("top", legend=colnames(stdev), y.intersp=0.4,
       bg="white", lty=1, lwd=6, inset=0.1, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
# Plot volatility around 2010 flash crash
quantmod::chart_Series(stdev["2010-04/2010-06"],
  theme=plot_theme, name="Volatility Around 2010 Flash Crash")
legend("top", legend=colnames(stdev), y.intersp=0.4,
       bg="white", lty=1, lwd=6, inset=0.1, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
# Plot density of volatility distributions
plot(density(stdev[, 1]), xlab="", ylab="",
  main="Density of Volatility Distributions",
  xlim=c(-0.05, range(stdev[, 1])[2]/3), type="l", lwd=2, col="blue")
lines(density(stdev[, 2]), col='red', lwd=2)
legend("top", legend=c("Close-to-Close", "Yang-Zhang"),
       bg="white", lty=1, lwd=6, inset=0.1, cex=0.8, y.intersp=0.4,
       col=plot_theme$col$line.col, bty="n")
# ? range volatility estimator has lower standard error ?
c(sd(varcl)/mean(varcl), sd(var_yang_zhang)/mean(var_yang_zhang))
foo <- stdev[varcl < range(varcl)[2]/3, ]
c(sd(foo[, 1])/mean(foo[, 1]), sd(foo[, 2])/mean(foo[, 2]))
plot(density(foo[, 1]), xlab="", ylab="",
  main="Mixture of Normal Returns",
  xlim=c(-0.05, range(foo[, 1])[2]/2), type="l", lwd=2, col="blue")
lines(density(foo[, 2]), col='red', lwd=2)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_close_yz.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/vol_density.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{Log-range} Volatility Proxies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    % wippp
      To-do: plot time series of \emph{intra-day range} volatility estimator and standard close-to-close volatility estimator.  Emphasize flash-crash of 2010.
      \vskip1ex
      An alternative range volatility estimator can be created by calculating the logarithm of the range, (as opposed to the range percentage, or the logarithm of the price ratios).
      \vskip1ex
      To-do: plot scatterplot of \emph{intra-day range} volatility estimator and standard close-to-close volatility estimator.
      \vskip1ex
      Emphasize the two are different: the intra-day range volatility estimator captures volatility events which aren't captured by close-to-close volatility estimator, and vice versa.
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{H_i - L_i}{H_i + L_i})^2
      \end{displaymath}
      The range logarithm fits better into the normal distribution than the range percentage.
      <<echo=TRUE,eval=FALSE>>=
ohlc <- rutils::etfenv$VTI
retp <- log((ohlc[, 2] - ohlc[, 3]) / (ohlc[, 2] + ohlc[, 3]))
foo <- rutils::diffit(log(ohlc[, 4]))
plot(as.numeric(foo)^2, as.numeric(retp)^2)
bar <- lm(retp ~ foo)
summary(bar)


# Perform normality tests
shapiro.test(coredata(retp))
tseries::jarque.bera.test(retp)
# Fit VTI returns using MASS::fitdistr()
fitobj <- MASS::fitdistr(retp, densfun="t", df=2)
fitobj$estimate; fitobj$sd
# Calculate moments of standardized returns
sapply(3:4, moments::moment, x=(retp - mean(retp))/sd(retp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/log_range.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of VTI returns
colorv <- c("lightgray", "blue", "green", "red")
PerformanceAnalytics::chart.Histogram(retp,
  main="", xlim=c(-7, -3), col=colorv[1:3],
  methods = c("add.density", "add.normal"))
curve(expr=dt((x-fitobj$estimate[1])/
  fitobj$estimate[2], df=2)/fitobj$estimate[2],
      type="l", xlab="", ylab="", lwd=2,
      col=colorv[4], add=TRUE)
# Add title and legend
title(main="VTI logarithm of range",
      cex.main=1.3, line=-1)
legend("topright", inset=0.05, y.intersp=0.4,
  legend=c("density", "normal", "t-distr"),
  lwd=6, lty=1, col=colorv[2:4], bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Autocorrelations of Alternative \protect\emph{Range} Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The logarithm of the range exhibits very significant autocorrelations, unlike the range percentage.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI range variance partial autocorrelations
pacf(retp^2, lag=10, xlab=NA, ylab=NA,
     main="PACF of VTI log range")
quantmod::chart_Series(retp^2, name="VTI log of range squared")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/pacf_log_range.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Standard Errors of Volatility Estimators Using Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of estimators can be calculated using a \emph{bootstrap} simulation.
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed data set.
      \vskip1ex
      The \emph{bootstrapped} data is then used to recalculate the estimator many times, producing a vector of values.
      \vskip1ex
      The \emph{bootstrapped} estimator values can then be used to calculate the probability distribution of the estimator and its standard error.
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators that are sensitive to the ordering and correlations in the data.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Standard errors of variance estimators using bootstrap
bootd <- sapply(1:1e2, function(x) {
  # Create random OHLC
  ohlc <- HighFreq::random_ohlc()
  # Calculate variance estimate
  c(var=var(ohlc[, 4]),
    yang_zhang=HighFreq::calcvariance(
      ohlc, method="yang_zhang", scalev=FALSE))
})  # end sapply
# Analyze bootstrapped variance
bootd <- t(bootd)
head(bootd)
colMeans(bootd)
apply(bootd, MARGIN=2, sd) /
  colMeans(bootd)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Autocorrelations of \protect\emph{Close-to-Close} and \protect\emph{Range} Variances}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{Close-to-Close} estimator exhibits very significant autocorrelations, but the \emph{range} estimators are not autocorrelated.
      \vskip1ex
      That is because the time series of squared intra-period ranges is not autocorrelated.
      <<echo=(-(1:2)),eval=FALSE>>=
par(oma=c(1, 1, 1, 1), mar=c(2, 2, 1, 1), mgp=c(0, 0.5, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
par(mfrow=c(2,1))  # Set plot panels
# Close variance estimator partial autocorrelations
pacf(varcl, lag=10, xlab=NA, ylab=NA)
title(main="VTI close variance partial autocorrelations")

# Range variance estimator partial autocorrelations
pacf(var_yang_zhang, lag=10, xlab=NA, ylab=NA)
title(main="VTI YZ variance partial autocorrelations")

# Squared range partial autocorrelations
retp <- log(rutils::etfenv$VTI[,2] /
                  rutils::etfenv$VTI[,3])
pacf(retp^2, lag=10, xlab=NA, ylab=NA)
title(main="VTI squared range partial autocorrelations")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/var_pacf.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Performing Aggregations Over Time Series}


%%%%%%%%%%%%%%%
\subsection{Defining Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A time \emph{period} is the time between two neighboring points in time.
      \vskip1ex
      A time \emph{interval} is the time spanned by one or more time \emph{periods}.
      \vskip1ex
      A \emph{look-back interval} is a time \emph{interval} for performing aggregations over the past, starting from a \emph{start point} and ending at an \emph{end point}.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      The look-back \emph{intervals} may or may not \emph{overlap} with their neighboring intervals.
    \column{0.5\textwidth}
      A \emph{trailing aggregation} is performed over a vector of \emph{end points} in time.
      \vskip1ex
      An example of a trailing aggregation are moving average prices.
      \vskip1ex
      An \emph{interval aggregation} is specified by \emph{end points} separated by many time \emph{periods}.
      \vskip1ex
      Examples of interval aggregations are monthly asset returns, or trailing 12-month asset returns calculated every month.
  \end{columns}
    \vspace{-1em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Trailing} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{trailing aggregation} is performed over a vector of \emph{end points} in time.
      \vskip1ex
      The first \emph{end point} is equal to zero $0$.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of a trailing aggregation are moving average prices.
    \column{0.5\textwidth}
\vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
ohlc <- rutils::etfenv$VTI
# Number of data points
nrows <- NROW(ohlc["2018-06/"])
# Define endd at each point in time
endd <- 0:nrows
# Number of data points in look_back interval
look_back <- 22
# startp are endd lagged by look_back
startp <- c(rep_len(0, look_back), endd[1:(NROW(endd)-look_back)])
head(startp, 33)
      @
  \end{columns}
    \vspace{-1em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_rolling.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Equally Spaced \protect\emph{end points} of a Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The neighboring \emph{end points} may be separated by a fixed number of periods, equal to \texttt{npoints}.
      \vskip1ex
      If the total number of data points is not an integer multiple of \texttt{npoints}, then a stub interval must be added either at the beginning or at the end of the \emph{end points}.
      \vskip1ex
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
closep <- quantmod::Cl(ohlc["2018/"])
nrows <- NROW(closep)
# Number of periods between endpoints
npoints <- 21
# Number of npoints that fit over nrows
nagg <- nrows %/% npoints
# If(nrows==npoints*nagg then whole number
endd <- (0:nagg)*npoints
# Stub interval at beginning
endd <- c(0, nrows-npoints*nagg + (0:nagg)*npoints)
# Else stub interval at end
endd <- c((0:nagg)*npoints, nrows)
# Or use xts::endpoints()
endd <- xts::endpoints(closep, on="weeks")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/intervals_end_points.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot data and endpoints as vertical lines
plot.xts(closep, col="blue", lwd=2, xlab="", ylab="",
         main="Prices with Endpoints as Vertical Lines")
addEventLines(xts(rep("endpoint", NROW(endd)-1), zoo::index(closep)[endd]),
              col="red", lwd=2, pos=4)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- "blue"
quantmod::chart_Series(closep, theme=plot_theme,
  name="prices with endpoints as vertical lines")
abline(v=endd, col="red", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Overlapping} time intervals can be defined if the \emph{start points} are equal to the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of an overlapping interval aggregation are trailing 12-month asset returns calculated every month.
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
nrows <- NROW(rutils::etfenv$VTI["2019/"])
# Number of npoints that fit over nrows
npoints <- 21
nagg <- nrows %/% npoints
# Stub interval at beginning
endd <- c(0, nrows-npoints*nagg + (0:nagg)*npoints)
      @
    \column{0.5\textwidth}
      The length of the \emph{look-back interval} can be defined either as the number of data points, or as the number of \emph{end points} to look back over.
      <<echo=TRUE,eval=FALSE>>=
# look_back defined as number of data points
look_back <- 252
# startp are endd lagged by look_back
startp <- (endd - look_back + 1)
startp <- ifelse(startp < 0, 0, startp)
# look_back defined as number of endd
look_back <- 12
startp <- c(rep_len(0, look_back), endd[1:(NROW(endd)- look_back)])
# Bind startp with endd
cbind(startp, endd)
      @
  \end{columns}
    \vspace{-1em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Non-overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Non-overlapping} time intervals can be defined if \emph{start points} are equal to the previous \emph{end points}.
      \vskip1ex
      In that case the look-back \emph{intervals} are non-overlapping and \emph{contiguous} (each \emph{start point} is the \emph{end point} of the previous interval).
      \vskip1ex
      If the \emph{start points} are defined as the previous \emph{end points} plus $1$, then the \emph{intervals} are \emph{exclusive}.
      \vskip1ex
      \emph{Exclusive intervals} are used for calculating \emph{out-of-sample} aggregations over future intervals.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Number of data points
nrows <- NROW(rutils::etfenv$VTI["2019/"])
# Number of data points per interval
npoints <- 21
# Number of npointss that fit over nrows
nagg <- nrows %/% npoints
# Define endd with beginning stub
endd <- c(0, nrows-npoints*nagg + (0:nagg)*npoints)
# Define contiguous startp
startp <- c(0, endd[1:(NROW(endd)-1)])
# Define exclusive startp
startp <- c(0, endd[1:(NROW(endd)-1)]+1)
      @
  \end{columns}
    \vspace{-1em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_non_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Aggregations performed over time series can be extremely slow if done improperly, therefore it's very important to find the fastest methods of performing aggregations.
      \vskip1ex
      The \texttt{sapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The variable \texttt{look\_back} is the size of the look-back interval, equal to the number of data points used for applying the aggregation function (including the current point).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
endd <- 0:NROW(closep)  # End points at each point
npts <- NROW(endd)
look_back <- 22  # Number of data points per look-back interval
# startp are multi-period lag of endd
startp <- c(rep_len(0, look_back), endd[1:(npts - look_back)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(2:npts, function(it) {
    startp[it]:endd[it]
})  # end lapply
# Define aggregation function
aggfun <- function(xtsv) c(max=max(xtsv), min=min(xtsv))
# Perform aggregations over look_backs list
aggs <- sapply(look_backs,
    function(look_back) aggfun(closep[look_back])
)  # end sapply
# Coerce aggs into matrix and transpose it
if (is.vector(aggs))
  aggs <- t(aggs)
aggs <- t(aggs)
# Coerce aggs into xts series
aggs <- xts(aggs, order.by=zoo::index(closep[endd]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{lapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Perform aggregations over look_backs list
aggs <- lapply(look_backs,
    function(look_back) aggfun(closep[look_back])
)  # end lapply
# rbind list into single xts or matrix
aggs <- rutils::do_call(rbind, aggs)
# Convert into xts
aggs <- xts::xts(aggs, order.by=zoo::index(closep))
aggs <- cbind(aggs, closep)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
x11(width=6, height=5)
quantmod::chart_Series(aggs, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(aggs),
  bg="white", lty=1, lwd=6, y.intersp=0.4,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Functionals for Trailing Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{roll\_agg()} performs trailing aggregations of its function argument \texttt{FUN}, over an \emph{xts} series (\texttt{x\_ts}), and a look-back interval (\texttt{look\_back}).
      \vskip1ex
      The argument \texttt{FUN} is an aggregation function over a subset of \texttt{x\_ts} series.
      \vskip1ex
      The dots \texttt{"..."} argument is passed into \texttt{FUN} as additional arguments.
      \vskip1ex
      The argument \texttt{look\_back} is equal to the number of periods of \texttt{x\_ts} series which are passed to the aggregation function \texttt{FUN}.
      \vskip1ex
      The functional \texttt{roll\_agg()} calls \texttt{lapply()}, which loops over the length of series \texttt{x\_ts}.
      \vskip1ex
      Note that two different intervals may be used with \texttt{roll\_agg()}.
      \vskip1ex
      The first interval is the argument \texttt{look\_back}.
      \vskip1ex
      A second interval may be one of the variables bound to the dots \texttt{"..."} argument, and passed to the aggregation function \texttt{FUN} (for example, an \emph{EWMA} window).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define functional for trailing aggregations
roll_agg <- function(xtsv, look_back, FUN, ...) {
# Define end points at every period
  endd <- 0:NROW(xtsv)
  npts <- NROW(endd)
# Define starting points as lag of endd
  startp <- c(rep_len(0, look_back), endd[1:(npts- look_back)])
# Perform aggregations over look_backs list
  aggs <- lapply(2:npts, function(it)
    FUN(xtsv[startp[it]:endd[it]], ...)
  )  # end lapply
# rbind list into single xts or matrix
  aggs <- rutils::do_call(rbind, aggs)
# Coerce aggs into xts series
  if (!is.xts(aggs))
    aggs <- xts(aggs, order.by=zoo::index(xtsv))
  aggs
}  # end roll_agg
# Define aggregation function
aggfun <- function(xtsv)
  c(max=max(xtsv), min=min(xtsv))
# Perform aggregations over trailing interval
aggs <- roll_agg(closep, look_back=look_back, FUN=aggfun)
class(aggs)
dim(aggs)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Speed of Trailing Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of trailing aggregations using \texttt{apply()} loops can be greatly increased by simplifying the aggregation function
      \vskip1ex
      For example, an aggregation function that returns a vector is over \texttt{13} times faster than a function that returns an \emph{xts} object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define aggregation function that returns a vector
agg_vector <- function(xtsv)
  c(max=max(xtsv), min=min(xtsv))
# Define aggregation function that returns an xts
agg_xts <- function(xtsv)
  xts(t(c(max=max(xtsv), min=min(xtsv))), order.by=end(xtsv))
# Benchmark the speed of aggregation functions
library(microbenchmark)
summary(microbenchmark(
  agg_vector=roll_agg(closep, look_back=look_back, FUN=agg_vector),
  agg_xts=roll_agg(closep, look_back=look_back, FUN=agg_xts),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Functionals for Trailing Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      Several packages contain functionals designed for performing trailing aggregations:
      \begin{itemize}
        \item \texttt{rollapply.zoo()} from package \emph{zoo},
        \item \texttt{rollapply.xts()} from package \emph{xts},
        \item \texttt{apply.rolling()} from package \emph{PerformanceAnalytics},
      \end{itemize}
      These functionals don't require specifying the \emph{end points}, and instead calculate the \emph{end points} from the trailing interval width.
      \vskip1ex
      These functionals can only apply functions that return a single value, not a vector.
      \vskip1ex
      These functionals return an \emph{xts} series with leading \texttt{NA} values at points before the trailing interval can fit over the data.
      \vskip1ex
      The argument \texttt{align="right"} of \texttt{rollapply()} determines that aggregations are taken from the past.
      \vskip1ex
      The functional \texttt{rollapply.xts} is the fastest, about as fast as performing an \texttt{lapply()} loop directly.
    \column{0.55\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define aggregation function that returns a single value
aggfun <- function(xtsv)  max(xtsv)
# Perform aggregations over a trailing interval
aggs <- xts:::rollapply.xts(closep, width=look_back,
                    FUN=aggfun, align="right")
# Perform aggregations over a trailing interval
library(PerformanceAnalytics)  # Load package PerformanceAnalytics
aggs <- apply.rolling(closep, width=look_back, FUN=aggfun)
# Benchmark the speed of the functionals
library(microbenchmark)
summary(microbenchmark(
  roll_agg=roll_agg(closep, look_back=look_back, FUN=max),
  roll_xts=xts:::rollapply.xts(closep, width=look_back, FUN=max, align="right"),
  apply_rolling=apply.rolling(closep, width=look_back, FUN=max),
  times=10))[, c(1, 4, 5)]
@
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Trailing Aggregations Using \protect\emph{Vectorized} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic functions \texttt{cumsum()}, \texttt{cummax()}, and \texttt{cummin()} return the cumulative sums, minima, and maxima of \emph{vectors} and \emph{time series} objects.
      \vskip1ex
      The methods for these functions are implemented as \emph{vectorized compiled} functions, and are therefore much faster than \texttt{apply()} loops.
      \vskip1ex
      The \texttt{cumsum()} function can be used to efficiently calculate the trailing sum of an an \emph{xts} series.
      \vskip1ex
      Using the function \texttt{cumsum()} is over \texttt{25} times faster than using \texttt{apply()} loops.
      \vskip1ex
      But trailing volatilities and higher moments can't be easily calculated using \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Trailing sum using cumsum()
roll_sum <- function(xtsv, look_back) {
  cumsumv <- cumsum(na.omit(xtsv))
  output <- cumsumv - rutils::lagit(x=cumsumv, lagg=look_back)
  output[1:look_back, ] <- cumsumv[1:look_back, ]
  colnames(output) <- paste0(colnames(xtsv), "_stdev")
  output
}  # end roll_sum
aggs <- roll_sum(closep, look_back=look_back)
# Perform trailing aggregations using lapply loop
aggs <- lapply(2:npts, function(it)
    sum(closep[startp[it]:endd[it]])
)  # end lapply
# rbind list into single xts or matrix
aggs <- rutils::do_call(rbind, aggs)
head(aggs)
tail(aggs)
# Benchmark the speed of both methods
library(microbenchmark)
summary(microbenchmark(
  roll_sum=roll_sum(closep, look_back=look_back),
  s_apply=sapply(look_backs,
    function(look_back) sum(closep[look_back])),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series Using Function \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_t$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_t = \varphi_1 r_{t-1} + \varphi_2 r_{t-2} + \ldots + \varphi_p r_{t-p}
      \end{displaymath}
      Where $f_t$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calculates a \emph{recursive} filter over the vector of random \emph{innovations} $\xi_t$ as follows:
      \begin{displaymath}
        r_t = \varphi_1 r_{t-1} + \varphi_2 r_{t-2} + \ldots + \varphi_p r_{t-p} + \xi_t
      \end{displaymath}
      Where $r_t$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
# Calculate EWMA prices using filter()
look_back <- 21
weightv <- exp(-0.1*1:look_back)
weightv <- weightv/sum(weights)
pricef <- stats::filter(closep, filter=weightv,
                         method="convolution", sides=1)
pricef <- as.numeric(pricef)
# filter() returns time series of class "ts"
class(pricef)
# Filter using compiled C++ function directly
getAnywhere(C_cfilter)
str(stats:::C_cfilter)
priceff <- .Call(stats:::C_cfilter, closep, 
                     filter=weightv, sides=1, circular=FALSE)
all.equal(as.numeric(pricef), priceff, check.attributes=FALSE)
# Calculate EWMA prices using HighFreq::roll_conv()
pricecpp <- HighFreq::roll_conv(closep, weightv=weightv)
all.equal(pricef[-(1:look_back)],
          as.numeric(pricecpp)[-(1:look_back)],
          check.attributes=FALSE)
# Benchmark speed of trailing calculations
library(microbenchmark)
summary(microbenchmark(
  filter=filter(closep, filter=weightv, method="convolution", sides=1),
  priceff=.Call(stats:::C_cfilter, closep, filter=weightv, sides=1, circular=FALSE),
  cumsumv=cumsum(closep),
  rcpp=HighFreq::roll_conv(closep, weightv=weightv)
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Using Package \protect\emph{TTR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{TTR} contains functions for calculating trailing aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{runSum()} for trailing sums,
        \item \texttt{runMin()} and \texttt{runMax()} for trailing minima and maxima,
        \item \texttt{runSD()} for trailing standard deviations,
        \item \texttt{runMedian()} and \texttt{runMAD()} for trailing medians and Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runCor()} for trailing correlations,
      \end{itemize}
      The trailing \emph{TTR} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} or \texttt{Fortran} code).
      \vskip1ex
      But the trailing \emph{TTR} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the trailing maximum and minimum over a vector of data
roll_maxminr <- function(vectorv, look_back) {
  nrows <- NROW(vectorv)
  max_min <- matrix(numeric(2:nrows), nc=2)
  # Loop over periods
  for (it in 1:nrows) {
    sub_vec <- vectorv[max(1, it-look_back+1):it]
    max_min[it, 1] <- max(sub_vec)
    max_min[it, 2] <- min(sub_vec)
  }  # end for
  return(max_min)
}  # end roll_maxminr
max_minr <- roll_maxminr(closep, look_back)
max_minr <- xts::xts(max_minr, zoo::index(closep))
library(TTR)  # Load package TTR
max_min <- cbind(TTR::runMax(x=closep, n=look_back),
                 TTR::runMin(x=closep, n=look_back))
all.equal(max_min[-(1:look_back), ], max_minr[-(1:look_back), ], check.attributes=FALSE)
# Benchmark the speed of TTR::runMax
library(microbenchmark)
summary(microbenchmark(
  rcode=roll_maxminr(closep, look_back),
  ttr=TTR::runMax(closep, n=look_back),
  times=10))[, c(1, 4, 5)]
# Benchmark the speed of TTR::runSum
summary(microbenchmark(
  vector_r=cumsum(coredata(closep)),
  rutils=rutils::roll_sum(closep, look_back=look_back),
  ttr=TTR::runSum(closep, n=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing \protect\emph{Weighted} Aggregations Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} trailing aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()}, \texttt{roll\_max()}, \texttt{roll\_mean()}, and \texttt{roll\_median()} for \emph{weighted} trailing sums, maximums, means, and medians,
        \item \texttt{roll\_var()} for \emph{weighted} trailing variance,
        \item \texttt{roll\_scale()} for trailing scaling and centering of time series,
        \item \texttt{roll\_lm()} for trailing regression,
        \item \texttt{roll\_pcr()} for trailing principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate trailing VTI variance using package roll
library(roll)  # Load roll
retp <- na.omit(rutils::etfenv$returns$VTI)
look_back <- 22
# Calculate trailing sum using roll::roll_sum
sum_roll <- roll::roll_sum(retp, width=look_back, min_obs=1)
# Calculate trailing sum using rutils
sum_rutils <- rutils::roll_sum(retp, look_back=look_back)
all.equal(sum_roll[-(1:look_back), ], 
          sum_rutils[-(1:look_back), ], check.attributes=FALSE)
# Benchmark speed of trailing calculations
library(microbenchmark)
summary(microbenchmark(
  cumsumv=cumsum(retp),
  roll=roll::roll_sum(retp, width=look_back),
  RcppRoll=RcppRoll::roll_sum(retp, n=look_back),
  rutils=rutils::roll_sum(retp, look_back=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trailing \protect\emph{Weighted} Aggregations Using Package \protect\emph{RcppRoll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package
      \href{https://cran.r-project.org/web/packages/RcppRoll/index.html}{\color{blue}{\emph{RcppRoll}}}
      contains functions for calculating \emph{weighted} trailing aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for \emph{weighted} trailing sums,
        \item \texttt{roll\_min()} and \texttt{roll\_max()} for \emph{weighted} trailing minima and maxima,
        \item \texttt{roll\_sd()} for \emph{weighted} trailing standard deviations,
        \item \texttt{roll\_median()} for \emph{weighted} trailing medians,
      \end{itemize}
      The \emph{RcppRoll} functions accept \emph{xts} objects, but they return matrices, not \emph{xts} objects.
      \vskip1ex
      The trailing \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      But the trailing \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(RcppRoll)  # Load package RcppRoll
# Calculate trailing sum using RcppRoll
sum_roll <- RcppRoll::roll_sum(retp, align="right", n=look_back)
# Calculate trailing sum using rutils
sum_rutils <- rutils::roll_sum(retp, look_back=look_back)
all.equal(sum_roll, coredata(sum_rutils[-(1:(look_back-1))]), 
          check.attributes=FALSE)
# Benchmark speed of trailing calculations
library(microbenchmark)
summary(microbenchmark(
  cumsumv=cumsum(retp),
  RcppRoll=RcppRoll::roll_sum(retp, n=look_back),
  rutils=rutils::roll_sum(retp, look_back=look_back),
  times=10))[, c(1, 4, 5)]
# Calculate EWMA prices using RcppRoll
closep <- quantmod::Cl(rutils::etfenv$VTI)
weightv <- exp(0.1*1:look_back)
pricewma <- RcppRoll::roll_mean(closep,
      align="right", n=look_back, weights=weightv)
pricewma <- cbind(closep,
  rbind(coredata(closep[1:(look_back-1), ]), pricewma))
colnames(pricewma) <- c("VTI", "VTI EWMA")
# Plot an interactive dygraph plot
dygraphs::dygraph(pricewma)
# Or static plot of EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red")
quantmod::chart_Series(pricewma, theme=plot_theme, name="EWMA prices")
legend("top", legend=colnames(pricewma),
       bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Using Package \protect\emph{caTools}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{caTools} contains functions for calculating trailing interval aggregations over a \texttt{vector} of data:
      \begin{itemize}
        \item \texttt{runmin()} and \texttt{runmax()} for trailing minima and maxima,
        \item \texttt{runsd()} for trailing standard deviations,
        \item \texttt{runmad()} for trailing Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runquantile()} for trailing quantiles,
      \end{itemize}
      Time series need to be coerced to \emph{vectors} before they are passed to \emph{caTools} functions.
      \vskip1ex
      The trailing \emph{caTools} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      The argument \texttt{"endrule"} determines how the end values of the data are treated.
      \vskip1ex
      The argument \texttt{"align"} determines whether the interval is centered (default), left-aligned or right-aligned, with \texttt{align="center"} the fastest option.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
library(caTools)  # Load package "caTools"
# Get documentation for package "caTools"
packageDescription("caTools")  # Get short description
help(package="caTools")  # Load help page
data(package="caTools")  # List all datasets in "caTools"
ls("package:caTools")  # List all objects in "caTools"
detach("package:caTools")  # Remove caTools from search path
# Median filter
look_back <- 2
closep <- quantmod::Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
med_ian <- runmed(x=closep, k=look_back)
# Vector of trailing volatilities
sigmav <- runsd(x=closep, k=look_back,
                endrule="constant", align="center")
# Vector of trailing quantiles
quantvs <- runquantile(x=closep, k=look_back,
  probs=0.9, endrule="constant", align="center")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{RcppArmadillo} functions for calculating trailing aggregations are often the fastest.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <RcppArmadillo.h> // include C++ header file from Armadillo library
using namespace arma; // use C++ namespace from Armadillo library
// declare dependency on RcppArmadillo
// [[Rcpp::depends(RcppArmadillo)]]

// export the function roll_maxmin() to R
// [[Rcpp::export]]
arma::mat roll_maxmin(const arma::vec& vectorv,
                      const arma::uword& look_back) {
  arma::uword.n_rows = vectorv.size();
  arma::mat max_min[nrows, 2);
  arma::vec sub_vec;
  // startup period
  max_min(0, 0) = vectorv[0];
  max_min(0, 1) = vectorv[0];
  for (uword it = 1; it < look_back; it++) {
    sub_vec = vectorv.subvec(0, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  // remaining periods
  for (uword it = look_back; it <.n_rows; it++) {
    sub_vec = vectorv.subvec(it- look_back + 1, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  return max_min;
}  // end roll_maxmin
    \end{lstlisting}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/trailing_maxmin.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Compile Rcpp functions
Rcpp::sourceCpp(file="/Users/jerzy/Develop/R/Rcpp/roll_maxmin.cpp")
max_minarma <- roll_maxmin(closep, look_back)
max_minarma <- xts::xts(max_minr, zoo::index(closep))
max_min <- cbind(TTR::runMax(x=closep, n=look_back),
                 TTR::runMin(x=closep, n=look_back))
all.equal(max_min[-(1:look_back), ], max_minarma[-(1:look_back), ], check.attributes=FALSE)
# Benchmark the speed of TTR::runMax
library(microbenchmark)
summary(microbenchmark(
  arma=roll_maxmin(closep, look_back),
  ttr=TTR::runMax(closep, n=look_back),
  times=10))[, c(1, 4, 5)]
# Dygraphs plot with max_min lines
datav <- cbind(closep, max_minarma)
colnames(datav)[2:3] <- c("max", "min")
colorv <- c("blue", "red", "green")
dygraphs::dygraph(datav, main=paste(colnames(closep), "max and min lines")) %>%
  dyOptions(colors=colorv) %>% dyLegend(show="always", width=300)
# Standard plot with max_min lines
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(datav["2008/2009"], theme=plot_theme,
  name=paste(colnames(closep), "max and min lines"))
legend(x="topright", title=NULL, legend=colnames(datav),
       inset=0.1, cex=0.9, bg="white", bty="n",
       lwd=6, lty=1, col=colorv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Calendar \protect\emph{end points} of \protect\emph{xts} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vskip1ex
      For example:\\ \-\ \texttt{endpoints(x, on="hours")}\\
      extracts the indices of the last observations in each hour.
      \vskip1ex
      The \emph{end points} calculated by \texttt{endpoints()} aren't always equally spaced, and aren't the same as those calculated from fixed intervals.
      \vskip1ex
      For example, the last observations in each day aren't equally spaced due to weekends and holidays.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Indices of last observations in each hour
endd <- xts::endpoints(closep, on="hours")
head(endd)
# extract the last observations in each hour
head(closep[endd, ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
# Number of data points
nrows <- NROW(closep)
# Number of data points per interval
look_back <- 22
# Number of look_backs that fit over nrows
nagg <- nrows %/% look_back
# Define endd with beginning stub
endd <- c(0, nrows-look_back*nagg + (0:nagg)*look_back)
# Define contiguous startp
startp <- c(0, endd[1:(NROW(endd)-1)])
# Define list of look-back intervals for aggregations over past
look_backs <- lapply(2:NROW(endd), function(it) {
    startp[it]:endd[it]
})  # end lapply
look_backs[[1]]
look_backs[[2]]
# Perform sapply() loop over look_backs list
aggs <- sapply(look_backs, function(look_back) {
  xtsv <- closep[look_back]
  c(max=max(xtsv), min=min(xtsv))
})  # end sapply
# Coerce aggs into matrix and transpose it
if (is.vector(aggs))
  aggs <- t(aggs)
aggs <- t(aggs)
# Coerce aggs into xts series
aggs <- xts(aggs, order.by=zoo::index(closep[endd]))
head(aggs)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
quantmod::chart_Series(aggs, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(aggs),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Perform lapply() loop over look_backs list
aggs <- lapply(look_backs, function(look_back) {
  xtsv <- closep[look_back]
  c(max=max(xtsv), min=min(xtsv))
})  # end lapply
# rbind list into single xts or matrix
aggs <- rutils::do_call(rbind, aggs)
# Coerce aggs into xts series
aggs <- xts(aggs, order.by=zoo::index(closep[endd]))
head(aggs)
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
quantmod::chart_Series(aggs, theme=plot_theme, name="price aggregations")
legend("top", legend=colnames(aggs),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations Using \texttt{period.apply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} from package \emph{xts} performs \emph{aggregations} over non-overlapping intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      Internally \texttt{period.apply()} performs an \texttt{sapply()} loop, and is therefore about as fast as an \texttt{sapply()} loop.
      \vskip1ex
      The package \emph{xts} also has several specialized functionals for aggregating data over \emph{end points}:
      \begin{itemize}
        \item \texttt{period.sum()} calculate the sum for each period,
        \item \texttt{period.max()} calculate the maximum for each period,
        \item \texttt{period.min()} calculate the minimum for each period,
        \item \texttt{period.prod()} calculate the product for each period,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Define functional for trailing aggregations over endd
roll_agg <- function(xtsv, endd, FUN, ...) {
  nrows <- NROW(endd)
# startp are single-period lag of endd
  startp <- c(1, endd[1:(nrows-1)])
# Perform aggregations over look_backs list
  aggs <- lapply(look_backs,
    function(look_back) FUN(xtsv[look_back], ...))  # end lapply
# rbind list into single xts or matrix
  aggs <- rutils::do_call(rbind, aggs)
  if (!is.xts(aggs))
    aggs <-  # Coerce aggs into xts series
    xts(aggs, order.by=zoo::index(xtsv[endd]))
  aggs
}  # end roll_agg
# Apply sum() over endd
aggs <- roll_agg(closep, endd=endd, FUN=sum)
aggs <- period.apply(closep, INDEX=endd, FUN=sum)
# Benchmark the speed of aggregation functions
summary(microbenchmark(
  roll_agg=roll_agg(closep, endd=endd, FUN=sum),
  period_apply=period.apply(closep, INDEX=endd, FUN=sum),
  times=10))[, c(1, 4, 5)]
aggs <- period.sum(closep, INDEX=endd)
head(aggs)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations of \protect\emph{xts} Over Calendar Periods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{xts} has convenience wrapper functionals for \texttt{period.apply()}, that apply functions over calendar periods:
      \begin{itemize}
        \item \texttt{apply.daily()} applies functions over daily periods,
        \item \texttt{apply.weekly()} applies functions over weekly periods,
        \item \texttt{apply.monthly()} applies functions over monthly periods,
        \item \texttt{apply.quarterly()} applies functions over quarterly periods,
        \item \texttt{apply.yearly()} applies functions over yearly periods,
      \end{itemize}
      These functionals don't require specifying a vector of \emph{end points}, because they determine the \emph{end points} from the calendar periods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# library(rutils)  # Load package rutils
# Load package HighFreq
library(HighFreq)
# Extract closing minutely prices
closep <- quantmod::Cl(rutils::etfenv$VTI["2019"])
# Apply "mean" over daily periods
aggs <- apply.daily(closep, FUN=sum)
head(aggs)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations Over Overlapping Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} performs aggregations over \emph{non-overlapping} intervals.
      \vskip1ex
      But it's often necessary to perform aggregations over \emph{overlapping} intervals, defined by a vector of \emph{end points} and a \emph{look-back interval}.
      \vskip1ex
      The \emph{start points} are defined as the \emph{end points} lagged by the interval width (number of periods in the \emph{look-back interval}).
      \vskip1ex
      Each point in time has an associated \emph{look-back interval}, which starts at a certain number of periods in the past (\emph{start\_point}) and ends at that point (\emph{end\_point}).
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the \emph{look-back interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define endd with beginning stub
npoints <- 5
nrows <- NROW(closep)
nagg <- nrows %/% npoints
endd <- c(0, nrows-npoints*nagg + (0:nagg)*npoints)
# Number of data points in look_back interval
look_back <- 22
# startp are endd lagged by look_back
startp <- (endd - look_back + 1)
startp <- ifelse(startp < 0, 0, startp)
# Perform lapply() loop over look_backs list
aggs <- lapply(2:NROW(endd), function(it) {
      xtsv <- closep[startp[it]:endd[it]]
      c(max=max(xtsv), min=min(xtsv))
})  # end lapply
# rbind list into single xts or matrix
aggs <- rutils::do_call(rbind, aggs)
# Coerce aggs into xts series
aggs <- xts(aggs, order.by=zoo::index(closep[endd]))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("red", "green")
quantmod::chart_Series(aggs, theme=plot_theme,
             name="price aggregations")
legend("top", legend=colnames(aggs),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extending Interval Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Interval aggregations produce values only at the \emph{end points}, but they can be carried forward in time using the function \texttt{na.locf.xts()} from package \emph{xts}.
      <<echo=TRUE,eval=FALSE>>=
aggs <- cbind(closep, aggs)
tail(aggs)
aggs <- na.omit(xts:::na.locf.xts(aggs))
# Plot aggregations with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "red", "green")
quantmod::chart_Series(aggs, theme=plot_theme, name="price aggregations")
legend("top", legend=colnames(aggs),
  bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/agg_interval_carryfwd.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations of \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The method \texttt{aggregate.zoo()} performs aggregations of \emph{zoo} series over non-overlapping intervals defined by a vector of aggregation groups (minutes, hours, days, etc.).
      \vskip1ex
      For example, \texttt{aggregate.zoo()} can calculate the average monthly returns.
      <<echo=(-(1:3)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(zoo)  # Load package zoo
# Create zoo time series of random returns
datev <- Sys.Date() + 0:365
zoo_series <- zoo(rnorm(NROW(datev)), order.by=datev)
# Create monthly dates
dates_agg <- as.Date(as.yearmon(zoo::index(zoo_series)))
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=datev_agg, FUN=mean)
# Merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# Replace NA's using locf
zoo_agg <- na.locf(zoo_agg, na.rm=FALSE)
# Extract aggregated zoo
zoo_agg <- zoo_agg[zoo::index(zoo_series), 2]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_agg-1}
      \vspace{-7em}
      <<zoo_agg,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
# library(rutils)  # Load package rutils
# Plot original and aggregated cumulative returns
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, bty="n",
       title="Aggregated Prices", y.intersp=0.4,
       leg=c("orig prices", "agg prices"),
       lwd=2, bg="white", col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpolating \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has two functions for replacing \texttt{NA} values using interpolation:
      \begin{itemize}
        \item \texttt{na.approx()} performs linear interpolation,
        \item \texttt{na.spline()} performs spline interpolation,
      \end{itemize}
      \vspace{-1em}
      <<zoo_interpol,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Perform monthly mean aggregation
zoo_agg <- aggregate(zoo_series, by=datev_agg, FUN=mean)
# Merge with original zoo - union of dates
zoo_agg <- cbind(zoo_series, zoo_agg)
# Replace NA's using linear interpolation
zoo_agg <- na.approx(zoo_agg)
# Extract interpolated zoo
zoo_agg <- zoo_agg[zoo::index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_agg), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Interpolated Prices",
       leg=c("orig prices", "interpol prices"), lwd=2, bg="white",
       col=c("black", "red"), bty="n", y.intersp=0.4)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_interpol-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Trailing Aggregations Over \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has several functions for trailing calculations:
      \begin{itemize}
        \item \texttt{rollapply()} performing aggregations over a trailing (sliding) interval,
        \item \texttt{rollmean()} calculating trailing means,
        \item \texttt{rollmedian()} calculating trailing median,
        \item \texttt{rollmax()} calculating trailing max,
      \end{itemize}
      \vspace{-1em}
      <<zoo_roll,echo=(-(1:1)),eval=FALSE,fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# "mean" aggregation over interval with width=11
zoo_mean <- rollapply(zoo_series, width=11, FUN=mean, align="right")
# Merge with original zoo - union of dates
zoo_mean <- cbind(zoo_series, zoo_mean)
# Replace NA's using na.locf
zoo_mean <- na.locf(zoo_mean, na.rm=FALSE, fromLast=TRUE)
# Extract mean zoo
zoo_mean <- zoo_mean[zoo::index(zoo_series), 2]
# Plot original and interpolated zoo
plot(cumsum(zoo_series), xlab="", ylab="")
lines(cumsum(zoo_mean), lwd=2, col="red")
# Add legend
legend("topright", inset=0.05, cex=0.8, title="Mean Prices",
       leg=c("orig prices", "mean prices"), lwd=2, bg="white",
       col=c("black", "red"), bty="n", y.intersp=0.4)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_roll-1}
      \vspace{-3em}
      The argument \texttt{align="right"} determines that aggregations are taken from the past,
  \end{columns}
\end{block}

\end{frame}


\end{document}
