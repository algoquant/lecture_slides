% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% tikz package for plotting and tables
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% bbm and bbold packages for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Univariate Investment Strategies]{Univariate Investment Strategies}
\subtitle{FRE7241, Spring 2022}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Investor Risk Preferences and Portfolio Selection}


%%%%%%%%%%%%%%%
\subsection{Single Period Binary Gamble}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a single investment (gamble) with a binary outcome: \\
      The investor makes no up-front payments, and either wins an amount $a$ (with probability $p$), or loses an amount $b$ (with probability $q = 1-p$).
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gambl_e <- data.frame(win=c("p", "a", "1 + a"), lose=c("q = 1 - p", "-b", "1 - b"))
rownames(gambl_e) <- c("probability", "payout", "terminal wealth")
# print(xtable(gambl_e), comment=FALSE, size="tiny")
print(xtable(gambl_e), comment=FALSE)
      @
      The initial wealth is equal to $1$ dollar, and the terminal wealth after the gamble is either $1 + a$ (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      \vskip1ex
      The amounts $a$ and $b$ are expressed as percentages of the wealth risked in the gamble, and the ratio $a / b$ is called the \emph{betting odds}.
      \vskip1ex
      The expected return on the gamble is called the \emph{edge} and is equal to: $\mu = p \, a - q \, b$, and the variance of returns is equal to: $\sigma^2 = p \, q \, (a + b)^2$.
    \column{0.5\textwidth}
      If the investor chooses to risk only a fraction $k_f$ of wealth, then the return on the gamble is either $k_f a$ (with probability $p$), or $- k_f b$ (with probability $q = 1-p$).
      \vskip1ex
      The fraction $k_f$ can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      And the expected return on the gamble is equal to: $p \, k_f a - q \, k_f b = k_f \, \mu$.
      \vskip1ex
      If an investor makes decisions exclusively based on the expected return $\mu$, then they would either invest all their wealth ($k_f = 1$) on the gamble if $\mu > 0$, or choose not to invest at all ($k_f = 0$) if $\mu < 0$.
      \vskip1ex
      Without loss of generality we can assume that $p = q = \frac{1}{2}$.
      \vskip1ex
      And then $\mu = 0.5 \, (a - b)$, and $\sigma^2 = 0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{\mu}{\sigma} = \frac{(a - b)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Utility and Fractional Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{expected utility} hypothesis states that investors try to maximize the expected \emph{utility} of wealth, not the expected wealth.
      \vskip1ex
      In 1738 Daniel Bernoulli introduced the concept of \emph{logarithmic utility} in his work \emph{"Specimen Theoriae Novae de Mensura Sortis"} (New Theory of the Measurement of Risk).
      \vskip1ex
      The \emph{logarithmic utility} function is defined as the logarithm of wealth: $u(w) = \log(w)$.
      \vskip1ex
      Under \emph{logarithmic utility} investor preferences depend on the percentage change of wealth, instead of the absolute change of wealth: $\mathrm{d} u(w) = \frac{\mathrm{d}w}{w}$.
      \vskip1ex
      An investor with \emph{logarithmic utility} invests only a fraction $k_f$ of their wealth in a gamble, depending on the risk-return of the gamble.
      \vskip1ex
      If the initial wealth is equal to $1$, then the expected value of \emph{logarithmic utility} for the binary gamble is equal to: $u(k_f) = p \, \log(1 + k_f a) + q \, \log(1 - k_f b)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/util_log.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define logarithmic utility
utili_ty <- function(frac, p=0.3, a=20, b=1) {
  p*log(1+frac*a) + (1-p)*log(1-frac*b)
}  # end utili_ty
# Plot utility
curve(expr=utili_ty, xlim=c(0, 1),
      ylim=c(-0.5, 0.4), xlab="betting fraction",
      ylab="utility", main="", lwd=2)
title(main="Logarithmic Utility", line=0.5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Fractional Betting Under Logarithmic Utility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The betting fraction that maximizes the \emph{utility} can be found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u(k_f)}{\mathrm{d} k_f} = \frac{p \, a}{1 + k_f a} - \frac{q \, b}{1 - k_f b} = 0
      \end{displaymath}
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a} = \frac{p \, a - q \, b}{b \, a} = \frac{\mu}{b \, a}
      \end{displaymath}
      The optimal $k_f$ is called the \emph{Kelly fraction}, and it depends on the parameters of the gamble.
      \vskip1ex
      The \emph{Kelly fraction} can be greater than $1$ (leveraged investing), or it can be negative (shorting).
      \vskip1ex
      If we assume that $b=1$, then the betting odds are equal to $a$ and the \emph{Kelly fraction} is: $k_f = \frac{p (a + 1) - 1}{a}$
      \vskip1ex
      The \emph{Kelly fraction} is then equal to the expected payout divided by the betting odds.
      \vskip1ex
      If the expected payout of the gamble is not positive, then an investor with logarithmic utility should not allocate any capital to the gamble.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_fraction.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define and plot Kelly fraction
kelly_frac <- function(a, p=0.5, b=1) {
  p/b - (1-p)/a
}  # end kelly_frac
curve(expr=kelly_frac, xlim=c(0, 5),
      ylim=c(-2, 1), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="max Kelly fraction=0.5")
title(main="Kelly fraction", line=-0.8)
      @
\end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kelly criterion} states that investors should bet the optimal \emph{Kelly fraction} of their capital in a gamble.
      \vskip1ex
      Investors with concave utility functions (for example logarithmic utility) are sensitive to the risk of ruin (losing all their capital).
      \vskip1ex
      Applying the \emph{Kelly criterion} and betting only a fraction of their capital reduces the risk of ruin (but it doesn't eliminate the risk if prices drop suddenly).
      \vskip1ex
      The loss amount $b$ determines the risk of ruin, with larger values of $b$ increasing the risk of ruin.
      \vskip1ex
      Therefore investors will choose a smaller betting fraction $k_f$ for larger values of $b$.
      \vskip1ex
      This means that even for huge odds in their favor, investors may not choose to invest all their capital, because of the risk of ruin.
      \vskip1ex
      For example, if the betting odds are very large $a \to \infty$, then the \emph{Kelly fraction}: $k_f = \frac{p}{b}$.
    \column{0.5\textwidth}
    \vspace{-1em}
    \includegraphics[width=0.4\paperwidth]{figure/kelly_fraction_max.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5),
      ylim=c(-1, 1.5), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $r_i$ be the random return on the gamble in period $i$, and let $w_i = (1 + k_f r_i)$ be the random wealth increment.
      \vskip1ex
      Then the terminal wealth after $n$ rounds is equal to the compounded wealth increments: $w_n = \prod_{i=1}^n w_i = \prod_{i=1}^n (1 + k_f r_i)$.
      \vskip1ex
      And the utility is equal to the sum of the individual utilities:
      \begin{displaymath}
        u_n = \log(w_n) = \sum_{i=1}^n \log(w_i) = \sum_{i=1}^n \log(1 + k_f r_i) = \sum_{i=1}^n u_i
      \end{displaymath}
      The individual utilities are all maximized by the same \emph{Kelly fraction} $k_f$, so the \emph{Kelly fraction} for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Wealth of Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In multiperiod betting the investor participates in $n$ rounds of gambles, and in each round they risk a fixed fraction $k_f$ of their current outstanding wealth.
      \vskip1ex
      In each round the wealth is multiplied by either $(1 + k_f a)$ (win) or $(1 - k_f b)$ (loss), so that the current outstanding wealth changes over time.
      \vskip1ex
      The terminal wealth after $n$ rounds with $m$ wins is equal to: $w(k_f) = (1 + k_f a)^m (1 - k_f b)^{n-m}$.
      \vskip1ex
      If the number of rounds $n$ is very large, then the number of wins is almost always equal to $m = n \, p$, and the terminal wealth is equal to: $w(k_f) = (1 + k_f a)^{np} (1 - k_f b)^{nq}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_multi.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Wealth of multiperiod binary betting
wealth <- function(f, a=0.8, b=0.1, n=1e3, i=150) {
  (1+f*a)^i * (1-f*b)^(n-i)
}  # end wealth
curve(expr=wealth, xlim=c(0, 1),
      xlab="betting fraction",
      ylab="wealth", main="", lwd=2)
title(main="Wealth of Multiperiod Betting", line=0.1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Multiperiod Betting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.9\textwidth}
      The betting fraction $k_f$ that maximizes the terminal wealth is found by setting the derivative of $w(k_f)$ to zero:
      \begin{flalign*}
        & \frac{\mathrm{d} w(k_f)}{\mathrm{d} k_f} = n p a (1 + k_f a)^{np-1} (1 - k_f b)^{nq} - n q b (1 + k_f a)^{np} (1 - k_f b)^{nq-1} & \\
        & = (\frac{n p a}{1 + k_f a} - \frac{n q b}{1 - k_f b}) (1 + k_f a)^{np} (1 - k_f b)^{nq} = 0 &
      \end{flalign*}
      We can then solve for the optimal betting fraction $k_f$:
      \begin{flalign*}
        \frac{p a}{1 + k_f a} - \frac{q b}{1 - k_f b} = 0 \\
        p a (1 - k_f b) - q b (1 + k_f a) = 0 \\
        p a - q b - k_f a b = 0 \\
        k_f = \frac{p a - q b}{a b} = \frac{p}{b} - \frac{q}{a}
      \end{flalign*}
      The above is just the \emph{Kelly fraction} $k_f$ that maximizes the utility.
      \vskip1ex
      So the \emph{Kelly fraction} $k_f$ that maximizes the utility also maximizes the terminal wealth.
    \column{0.1\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.9\textwidth}
      The terminal wealth after $n$ repeated gambles with $m$ wins is equal to: $(1 + k_f a)^m (1 - k_f b)^{n-m}$.
      \vskip1ex
      And the expected value of the wealth is equal to:
      \begin{displaymath}
        w(k_f) = \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}}}
      \end{displaymath}
      We can then find the fraction $k_f$ which maximizes the expected wealth $w(k_f)$:
      \begin{flalign*}
        \frac{\mathrm{d} w(k_f)}{\mathrm{d} k_f} = \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}} (\frac{a m}{1 + k_f a} - \frac{b (n-m)}{1 - k_f b}) = \\
        \frac{a}{1 + k_f a} \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {m} - \\
        \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} {(1 + k_f a)^m (1 - k_f b)^{n-m}} (\frac{a m}{1 + k_f a} - \frac{b (n-m)}{1 - k_f b}) \\
      \end{flalign*}
      If the investor chooses to risk only a fraction $k_f$ of wealth, then the wealth after the gamble is either $1 + k_f a$ (with probability $p$), or $1 - k_f b$ (with probability $q = 1-p$).
      \vskip1ex
      (with probability $p$), or $1 - b$ (with probability $q = 1-p$).
      initial wealth is equal to $1$, and the
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(k_f) &= \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} \log((1 + k_f a)^m (1 - k_f b)^{n-m}) &\\
        &= \log(1 + k_f a) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} m} + &\\
        & \log(1 - k_f b) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} (n-m)} &\\
        &= n \, p \, \log(1 + k_f a) + n \, q \, \log(1 - k_f b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $k_f$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.1\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Utility of Multiperiod Binary Gambles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      The \emph{Kelly fraction} for multiperiod betting can be found by maximizing the expected \emph{utility} of the final wealth distribution:
      \begin{flalign*}
        u(k_f) &= \sum_{m=0}^n \binom{n}{m} p^m q^{n-m} \log((1 + k_f a)^m (1 - k_f b)^{n-m}) &\\
        &= \log(1 + k_f a) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} m} + &\\
        & \log(1 - k_f b) \sum_{m=0}^n {\binom{n}{m} p^m q^{n-m} (n-m)} &\\
        &= n \, p \, \log(1 + k_f a) + n \, q \, \log(1 - k_f b)
      \end{flalign*}
      The above is just the single period \emph{utility} multiplied by the number of rounds of betting $n$.
      \vskip1ex
      The \emph{Kelly fraction} $k_f$ for multiperiod betting is the same as for single period betting:
      \begin{displaymath}
        k_f = \frac{p}{b}-\frac{q}{a}
      \end{displaymath}
    \column{0.3\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Margin}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $r_i$ be the percentage returns on a \emph{risky asset}, so that the asset price $p_t$ at time $t$ is given by:
      \begin{displaymath}
        p_t = p_0 \prod_{i=1}^t {(1 + r_i)}
      \end{displaymath}
      The initial investor wealth at time $t=0$ is equal to $1$ dollar, and they also borrow on margin $m$ dollars to invest in the \emph{risky asset}.
      \vskip1ex
      The investor's \emph{wealth} at time $t$ is equal to (the margin borrowing rate is assumed to be zero):
      \begin{displaymath}
        w_t = 1 + m \, \frac{p_t - p_0}{p_0}
      \end{displaymath}
      The \emph{leverage} $k_f$ is equal to the \emph{margin debt} $m$ divided by the total wealth $w_t$: $k_f = m / w_t$.
      \vskip1ex
      If the asset price drops then the \emph{leverage} increases, because the \emph{margin debt} is fixed while the wealth drops.
      \vskip1ex
      If the asset price drops enough so that the wealth reaches zero, then the investment is liquidated and the investor is ruined.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/simu_prices.png}
      <<echo=(-(1:5)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
set.seed(1121)  # Reset random number generator
# Simulate asset prices
calc_prices <- function(x) cumprod(1 + rnorm(1e3, sd=0.01))
price_paths <- sapply(1:3, calc_prices)
plot(price_paths[, 1], type="l", lwd=3,
     main="Simulated Asset Prices",
     ylim=range(price_paths),
     lty="solid", xlab="time", ylab="price")
lines(price_paths[, 2], col="blue", lwd=3)
lines(price_paths[, 3], col="orange", lwd=3)
abline(h=0.5, col="red", lwd=3)
text(x=200, y=0.5, pos=3, labels="liquidation threshold")
      @
  \end{columns}
\end{block}


\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investing With Fixed Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In order to avoid ruin, the investor may choose to maintain a fixed \emph{leverage ratio} equal to $k_f$, so that the amount invested in the \emph{risky asset} is proportional to the \emph{wealth}: $k_f \, w_t$.
      \vskip1ex
      This requires buying the \emph{risky asset} when its price increases, and selling it when it drops.
      \vskip1ex
      The return on the \emph{risky asset} in a single period is equal to: $k_f \, w_t \, r_t$, so the \emph{terminal wealth} at time $t$ is equal to the compounded returns:
      \begin{displaymath}
        w_t = (1 + k_f \, r1) \ldots (1 + k_f \, r_t) = \prod_{i=1}^t {(1 + k_f \, r_i)}
      \end{displaymath}
      The utility of the \emph{terminal wealth} is equal to the sum of the utilities of single periods:
      \begin{flalign*}
        & \mathbbm{E}[\log{w_t}] = \mathbbm{E}[\log((1 + k_f \, r1) \ldots (1 + k_f \, r_t))] &\\
        & = \sum_{i=1}^t {\mathbbm{E}[\log{(1 + k_f \, r_i)}]} = t \, \mathbbm{E}[\log{(1 + k_f \, r)}]
      \end{flalign*}
      The last equality holds because all the utilities of single periods are the same.
    \column{0.5\textwidth}
      Let the returns over a short time period be equal to $r$, with probability distribution $p(r)$.
      \vskip1ex
      The mean return $\bar{r}$, and variance $\sigma^2$ are:
      \begin{displaymath}
        \bar{r} = \int {r \, p(r) \, \mathrm{d}r} \; ; \quad
        \sigma^2 = \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      Since the returns are over a short time period, we have: $r \ll 1$ and $\bar{r} \ll \sigma$, so that we can replace $r - \bar{r}$ with $r$ as follows:
      \begin{displaymath}
        \int {(r - \bar{r})^2 \, p(r) \, \mathrm{d}r} \approx \int {r^2 \, p(r) \, \mathrm{d}r}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Leveraged Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      So the utility of the \emph{terminal wealth} $u_t$ is equal to the utility of a single period times the number of periods:
      \begin{displaymath}
        u_t = \mathbbm{E}[\log{w_t}] = t \, \mathbbm{E}[\log{(1 + k_f \, r)}] = t \, u_r
      \end{displaymath}
      The utility of the asset returns $u_r$ is equal to:
      \begin{displaymath}
        u_r = \mathbbm{E}[\log{(1 + k_f \, r)}] = \int {\log(1 + k_f \, r) \, p(r) \, \mathrm{d}r}
      \end{displaymath}
      The leverage $k_f$ is limited so that $(1 + k_f \, r) > 0$ for all return values $r$.
      \vskip1ex
      If the mean returns are positive, then at first the utility increases with leverage, but only up to a point.
      \vskip1ex
      With higher leverage, the negative utility of time periods with negative returns becomes significant, forcing the aggregate utility to drop.
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)
# Calculate the VTI returns
vtis <- rutils::etfenv$returns$VTI
vtis <- na.omit(vtis)
c(mean=mean(vtis), std=sd(vtis))
range(vtis)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/util_rets.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define vectorized logarithmic utility function
utili_ty <- function(kell_y, returns) {
  sapply(kell_y, function(x)
    sum(log(1 + x*returns)))
}  # end utili_ty
utili_ty(1, vtis)
utili_ty(c(1, 4), vtis)
# Plot the logarithmic utility
curve(expr=utili_ty(x, returns=vtis),
      xlim=c(0.1, 5), xlab="leverage", ylab="utility",
      main="Utility of Asset Returns", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Criterion for Optimal Leverage of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logarithmic utility}  $u_r$ can be expanded in the moments of the return distribution:
      \begin{flalign*}
        & u_r = \mathbbm{E}[\log{(1 + k_f \, r)}] = \int {\log(1 + k_f \, r) \, p(r) \, \mathrm{d}r} & \\
        & = \int {(k_f \, r - \frac{(k_f \, r)^2}{2} + \frac{(k_f \, r)^3}{3} - \frac{(k_f \, r)^4}{4}) \, p(r) \, \mathrm{d}r} & \\
        & = k_f \bar{r} - \frac{k_f^2 \sigma^2}{2} + \frac{k_f^3 \sigma^3 \varsigma}{3} - \frac{k_f^4 \sigma^4 \kappa}{4}
      \end{flalign*}
      Where $\varsigma = \int {\frac{r^3}{\sigma^3} \, p(r) \, \mathrm{d}r}$ is the \emph{skewness}, and $\kappa = \int {\frac{r^4}{\sigma^4} \, p(r) \, \mathrm{d}r}$ is the \emph{kurtosis}.
      \vskip1ex
      The \emph{Kelly leverage} which maximizes the \emph{utility} is found by equating the derivative of \emph{utility} to zero:
      \begin{displaymath}
        \frac{\mathrm{d} u_r}{\mathrm{d}k_f} = \bar{r} - k_f \sigma^2 + k_f^2 \sigma^3 \varsigma - k_f^3 \sigma^4 \kappa = 0
      \end{displaymath}
      % wippp
      This shows that the logarithmic utility has positive odd derivatives and negative even derivatives.
    \column{0.5\textwidth}
      Assuming that the third and fourth moments $\sigma^4 \varsigma$ and $\sigma^4 \kappa$ are small and can be neglected, we get:
      \begin{displaymath}
        k_f = \frac{\bar{r}}{\sigma^2} = \frac{S_r}{\sigma} \; ; \quad u_r = \frac{1}{2} \frac{{\bar{r}}^2}{\sigma^2} = \frac{1}{2} S_r^2
      \end{displaymath}
      The \emph{Kelly leverage} is \emph{approximately} equal to the \emph{Sharpe ratio} divided by the \emph{standard deviation}.
      \vskip1ex
      The optimal utility $u_r$ is \emph{approximately} equal to half the \emph{Sharpe ratio} $S_r$ squared.
      \vskip1ex
      The \emph{standard deviation} and \emph{Sharpe ratio} are calculated over the same time interval as the returns (not annualized).
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly leverage
mean(vtis)/var(vtis)
PerformanceAnalytics::KellyRatio(R=vtis, method="full")
# Kelly leverage
unlist(optimize(
  f=function(x) -utili_ty(x, vtis),
  interval=c(1, 4)))
      @
    \vspace{-1em}
%    \vspace{-1em}
%    \includegraphics[width=0.4\paperwidth]{figure/kelly_returns-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy Wealth Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth of a Kelly Strategy with a fixed leverage ratio $k_f$ is equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_i)}
      \end{displaymath}
      The \emph{Kelly fraction} $k_f$ provides the optimal leverage to maximize the utility of wealth, by balancing the benefit of leveraging higher positive returns, with the risk of ruin due to excessive leverage.
      \vskip1ex
      If the mean asset returns are positive, then a higher leverage ratio provides higher returns.
      \vskip1ex
      But if the leverage is too high, then the losses in periods with negative returns wipe out most of the wealth, so then it's slow to recover.
      <<echo=(-(1:2)),eval=FALSE>>=
# Calculate the VTI returns
vtis <- rutils::etfenv$returns$VTI
vtis <- na.omit(vtis)
# Calculate wealth paths
kelly_ratio <- drop(mean(vtis)/var(vtis))
kelly_wealth <- cumprod(1 + kelly_ratio*vtis)
hyper_kelly <- cumprod(1 + (kelly_ratio+2)*vtis)
sub_kelly <- cumprod(1 + (kelly_ratio-2)*vtis)
kelly_paths <- cbind(kelly_wealth, hyper_kelly, sub_kelly)
colnames(kelly_paths) <- c("kelly", "hyper-kelly", "sub-kelly")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth paths
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "orange", "blue")
quantmod::chart_Series(kelly_paths, theme=plot_theme, name="Wealth Paths")
legend("topleft", legend=colnames(kelly_paths),
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy With Margin Account}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{margin debt} $m_t$ is equal to the dollar amount borrowed to purchase the \emph{risky asset}.
      \vskip1ex
      The wealth $w_t$ at time $t$ is equal to the initial wealth $w_0 = 1$ plus the dollar amount of the \emph{risky asset} $a_t$, minus the \emph{margin debt} $m_t$: $w_t = 1 + a_t - m_t$.
      \vskip1ex
      The dollar amount of the \emph{risky asset} $a_t$ is equal to the wealth $w_t$ times the \emph{leverage} $k_f$: $a_t = k_f w_t$.
      \vskip1ex
      So the \emph{margin debt} $m_t$ is proportional to the wealth $w_t$: $m_t = (k_f - 1) w_t + 1$.
      \vskip1ex
      The wealth changes from $w_{t-1}$ to: $w_t = w_{t-1} (1 + k_f \, r_t)$, while the dollar amount of the \emph{risky asset} changes from $a_{t-1} = k_f w_{t-1}$ to: $a_t = k_f w_{t-1} (1 + r_t)$, so that the leverage changes from $k_f$ to:
      \begin{displaymath}
        \frac{k_f w_{t-1} (1 + r_t)}{w_{t-1} (1 + k_f \, r_t)} = \frac{k_f (1 + r_t)}{1 + k_f \, r_t}
      \end{displaymath}
    \column{0.5\textwidth}
      In order to maintain a fixed \emph{leverage ratio} equal to $k_f$, the investor must actively trade the \emph{risky asset}, and the \emph{margin debt} $m_t$ changes over time.
      \vskip1ex
      The change in margin in a single time period is equal to:
      \begin{displaymath}
        \Delta m_t = (k_f - 1) \Delta w_t = k_f (k_f - 1) w_{t-1} r_t
      \end{displaymath}
      The dollar amount of the \emph{risky asset} traded is equal to the change in \emph{margin}.
      \vskip1ex
      Therefore the investor must borrow on margin and buy the \emph{risky asset} when its price increases, and sell it when it drops.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Strategy With Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} is the percentage difference between the \emph{offer} minus the \emph{bid} price, divided by the \emph{mid} price.
      \vskip1ex
      The \emph{bid-offer spread} for liquid stocks can be assumed to be about \texttt{10} basis points (bps).
      \vskip1ex
      The \emph{transaction costs} $c^r$ due to the \emph{bid-offer spread} are equal to half the \emph{bid-offer spread} $\delta$ times the absolute value of the traded dollar amount of the \emph{risky asset}:
      \begin{displaymath}
        c^r = \frac{\delta}{2} \left| \Delta m_t \right|
      \end{displaymath}
      If the transaction costs are much less than the change in wealth $c^r \ll \left| \Delta w_t \right|$, then we can write approximately:
      \begin{displaymath}
        c^r = \frac{\delta}{2} k_f (k_f - 1) w_{t-1} \left| r_t \right|
      \end{displaymath}
    \column{0.5\textwidth}
      The wealth of the Kelly Strategy after accounting for the \emph{bid-offer spread} is then equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_i - \frac{\delta}{2} k_f (k_f - 1) \left| r_i \right|)}
      \end{displaymath}
      The effect of the \emph{bid-offer spread} is to reduce the effective asset returns by an amount proportional to the \emph{bid-offer spread}.
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate wealth paths
kelly_ratio <- drop(mean(vtis)/var(vtis))
wealth <- cumprod(1 + kelly_ratio*vtis)
wealth_trans <- cumprod(1 + kelly_ratio*vtis - 
  0.5*bid_offer*kelly_ratio*(kelly_ratio-1)*abs(vtis))
# Calculate compounded wealth from returns
wealth <- cbind(wealth, wealth_trans)
colnames(wealth) <- c("Kelly", "Including bid-offer")
# Plot compounded wealth
dygraphs::dygraph(wealth, main="Kelly Strategy With Transaction Costs") %>%
  dyOptions(colors=c("green", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Half-Kelly Criterion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In reality investors don't know the probability of winning or the odds of the gamble, so they can't accurately calculate the optimal \emph{Kelly fraction}.
      \vskip1ex
      The \emph{Kelly fraction}: $k_f = \frac{\bar{r}}{\sigma^2}$ is especially sensitive to the uncertainty of the expected returns $\bar{r}$.
      \vskip1ex
      If the expected returns are over-estimated, then it can produce an inflated value of the \emph{Kelly fraction}, leading to ruin.
      \vskip1ex
      The risk of applying too much leverage (over-betting) is much greater than the risk of applying too little leverage (under-betting).
      \vskip1ex
      Too much leverage (over-betting) not only reduces returns, but it increases the risk of ruin.
      \vskip1ex
      So in practice many investors apply only half the theoretical \emph{Kelly fraction} (the Half-Kelly), to reduce the risk of ruin.
    \column{0.5\textwidth}
      Perform bootstrap simulation to obtain the standard error of the \emph{Kelly fraction}.
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot several Kelly curves
curve(expr=kelly_frac(x, b=1), xlim=c(0, 5),
      ylim=c(-1, 1.5), xlab="betting odds",
      ylab="kelly fraction", main="", lwd=2)
abline(h=0.5, lwd=2, col="red")
text(x=1.5, y=0.5, pos=3, cex=0.8, labels="b=1.0; max fraction=0.5")
curve(expr=kelly_frac(x, b=0.5), add=TRUE, main="", lwd=2)
abline(h=1.0, lwd=2, col="red")
text(x=1.5, y=1.0, pos=3, cex=0.8, labels="b=0.5; max fraction=1.0")
title(main="Kelly fraction", line=-0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Risk aversion is the investor preference to avoid losses more than to seek similar percentage gains in wealth.
      \vskip1ex
      For example, for a risk averse investor, a $10\%$ loss of wealth is more important than a $10\%$ gain.
      \vskip1ex
      Risk aversion is associated with the \emph{diminishing marginal utility} of the percentage change in wealth $\Delta w$.
      \vskip1ex
      This manifests itself as a concave utility function, with a negative second derivative $u''(w) < 0$.
      \vskip1ex
      For example, the \emph{logarithmic utility} function is concave.
      \vskip1ex
      The Arrow-Pratt coefficient of relative risk aversion is proportional to the convexity $u''(w)$ of the utility, and is defined as: $\eta = - \frac{w \, u''(w)}{u'(w)}$.
      \vskip1ex
      The relative risk aversion of \emph{logarithmic utility} is equal to one: $\eta = 1$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/util_log2.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Plot logarithmic utility function
curve(expr=log, lwd=3, col="blue", xlim=c(0.5, 5),
      xlab="wealth", ylab="utility",
      main="Logarithmic Utility")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Relative Risk Aversion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      It's not a given that all investors have a risk aversion coefficient equal to $1$, and other \emph{utility functions} are possible.
      \vskip1ex
      The Constant Relative Risk Aversion (\emph{CRRA}) utility function is a generalization of logarithmic utility:
      \begin{displaymath}
        u(w) = \frac{w^{1 - \eta} - 1}{1 - \eta}
      \end{displaymath}
      Where $\eta$ is the risk aversion parameter.
      \vskip1ex
      The relative risk aversion of the \emph{CRRA} utility function is constant and equal to $\eta$.
      \vskip1ex
      When the risk aversion parameter is equal to one $\eta = 1$, then the \emph{CRRA} utility function is equal to the logarithmic utility.
      \vskip1ex
      In practice, the risk aversion parameter $\eta$ is not known, and must be estimated through empirical studies.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/util_crra.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define CRRA utility
cr_ra <- function(w, ra) {
  (w^(1-ra) - 1)/(1-ra)
}  # end cr_ra
# Plot utility functions
curve(expr=cr_ra(x, ra=0.7), xlim=c(0.5, 5), lwd=3,
      xlab="wealth", ylab="utility", main="", col="blue")
curve(expr=log, add=TRUE, lwd=3)
curve(expr=cr_ra(x, ra=1.3), add=TRUE, lwd=3, col="red")
# Add title and legend
title(main="CRRA Utility", line=0.5)
legend(x="topleft", legend=c("risk seeking", "logarithmic", "risk averse"),
       title="Risk Aversion", inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, bty="n", col=c("blue", "black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: CRRA Optimal Leverage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      It's not a given that all investors have a risk aversion coefficient equal to $1$, and other \emph{utility functions} are possible.
      \vskip1ex
      The Constant Relative Risk Aversion (\emph{CRRA}) utility function is a generalization of logarithmic utility:
      \begin{displaymath}
        u(w) = \frac{w^{1 - \eta} - 1}{1 - \eta}
      \end{displaymath}
      Where $\eta$ is the risk aversion parameter.
      \vskip1ex
      The relative risk aversion of the \emph{CRRA} utility function is constant and equal to $\eta$.
      \vskip1ex
      When the risk aversion parameter is equal to one $\eta = 1$, then the \emph{CRRA} utility function is equal to the logarithmic utility.
      \vskip1ex
      In practice, the risk aversion parameter $\eta$ is not known, and must be estimated through empirical studies.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/util_crra.png}
      <<echo=(-(1:4)),eval=FALSE>>=
# Open x11 for plotting
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
# Define CRRA utility
cr_ra <- function(w, ra) {
  (w^(1-ra) - 1)/(1-ra)
}  # end cr_ra
# Plot utility functions
curve(expr=cr_ra(x, ra=0.7), xlim=c(0.5, 5), lwd=3,
      xlab="wealth", ylab="utility", main="", col="blue")
curve(expr=log, add=TRUE, lwd=3)
curve(expr=cr_ra(x, ra=1.3), add=TRUE, lwd=3, col="red")
# Add title and legend
title(main="CRRA Utility", line=0.5)
legend(x="topleft", legend=c("risk seeking", "logarithmic", "risk averse"),
       title="Risk Aversion", inset=0.05, cex=0.8, bg="white",
       lwd=6, lty=1, bty="n", col=c("blue", "black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{draft: CRRA Strategy Wealth Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth of a Kelly Strategy with a fixed leverage ratio $k_f$ is equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_i)}
      \end{displaymath}
      The \emph{Kelly fraction} $k_f$ provides the optimal leverage to maximize the utility of wealth, by balancing the benefit of leveraging higher positive returns, with the risk of ruin due to excessive leverage.
      \vskip1ex
      If the mean asset returns are positive, then a higher leverage ratio provides higher returns.
      \vskip1ex
      But if the leverage is too high, then the losses in periods with negative returns wipe out most of the wealth, so then it's slow to recover.
      <<echo=(-(1:2)),eval=FALSE>>=
# Calculate the VTI returns
vtis <- rutils::etfenv$returns$VTI
vtis <- na.omit(vtis)
# Calculate wealth paths
kelly_ratio <- drop(mean(vtis)/var(vtis))
kelly_wealth <- cumprod(1 + kelly_ratio*vtis)
hyper_kelly <- cumprod(1 + (kelly_ratio+2)*vtis)
sub_kelly <- cumprod(1 + (kelly_ratio-2)*vtis)
kelly_paths <- cbind(kelly_wealth, hyper_kelly, sub_kelly)
colnames(kelly_paths) <- c("kelly", "hyper-kelly", "sub-kelly")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth paths
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("black", "orange", "blue")
quantmod::chart_Series(kelly_paths, theme=plot_theme,
             name="Wealth Paths")
legend("topleft", legend=colnames(kelly_paths),
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Utility of Lottery Tickets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Lottery tickets are equivalent to binary gambles with a very small probability of winning $p$, but a very large winning amount $a$, and a small loss amount $b$ equal to the ticket price.
      \vskip1ex
      The expected payout $\mu = p \, a - q \, b$ of most lottery tickets is negative.
      \vskip1ex
      So under \emph{logarithmic utility}, the Kelly fraction $k_f$ for most lottery tickets is also negative, meaning that investors should not be expected to buy these lottery tickets.
      \vskip1ex
      But in reality many people do buy lottery tickets with negative expected payouts, which means that their utility functions are not logarithmic.
      \vskip1ex
      The demand for lottery tickets can be explained by assuming a strong demand for positive \emph{skewness}, which exceeds the demand for a positive payout.
      \vskip1ex
      People buy lottery tickets because they want a small chance of a very large payout, even if the average payout is negative.
    \column{0.5\textwidth}
      Without loss of generality we can assume that the lottery ticket price is one dollar $b = 1$, that it pays out $a$ dollars, and that the expected payout is equal to zero: $\mu = p \, a - q \, b = 0$.
      \vskip1ex
      Then the probabilities of winning and losing are equal to:
      $p = \frac{1}{a + 1}$ and $q = \frac{a}{a + 1}$.
      \vskip1ex
      The variance is equal to: $\sigma^2 = p \, q \, (a + 1)^2 = a$.
      \vskip1ex
      And the \emph{skewness} is equal to:
      $\varsigma = \frac{1}{\sigma^3} (\frac{a^3}{a + 1} - \frac{a}{a + 1}) = \frac{a - 1}{\sqrt{a}}$.
      \vskip1ex
      So the positive \emph{skewness} of a lottery ticket increases as the square root of the \emph{betting odds} $a$, and it can become very large for large \emph{betting odds}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor \protect\emph{Risk Aversion}, \protect\emph{Prudence} and \protect\emph{Temperance}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investor risk and return preferences depend on the signs of the derivatives of their \emph{utility} function.
      \vskip1ex
      Investors with \emph{logarithmic utility} have positive \emph{odd} derivatives ($u'(w) > 0$ and $u'''(w) > 0$) and negative \emph{even} derivatives ($u''(w) < 0$ and $u''''(w) < 0$), which is typical for most other investors as well.
      \vskip1ex
      \emph{Risk averse} investors have a negative second derivative of utility $u''(w) < 0$.
      \vskip1ex
      The demand for lottery tickets shows that investors' utility typically has a positive third derivative $u'''(w) > 0$.
      \vskip1ex
      Positive \emph{odd} derivatives imply a preference for larger \emph{odd moments} of the change in the wealth distribution (mean, skewness).
      \vskip1ex
      Negative \emph{even} derivatives imply a preference for smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      The preference for smaller \emph{variance} is called \emph{risk aversion}, for larger \emph{skewness} is called \emph{prudence}, and for smaller \emph{kurtosis} is called \emph{temperance}.
    \column{0.5\textwidth}
      The expected change of the \emph{utility} of wealth $\mathbb{E}[\Delta u(w)]$ can be expanded in the moments of the wealth distribution $\Delta w$:
      \begin{flalign*}
        \mathbb{E}[\Delta u(w)] &= u'(w) \mathbb{E}[\Delta w] + \frac{u''(w)}{2} \sigma^2 &\\
        & + \frac{u'''(w)}{3!} \mu3 + \frac{u''''(w)}{4!} \mu3
      \end{flalign*}
      Where $\mathbb{E}[\Delta w]$ is the expected change of wealth, $\sigma^2 = \int {{\Delta w}^2 \, p(w) \, \mathrm{d}w}$ is the \emph{variance} of The change in wealth, and $\mu3 = \int {{\Delta w}^3 \, p(w) \, \mathrm{d}w} = \sigma^3 \varsigma$ and $\mu4 = \int {{\Delta w}^4 \, p(w) \, \mathrm{d}w} = \sigma^4 \kappa$ are the third and fourth moments, proportional to the \emph{skewness} $\varsigma$ and the \emph{kurtosis} $\kappa$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Preferences and Empirical Return Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The investor preference for higher \emph{returns} and for lower \emph{volatility} is expressed by maximizing the \emph{Sharpe ratio}.
      \vskip1ex
      The third and fourth moments of asset returns are usually much smaller than the \emph{variance}, so they typically have a smaller effect on the investor risk and return preferences.
      \vskip1ex
      Nevertheless, there is evidence that investors also have significant preferences for positive \emph{skewness} and lower \emph{kurtosis}.
      \vskip1ex
      But stock returns typically have negative \emph{skewness} and excess \emph{kurtosis}, the opposite of what investors prefer.
      \vskip1ex
      Many investors may prefer positive \emph{skewness}, even at the expense of lower \emph{returns}, similar to the buyers of lottery tickets.
      \vskip1ex
      A paper by Amaya asks if the
      \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1898735}{\emph{Realized Skewness Predicts the Cross-Section of Equity Returns?}}
      \vskip1ex
      But higher moments are hard to estimate accurately from low frequency (daily) returns, which makes empirical investigations more difficult.
    \column{0.5\textwidth}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
vtis <- rutils::etfenv$vtis$VTI
vtis <- na.omit(vtis)
# Calculate higher moments of VTI returns
c(mean=sum(vtis),
  variance=sum(vtis^2),
  mom3=sum(vtis^3),
  mom4=sum(vtis^4))/NROW(vtis)
# Calculate higher moments of minutely SPY returns
sp_y <- HighFreq::SPY[, 4]
sp_y <- na.omit(sp_y)
sp_y <- HighFreq::diffit(log(sp_y))
c(mean=sum(sp_y),
  variance=sum(sp_y^2),
  mom3=sum(sp_y^3),
  mom4=sum(sp_y^4))/NROW(sp_y)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Utility of Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The utility $u$ of the stock and bond portfolio with weights $w_s, w_b$ is equal to:
      \begin{displaymath}
        u = \sum_{i=1}^n {\log(1 + w_s \, r^s_i + w_b \, r^b_i)}
      \end{displaymath}
      Where $r^s_i, r^b_i$ are the stock and bond returns.
      <<echo=TRUE,eval=FALSE>>=
returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
# Logarithmic utility of stock and bond portfolio
utili_ty <- function(w_s, w_b) {
  -sum(log(1 + w_s*returns$VTI + w_b*returns$IEF))
}  # end utili_ty
# Create matrix of utility values
w_s <- seq(from=3, to=7, by=0.2)
w_b <- seq(from=12, to=20, by=0.2)
utility_mat <- sapply(w_b, function(y) sapply(w_s,
  function(x) utili_ty(x, y)))
# Set rgl options and load package rgl
options(rgl.useNULL=TRUE)
library(rgl)
# Draw 3d surface plot of utility
rgl::persp3d(w_s, w_b, utility_mat, col="green",
        xlab="stocks", ylab="bonds", zlab="utility")
# Render the surface plot
rgl::rglwidget(elementId="plot3drgl")
# Save the surface plot to png file
rgl::rgl.snapshot("utility_surface.png")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/utility_surface.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly optimal stock and bond portfolio weights $w_s, w_b$ can be calculated by maximizing the utility $u$.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
weights <- sapply(returns, function(x) mean(x)/var(x))
# Kelly weight for stocks
unlist(optimize(f=function(x) utili_ty(x, w_b=0), interval=c(1, 4)))
# Kelly weight for bonds
unlist(optimize(f=function(x) utili_ty(x, w_s=0), interval=c(1, 14)))
# Vectorized utility of stock and bond portfolio
utility_vec <- function(weights) {
  utili_ty(weights[1], weights[2])
}  # end utility_vec
# Optimize with respect to vector argument
optimd <- optim(fn=utility_vec, par=c(3, 10),
                method="L-BFGS-B",
                upper=c(8, 20), lower=c(2, 5))
# Exact Kelly weights
optimd$par
      @
    \column{0.5\textwidth}
      The Kelly optimal weights can be calculated approximately by first calculating the individual stock and bond weights, and then multiplying them by the Kelly weight of the combined portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Approximate Kelly weights
p_rets <- (returns %*% weights)
drop(mean(p_rets)/var(p_rets))*weights
# Exact Kelly weights
optimd$par
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Kelly Optimal Stock and Bond Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice, the Kelly optimal weights under logarithmic utility are too aggressive and they require very active trading, so half-Kelly or even quarter-Kelly weights are used instead.
      <<echo=TRUE,eval=FALSE>>=
# Quarter-Kelly sub-optimal weights
weights <- optimd$par/4
# Plot Kelly optimal portfolio
returns <- cbind(returns,
  weights[1]*returns$VTI + weights[2]*returns$IEF)
colnames(returns)[3] <- "Kelly_sub_optimal"
# Calculate compounded wealth from returns
wealth <- cumprod(1 + returns)
# Plot compounded wealth
dygraphs::dygraph(wealth, main="Stock and Bond Portfolio") %>%
  dyOptions(colors=c("green", "blue", "green")) %>%
  dySeries("Kelly_sub_optimal", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_stocks_bonds.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Kelly weights $k_f$ are calculated daily over a rolling look-back interval:
      \begin{displaymath}
        k_f = \frac{\bar{r_t}}{\sigma^2_t}
      \end{displaymath}
      \vskip1ex
      The distribution of the Kelly weights depends on the rolling returns $\bar{r_t}$ and variance $\sigma^2_t$.
      <<echo=TRUE,eval=FALSE>>=
returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
# Calculate rolling returns and variance
look_back <- 200
var_rolling <- roll::roll_var(returns, width=look_back)
weights <- roll::roll_sum(returns, width=look_back)/look_back
weights <- weights/var_rolling
weights[1, ] <- 1/NCOL(weights)
weights <- zoo::na.locf(weights)
sum(is.na(weights))
range(weights)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_distr.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the weights
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot(density(returns$IEF), t="l", lwd=3, col="red",
     xlab="weights", ylab="density",
     ylim=c(0, max(density(returns$VTI)$y)),
     main="Kelly Weight Distributions")
lines(density(returns$VTI), t="l", col="blue", lwd=3)
legend("topright", legend=c("VTI", "IEF"),
       inset=0.1, bg="white", lty=1, lwd=6,
       col=c("blue", "red"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy For Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling Kelly strategy, the leverage of the risky asset $k_f$ changes over time.
      \vskip1ex
      The leverage is equal to the updated weight from the previous period.
      <<echo=TRUE,eval=FALSE>>=
# Scale and lag the Kelly weights
weights <- lapply(weights,
  function(x) 10*x/sum(abs(range(x))))
weights <- do.call(cbind, weights)
weights <- rutils::lagit(weights)
# Calculate the compounded Kelly wealth and VTI
wealth <- cbind(cumprod(1 + weights$VTI*returns$VTI),
                 cumprod(1 + returns$VTI))
colnames(wealth) <- c("Kelly Strategy", "VTI")
dygraphs::dygraph(wealth, main="VTI Strategy Using Rolling Kelly Weight") %>%
  dyAxis("y", label="Kelly Strategy", independentTicks=TRUE) %>%
  dyAxis("y2", label="VTI", independentTicks=TRUE) %>%
  dySeries(name="Kelly Strategy", axis="y", label="Kelly Strategy", strokeWidth=1, col="red") %>%
  dySeries(name="VTI", axis="y2", label="VTI", strokeWidth=1, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_roll_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy With Transaction Costs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{margin debt} $m_t$ is proportional to the wealth $w_t$: $m_t = (k_f - 1) w_t + 1$.
      \vskip1ex
      The dollar amount of the \emph{risky asset} traded is equal to the change in \emph{margin}, equal to: $\Delta m_t = \Delta [(k_f - 1) w_t]$.
      \vskip1ex
      If the transaction costs are large, then they will reduce the wealth and reduce the dollar amount of the \emph{risky asset} held by the investor.
      \vskip1ex
      The transaction costs depend on the change in wealth, and the wealth is decreased by the transaction costs.
      \vskip1ex
      So the transaction costs in each time period must be calculated recursively in a loop from the wealth in the past period.
      \vskip1ex
      If the transaction costs are much less than the change in wealth $c^r \ll \left| \Delta w_t \right|$, then they can be calculated approximately as the absolute value of the change in \emph{margin} $m_t^{nc}$ for a wealth path with no transaction costs:
      \begin{displaymath}
        c^r = \frac{\delta}{2} \left| \Delta m_t^{nc} \right|
      \end{displaymath}
    \column{0.5\textwidth}
      The transaction costs as a percentage of wealth are equal to: $c_t/w_t^{nc}$, where $w_t^{nc}$ is the wealth assuming no transaction costs.
      \vskip1ex
      The wealth of the Kelly Strategy after accounting for the \emph{bid-offer spread} is then equal to:
      \begin{displaymath}
        w_t = \prod_{i=1}^t {(1 + k_f \, r_i - \frac{\delta}{2} \frac{\left| \Delta m_i^{nc} \right|}{w_i^{nc}})}
      \end{displaymath}
      The effect of the \emph{bid-offer spread} is to reduce the effective asset returns by an amount proportional to the \emph{bid-offer spread}.
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate the compounded Kelly wealth and margin
wealth <- cumprod(1 + weights$VTI*returns$VTI)
mar_gin <- (returns$VTI - 1)*wealth + 1
# Calculate the transaction costs
costs <- bid_offer*drop(rutils::diffit(mar_gin))/2
wealth_diff <- drop(rutils::diffit(wealth))
costs_rel <- ifelse(wealth_diff>0, costs/wealth_diff, 0)
range(costs_rel)
hist(costs_rel, breaks=10000, xlim=c(-0.02, 0.02))
# Scale and lag the transaction costs
costs <- rutils::lagit(abs(costs)/wealth)
# Recalculate the compounded Kelly wealth
wealth_trans <- cumprod(1 + returns$VTI*returns$VTI - costs)
# Plot compounded wealth
wealth <- cbind(wealth, wealth_trans)
colnames(wealth) <- c("Kelly", "Including bid-offer")
dygraphs::dygraph(wealth, main="Kelly Strategy With Transaction Costs") %>%
  dyOptions(colors=c("green", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Kelly Strategy For Stocks and Bonds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling Kelly strategy, the leverage of the risky asset $k_f$ changes over time.
      \vskip1ex
      The leverage is equal to the updated weight from the previous period.
      <<echo=TRUE,eval=FALSE>>=
# Calculate compounded wealth from returns
wealth <- cumprod(1 + rowSums(weights*returns))
wealth <- xts::xts(wealth, index(returns))
quantmod::chart_Series(wealth, name="Rolling Kelly Strategy For VTI and IEF")
# Calculate the compounded Kelly wealth and VTI
wealth <- cbind(wealth,
  cumprod(1 + 0.6*returns$IEF + 0.4*returns$VTI))
colnames(wealth) <- c("Kelly Strategy", "VTI plus IEF")
dygraphs::dygraph(wealth, main="Rolling Kelly Strategy For VTI and IEF") %>%
  dyAxis("y", label="Kelly Strategy", independentTicks=TRUE) %>%
  dyAxis("y2", label="VTI plus IEF", independentTicks=TRUE) %>%
  dySeries(name="Kelly Strategy", axis="y", label="Kelly Strategy", strokeWidth=1, col="red") %>%
  dySeries(name="VTI plus IEF", axis="y2", label="VTI plus IEF", strokeWidth=1, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/kelly_roll_vti_ief.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Evaluating Manager Skill}


%%%%%%%%%%%%%%%
\subsection{Tests for Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{market timing} skill can be measured by performing a \emph{linear regression} of a strategy's returns against a strategy with perfect \emph{market timing} skill.
      \vskip1ex
      The \emph{Merton-Henriksson} market timing test uses a linear \emph{market timing} term:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma \max{(0, R_m - R_f)} + {\varepsilon}
      \end{displaymath}
      Where $R$ are the strategy returns, $R_m$ are the market returns, and $R_f$ are the risk-free returns.
      \vskip1ex
      If the coefficient $\gamma$ is statistically significant, then it's very likely due to \emph{market timing} skill.
      \vskip1ex
      The \emph{market timing} regression is a generalization of the \emph{Capital Asset Pricing Model}.
      \vskip1ex
      The \emph{Treynor-Mazuy} test uses a quadratic term, which makes it more sensitive to the magnitude of returns:
      \begin{displaymath}
        R - R_f = {\alpha} + \beta (R_m - R_f) + \gamma (R_m - R_f)^2 + {\varepsilon}
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Test if IEF can time VTI
returns <- na.omit(rutils::etfenv$returns[, c("IEF", "VTI")])
vtis <- returns$VTI
design <- cbind(returns, 0.5*(vtis+abs(vtis)), vtis^2)
colnames(design)[3:4] <- c("merton", "treynor")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/timing_skill_ief_vti.png}
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
model <- lm(IEF ~ VTI + merton, data=design); summary(model)
# Treynor-Mazuy test
model <- lm(IEF ~ VTI + treynor, data=design); summary(model)
# Plot residual scatterplot
x11(width=6, height=5)
residuals <- (design$IEF - model$coeff[2]*design$VTI)
plot.default(x=design$VTI, y=residuals, xlab="VTI", ylab="IEF")
title(main="Treynor-Mazuy Market Timing Test\n for IEF vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*vtis^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("Treynor test t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Identifying Managers With Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      % Adapt from RFinance\2017.Rmd and from file scratch.R.
      \vskip1ex
      \vskip1ex
      Consider a binary investment (gamble) with the probability of winning equal to $p$, the winning amount (gain) equal to $a$, and the loss equal to $b$.
      \vskip1ex
      The investor makes no up-front payments, and either wins an amount $a$, or loses an amount $b$.
      \vskip1ex
      Assuming that an investor makes decisions exclusively on the basis of the expected value of future wealth, then they would choose to invest all their wealth on the gamble if its expected value is positive, and choose not to invest at all if its expected value is negative.
    \column{0.5\textwidth}
      <<results='asis',echo=FALSE,eval=TRUE>>=
library(xtable)
gambl_e <- data.frame(win=c("p", "a"), lose=c("q = 1 - p", "-b"))
rownames(gambl_e) <- c("probability", "payout")
# print(xtable(gambl_e), comment=FALSE, size="tiny")
print(xtable(gambl_e), comment=FALSE)
      @
      The expected value of the gamble is equal to: $m = p \, a - q \, b$.
      \vskip1ex
      The variance of the gamble is equal to: $var=p \, q \, (a + b)^2$.
      \vskip1ex
      Without loss of generality we can assume that $p=q = \frac{1}{2}$,\\
      $m = 0.5 \, (b - a)$,\\
      $var=0.25 \, (a + b)^2$.
      \vskip1ex
      The \emph{Sharpe ratio} of the gamble is then equal to:
      \begin{displaymath}
        S_r = \frac{m}{sqrt(var)} = \frac{(b - a)}{(a + b)}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Rebalancing Strategies}


%%%%%%%%%%%%%%%
\subsection{Calculating Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Given a time series of asset prices $p_i$, the simple (dollar) returns $r^s_i$, the percentage returns $r^p_i$, and the log returns $r^l_i$ are defined as:
      \begin{displaymath}
        r^s_i = p_i - p_{i-1} \quad r^p_i = \frac{p_i - p_{i-1}}{p_{i-1}} \quad r^l_i = \log(\frac{p_i}{p_{i-1}})
      \end{displaymath}
      The initial returns are all equal to zero.
      \vskip1ex
      If the log returns are small $r^l \ll 1$, then they are approximately equal to the percentage returns: $r^l \approx r^p$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)
# Extract ETF prices from rutils::etfenv$prices
prices <- rutils::etfenv$prices
prices <- zoo::na.locf(prices, na.rm=FALSE)
prices <- zoo::na.locf(prices, fromLast=TRUE)
# Calculate simple dollar returns
rets_dollar <- rutils::diffit(prices)
# Or
# rets_dollar <- lapply(prices, rutils::diffit)
# rets_dollar <- rutils::do_call(cbind, rets_dollar)
# Calculate percentage returns
rets_percent <- rets_dollar/
  rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
# Calculate log returns
rets_log <- rutils::diffit(log(prices))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Compounding Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The sum of the simple (dollar) returns:
      $\sum_{i=1}^n r^s_i$
      represents the increase in wealth from owning a \emph{fixed number of shares}.
      \vskip1ex
      The compounded percentage returns:
      $\prod_{i=1}^n (1 + r^p_i$)
      also represents the incremental wealth from owning a \emph{fixed number of shares}.
      \vskip1ex
      The sum of the percentage returns (without compounding):
      $\sum_{i=1}^n r^p_i$
      represents the incremental wealth from owning a \emph{fixed dollar amount} of stock.
      \vskip1ex
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing} - selling shares when their price goes up, and vice versa.
      \vskip1ex
      This \emph{rebalancing} therefore acts as a mean reverting strategy.
      \vskip1ex
      Rebalancing requires borrowing from a \emph{margin account}, and it also incurs trading costs.
      \vskip1ex
      The logarithm of the wealth of a \emph{fixed number of shares} is often used to compare investments, and it's approximately equal to the sum of the percentage returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_log_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Set the initial dollar returns
rets_dollar[1, ] <- prices[1, ]
# Calculate prices from dollar returns
new_prices <- cumsum(rets_dollar)
all.equal(new_prices, prices)
# Compound the percentage returns
new_prices <- cumprod(1+rets_percent)
# Set the initial prices
init_prices <- as.numeric(prices[1, ])
new_prices <- lapply(1:NCOL(new_prices), function (i)
    init_prices[i]*new_prices[, i])
new_prices <- rutils::do_call(cbind, new_prices)
# Or
# new_prices <- t(t(new_prices)*init_prices)
all.equal(new_prices, prices, check.attributes=FALSE)
# Plot log VTI prices
dygraphs::dygraph(log(quantmod::Cl(rutils::etfenv$VTI)),
  main="Logarithm of VTI Prices") %>%
  dyOptions(colors="blue", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Funding Costs of Single Asset Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth accumulated from owning a \emph{fixed dollar amount} of stock is equal to the cash earned from rebalancing, which is proportional to the sum of the percentage returns, and it's kept in a \emph{margin account}: $m_t = \sum_{i=1}^t r^p_i$.
      \vskip1ex
      The cash in the \emph{margin account} can be positive (accumulated profits) or negative (losses).
      \vskip1ex
      The \emph{funding costs} $c^f_t$ are approximately equal to the \emph{margin account} $m_t$ times the \emph{funding rate} $f$: $c^f_t = f \, m_t = f \, \sum_{i=1}^t r^p_i$.
      \vskip1ex
      Positive \emph{funding costs} represent interest profits earned on the \emph{margin account}, while negative costs represent the interest paid for funding stock purchases.
      \vskip1ex
      The \emph{cumulative funding costs} $\sum_{i=1}^t c^f_i$ must be added to the \emph{margin account}: $m_t + \sum_{i=1}^t c^f_i$.
      <<echo=TRUE,eval=FALSE>>=
# Calculate percentage VTI returns
prices <- rutils::etfenv$prices$VTI
prices <- na.omit(prices)
returns <- rutils::diffit(prices)/
  rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_margin.png}
      <<echo=TRUE,eval=FALSE>>=
# Funding rate per day
f_rate <- 0.01/252
# Margin account
mar_gin <- cumsum(returns)
# Cumulative funding costs
f_costs <- cumsum(f_rate*mar_gin)
# Add funding costs to margin account
mar_gin <- (mar_gin + f_costs)
# dygraph plot of margin and funding costs
datav <- cbind(mar_gin, f_costs)
colnamev <- c("Margin", "Cumulative Funding")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="VTI Margin Funding Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The total \emph{transaction costs} are the sum of the \emph{broker commissions}, the \emph{bid-offer spread} (for market orders), \emph{lost trades} (for limit orders), and \emph{market impact}.
      \vskip1ex
      Broker commissions depend on the broker, the size of the trades, and on the type of investors, with institutional investors usually enjoying smaller commissions.
      \vskip1ex
      The \emph{bid-offer spread} is the percentage difference between the \emph{offer} minus the \emph{bid} price, divided by the \emph{mid} price.
      \vskip1ex
      Market impact is the effect of large trades pushing the market prices (the limit order book) against the trades, making the filled price worse.
      \vskip1ex
      Limit orders are not subject to the bid-offer spread but they are exposed to \emph{lost trades}.
      \vskip1ex
      \emph{Lost trades} are limit orders that don't get executed, resulting in lost potential profits.
      \vskip1ex
      Limit orders may receive rebates from some exchanges, which may reduce transaction costs.
    \column{0.5\textwidth}
      The \emph{bid-offer spread} for liquid stocks can be assumed to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and the time of day.
      \vskip1ex
      The \emph{transaction costs} due to the \emph{bid-offer spread} are equal to the number of traded shares times their price, times half the \emph{bid-offer spread}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Single Asset Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing}, selling shares when their price goes up, and vice versa.
      \vskip1ex
      The dollar amount of stock that must be traded in a given period is equal to the absolute of the percentage returns: $\left| r_t \right|$.
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ due to rebalancing are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \left| r_t \right|$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{margin account} $m_t$: $m_t - \sum_{i=1}^t c^r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_single_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Cumulative transaction costs
costs <- bid_offer*cumsum(abs(returns))/2
# Subtract transaction costs from margin account
mar_gin <- cumsum(returns)
mar_gin <- (mar_gin - costs)
# dygraph plot of margin and transaction costs
datav <- cbind(mar_gin, costs)
colnamev <- c("Margin", "Cumulative Transaction Costs")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="VTI Transaction Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining the Returns of Multiple Assets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      There are several ways of combining the returns of multiple assets.
      \vskip1ex
      Adding the weighted simple (dollar) returns is equivalent to buying a \emph{fixed number of shares} (aka \emph{Fixed Share Allocation} or FSA) proportional to the weights.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} (aka \emph{Fixed Dollar Amount} or FDA) proportional to the weights.
      \vskip1ex
      The portfolio allocations must be periodically rebalanced to keep the dollar amounts of the stocks proportional to the weights.
      \vskip1ex
      This \emph{rebalancing} acts as a mean reverting strategy - selling shares when their price goes up, and vice versa.
      \vskip1ex
      The portfolio with a fixed number of shares has higher absolute returns but a lower Sharpe ratio than the portfolio with fixed dollar amounts of stock.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI and IEF dollar returns
prices <- rutils::etfenv$prices[, c("VTI", "IEF")]
prices <- na.omit(prices)
rets_dollar <- rutils::diffit(prices)
# Calculate VTI and IEF percentage returns
rets_percent <- rets_dollar/
  rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_weighted_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
weights <- c(0.5, 0.5)
init_prices <- as.numeric(prices[1, ])
wealth_fsa <- cumsum(rets_dollar %*% (weights/init_prices))
# Wealth of fixed dollars (with rebalancing)
wealth_fda <- cumsum(rets_percent %*% weights)
# Plot log wealth
wealth <- cbind(wealth_fda, wealth_fsa)
wealth <- xts::xts(wealth, index(prices))
colnames(wealth) <- c("Fixed dollars", "Fixed shares")
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealth), 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
colnamev <- colnames(wealth)
dygraphs::dygraph(wealth, main="Wealth of Weighted Portfolios") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Weighted Portfolio Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing}, selling shares when their price goes up, and vice versa.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} proportional to the weights.
      \vskip1ex
      The dollar amount of stock that must be traded in a given period is equal to the weighted sum of the absolute percentage returns: $w1 \left| r^1_t \right| + w2 \left| r^2_t \right|$.
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ due to rebalancing are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} (w1 \left| r^1_t \right| + w2 \left| r^2_t \right|)$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{margin account} $m_t$: $m_t - \sum_{i=1}^t c^r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# Margin account for fixed dollars (with rebalancing)
mar_gin <- cumsum(rets_percent %*% weights)
# Cumulative transaction costs
costs <- bid_offer*cumsum(abs(rets_percent) %*% weights)/2
# Subtract transaction costs from margin account
mar_gin <- (mar_gin - costs)
# dygraph plot of margin and transaction costs
datav <- cbind(mar_gin, costs)
datav <- xts::xts(datav, index(prices))
colnamev <- c("Margin", "Cumulative Transaction Costs")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="Fixed Dollar Portfolio Transaction Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{proportional dollar allocation} strategy (\emph{PDA}), the total wealth $w_t$ is allocated to the assets $w_i$ proportional to the portfolio weights $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      The total wealth $w_t$ is not fixed and is equal to the portfolio market value $w_t = \sum w_i$, so there's no margin account.
      \vskip1ex
      The portfolio is rebalanced daily to maintain the dollar allocations $w_i$ equal to the total wealth $w_t = \sum w_i$ times the portfolio weights: $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      Let $r_i$ be the percentage returns, $\omega_i$ be the portfolio weights, and $\bar{r}_t = \sum_{i=1}^n \omega_i r_i$ be the weighted percentage returns at time $t$.
      \vskip1ex
      The total portfolio wealth at time $t$ is equal to the wealth at time $t-1$ multiplied by the weighted returns: $w_t = w_{t-1} (1 + \bar{r}_t)$.
      \vskip1ex
      The dollar amount of stock $i$ at time $t$ increases by $\omega_i r_i$ so it's equal to $\omega_i w_{t-1} (1 + r_i)$, while the target amount is $\omega_i w_t = \omega_i w_{t-1} (1 + \bar{r}_t)$
      \vskip1ex
      The dollar amount of stock $i$ needed to trade to rebalance back to the target weight is equal to:
      \begin{flalign*}
        \varepsilon_i &= \left| \omega_i w_{t-1} (1 + \bar{r}_t) - \omega_i w_{t-1} (1 + r_i) \right|\\
        &= \omega_i w_{t-1} \left| \bar{r}_t - r_i \right|
      \end{flalign*}
      If $\bar{r}_t > r_i$ then an amount $\varepsilon_i$ of the stock $i$ needs to be bought, and if $\bar{r}_t < r_i$ then it needs to be sold.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_allocations.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
wealth_fsa <- cumsum(rets_dollar %*% (weights/init_prices))
# Or compound the percentage returns
wealth_fsa <- drop(apply(rets_percent, 2, function(x) cumprod(1+x)) %*% weights)-1
# Wealth of proportional allocations (with rebalancing)
wealth_pda <- cumprod(1 + rets_percent %*% weights) - 1
# Plot log wealth
wealth <- cbind(wealth_fsa, wealth_pda)
wealth <- xts::xts(wealth, index(prices))
colnames(wealth) <- c("Fixed Shares", "Proportional Allocations")
dygraphs::dygraph(wealth, main="Wealth of Proportional Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In each period the stocks must be rebalanced to maintain the proportional dollar allocations.
      \vskip1ex
      The total dollar amount of stocks that need to be traded to rebalance back to the target weight is equal to: $\sum_{i=1}^n \varepsilon_i = w_{t-1} \sum_{i=1}^n \omega_i \left| \bar{r}_t - r_i \right|$
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \sum_{i=1}^n \varepsilon_i$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{wealth} $w_t$: $w_t - \sum_{i=1}^t c^r_i$.
      <<echo=TRUE,eval=FALSE>>=
# Returns in excess of weighted returns
rets_weighted <- rets_percent %*% weights
excess <- lapply(rets_percent, function(x) (rets_weighted - x))
excess <- do.call(cbind, excess)
sum(excess %*% weights)
# Calculate weighted sum of absolute excess returns
excess <- abs(excess) %*% weights
# Total dollar amount of stocks that need to be traded
excess <- excess*rutils::lagit(wealth_pda)
# Cumulative transaction costs
costs <- bid_offer*cumsum(excess)/2
# Subtract transaction costs from wealth
wealth_pda <- (wealth_pda - costs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_allocations_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of wealth and transaction costs
wealth <- cbind(wealth_pda, costs)
wealth <- xts::xts(wealth, index(prices))
colnamev <- c("Wealth", "Cumulative Transaction Costs")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Transaction Costs With Proportional Allocations") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Proportional Target Allocation Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{fixed share strategy} (\emph{FSA}), the number of shares is fixed, with their initial dollar value equal to the portfolio weights.
      \vskip1ex
      In the \emph{proportional dollar allocation} strategy (\emph{PDA}), the portfolio is rebalanced daily to maintain the dollar allocations $w_i$ equal to the total wealth $w_t = \sum w_i$ times the portfolio weights: $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      In the \emph{proportional target allocation} strategy (\emph{PTA}), the portfolio is rebalanced only if the dollar allocations $w_i$ differ from their targets $\omega_i w_t$ more than the threshold value $\tau$: $\tau > \frac{\sum \left| w_i - \omega_i w_t \right|}{w_t}$.
      \vskip1ex
      The \emph{PTA} strategy is path-dependent so it must be simulated using an explicit loop. 
      \vskip1ex
      The \emph{PTA} strategy is contrarian, since it sells assets that have outperformed, and it buys assets that have underperformed.
      \vskip1ex
      If the threshold level is very small then the \emph{PTA} strategy rebalances daily and it's the same as the \emph{PDA}.
      \vskip1ex
      If the threshold level is very large then the \emph{PTA} strategy does not rebalance and it's the same as the \emph{FSA}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
wealth_fsa <- drop(apply(rets_percent, 2, function(x) cumprod(1+x)) %*% weights)-1
# Wealth of proportional dollar allocations (with rebalancing)
wealth_pda <- cumprod(1 + rets_percent %*% weights) - 1
# Wealth of proportional target allocation (with rebalancing)
rets_percent <- zoo::coredata(rets_percent)
threshold <- 0.05
wealth <- matrix(nrow=NROW(rets_percent), ncol=2)
colnames(wealth) <- colnames(rets_percent)
wealth[1, ] <- weights
for (it in 2:NROW(rets_percent)) {
  # Accrue wealth without rebalancing
  wealth[it, ] <- wealth[it-1, ]*(1 + rets_percent[it, ])
  # Rebalance if wealth allocations differ from weights
  if (sum(abs(wealth[it, ] - sum(wealth[it, ])*weights))/sum(wealth[it, ]) > threshold) {
    # cat("Rebalance at:", it, "\n")
    wealth[it, ] <- sum(wealth[it, ])*weights
  } # end if
} # end for
wealth <- rowSums(wealth) - 1
wealth <- cbind(wealth_pda, wealth)
wealth <- xts::xts(wealth, index(prices))
colnames(wealth) <- c("Proportional Allocations", "Proportional Target")
dygraphs::dygraph(wealth, main="Wealth of Proportional Target Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Stock Index Weighting Methods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Split this slide to explain equal-weighted indices:
      https://www.investopedia.com/terms/e/equalweight.asp
      \vskip1ex
      Stock market indices can be capitalization-weighted (\emph{S\&P500}), price-weighted (\emph{DJIA}), or equal-weighted.
      \vskip1ex
      The cap-weighted and price-weighted indices own a fixed number of shares (excluding stock splits).
      \vskip1ex
      Equal-weighted indices own the same dollar amount of each stock, so they must be rebalanced as market prices change.
      \vskip1ex
      Cap-weighted index = Sum \{ (Stock Price * Number of shares) / Index Divisor \}
      \vskip1ex
      Price-weighted index = Sum \{Stock Price / Index Divisor \}
      \vskip1ex
      Equal-weighted index = Sum \{ (Stock Price * factor) / Index Divisor \}
      \vskip1ex
      Cap-weighted indices are over-weight large-cap stocks, while equal-weighted indices are over-weight small-cap stocks.
      \vskip1ex
      Cap-weighted indices are \emph{trend following}, while equal-weighted indices are \emph{mean reverting} (contrarian).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Create name corresponding to "^GSPC" symbol
setSymbolLookup(
  SP500=list(name="^GSPC", src="yahoo"))
getSymbolLookup()
# view and clear options
options("getSymbols.sources")
options(getSymbols.sources=NULL)
# Download S&P500 prices into etfenv
quantmod::getSymbols("SP500", env=etfenv,
    adjust=TRUE, auto.assign=TRUE, from="1990-01-01")
quantmod::chart_Series(x=etfenv$SP500["2016/"],
             TA="add_Vo()",
             name="S&P500 index")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stock and Bond Portfolio With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolios combining stocks and bonds can provide a much better risk versus return tradeoff than either of the assets separately, because the returns of stocks and bonds are usually negatively correlated, so they are natural hedges of each other.
      \vskip1ex
      The fixed portfolio weights represent the percentage dollar allocations to stocks and bonds, while the portfolio wealth grows over time.
      \vskip1ex
      The weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios.
      <<echo=TRUE,eval=FALSE>>=
# Calculate stock and bond returns
returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
weights <- c(0.4, 0.6)
returns <- cbind(returns, returns %*% weights)
colnames(returns)[3] <- "Combined"
# Calculate correlations
cor(returns)
# Calculate Sharpe ratios
sqrt(252)*sapply(returns, function(x) mean(x)/sd(x))
# Calculate standard deviation, skewness, and kurtosis
sapply(returns, function(x) {
  # Calculate standard deviation
  stddev <- sd(x)
  # Standardize the returns
  x <- (x - mean(x))/stddev
  c(stddev=stddev, skew=mean(x^3), kurt=mean(x^4))
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_stocks_bonds.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of proportional allocations
wealth <- cumprod(1 + returns)
# Plot cumulative wealth
dygraphs::dygraph(log(wealth), main="Stocks and Bonds With Proportional Allocations") %>%
  dyOptions(colors=c("blue", "green", "blue", "red")) %>%
  dySeries("Combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Distributions of Terminal Wealth}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Perform a bootsrap to get the Distributions of Terminal Wealth
      \vskip1ex
      In the past, the returns of stocks and bonds have usually been negatively correlated.
      \vskip1ex
      Static portfolios consisting of stocks and bonds provide a much better risk versus return tradeoff than either of the assets separately.
      \vskip1ex
      The static weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios.
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Calculate ETF returns
returns <- na.omit(rutils::etfenv$returns[, c("IEF", "VTI")])
returns <- cbind(returns, 0.6*returns$IEF+0.4*returns$VTI)
colnames(returns)[3] <- "combined"
# Calculate correlations
cor(returns)
# Calculate Sharpe ratios
sqrt(252)*sapply(returns, function(x) mean(x)/sd(x))
# Calculate skewness and kurtosis
sapply(returns, sd)
# Calculate skewness and kurtosis
t(sapply(c(skew=3, kurt=4), function(x)
  moments::moment(returns, order=x, central=TRUE)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_stocks_bonds.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate prices from returns
prices <- lapply(returns, function(x) exp(cumsum(x)))
prices <- do.call(cbind, prices)
# Plot prices
dygraphs::dygraph(prices, main="Stock and Bond Portfolio") %>%
  dyOptions(colors=c("green", "blue", "green")) %>%
  dySeries("combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a portfolio with proportional allocations of stocks ($30\%$), bonds ($55\%$), and commodities and precious metals ($15\%$) (approximately).
      \vskip1ex
      The \emph{All-Weather} portfolio was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      {\tiny
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/} \\
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511} \\
      }
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vskip1ex
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
returns <- na.omit(rutils::etfenv$returns[, symbolv])
# Calculate all-weather portfolio wealth
weightsaw <- c(0.30, 0.55, 0.15)
returns <- cbind(returns, returns %*% weightsaw)
colnames(returns)[4] <- "All Weather"
# Calculate Sharpe ratios
sqrt(252)*sapply(returns, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_all_weather.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative wealth from returns
wealth <- cumprod(1+returns)
# dygraph all-weather wealth
dygraphs::dygraph(wealth, main="All-Weather Portfolio") %>%
  dyOptions(colors=c("blue", "green", "orange", "red")) %>%
  dySeries("All Weather", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot all-weather wealth
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "red")
quantmod::chart_Series(wealth, theme=plot_theme, lwd=c(2, 2, 2, 4),
             name="All-Weather Portfolio")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Combining the Standardized Returns of Multiple Assets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Adding the weighted \emph{standardized dollar returns} is equivalent to buying \emph{fixed number of shares} such that their dollar volatilities are proportional to the weights.
      \vskip1ex
      If the asset volatilities change over time then the portfolio allocations must be rebalanced to ensure that the volatilities remain proportional to the target weights.
      \vskip1ex
      Adding the weighted \emph{standardized returns} of multiple assets is equivalent to buying stock amounts such that their volatilities are proportional to the weights.
      \vskip1ex
      Adding the weighted \emph{standardized percentage returns} is equivalent to buying \emph{fixed dollar amounts of stock} such that their percentage volatilities are proportional to the weights.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate standardized simple dollar returns
rets_dollar_std <- lapply(rets_dollar, function(x) x/sd(x))
rets_dollar_std <- do.call(cbind, rets_dollar_std)
sapply(rets_dollar_std, sd)
# Wealth of fixed number of shares (without rebalancing)
weights <- c(0.5, 0.5)
wealth_fsa <- cumsum(rets_dollar %*% (weights/init_prices))
# Calculate standardized percentage returns
rets_percent_std <- lapply(rets_percent, function(x) x/sd(x))
rets_percent_std <- do.call(cbind, rets_percent_std)
sapply(rets_percent_std, sd)
# Wealth of fixed dollar amount of shares (with rebalancing)
wealth_fda <- cumsum(rets_percent_std %*% weights)
# Plot log wealth
wealth <- cbind(wealth_fda, log(wealth_fsa))
# wealth <- xts::xts(wealth, index(prices))
colnames(wealth) <- c("With rebalancing", "Without rebalancing")
dygraphs::dygraph(wealth, main="Wealth of Equal Dollar Amount of Shares") %>%
  dyOptions(colors=c("green", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Proportion Portfolio Insurance Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Constant Proportion Portfolio Insurance} (CPPI) strategy the portfolio is rebalanced between stocks and zero-coupon bonds, to protect against the loss of principal.
      \vskip1ex
      A zero-coupon bond pays no coupon, but it's bought at a discount to par ($100\%$), and pays par at maturity.  The investor receives capital appreciation instead of coupons.
      \vskip1ex
      Let $P$ be the investor principal amount (total initial invested dollar amount), and let $F$ be the zero-coupon \emph{bond floor}.  
      The zero-coupon bond floor $F$ is set so that its value at maturity is equal to the principal $P$.  
      This guarantees that the investor is paid back at least the full principal $P$.
      \vskip1ex
      The stock investment is levered by the \emph{multiplier} $C$.  
      The initial dollar amount invested in stocks is equal to the \emph{cushion} $(P - F)$ times the \emph{multiplier} $C$: $C * (P - F)$.
      The remaining amount of the principal is invested in zero-coupon bonds and is equal to: $P - C * (P - F)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
returns <- na.omit(rutils::etfenv$returns$VTI["2008/2009"])
dates <- index(returns)
nrows <- NROW(returns)
returns <- drop(zoo::coredata(returns))
bfloor <- 60  # bond floor
coeff <- 2  # multiplier
portf_value <- numeric(nrows)
portf_value[1] <- 100  # principal
stock_value <- numeric(nrows)
stock_value[1] <- coeff*(portf_value[1] - bfloor)
bond_value <- numeric(nrows)
bond_value[1] <- (portf_value[1] - stock_value[1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{CPPI Strategy Dynamics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the stock price changes and the portfolio value becomes $P_t$, then the dollar amount invested in stocks must be adjusted to: $C * (P_t - F)$.  
      The amount invested in stocks changes both because the stock price changes and because of rebalancing with the zero-coupon bonds. 
      \vskip1ex
      The amount invested in zero-coupon bonds is then equal to: $P_t - C * (P_t - F)$.
      If the portfolio value drops to the \emph{bond floor} $P_t = F$, then all the stocks must be sold, with only the zero-coupon bonds remaining.
      But if the stock price rises, more stocks must be purchased, and vice versa.
      \vskip1ex
      Therefore the \emph{CPPI} strategy is a \emph{trend following} strategy, buying stocks when their prices are rising, and selling when their prices are dropping.
      \vskip1ex
      The \emph{CPPI} strategy can be considered a dynamic replication of a portfolio with a zero-coupon bond and a stock call option.
      \vskip1ex
      The \emph{CPPI} strategy is exposed to \emph{gap risk}, if stock prices drop suddenly by a large amount.  
      The \emph{gap risk} is exacerbated by high leverage, when the \emph{multiplier} $C$ is large, say greater than $5$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_cppi.png}
      <<echo=TRUE,eval=FALSE>>=
# Simulate CPPI strategy
for (t in 2:nrows) {
  portf_value[t] <- portf_value[t-1] + stock_value[t-1]*returns[t]
  stock_value[t] <- min(coeff*(portf_value[t] - bfloor), portf_value[t])
  bond_value[t] <- (portf_value[t] - stock_value[t])
}  # end for
# dygraph plot of CPPI strategy
vtis <- 100*cumprod(1+returns)
datav <- xts::xts(cbind(stock_value, bond_value, portf_value, vtis), dates)
colnames(datav) <- c("stocks", "bonds", "CPPI", "VTI")
dygraphs::dygraph(datav, main="CPPI strategy") %>%
  dyOptions(colors=c("red", "green", "blue", "orange"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Risk Parity} strategy the dollar portfolio allocations are rebalanced daily so that their dollar volatilities remain the same.
      \vskip1ex
      This means that the allocations $a_i$ are proportional to the \emph{standardized prices} ($\frac{p_i}{\sigma^d_i}$ - the dollar amounts of stocks with unit dollar volatilities):
      $a_i \propto \frac{p_i}{\sigma^d_i}$, 
      where $\sigma^d_i$ is the dollar volatility.
      \vskip1ex
      But the \emph{standardized prices} are equal to the inverse of the percentage volatilities $\sigma_i$:
      $\frac{p_i}{\sigma^d_i} = \frac{1}{\sigma_i}$,
      so the allocations $a_i$ are proportional to the inverse of the percentage volatilities $a_i \propto \frac{1}{\sigma_i}$.
      \vskip1ex
      In general, the dollar allocations $a_i$ may be set proportional to some target weights $\omega_i$:
      \begin{displaymath}
        a_i \propto \frac{\omega_i}{\sigma_i}
      \end{displaymath}
      The risk parity strategy is also called the equal risk contributions (ERC) strategy.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate dollar and percentage returns for VTI and IEF.
prices <- rutils::etfenv$prices[, c("VTI", "IEF")]
prices <- na.omit(prices)
rets_dollar <- rutils::diffit(prices)
rets_percent <- rets_dollar/rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
# Calculate wealth of proportional allocations.
weights <- c(0.5, 0.5)
rets_weighted <- rets_percent %*% weights
wealth_pda <- cumprod(1 + rets_weighted)
# Calculate rolling percentage volatility.
look_back <- 21
vo_l <- roll::roll_sd(rets_percent, width=look_back)
vo_l <- zoo::na.locf(vo_l, na.rm=FALSE)
vo_l <- zoo::na.locf(vo_l, fromLast=TRUE)
# Calculate the risk parity portfolio allocations.
allocation_s <- lapply(1:NCOL(prices), 
  function(x) weights[x]/vo_l[, x])
allocation_s <- do.call(cbind, allocation_s)
# Scale allocations to 1 dollar total.
allocation_s <- allocation_s/rowSums(allocation_s)
# Lag the allocations
allocation_s <- rutils::lagit(allocation_s)
# Calculate wealth of risk parity.
rets_weighted <- rowSums(rets_percent*allocation_s)
wealth_risk_parity <- cumprod(1 + rets_weighted)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy for \emph{VTI} and \emph{IEF} has a higher \emph{Sharpe ratio} than the fixed ratio strategy because it's more overweight bonds, which is also why it has lower absolute returns. 
      \vskip1ex
      Risk parity works better for assets with low correlations and very different volatilities, like stocks and bonds.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_risk_parity.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log wealths.
wealth <- log(cbind(wealth_pda, wealth_risk_parity))
wealth <- xts::xts(wealth, index(prices))
colnames(wealth) <- c("Fixed Ratio", "Risk Parity")
# Calculate the Sharpe ratios.
sqrt(252)*sapply(rutils::diffit(wealth), function (x) mean(x)/sd(x))
# Plot a dygraph of the log wealths.
dygraphs::dygraph(wealth, main="Log Wealth of Risk Parity vs Proportional Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy reduces allocations to assets with rising volatilities, which is often accompanied by negative returns.
      \vskip1ex
      This allows the risk parity strategy to better time the markets - selling when prices are about to drop and buying when prices are rising.
      \vskip1ex
      The t-value of the \emph{Treynor-Mazuy} test is slightly significant, indicating some market timing skill of the risk parity strategy for \emph{VTI} and \emph{IEF}. 
      <<echo=TRUE,eval=FALSE>>=
# Test risk parity market timing of VTI using Treynor-Mazuy test
returns <- rutils::diffit(wealth)
vtis <- rets_percent$VTI
design <- cbind(returns, vtis, vtis^2)
design <- na.omit(design)
colnames(design)[1:2] <- c("fixed", "risk_parity")
colnames(design)[4] <- "treynor"
model <- lm(risk_parity ~ VTI + treynor, data=design)
summary(model)
# Plot residual scatterplot
residuals <- (design$risk_parity - model$coeff[2]*design$VTI)
residuals <- model$residuals
x11(width=6, height=5)
plot.default(x=design$VTI, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Risk Parity vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vtis^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("Risk Parity t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_risk_parity_timing_skill.png}
      <<echo=TRUE,eval=FALSE>>=
# Test for fixed ratio market timing of VTI using Treynor-Mazuy test
model <- lm(fixed ~ VTI + treynor, data=design)
summary(model)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vtis^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="blue")
text(x=0.05, y=0.8*max(residuals), paste("Fixed Ratio t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Transaction Costs of Risk Parity Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In each period the stocks must be rebalanced to maintain the proportional allocations.
      \vskip1ex
      The total dollar amount of stocks that need to be traded to rebalance back to the target weight is equal to: $\sum_{i=1}^n \varepsilon_i = w_{t-1} \sum_{i=1}^n \omega_i \left| \bar{r}_t - r_i \right|$
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \sum_{i=1}^n \varepsilon_i$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{wealth} $w_t$: $w_t - \sum_{i=1}^t c^r_i$.
      <<echo=TRUE,eval=FALSE>>=
# Returns in excess of weighted returns
excess <- lapply(rets_percent, function(x) (rets_weighted - x))
excess <- do.call(cbind, excess)
sum(excess %*% weights)
# Calculate weighted sum of absolute excess returns
excess <- abs(excess) %*% weights
# Total dollar amount of stocks that need to be traded
excess <- excess*rutils::lagit(wealth_pda)
# Cumulative transaction costs
costs <- bid_offer*cumsum(excess)/2
# Subtract transaction costs from wealth
wealth_pda <- (wealth_pda - costs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_allocations_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of wealth and transaction costs
wealth <- cbind(wealth_pda, costs)
wealth <- xts::xts(wealth, index(prices))
colnamev <- c("Wealth", "Cumulative Transaction Costs")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Transaction Costs With Proportional Allocations") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calendar Strategies}


%%%%%%%%%%%%%%%
\subsection{Sell in May Calendar Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \href{https://en.wikipedia.org/wiki/Sell_in_May}{\emph{Sell in May}} is a \emph{market timing} \emph{calendar strategy}, in which stocks are sold at the beginning of May, and then bought back at the beginning of November.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions
vtis <- na.omit(rutils::etfenv$returns$VTI)
po_s <- rep(NA_integer_, NROW(vtis))
dates <- index(vtis)
dates <- format(dates, "%m-%d")
po_s[dates == "05-01"] <- 0
po_s[dates == "05-03"] <- 0
po_s[dates == "11-01"] <- 1
po_s[dates == "11-03"] <- 1
# Carry forward and backward non-NA po_s
po_s <- zoo::na.locf(po_s, na.rm=FALSE)
po_s <- zoo::na.locf(po_s, fromLast=TRUE)
# Calculate strategy returns
sell_inmay <- po_s*vtis
wealth <- cbind(vtis, sell_inmay)
colnames(wealth) <- c("VTI", "sell_in_may")
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_sell_inmay.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth of Sell in May strategy
dygraphs::dygraph(cumsum(wealth), main="Sell in May Strategy") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# OR: Open x11 for plotting
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue", "red")
quantmod::chart_Series(wealth, theme=plot_theme, name="Sell in May Strategy")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sell in May Strategy Market Timing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sell in May} strategy doesn't demonstrate any ability of \emph{timing} the \emph{VTI} ETF.
      <<echo=TRUE,eval=FALSE>>=
# Test if Sell in May strategy can time VTI
design <- cbind(vtis, 0.5*(vtis+abs(vtis)), vtis^2)
colnames(design) <- c("VTI", "merton", "treynor")
# Perform Merton-Henriksson test
model <- lm(sell_inmay ~ VTI + merton, data=design)
summary(model)
# Perform Treynor-Mazuy test
model <- lm(sell_inmay ~ VTI + treynor, data=design)
summary(model)
# Plot Treynor-Mazuy residual scatterplot
residuals <- (sell_inmay - model$coeff[2]*vtis)
plot.default(x=vtis, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Sell in May vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*vtis^2)
points.default(x=vtis, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("Treynor test t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/timing_skill_sell_inmay.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Seasonal Overnight Market Anomaly}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/market-sentiment-and-an-overnight-anomaly/}{\emph{Overnight Market Anomaly}}
      is the consistent outperformance of overnight returns relative to the daytime returns.
      \vskip1ex
      The Overnight Strategy consists of holding a long position only overnight (buying at the close and selling at the open the next day).
      \vskip1ex
      The Daytime Strategy consists of holding a long position only during the daytime (buying at the open and selling at the close the same day).
      \vskip1ex
      The \emph{Overnight Market Anomaly} has been observed for many decades for most stock market indices, but not always for all stock sectors.
      \vskip1ex
      The \emph{Overnight Market Anomaly} has mostly disappeared after the \texttt{2008-2009} financial crisis.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log of OHLC VTI prices
ohlc <- log(rutils::etfenv$VTI)
openp <- quantmod::Op(ohlc)
highp <- quantmod::Hi(ohlc)
lowp <- quantmod::Lo(ohlc)
closep <- quantmod::Cl(ohlc)
# Calculate the close-to-close log returns, the intraday 
# open-to-close returns and the overnight close-to-open returns.
close_close <- rutils::diffit(closep)
colnames(close_close) <- "close_close"
open_close <- (closep - openp)
colnames(open_close) <- "open_close"
close_open <- (openp - rutils::lagit(closep, lagg=1, pad_zeros=FALSE))
colnames(close_open) <- "close_open"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_overnight.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate Sharpe and Sortino ratios
wealth <- cbind(close_close, close_open, open_close)
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot log wealth
dygraphs::dygraph(cumsum(wealth), 
  main="Wealth of Close-to-Close, Close-to-Open, and Open-to-Close Strategies") %>%
  dySeries(name="close_close", label="Close-to-Close (static)", strokeWidth=2, col="blue") %>%
  dySeries(name="close_open", label="Close-to-Open (overnight)", strokeWidth=2, col="red") %>%
  dySeries(name="open_close", label="Open-to-Close (daytime)", strokeWidth=2, col="green") %>%
  dyLegend(width=600)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Turn of the Month Effect}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/strategies/turn-of-the-month-in-equity-indexes/}{\emph{Turn of the Month} (TOM) effect}
      is the outperformance of stocks on the last trading day of the month and on the first three days of the following month.
      \vskip1ex
      The \emph{TOM} effect was observed for the period from 1928 to 1975, but it has been less pronounced since the year \texttt{2000}.
      \vskip1ex
      The \emph{TOM} effect has been attributed to the investment of funds deposited at the end of the month.
      \vskip1ex
      This would explain why the \emph{TOM} effect has been more pronounced for less liquid small-cap stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
vtis <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(vtis)
# Calculate first business day of every month
day_s <- as.numeric(format(dates, "%d"))
indeks <- which(rutils::diffit(day_s) < 0)
dates[head(indeks)]
# Calculate Turn of the Month dates
indeks <- lapply((-1):2, function(x) indeks + x)
indeks <- do.call(c, indeks)
sum(indeks > NROW(dates))
indeks <- sort(indeks)
dates[head(indeks, 11)]
# Calculate Turn of the Month pnls
pnls <- numeric(NROW(vtis))
pnls[indeks] <- vtis[indeks, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_tom.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealth <- cbind(vtis, pnls)
colnamev <- c("VTI", "Strategy")
colnames(wealth) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI Turn of the Month strategy
dygraphs::dygraph(cumsum(wealth), main="Turn of the Month Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Stop-loss Rules}


%%%%%%%%%%%%%%%
\subsection{Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules are used to reduce losses in case of a significant drawdown in returns.
      \vskip1ex
      For example, a simple stop-loss rule is to sell the stock if its price drops by $5\%$ below the recent maximum price, and buy it back when the price recovers.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
vtis <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(vtis)
vtis <- drop(coredata(vtis))
nrows <- NROW(vtis)
# Simulate stop-loss strategy
stopl <- 0.05
maxp <- 0.0
cumrets <- 0.0
pnls <- vtis
for (i in 1:nrows) {
# Calculate drawdown
  cumrets <- cumrets + vtis[i]
  maxp <- max(maxp, cumrets)
  dd <- (cumrets - maxp)
# Check for stop-loss
  if (dd < -stopl*maxp)
    pnls[i+1] <- 0
}  # end for
# Same but without using explicit loops
cumsumv <- cumsum(vtis)
cum_max <- cummax(cumsum(vtis))
dd <- (cumsumv - cum_max)
pnls2 <- vtis
is_dd <- rutils::lagit(dd < -stopl*cum_max)
pnls2 <- ifelse(is_dd, 0, pnls2)
all.equal(pnls, pnls2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/stop_loss.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealth <- xts::xts(cbind(vtis, pnls), dates)
colnamev <- c("VTI", "Strategy")
colnames(wealth) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI stop-loss strategy
dygraphs::dygraph(cumsum(wealth), main="VTI Stop-loss Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules can reduce the largest losses but they also tend to reduce cumulative returns.
      <<echo=TRUE,eval=FALSE>>=
# Simulate multiple stop-loss strategies
cumsumv <- cumsum(vtis)
cum_max <- cummax(cumsum(vtis))
dd <- (cumsumv - cum_max)
cum_pnls <- sapply(0.01*(1:20), function(stopl) {
  pnls <- vtis
  is_dd <- rutils::lagit(dd < -stopl*cum_max)
  pnls <- ifelse(is_dd, 0, pnls)
  sum(pnls)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/stop_loss_cum.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative pnls for stop-loss strategies
plot(x=0.01*(1:20), y=cum_pnls, 
     main="Cumulative PnLs for Stop-loss Strategies",
     xlab="stop-loss level", ylab="cumulative pnl", 
     t="l", lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Stop-loss and Gain Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules are used to reduce losses in case of a significant drawdown in returns.
      \vskip1ex
      For example, a simple stop-loss rule is to sell the stock if its price drops by $5\%$ below the recent maximum price.
      <<echo=TRUE,eval=FALSE>>=
# Simulate stop-loss strategy
stopl <- 0.05
maxp <- 0.0
minp <- 0.0
cumrets <- 0.0
pnls <- vtis
for (i in 1:(nrows-1)) {
# Calculate drawdown
  cumrets <- cumrets + vtis[i]
  maxp <- max(maxp, cumrets)
  dd <- (cumrets - maxp)
# Check for stop-loss
  if (dd < -stopl*maxp) {
    pnls[i+1] <- 0
    minp <- min(minp, cumrets)
    du <- (cumrets - minp)
# Check for gain
    if (du > stopl*minp) {
      pnls[i+1] <- vtis[i+1]
    }  # end if
  } else {
    minp <- cumrets
  }  # end if
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/stop_loss.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealth <- xts::xts(cbind(vtis, pnls), dates)
colnamev <- c("VTI", "Strategy")
colnames(wealth) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI stop-loss strategy
dygraphs::dygraph(cumsum(wealth), main="VTI Stop-loss Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Classification Strategies}


%%%%%%%%%%%%%%%
\subsection{Centered Price Z-scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An extreme local price is a price which differs significantly from neighboring prices.
      \vskip1ex
      Extreme prices can be identified in-sample using the centered \emph{price z-score} equal to the price difference with neighboring prices divided by the volatility of returns $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{2 p_i - p_{i-k} - p_{i+k}}{\sigma_i}
      \end{displaymath}
      Where $p_{i-k}$ and $p_{i+k}$ are the lagged and advanced prices.
      \vskip1ex
      The lag parameter $k$ determines the scale of the extreme local prices, with smaller $k$ producing larger z-scores for more local price extremes.
      <<echo=TRUE,eval=FALSE>>=
# Extract VTI log OHLC prices
ohlc <- log(rutils::etfenv$VTI)
closep <- quantmod::Cl(ohlc)
returns <- rutils::diffit(closep)
# Calculate the centered volatility
look_back <- 21
half_back <- look_back %/% 2
volat <- roll::roll_sd(returns, width=look_back, min_obs=1)
volat <- rutils::lagit(volat, lagg=(-half_back))
# Calculate the z-scores of prices
pricescores <- (2*closep - 
  rutils::lagit(closep, half_back, pad_zeros=FALSE) - 
  rutils::lagit(closep, -half_back, pad_zeros=FALSE))
pricescores <- ifelse(volat > 0, pricescores/volat, 0)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/features_price_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of z-scores of VTI prices
prices <- cbind(closep, pricescores)
colnames(prices) <- c("VTI", "Z-scores")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI Price Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Labeling the Tops and Bottoms of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The local tops and bottoms of prices can be labeled approximately in-sample using the z-scores of prices and threshold values.
      \vskip1ex
      The local tops of prices represent \emph{overbought} conditions, while the bottoms represent \emph{oversold} conditions.
      \vskip1ex
      The labeled data can be used as a response or target variable in machine learning classifier models.
      \vskip1ex
      But it's not feasible to classify the prices out-of-sample exactly according to their in-sample labels.
      <<echo=TRUE,eval=FALSE>>=
# Calculate thresholds for labeling tops and bottoms
threshold_s <- quantile(pricescores, c(0.1, 0.9))
# Calculate the vectors of tops and bottoms
top_s <- zoo::coredata(pricescores > threshold_s[2])
colnames(top_s) <- "tops"
bottom_s <- zoo::coredata(pricescores < threshold_s[1])
colnames(bottom_s) <- "bottoms"
# Simulate in-sample VTI strategy
po_s <- rep(NA_integer_, NROW(returns))
po_s[1] <- 0
po_s[top_s] <- (-1)
po_s[bottom_s] <- 1
po_s <- zoo::na.locf(po_s)
po_s <- rutils::lagit(po_s)
pnls <- returns*po_s
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/features_labels.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
wealth <- cbind(returns, pnls)
colnames(wealth) <- c("VTI", "Strategy")
dygraphs::dygraph(cumsum(wealth), main="VTI Strategy Using In-sample Labels") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Predictors of Price Extremes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The return volatility and trading volumes may be used as predictors in a classification model, in order to identify \emph{overbought} and \emph{oversold} conditions.
      \vskip1ex
      The trailing \emph{volume z-score} is equal to the volume $v_i$ minus the trailing average volumes $\bar{v_i}$ divided by the volatility of the volumes $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      Trading volumes are typically higher when prices drop and they are also positively correlated with the return volatility.
      \vskip1ex
      The \emph{volatility z-score} is equal to the spot volatility $v_i$ minus the trailing average volatility $\bar{v_i}$ divided by the standard deviation of the volatility $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      Volatility is typically higher when prices drop and it's also positively correlated with the trading volumes.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate volatility z-scores
volat <- HighFreq::roll_var_ohlc(ohlc=ohlc, look_back=look_back, scale=FALSE)
volat_mean <- roll::roll_mean(volat, width=look_back, min_obs=1)
volat_sd <- roll::roll_sd(rutils::diffit(volat), width=look_back, min_obs=1)
volat_sd[1] <- 0
volat_scores <- ifelse(volat_sd > 0, (volat - volat_mean)/volat_sd, 0)
colnames(volat_scores) <- "volat"
# Calculate volume z-scores
volumes <- quantmod::Vo(ohlc)
volume_mean <- roll::roll_mean(volumes, width=look_back, min_obs=1)
volume_sd <- roll::roll_sd(rutils::diffit(volumes), width=look_back, min_obs=1)
volume_sd[1] <- 0
volume_scores <- ifelse(volume_sd > 0, (volumes - volume_mean)/volume_sd, 0)
colnames(volume_scores) <- "volume"
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Price Extremes Using Logistic Regression}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weighted average of the volatility and trading volume z-scores can be used to calculate the probability of a top (overbought condition)
      \vskip1ex
      The residuals are the differences between the actual response values ($0$ and $1$), and the calculated probabilities of default.
      \vskip1ex
      The residuals are not normally distributed, so the data is fitted using the \emph{maximum likelihood} method, instead of least squares.
      <<echo=TRUE,eval=FALSE>>=
# Define design matrix for tops including intercept column
design <- cbind(top_s, intercept=rep(1, NROW(top_s)), 
                 volat_scores, volume_scores)
# Define regression formula
colnamev <- colnames(design)
formulav <- as.formula(paste(paste(colnamev[1],
  paste(colnamev[-1], collapse="+"), sep=" ~ "), "-1"))
# Fit in-sample logistic regression for tops
glmod <- glm(formulav, data=design, family=binomial(logit))
summary(glmod)
coeff <- glmod$coefficients
predictv <- drop(design[, -1] %*% coeff)
ordern <- order(predictv)
# Calculate in-sample forecasts from logistic regression model
forecasts <- 1/(1+exp(-predictv))
all.equal(glmod$fitted.values, forecasts, check.attributes=FALSE)
hist(forecasts)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/logistic_stocktops.png}
      <<echo=TRUE,eval=FALSE>>=
x11(width=6, height=5)
plot(x=predictv[ordern], y=top_s[ordern],
     main="Logistic Regression of Stock Tops", 
     col="orange", xlab="predictor", ylab="top")
lines(x=predictv[ordern], y=glmod$fitted.values[ordern], col="blue", lwd=3)
legend(x="topleft", inset=0.1, bty="n", lwd=6,
       legend=c("tops", "logit fitted values"),
       col=c("orange", "blue"), lty=c(NA, 1), pch=c(1, NA))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Errors}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{binary classification model} categorizes cases based on its forecasts whether the \emph{null hypothesis} is \texttt{TRUE} or \texttt{FALSE}.
      \vskip1ex
      Let the \emph{null hypothesis} be that the data point is not a top: \texttt{top\_s = FALSE}.
      \vskip1ex
      A \emph{positive} result corresponds to rejecting the null hypothesis, while a \emph{negative} result corresponds to accepting the null hypothesis.
      \vskip1ex
      The forecasts are subject to two different types of errors: \emph{type I} and \emph{type II} errors.
      \vskip1ex
      A \emph{type I} error is the incorrect rejection of a \texttt{TRUE} \emph{null hypothesis} (i.e. a "false positive"), when there is no default but it's classified as a default.
      \vskip1ex
      A \emph{type II} error is the incorrect acceptance of a \texttt{FALSE} \emph{null hypothesis} (i.e. a "false negative"), when there is a default but it's classified as no default.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define discrimination threshold value
threshold <- quantile(forecasts, 0.95)
# Calculate confusion matrix in-sample
confu_sion <- table(actual=!top_s, forecast=(forecasts < threshold))
confu_sion
# Calculate FALSE positive (type I error)
sum(!top_s & (forecasts > threshold))
# Calculate FALSE negative (type II error)
sum(top_s & (forecasts < threshold))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Confusion Matrix of a Binary Classification Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The confusion matrix summarizes the performance of a classification model on a set of test data for which the actual values of the \emph{null hypothesis} are known.
      \vskip1ex
      \newcommand\MyBox[2]{
        \fbox{\lower0.75cm
          \vbox to 1.2cm{\vfil
            \hbox to 1.7cm{\parbox{\textwidth}{#1\\#2}}
            \vfil}
        }
      }
      \renewcommand\arraystretch{0.3}
      \setlength\tabcolsep{0pt}
      {\tiny
      \begin{tabular}{c >{\bfseries}r @{\hspace{0.5em}}c @{\hspace{0.4em}}c @{\hspace{0.5em}}l}
      \multirow{10}{*}{\parbox{0.5cm}{\bfseries Actual}} &
      & \multicolumn{2}{c}{\bfseries Forecast} & \\
      & & \bfseries Null is FALSE & \bfseries Null is TRUE \\
      & {\bfseries Null is FALSE} & \MyBox{True Positive}{(sensitivity)} & \MyBox{False Negative}{(type II error)} \\[2.4em]
      & {\bfseries Null is TRUE} & \MyBox{False Positive}{(type I error)} & \MyBox{True Negative}{(specificity)}
      \end{tabular}}
      <<echo=TRUE,eval=FALSE>>=
# Calculate FALSE positive and FALSE negative rates
confu_sion <- confu_sion / rowSums(confu_sion)
c(typeI=confu_sion[2, 1], typeII=confu_sion[1, 2])
      @
      <<echo=FALSE,eval=FALSE,results='asis'>>=
# Below is an unsuccessful attempt to draw confusion matrix using xtable
confusion_matrix <- matrix(c("| true positive \\\\ (sensitivity)", "| false negative \\\\ (type II error)", "| false positive \\\\ (type I error)", "| true negative \\\\ (specificity)"), nc=2)
dimnames(confusion_matrix) <- list(forecast=c("FALSE", "TRUE"),
                                   actual=c("FALSE", "TRUE"))
print(xtable::xtable(confusion_matrix,
      caption="Confusion Matrix"),
      caption.placement="top",
      comment=FALSE, size="scriptsize",
      include.rownames=TRUE,
      include.colnames=TRUE)
# end unsuccessful attempt to draw confusion table using xtable
      @
    \column{0.5\textwidth}
      Let the \emph{null hypothesis} be that the data point is not a top: \texttt{top\_s = FALSE}.
      \vskip1ex
      The \emph{true positive} rate (known as the \emph{sensitivity}) is the fraction of \texttt{FALSE} \emph{null hypothesis} cases that are correctly classified as \texttt{FALSE}.
      \vskip1ex
      The \emph{false negative} rate is the fraction of \texttt{FALSE} \emph{null hypothesis} cases that are incorrectly classified as \texttt{TRUE} (\emph{type II} error).
      \vskip1ex
      The sum of the \emph{true positive} plus the \emph{false negative} rate is equal to $1$.
      \vskip1ex
      The \emph{true negative} rate (known as the \emph{specificity}) is the fraction of \texttt{TRUE} \emph{null hypothesis} cases that are correctly classified as \texttt{TRUE}.
      \vskip1ex
      The \emph{false positive} rate is the fraction of \texttt{TRUE} \emph{null hypothesis} cases that are incorrectly classified as \texttt{FALSE} (\emph{type I} error).
      \vskip1ex
      The sum of the \emph{true negative} plus the \emph{false positive} rate is equal to $1$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Receiver Operating Characteristic (ROC) Curve for Stock Tops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ROC curve} is the plot of the \emph{true positive} rate, as a function of the \emph{false positive} rate, and illustrates the performance of a binary classifier.
      \vskip1ex
      The area under the \emph{ROC curve} (AUC) measures the classification ability of a binary classifier.
      \vskip1ex
      The \emph{informedness} is equal to the sum of the sensitivity plus the specificity, and measures the performance of a binary classification model. 
      <<echo=TRUE,eval=FALSE>>=
# Confusion matrix as function of threshold
con_fuse <- function(actual, forecasts, threshold) {
    conf <- table(actual, (forecasts < threshold))
    conf <- conf / rowSums(conf)
    c(typeI=conf[2, 1], typeII=conf[1, 2])
  }  # end con_fuse
con_fuse(!top_s, forecasts, threshold=threshold)
# Define vector of discrimination thresholds
threshold_s <- quantile(forecasts, seq(0.1, 0.99, by=0.01))
# Calculate error rates
error_rates <- sapply(threshold_s, con_fuse,
  actual=!top_s, forecasts=forecasts)  # end sapply
error_rates <- t(error_rates)
rownames(error_rates) <- threshold_s
# Calculate the informedness
inform_ed <- 2 - rowSums(error_rates[, c("typeI", "typeII")])
plot(threshold_s, inform_ed, t="l", main="Informedness")
# Find the threshold corresponding to highest informedness
threshold_top <- threshold_s[which.max(inform_ed)]
tops_forecast <- (forecasts > threshold_top)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/logistic_stocktops_roc.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate area under ROC curve (AUC)
error_rates <- rbind(c(1, 0), error_rates)
error_rates <- rbind(error_rates, c(0, 1))
true_pos <- (1 - error_rates[, "typeII"])
true_pos <- (true_pos + rutils::lagit(true_pos))/2
false_pos <- rutils::diffit(error_rates[, "typeI"])
abs(sum(true_pos*false_pos))
# Plot ROC Curve for stock tops
x11(width=5, height=5)
plot(x=error_rates[, "typeI"], y=1-error_rates[, "typeII"],
     xlab="FALSE positive rate", ylab="TRUE positive rate",
     main="ROC Curve for Stock Tops", type="l", lwd=3, col="blue")
abline(a=0.0, b=1.0, lwd=3, col="orange")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{Receiver Operating Characteristic (ROC) Curve for Stock Bottoms}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ROC curve} is the plot of the \emph{true positive} rate, as a function of the \emph{false positive} rate, and illustrates the performance of a binary classifier.
      \vskip1ex
      The area under the \emph{ROC curve} (AUC) measures the classification ability of a binary classifier.
      \vskip1ex
      The \emph{informedness} is equal to the sum of the sensitivity plus the specificity, and measures the performance of a binary classification model. 
      <<echo=TRUE,eval=FALSE>>=
# Define design matrix for tops including intercept column
design <- cbind(bottom_s, intercept=rep(1, NROW(bottom_s)), 
                 volat_scores, volume_scores)
# Define regression formula
colnamev <- colnames(design)
formulav <- as.formula(paste(paste(colnamev[1],
  paste(colnamev[-1], collapse="+"), sep=" ~ "), "-1"))
# Fit in-sample logistic regression for tops
glmod <- glm(formulav, data=design, family=binomial(logit))
summary(glmod)
# Calculate in-sample forecast from logistic regression model
predictv <- drop(design[, -1] %*% glmod$coefficients)
forecasts <- 1/(1+exp(-predictv))
# Calculate error rates
error_rates <- sapply(threshold_s, con_fuse,
  actual=!bottom_s, forecasts=forecasts)  # end sapply
error_rates <- t(error_rates)
rownames(error_rates) <- threshold_s
# Calculate the informedness
inform_ed <- 2 - rowSums(error_rates[, c("typeI", "typeII")])
plot(threshold_s, inform_ed, t="l", main="Informedness")
# Find the threshold corresponding to highest informedness
threshold_bottom <- threshold_s[which.max(inform_ed)]
bottoms_forecast <- (forecasts > threshold_bottom)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/logistic_bottoms_roc.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate area under ROC curve (AUC)
error_rates <- rbind(c(1, 0), error_rates)
error_rates <- rbind(error_rates, c(0, 1))
true_pos <- (1 - error_rates[, "typeII"])
true_pos <- (true_pos + rutils::lagit(true_pos))/2
false_pos <- rutils::diffit(error_rates[, "typeI"])
abs(sum(true_pos*false_pos))
# Plot ROC Curve for stock tops
x11(width=5, height=5)
plot(x=error_rates[, "typeI"], y=1-error_rates[, "typeII"],
     xlab="FALSE positive rate", ylab="TRUE positive rate",
     main="ROC Curve for Stock Bottoms", type="l", lwd=3, col="blue")
abline(a=0.0, b=1.0, lwd=3, col="orange")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Labeling the Tops and Bottoms of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The local tops and bottoms of prices can be labeled approximately in-sample using the z-scores of prices and threshold values.
      \vskip1ex
      The local tops of prices represent \emph{overbought} conditions, while the bottoms represent \emph{oversold} conditions.
      \vskip1ex
      The labeled data can be used as a response or target variable in machine learning classifier models.
      \vskip1ex
      But it's not feasible to classify the prices out-of-sample exactly according to their in-sample labels.
      <<echo=TRUE,eval=FALSE>>=
# Simulate in-sample VTI strategy
po_s <- rep(NA_integer_, NROW(returns))
po_s[1] <- 0
po_s[tops_forecast] <- (-1)
po_s[bottoms_forecast] <- 1
po_s <- zoo::na.locf(po_s)
po_s <- rutils::lagit(po_s)
pnls <- returns*po_s
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/logistic_stocklabels.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
wealth <- cbind(returns, pnls)
colnames(wealth) <- c("VTI", "Strategy")
dygraphs::dygraph(cumsum(wealth), main="Logistic Strategy Using Top and Bottom Labels") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Forecasting Logistic Regression Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{predict()} is a \emph{generic function} for forecasting based on a given model.
      \vskip1ex
      The method \texttt{predict.glm()} produces forecasts for a generalized linear (\emph{glm}) model, in the form of \texttt{numeric} probabilities, not the \texttt{Boolean} response variable.
      \vskip1ex
      The \texttt{Boolean} forecasts are obtained by comparing the \emph{forecast probabilities} with a \emph{discrimination threshold}.
      \vskip1ex
      Let the \emph{null hypothesis} be that the data point is not a top: \texttt{top\_s = FALSE}.
      \vskip1ex
      If the \emph{forecast probability} is greater than the \emph{discrimination threshold}, then the forecast is that the data point is not a top and that the \emph{null hypothesis} is \texttt{TRUE}.
      \vskip1ex
      The \emph{in-sample forecasts} are just the \emph{fitted values} of the \emph{glm} model.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Fit logistic regression over training data
set.seed(1121)  # Reset random number generator
nrows <- NROW(Default)
samplev <- sample.int(n=nrows, size=nrows/2)
traindata <- Default[samplev, ]
glmod <- glm(formulav, data=traindata, family=binomial(logit))
# Forecast over test data out-of-sample
test_data <- Default[-samplev, ]
forecasts <- predict(glmod, newdata=test_data, type="response")
# Calculate confusion matrix out-of-sample
table(actual=!test_data$de_fault, 
      forecast=(forecasts < threshold))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hampel Filter Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Hampel filter strategy is a contrarian strategy that uses Hampel z-scores to establish long and short positions.
      \vskip1ex
      The Hampel strategy has two meta-parameters: the look-back interval and the threshold level.
      \vskip1ex
      The best choice of the meta-parameters can be determined through simulation.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
closep <- log(na.omit(rutils::etfenv$prices$VTI))
returns <- rutils::diffit(closep)
# Define look-back window
look_back <- 11
# Calculate time series of medians
medi_an <- roll::roll_median(closep, width=look_back)
# medi_an <- TTR::runMedian(closep, n=look_back)
# Calculate time series of MAD
madv <- HighFreq::roll_var(closep, look_back=look_back, method="nonparametric")
# madv <- TTR::runMAD(closep, n=look_back)
# Calculate time series of z-scores
zscores <- (closep - medi_an)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
# Define threshold value
threshold <- sum(abs(range(zscores)))/8
# Simulate VTI strategy
position_s <- rep(NA_integer_, NROW(closep))
position_s[1] <- 0
position_s[zscores < -threshold] <- 1
position_s[zscores > threshold] <- (-1)
position_s <- zoo::na.locf(position_s)
position_s <- rutils::lagit(position_s)
pnls <- returns*position_s
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_hampel.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of Hampel strategy pnls
wealth <- cbind(returns, pnls)
colnames(wealth) <- c("VTI", "Strategy")
dygraphs::dygraph(cumsum(wealth), main="VTI Hampel Strategy") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Trend Following Strategies}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Leverage} Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{leverage function} maps the \emph{predictor} value $p$ into a dollar amount for investment.
      \vskip1ex
      A possible choice for the leverage function is the hyperbolic tangent function:
      \begin{displaymath}
        L(x) = \frac{\exp(\lambda p) - \exp(-\lambda p)}{\exp(\lambda p) + \exp(-\lambda p)}
      \end{displaymath}
      Where $\lambda$ is the leverage parameter.
      \vskip1ex
      The hyperbolic tangent is close to linear for small values of the \emph{predictor} $p$, and saturates to $+1\$ / -1\$$ for very large positive and negative values of the \emph{predictor}.
      \vskip1ex
      The saturation effect limits (caps) the leverage in the strategy to $+1\$ / -1\$$.
      \vskip1ex
      For very small values of the leverage parameter $\lambda$, the invested dollar amount is linear for a wide range of \emph{predictor} values $p$.  So the strategy is mostly invested in dollar amounts proportional to the \emph{predictor} values.
      \vskip1ex
      For very large values of the leverage parameter $\lambda$, the invested dollar amount jumps from $-1\$$ for negative \emph{predictor} values to $+1\$$ for positive \emph{predictor} values.  So the strategy is invested in either $-1\$$ or $+1\$$ dollar amounts.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/leverage_func.png}
        <<echo=TRUE,eval=FALSE>>=
lambdas <- c(0.5, 1, 1.5)
colors <- c("red", "blue", "green")
# Define the leverage function
leverage <- function(p, lambda) tanh(lambda*p)
# Plot three curves in loop
for (indeks in 1:3) {
  curve(expr=leverage(x, lambda=lambdas[indeks]),
        xlim=c(-4, 4), type="l", lwd=4,
        xlab="predictor", ylab="dollar amount", 
        col=colors[indeks], add=(indeks>1))
}  # end for
# Add title
title(main="Leverage function", line=0.5)
# Add legend
legend("topleft", title="Leverage parameters",
       paste("lambda", lambdas, sep="="),
       inset=0.05, cex=0.8, lwd=6, bty="n",
       lty=1, col=colors)
      @
  \end{columns}
\end{block}

\end{frame}




%%%%%%%%%%%%%%%
\section{Moving Average Crossover Strategies}


%%%%%%%%%%%%%%%
\subsection{EWMA Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        p^{EWMA}_i = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j p_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      \vskip1ex
      The function \texttt{HighFreq::roll\_wsum()} calculates the convolution of a time series with a vector of weights.
      <<echo=TRUE,eval=FALSE>>=
# Extract log VTI prices
ohlc <- log(rutils::etfenv$VTI)
closep <- quantmod::Cl(ohlc)
colnames(closep) <- "VTI"
nrows <- NROW(closep)
# Calculate EWMA weights
look_back <- 333
lambda <- 0.9
weights <- lambda^(1:look_back)
weights <- weights/sum(weights)
# Calculate EWMA prices as the convolution
ew_ma <- HighFreq::roll_wsum(closep, weights=weights)
prices <- cbind(closep, ew_ma)
colnames(prices) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_feature.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=500)
# Standard plot of  EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
colors <- c("blue", "red")
plot_theme$col$line.col <- colors
quantmod::chart_Series(prices["2009"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
legend("topleft", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive EWMA Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{EWMA} prices can be calculated recursively as follows:
      \begin{displaymath}
        p^{EWMA}_i = (1-\lambda) p_i + \lambda p^{EWMA}_{i-1}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      \vskip1ex
      The recursive \emph{EWMA} prices are slightly different from those calculated as a convolution, because the convolution uses a fixed look-back interval.
      \vskip1ex
      The compiled \texttt{C++} function \texttt{stats:::C\_rfilter()} calculates the exponentially weighted moving average prices recursively.
      \vskip1ex
      The function \texttt{HighFreq::run\_mean()} calculates the exponentially weighted moving average prices recursively.
      <<echo=TRUE,eval=FALSE>>=
# Calculate EWMA prices recursively using C++ code
ewma_rfilter <- .Call(stats:::C_rfilter, closep, lambda, c(as.numeric(closep[1])/(1-lambda), double(NROW(closep))))[-1]
# Or R code
# ewma_rfilter <- filter(closep, filter=lambda, init=as.numeric(closep[1, 1])/(1-lambda), method="recursive")
ewma_rfilter <- (1-lambda)*ewma_rfilter
# Calculate EWMA prices recursively using RcppArmadillo
ew_ma <- HighFreq::run_mean(closep, lambda=lambda)
all.equal(drop(ew_ma), ewma_rfilter)
# Compare the speed of C++ code with RcppArmadillo
library(microbenchmark)
summary(microbenchmark(
  run_mean=HighFreq::run_mean(closep, lambda=lambda),
  rfilter=.Call(stats:::C_rfilter, closep, lambda, c(as.numeric(closep[1])/(1-lambda), double(NROW(closep)))),
  times=10))[, c(1, 4, 5)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_recursive.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
prices <- cbind(closep, ew_ma)
colnames(prices) <- c("VTI", "VTI EWMA")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="Recursive VTI EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=500)
# Standard plot of  EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
colors <- c("blue", "red")
plot_theme$col$line.col <- colors
quantmod::chart_Series(prices["2009"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
legend("topleft", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating the EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the trend following \emph{EWMA Crossover} strategy, the risk position switches depending if the current price is above or below the \emph{EWMA}.
      \vskip1ex
      If the current price crosses above the \emph{EWMA}, then the strategy switches its risk position to a fixed unit of long risk, and if it crosses below, to a fixed unit of short risk.
      \vskip1ex
      The strategy holds the same position until the \emph{EWMA} crosses over the current price (either from above or below), and then it switches its position.
      \vskip1ex
      The strategy is therefore always either in a long risk, or in a short risk position.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions, either: -1, 0, or 1
indic <- sign(closep - ew_ma)
po_s <- rutils::lagit(indic, lagg=1)
# Create colors for background shading
dates <- (rutils::diffit(po_s) != 0)
shad_e <- po_s[dates]
dates <- c(index(shad_e), end(po_s))
shad_e <- ifelse(drop(zoo::coredata(shad_e)) == 1, "lightgreen", "antiquewhite")
# Create dygraph object without plotting it
dyplot <- dygraphs::dygraph(prices["2007/"], main="VTI EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=4, col="red") %>%
  dyLegend(show="always", width=500)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
      <<echo=FALSE,eval=FALSE>>=
# Equivalent code to the above
# Determine trade dates right after EWMA has crossed prices
indic <- sign(closep - ew_ma)
dates <- (rutils::diffit(indic) != 0)
dates <- which(dates) + 1
dates <- dates[dates <.n_rows]
# Calculate positions, either: -1, 0, or 1
po_s <- rep(NA_integer_, nrows)
po_s[1] <- 0
po_s[dates] <- indic[dates-1]
po_s <- zoo::na.locf(po_s, na.rm=FALSE)
po_s <- xts::xts(po_s, order.by=index(closep))
# Create indicator for background shading
shad_e <- po_s[dates]
dates <- index(shad_e)
dates <- c(dates, end(po_s))
shad_e <- ifelse(drop(zoo::coredata(shad_e)) == 1, "lightgreen", "antiquewhite")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat.png}
      <<echo=TRUE,eval=FALSE>>=
# Standard plot of EWMA prices with position shading
x11(width=6, height=5)
quantmod::chart_Series(prices["2007/"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
add_TA(po_s > 0, on=-1, col="lightgreen", border="lightgreen")
add_TA(po_s < 0, on=-1, col="lightgrey", border="lightgrey")
legend("topleft", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of the EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The crossover strategy trades at the \emph{Close} price on the same day that prices cross the \emph{EWMA}, which may be difficult in practice.
      \vskip1ex
      The crossover strategy performance is worse than the underlying asset (\emph{VTI} ), but it has a negative correlation to it, which is very valuable when building a portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate daily profits and losses of EWMA strategy
vtis <- rutils::diffit(closep)  # VTI returns
pnls <- vtis*po_s
colnames(pnls) <- "EWMA"
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "EWMA PnL")
# Annualized Sharpe ratio of EWMA strategy
sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
# The crossover strategy has a negative correlation to VTI
cor(wealth)
# Plot dygraph of EWMA strategy wealth
# Create dygraph object without plotting it
colors <- c("blue", "red")
dyplot <- dygraphs::dygraph(cumsum(wealth["2007/"]), main="Performance of EWMA Strategy") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% 
      dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat_pnl.png}
      <<echo=TRUE,eval=FALSE>>=
# Standard plot of EWMA strategy wealth
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(wealth["2007/"]), theme=plot_theme,
             name="Performance of EWMA Strategy")
add_TA(po_s > 0, on=-1, col="lightgreen", border="lightgreen")
add_TA(po_s < 0, on=-1, col="lightgrey", border="lightgrey")
legend("top", legend=colnames(wealth),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{EWMA Crossover Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The EWMA crossover strategy shorts the market during significant selloffs, but otherwise doesn't display market timing skill.
      \vskip1ex
      The t-value of the \emph{Treynor-Mazuy} test is negative, but not statistically significant. 
      <<echo=TRUE,eval=FALSE>>=
# Test EWMA crossover market timing of VTI using Treynor-Mazuy test
design <- cbind(pnls, vtis, vtis^2)
design <- na.omit(design)
colnames(design) <- c("EWMA", "VTI", "treynor")
model <- lm(EWMA ~ VTI + treynor, data=design)
summary(model)
# Plot residual scatterplot
residuals <- (design$EWMA - model$coeff[2]*design$VTI)
residuals <- model$residuals
x11(width=6, height=6)
plot.default(x=design$VTI, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for EWMA Crossover vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*vtis^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("EWMA crossover t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_timing_skill.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{EWMA Crossover Strategy With Lag}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The crossover strategy suffers losses when prices are range-bound without a trend, because whenever it switches position the prices soon change direction.  (This is called a \emph{"whipsaw"}.)
      \vskip1ex
      To prevent whipsaws and over-trading, the crossover strategy may choose to delay switching positions until the indicator repeats the same value for several periods.
      \vskip1ex
      There's a tradeoff between switching positions too early and risking a whipsaw, and waiting too long and missing an emerging trend.
      <<echo=TRUE,eval=FALSE>>=
# Determine trade dates right after EWMA has crossed prices
indic <- sign(closep - ew_ma)
# Calculate positions from lagged indicator
lagg <- 2
indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
# Calculate positions, either: -1, 0, or 1
po_s <- rep(NA_integer_, nrows)
po_s[1] <- 0
po_s <- ifelse(indic == lagg, 1, po_s)
po_s <- ifelse(indic == (-lagg), -1, po_s)
po_s <- zoo::na.locf(po_s, na.rm=FALSE)
po_s <- xts::xts(po_s, order.by=index(closep))
# Lag the positions to trade in next period
po_s <- rutils::lagit(po_s, lagg=1)
# Calculate PnLs of lagged strategy
pnls_lag <- vtis*po_s
colnames(pnls_lag) <- "Lagged Strategy"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat_lag.png}
      <<echo=TRUE,eval=FALSE>>=
wealth <- cbind(pnls, pnls_lag)
colnames(wealth) <- c("EWMA Strategy", "Lagged Strategy")
# Annualized Sharpe ratios of EWMA strategies
sharp_e <- sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
# Plot both strategies
dygraphs::dygraph(cumsum(wealth["2007/"]), main=paste("EWMA Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red"), strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{EWMA Strategy Trading at the Open Price}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice it may not be possible to trade immediately at the \emph{Close} price on the same day that prices cross the \emph{EWMA}.
      \vskip1ex
      Then the strategy may trade at the \emph{Open} price on the next day. 
      \vskip1ex
      The Profit and Loss (\emph{PnL}) on a trade date is the sum of the realized \emph{PnL} from closing the old position, plus the unrealized \emph{PnL} after opening the new position.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions, either: -1, 0, or 1
indic <- sign(closep - ew_ma)
po_s <- rutils::lagit(indic, lagg=1)
# Calculate daily pnl for days without trades
pnls_lag <- vtis*po_s
# Determine trade dates right after EWMA has crossed prices
dates <- which(rutils::diffit(po_s) != 0)
# Calculate realized pnl for days with trades
openp <- quantmod::Op(ohlc)
close_lag <- rutils::lagit(closep)
pos_lag <- rutils::lagit(po_s)
pnls_lag[dates] <- pos_lag[dates]*
  (openp[dates] - close_lag[dates])
# Calculate unrealized pnl for days with trades
pnls_lag[dates] <- pnls_lag[dates] + 
  po_s[dates]*(closep[dates] - openp[dates])
# Calculate the wealth
wealth <- cbind(vtis, pnls_lag)
colnames(wealth) <- c("VTI", "EWMA PnL")
# Annualized Sharpe ratio of EWMA strategy
sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
# The crossover strategy has a negative correlation to VTI
cor(wealth)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat_open_pnl.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of EWMA strategy wealth
dygraphs::dygraph(cumsum(wealth["2007/"]), main="EWMA Strategy Trading at the Open Price") %>%
  dyOptions(colors=colors, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Standard plot of EWMA strategy wealth
quantmod::chart_Series(wealth, theme=plot_theme,
             name="EWMA Strategy Trading at the Open Price")
legend("top", legend=colnames(wealth),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{EWMA Crossover Strategy With Transaction Costs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-offer spread} is the percentage difference between the \emph{offer} minus the \emph{bid} price, divided by the \emph{mid} price.
      \vskip1ex
      The \emph{bid-offer spread} for liquid stocks can be assumed to be about \texttt{10} basis points (bps).
      \vskip1ex
      The \emph{transaction costs} $c^r$ due to the \emph{bid-offer spread} are equal to half the \emph{bid-offer spread} $\delta$ times the absolute value of the traded dollar amount of the \emph{risky asset}:
      \begin{displaymath}
        c^r = \frac{\delta}{2} \left| \Delta n_t \right| p_t
      \end{displaymath}
      Where $\Delta n_t$ is the number of shares traded, and $p_t$ is their price.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat_transcosts.png}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate transaction costs
costs <- 0.5*bid_offer*abs(pos_lag - po_s)*closep
# Plot strategy with transaction costs
wealth <- cbind(pnls, pnls - costs)
colnames(wealth) <- c("EWMA", "EWMA w Costs")
colors <- c("blue", "red")
dygraphs::dygraph(cumsum(wealth["2007/"]), main="EWMA Strategy With Transaction Costs") %>%
  dyOptions(colors=colors, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulation Function for EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{EWMA} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters.
      \vskip1ex
      The function \texttt{sim\_ewma()} performs a simulation of the \emph{EWMA} strategy, given an \emph{OHLC} time series of prices, and a decay parameter $\lambda$.
      \vskip1ex
      The function \texttt{sim\_ewma()} returns the \emph{EWMA} strategy positions and returns, in a two-column \emph{xts} time series.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sim_ewma <- function(ohlc, lambda=0.01, look_back=333, bid_offer=0.001, 
                      trend=1, lagg=1) {
  close <- quantmod::Cl(ohlc)
  returns <- rutils::diffit(close)
  nrows <- NROW(ohlc)
  # Calculate EWMA prices
  weights <- exp(-lambda*(1:look_back))
  weights <- weights/sum(weights)
  ewma <- HighFreq::roll_wsum(close, weights=weights)
  # Calculate the indicator
  indic <- trend*sign(close - ewma)
  if (lagg > 1) {
    indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
    indic[1:lagg] <- 0
  }  # end if
  # Calculate positions, either: -1, 0, or 1
  pos <- rep(NA_integer_, nrows)
  pos[1] <- 0
  pos <- ifelse(indic == lagg, 1, pos)
  pos <- ifelse(indic == (-lagg), -1, pos)
  pos <- zoo::na.locf(pos, na.rm=FALSE)
  pos <- xts::xts(pos, order.by=index(close))
  # Lag the positions to trade on next day
  pos <- rutils::lagit(pos, lagg=1)
  # Calculate PnLs of strategy
  pnls <- returns*pos
  costs <- 0.5*bid_offer*abs(rutils::diffit(pos))*close
  pnls <- (pnls - costs)
  # Calculate strategy returns
  pnls <- cbind(pos, pnls)
  colnames(pnls) <- c("positions", "pnls")
  pnls
}  # end sim_ewma
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Multiple Trend Following EWMA Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{EWMA} strategies can be simulated by calling the function \texttt{sim\_ewma()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{sim\_ewma()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using the functions \texttt{do.call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("/Users/jerzy/Develop/lecture_slides/scripts/ewma_model.R")
lambdas <- seq(from=0.001, to=0.008, by=0.001)
# Perform lapply() loop over lambdas
pnls <- lapply(lambdas, function(lambda) {
  # Simulate EWMA strategy and calculate returns
  sim_ewma(ohlc=ohlc, lambda=lambda, look_back=look_back, bid_offer=0, lagg=2)[, "pnls"]
})  # end lapply
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("lambda=", lambdas)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_trend_returns.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of multiple EWMA strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls["2007/"]), main="Cumulative Returns of Trend Following EWMA Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(pnls), theme=plot_theme,
  name="Cumulative Returns of EWMA Strategies")
legend("topleft", legend=colnames(pnls), inset=0.1,
  bg="white", cex=0.8, lwd=rep(6, NCOL(pnls)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating EWMA Strategies Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulating \emph{EWMA} strategies naturally lends itself to parallel computing, since the simulations are independent from each other.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The resulting list of time series can then be collapsed into a single \emph{xts} series using the functions \texttt{rutils::do\_call()} and \texttt{cbind()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Initialize compute cluster under Windows
library(parallel)
cluster <- makeCluster(detectCores()-1)
clusterExport(cluster,
  varlist=c("ohlc", "look_back", "sim_ewma"))
# Perform parallel loop over lambdas under Windows
pnls <- parLapply(cluster, lambdas, function(lambda) {
  library(quantmod)
  # Simulate EWMA strategy and calculate returns
  sim_ewma(ohlc=ohlc, lambda=lambda, look_back=look_back)[, "pnls"]
})  # end parLapply
stopCluster(cluster)  # Stop R processes over cluster under Windows
# Perform parallel loop over lambdas under Mac-OSX or Linux
pnls <- mclapply(lambdas, function(lambda) {
  library(quantmod)
  # Simulate EWMA strategy and calculate returns
  sim_ewma(ohlc=ohlc, lambda=lambda, look_back=look_back)[, "pnls"]
})  # end mclapply
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("lambda=", lambdas)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Weights of Trend Following EWMA Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of trend following \emph{EWMA} strategies depends on the $\lambda$ parameter, with smaller $\lambda$ parameters performing better than larger ones.
      \vskip1ex
      The optimal $\lambda$ parameter applies significant weight to returns \texttt{8 - 12} months in the past, which is consistent with research on trend following strategies.
      \vskip1ex
      The \emph{Sharpe ratios} of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      <<echo=TRUE,eval=FALSE>>=
# Calculate annualized Sharpe ratios of strategy returns
sharper <- sqrt(252)*sapply(pnls, function(xtes) {
  mean(xtes)/sd(xtes)
})  # end sapply
# Plot Sharpe ratios
dev.new(width=6, height=5, noRStudioGD=TRUE)
plot(x=lambdas, y=sharper, t="l",
     xlab="lambda", ylab="Sharpe",
     main="Performance of EWMA Trend Following Strategies
     as Function of the Decay Parameter Lambda")
# Find optimal lambda
lambda <- lambdas[which.max(sharper)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_trendperformance.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot optimal weights
weights <- exp(-lambda*(1:look_back))
weights <- weights/sum(weights)
plot(weights, t="l", xlab="days", ylab="weights",
     main="Optimal Weights of EWMA Trend Following Strategy")
trend_returns <- pnls
trend_sharpe <- sharper
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Trend Following EWMA Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best performing trend following \emph{EWMA} strategy has a relatively small $\lambda$ parameter, corresponding to slower weight decay (giving more weight to past prices), and producing less frequent trading.
      <<echo=TRUE,eval=FALSE>>=
# Simulate best performing strategy
ewma_trend <- sim_ewma(ohlc=ohlc, lambda=lambda, look_back=look_back, bid_offer=0, lagg=2)
po_s <- ewma_trend[, "positions"]
pnls <- ewma_trend[, "pnls"]
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "EWMA PnL")
# Create colors for background shading
dates <- (rutils::diffit(po_s) != 0)
shad_e <- po_s[dates]
dates <- c(index(shad_e), end(po_s))
shad_e <- ifelse(drop(zoo::coredata(shad_e)) == 1, "lightgreen", "antiquewhite")
colors <- c("blue", "red")
# Plot dygraph of EWMA strategy wealth
# Create dygraph object without plotting it
dyplot <- dygraphs::dygraph(cumsum(wealth["2007/"]), main="Performance of Optimal Trend Following EWMA Strategy") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% 
      dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_strat_pnl.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot EWMA PnL with position shading
# Standard plot of EWMA strategy wealth
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(wealth["2007/"]), theme=plot_theme,
             name="Performance of EWMA Strategy")
add_TA(po_s > 0, on=-1, col="lightgreen", border="lightgreen")
add_TA(po_s < 0, on=-1, col="lightgrey", border="lightgrey")
legend("top", legend=colnames(wealth),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mean Reverting EWMA Crossover Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Mean reverting EWMA crossover strategies can be simulated using function \texttt{sim\_ewma()} with argument \texttt{trend=(-1)}.
      \vskip1ex
      The profitability of mean reverting strategies can be significantly improved by using limit orders, to reduce transaction costs.
      <<echo=TRUE,eval=FALSE>>=
source("/Users/jerzy/Develop/lecture_slides/scripts/ewma_model.R")
lambdas <- seq(0.05, 1.0, 0.05)
# Perform lapply() loop over lambdas
pnls <- lapply(lambdas, function(lambda) {
  # Simulate EWMA strategy and calculate returns
  sim_ewma(ohlc=ohlc, lambda=lambda, look_back=look_back, trend=(-1))[, "pnls"]
})  # end lapply
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("lambda=", lambdas)
# Plot dygraph of mean reverting EWMA strategies
column_s <- seq(1, NCOL(pnls), by=4)
colors <- colorRampPalette(c("blue", "red"))(NROW(column_s))
dygraphs::dygraph(cumsum(pnls["2007/", column_s]), main="Cumulative Returns of Mean Reverting EWMA Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(pnls[, column_s],
  theme=plot_theme, name="Cumulative Returns of Mean Reverting EWMA Strategies")
legend("topleft", legend=colnames(pnls[, column_s]),
  inset=0.1, bg="white", cex=0.8, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_revert_returns.png}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_revert_notranscosts.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Mean Reverting EWMA Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe ratios} of \emph{EWMA} strategies with different $\lambda$ parameters can be calculated by performing an \texttt{sapply()} loop over the \emph{columns} of returns.
      \vskip1ex
      \texttt{sapply()} treats the columns of \emph{xts} time series as list elements, and loops over the columns.
      \vskip1ex
      Performing loops in \texttt{R} over the \emph{columns} of returns is acceptable, but \texttt{R} loops over the \emph{rows} of returns should be avoided.
      \vskip1ex
      The performance of mean reverting \emph{EWMA} strategies depends on the $\lambda$ parameter, with performance decreasing for very small or very large $\lambda$ parameters.
      \vskip1ex
      For too large $\lambda$ parameters, the trading frequency is too high, causing high transaction costs.
      \vskip1ex
      For too small $\lambda$ parameters, the trading frequency is too low, causing the strategy to miss profitable trades.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_revert_performance.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate Sharpe ratios of strategy returns
sharper <- sqrt(252)*sapply(pnls, function(xtes) {
  mean(xtes)/sd(xtes)
})  # end sapply
plot(x=lambdas, y=sharper, t="l",
     xlab="lambda", ylab="Sharpe",
     main="Performance of EWMA Mean Reverting Strategies
     as Function of the Decay Parameter Lambda")
revert_returns <- pnls
revert_sharpe <- sharper
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Mean Reverting EWMA Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Reverting the direction of the trend following \emph{EWMA} strategy creates a mean reverting strategy.
      \vskip1ex
      The best performing mean reverting \emph{EWMA} strategy has a relatively large $\lambda$ parameter, corresponding to faster weight decay (giving more weight to recent prices), and producing more frequent trading.
      \vskip1ex
      But a too large $\lambda$ parameter also causes very high trading frequency, and high transaction costs.
      <<echo=TRUE,eval=FALSE>>=
# Find optimal lambda
lambda <- lambdas[which.max(sharper)]
# Simulate best performing strategy
ewma_revert <- sim_ewma(ohlc=ohlc, bid_offer=0.0,
  lambda=lambda, look_back=look_back, trend=(-1))
po_s <- ewma_revert[, "positions"]
pnls <- ewma_revert[, "pnls"]
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "EWMA PnL")
# Plot dygraph of EWMA strategy wealth
colors <- c("blue", "red")
dygraphs::dygraph(cumsum(wealth["2007/"]), main="Optimal Mean Reverting EWMA Strategy") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_revert_best.png}
      <<echo=TRUE,eval=FALSE>>=
# Standard plot of EWMA strategy wealth
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(wealth["2007/"]), theme=plot_theme,
             name="Optimal Mean Reverting EWMA Strategy")
add_TA(po_s > 0, on=-1, col="lightgreen", border="lightgreen")
add_TA(po_s < 0, on=-1, col="lightgrey", border="lightgrey")
legend("top", legend=colnames(wealth),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Trend Following and Mean Reverting Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The returns of trend following and mean reverting strategies are usually negatively correlated to each other, so combining them can achieve significant diversification of risk.
      \vskip1ex
      The main advantage of EWMA crossover strategies is that they provide positive returns and a diversification of risk with respect to static stock portfolios.
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlation between trend following and mean reverting strategies
trend_ing <- ewma_trend[, "pnls"]
colnames(trend_ing) <- "trend"
revert_ing <- ewma_revert[, "pnls"]
colnames(revert_ing) <- "revert"
cor(cbind(vtis, trend_ing, revert_ing))
# Calculate combined strategy
com_bined <- (vtis + trend_ing + revert_ing)/3
colnames(com_bined) <- "combined"
# Calculate annualized Sharpe ratio of strategy returns
returns <- cbind(vtis, trend_ing, revert_ing, com_bined)
colnames(returns) <- c("VTI", "Trending", "Reverting", "EWMA combined")
sqrt(252)*sapply(returns, function(xtes) mean(xtes)/sd(xtes))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_combined.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of EWMA strategy wealth
colors <- c("blue", "red", "green", "purple")
dygraphs::dygraph(cumsum(returns["2007/"]), main="Performance of Combined EWMA Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Standard plot of EWMA strategy wealth
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(pnls, theme=plot_theme,
             name="Performance of Combined EWMA Strategies")
legend("topleft", legend=colnames(pnls),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ensemble of EWMA Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Instead of selecting the best performing \emph{EWMA} strategy, one can choose a weighted average of strategies (ensemble), which corresponds to allocating positions according to the weights.
      \vskip1ex
      The weights can be chosen to be proportional to the Sharpe ratios of the \emph{EWMA} strategies.
      <<echo=TRUE,eval=FALSE>>=
weights <- c(trend_sharpe, revert_sharpe)
weights[weights<0] <- 0
weights <- weights/sum(weights)
returns <- cbind(trend_returns, revert_returns)
returns <- returns %*% weights
returns <- xts::xts(returns, order.by=index(vtis))
returns <- cbind(vtis, returns)
colnames(returns) <- c("VTI", "EWMA PnL")
# Plot dygraph of EWMA strategy wealth
colors <- c("blue", "red")
dygraphs::dygraph(cumsum(returns["2007/"]), main="Performance of Ensemble of EWMA Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Standard plot of EWMA strategy wealth
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(returns["2007/"]), theme=plot_theme,
             name="Performance of Ensemble of EWMA Strategies")
legend("topleft", legend=colnames(pnls),
       inset=0.05, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewma_ensemble.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating the Dual EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Dual EWMA Crossover} strategy, the risk position depends on the difference between two moving averages.
      \vskip1ex
      The risk position flips when the fast moving \emph{EWMA} crosses the slow moving \emph{EWMA}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate fast and slow EWMAs
look_back <- 333
lambda1 <- 0.04
lambda2 <- 0.004
weights <- exp(-lambda1*(1:look_back))
weights <- weights/sum(weights)
ewma1 <- HighFreq::roll_wsum(closep, weights=weights)
weights <- exp(-lambda2*(1:look_back))
weights <- weights/sum(weights)
ewma2 <- HighFreq::roll_wsum(closep, weights=weights)
# Calculate EWMA prices
prices <- cbind(closep, ewma1, ewma2)
colnames(prices) <- c("VTI", "EWMA fast", "EWMA slow")
# Calculate positions, either: -1, 0, or 1
indic <- sign(ewma1 - ewma2)
lagg <- 2
indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
po_s <- rep(NA_integer_, nrows)
po_s[1] <- 0
po_s <- ifelse(indic == lagg, 1, po_s)
po_s <- ifelse(indic == (-lagg), -1, po_s)
po_s <- zoo::na.locf(po_s, na.rm=FALSE)
po_s <- xts::xts(po_s, order.by=index(closep))
po_s <- rutils::lagit(po_s, lagg=1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewm_dual_strat.png}
      <<echo=TRUE,eval=FALSE>>=
# Create colors for background shading
dates <- (rutils::diffit(po_s) != 0)
shad_e <- po_s[dates]
dates <- c(index(shad_e), end(po_s))
shad_e <- ifelse(drop(zoo::coredata(shad_e)) == 1, "lightgreen", "antiquewhite")
# Plot dygraph
colnamev <- colnames(prices)
dyplot <- dygraphs::dygraph(prices["2007/"], main="VTI Dual EWMA Prices") %>%
  dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=4, col="red") %>%
  dySeries(name=colnamev[3], label=colnamev[3], strokeWidth=4, col="purple") %>%
  dyLegend(show="always", width=500)
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of the Dual EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The crossover strategy suffers losses when prices are range-bound without a trend, because whenever it switches position the prices soon change direction.  (This is called a \emph{"whipsaw"}.)
      \vskip1ex
      The crossover strategy performance is worse than the underlying asset (\emph{VTI} ), but it has a negative correlation to it, which is very valuable when building a portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate daily profits and losses of strategy
pnls <- vtis*po_s
colnames(pnls) <- "Strategy"
wealth <- cbind(vtis, pnls)
# Annualized Sharpe ratio of Dual EWMA strategy
sharp_e <- sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
# The crossover strategy has a negative correlation to VTI
cor(wealth)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewmadvual_strat_pnl.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot Dual EWMA strategy
dyplot <- dygraphs::dygraph(cumsum(wealth["2007/"]), main=paste("EWMA Dual Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red"), strokeWidth=1)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulation Function for the Dual EWMA Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Dual EWMA} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters.
      \vskip1ex
      The function \texttt{sim\_ewma2()} performs a simulation of the \emph{Dual EWMA} strategy, given an \emph{OHLC} time series of prices, and two decay parameters $\lambda1$ and $\lambda2$.
      \vskip1ex
      The function \texttt{sim\_ewma2()} returns the \emph{EWMA} strategy positions and returns, in a two-column \emph{xts} time series.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sim_ewma2 <- function(ohlc, lambda1=0.1, lambda2=0.01, look_back=333, 
                      bid_offer=0.001, trend=1, lagg=1) {
  close <- log(quantmod::Cl(ohlc))
  returns <- rutils::diffit(close)
  nrows <- NROW(ohlc)
  # Calculate EWMA prices
  weights <- exp(-lambda1*(1:look_back))
  weights <- weights/sum(weights)
  ewma1 <- HighFreq::roll_wsum(closep, weights=weights)
  weights <- exp(-lambda2*(1:look_back))
  weights <- weights/sum(weights)
  ewma2 <- HighFreq::roll_wsum(closep, weights=weights)
  # Calculate positions, either: -1, 0, or 1
  indic <- sign(ewma1 - ewma2)
  if (lagg > 1) {
    indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
    indic[1:lagg] <- 0
  }  # end if
  pos <- rep(NA_integer_, nrows)
  pos[1] <- 0
  pos <- ifelse(indic == lagg, 1, pos)
  pos <- ifelse(indic == (-lagg), -1, pos)
  pos <- zoo::na.locf(pos, na.rm=FALSE)
  pos <- xts::xts(pos, order.by=index(close))
  # Lag the positions to trade on next day
  pos <- rutils::lagit(pos, lagg=1)
  # Calculate PnLs of strategy
  pnls <- returns*pos
  costs <- 0.5*bid_offer*abs(rutils::diffit(pos))*close
  pnls <- (pnls - costs)
  # Calculate strategy returns
  pnls <- cbind(pos, pnls)
  colnames(pnls) <- c("positions", "pnls")
  pnls
}  # end sim_ewma2
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Dual EWMA Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{Dual EWMA} strategies can be simulated by calling the function \texttt{sim\_ewma2()} in two loops over the vectors of $\lambda$ parameters.
      \vskip1ex
      The best \emph{Dual EWMA} strategy performs better than the best \emph{single EWMA} strategy, because it has an extra parameter that can be adjusted to improve in-sample performance.  But this doesn't guarantee better out-of-sample performance.
      <<echo=TRUE,eval=FALSE>>=
source("/Users/jerzy/Develop/lecture_slides/scripts/ewma_model.R")
lambdas1 <- seq(from=0.05, to=0.15, by=0.01)
lambdas2 <- seq(from=0.03, to=0.1, by=0.01)
# Perform sapply() loops over lambdas
sharper <- sapply(lambdas1, function(lambda1) {
  sapply(lambdas2, function(lambda2) {
    if (lambda1 > lambda2) {
      # Simulate Dual EWMA strategy
      pnls <- sim_ewma2(ohlc=ohlc, lambda1=lambda1, lambda2=lambda2, 
                          look_back=look_back, bid_offer=0.0, trend=1, lagg=2)[, "pnls"]
      sqrt(252)*mean(pnls)/sd(pnls)
    } else NA
  })  # end sapply
})  # end sapply
colnames(sharper) <- lambdas1
rownames(sharper) <- lambdas2
# Calculate the PnLs for the optimal strategy
whichv <- which(sharper == max(sharper, na.rm=TRUE), arr.ind=TRUE)
lambda1 <- lambdas1[whichv[2]]
lambda2 <- lambdas2[whichv[1]]
pnls <- sim_ewma2(ohlc=ohlc, lambda1=lambda1, lambda2=lambda2, 
                    look_back=look_back, bid_offer=0.0, trend=1, lagg=2)[, "pnls"]
wealth <- cbind(vtis, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ewmadvual_strat_opt_pnl.png}
      <<echo=TRUE,eval=FALSE>>=
# Annualized Sharpe ratio of Dual EWMA strategy
sharp_e <- sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
# The crossover strategy has a negative correlation to VTI
cor(wealth)
# Plot Optimal Dual EWMA strategy
dyplot <- dygraphs::dygraph(cumsum(wealth["2007/"]), main=paste("Optimal EWMA Dual Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>% 
  dyOptions(colors=c("blue", "red"), strokeWidth=1)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volume-Weighted Average Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes:
      \begin{displaymath}
        p^{VWAP}_i = \frac{\sum_{j=0}^{n} v_j p_{i-j}}{\sum_{j=0}^{n} v_j}
      \end{displaymath}
      The \emph{VWAP} applies more weight to prices with higher trading volumes, which allows it to react more quickly to recent market volatility.
      \vskip1ex
      The drawback of the \emph{VWAP} indicator is that it applies large weights to prices far in the past.
      \vskip1ex
      The \emph{VWAP} is often used as a technical indicator in trend following strategies.
      <<echo=TRUE,eval=FALSE>>=
# Calculate log OHLC prices and volumes
ohlc <- rutils::etfenv$VTI
closep <- log(quantmod::Cl(ohlc))
colnames(closep) <- "VTI"
volumes <- quantmod::Vo(ohlc)
colnames(volumes) <- "Volume"
nrows <- NROW(closep)
# Calculate the VWAP prices
look_back <- 21
vwap <- roll::roll_sum(closep*volumes, width=look_back, min_obs=1)
volume_roll <- roll::roll_sum(volumes, width=look_back, min_obs=1)
vwap <- vwap/volume_roll
colnames(vwap) <- "VWAP"
prices <- cbind(closep, vwap)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_feature.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colors <- c("blue", "red")
dygraphs::dygraph(prices["2009"], main="VTI VWAP Prices") %>%
  dyOptions(colors=colors, strokeWidth=2)
# Plot VWAP prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(prices["2009"], theme=plot_theme,
             lwd=2, name="VTI VWAP Prices")
legend("bottomright", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive VWAP Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VWAP} prices $p^{VWAP}$ can also be calculated as the ratio of the volume weighted prices $\mu^{pv}$ divided by the mean trading volumes $\mu^v$:
      \begin{displaymath}
        p^{VWAP} = \frac{\mu^{pv}}{\mu^v}
      \end{displaymath}
      The volume weighted prices $\mu^{pv}$ and the mean trading volumes $\mu^v$ are both calculated recursively: 
      \begin{flalign*}
        \mu^v_i = (1-\lambda) v_i + \lambda \mu^v_{i-1} \\
        \mu^{pv}_i = (1-\lambda) v_i p_i + \lambda \mu^{pv}_{i-1}
      \end{flalign*}
      The recursive \emph{VWAP} prices are slightly different from those calculated as a convolution, because the convolution uses a fixed look-back interval.
      \vskip1ex
      The advantage of the recursive \emph{VWAP} indicator is that it gradually "forgets" about large trading volumes far in the past.
      \vskip1ex
      The compiled \texttt{C++} function \texttt{stats:::C\_rfilter()} calculates the weighted running values recursively.
      \vskip1ex
      The function \texttt{HighFreq::run\_mean()} also calculates the weighted running values recursively.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_recursive.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VWAP prices recursively using C++ code
volume_rec <- .Call(stats:::C_rfilter, volumes, lambda, c(as.numeric(volumes[1])/(1-lambda), double(NROW(volumes))))[-1]
price_rec <- .Call(stats:::C_rfilter, volumes*closep, lambda, c(as.numeric(volumes[1]*closep[1])/(1-lambda), double(NROW(closep))))[-1]
vwap_rec <- price_rec/volume_rec
# Calculate VWAP prices recursively using RcppArmadillo
vwap_arma <- HighFreq::run_mean(closep, lambda=lambda, weights=volumes)
all.equal(vwap_rec, drop(vwap_arma))
# Dygraphs plot the VWAP prices
prices <- xts(cbind(vwap, vwap_arma), zoo::index(ohlc))
colnames(prices) <- c("VWAP rolling", "VWAP running")
dygraphs::dygraph(prices["2009"], main="VWAP Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating the VWAP Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the trend following \emph{VWAP Crossover} strategy, the risk position switches depending if the current price is above or below the \emph{VWAP}.
      \vskip1ex
      If the current price crosses above the \emph{VWAP}, then the strategy switches its risk position to a fixed unit of long risk, and if it crosses below, to a fixed unit of short risk.
      \vskip1ex
      To prevent whipsaws and over-trading, the crossover strategy delays switching positions until the indicator repeats the same value for several periods.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions from lagged indicator
indic <- sign(closep - vwap)
lagg <- 2
indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
# Calculate positions, either: -1, 0, or 1
po_s <- rep(NA_integer_, nrows)
po_s[1] <- 0
po_s <- ifelse(indic == lagg, 1, po_s)
po_s <- ifelse(indic == (-lagg), -1, po_s)
po_s <- zoo::na.locf(po_s, na.rm=FALSE)
po_s <- xts::xts(po_s, order.by=index(closep))
# Lag the positions to trade in next period
po_s <- rutils::lagit(po_s, lagg=1)
# Calculate PnLs of VWAP strategy
vtis <- rutils::diffit(closep)  # VTI returns
pnls <- vtis*po_s
colnames(pnls) <- "VWAP Strategy"
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "VWAP Strategy")
colnamev <- colnames(wealth)
# Annualized Sharpe ratios of VTI and VWAP strategy
sharp_e <- sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_strat_lag.png}
      <<echo=TRUE,eval=FALSE>>=
# Create colors for background shading
dates <- (rutils::diffit(po_s) != 0)
shad_e <- po_s[dates]
dates <- c(index(shad_e), end(po_s))
shad_e <- ifelse(drop(zoo::coredata(shad_e)) == 1, "lightgreen", "antiquewhite")
# Plot dygraph of VWAP strategy
# Create dygraph object without plotting it
dyplot <- dygraphs::dygraph(cumsum(wealth["2007/"]), main=paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Add shading to dygraph object
for (i in 1:NROW(shad_e)) {
    dyplot <- dyplot %>% dyShading(from=dates[i], to=dates[i+1], color=shad_e[i])
}  # end for
# Plot the dygraph object
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining VWAP Crossover Strategy with Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Even though the \emph{VWAP} strategy doesn't perform as well as a static buy-and-hold strategy, it can provide risk reduction when combined with it.
      \vskip1ex
      This is because the \emph{VWAP} strategy has a negative correlation with respect to the underlying asset.
      \vskip1ex
      In addition, the \emph{VWAP} strategy performs well in periods of extreme market selloffs, so it can provide a hedge for a static buy-and-hold strategy.
      \vskip1ex
      The \emph{VWAP} strategy serves as a dynamic put option in periods of extreme market selloffs.
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlation of VWAP strategy with VTI
cor(vtis, pnls)
# Combine VWAP strategy with VTI
wealth <- cbind(vtis, pnls, 0.5*(vtis+pnls))
colnames(wealth) <- c("VTI", "VWAP", "Combined")
sharp_e <- sqrt(252)*sapply(wealth, function (x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_combined.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of VWAP strategy combined with VTI
# wippp
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>%
  dySeries(name=colnamev[1], label=colnamev[1], col="blue", strokeWidth=1) %>%
  dySeries(name=colnamev[2], label=colnamev[2], col="red", strokeWidth=1) %>%
  dySeries(name=colnamev[3], label=colnamev[3], col="purple", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Or
dygraphs::dygraph(cumsum(wealth), 
  main=paste("VWAP Crossover Strategy, Sharpe", paste(paste(names(sharp_e), round(sharp_e, 3), sep="="), collapse=", "))) %>%
  dyOptions(colors=c("blue", "red", "purple"), strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{VWAP Crossover Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The VWAP crossover strategy shorts the market during significant selloffs, but otherwise doesn't display market timing skill.
      \vskip1ex
      The t-value of the \emph{Treynor-Mazuy} test is negative, but not statistically significant. 
      <<echo=TRUE,eval=FALSE>>=
# Test VWAP crossover market timing of VTI using Treynor-Mazuy test
design <- cbind(pnls, vtis, vtis^2)
design <- na.omit(design)
colnames(design) <- c("VWAP", "VTI", "treynor")
model <- lm(VWAP ~ VTI + treynor, data=design)
summary(model)
# Plot residual scatterplot
residuals <- (design$VWAP - model$coeff[2]*design$VTI)
residuals <- model$residuals
x11(width=6, height=6)
plot.default(x=design$VTI, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for VWAP Crossover vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vtis^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("VWAP crossover t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_timing_skill.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulation Function for VWAP Crossover Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VWAP} strategy can be simulated by a single function, which allows the analysis of its performance depending on its parameters.
      \vskip1ex
      The function \texttt{sim\_vwap()} performs a simulation of the \emph{VWAP} strategy, given an \emph{OHLC} time series of prices, and the length of the look-back interval (\texttt{look\_back}).
      \vskip1ex
      The function \texttt{sim\_vwap()} returns the \emph{VWAP} strategy positions and returns, in a two-column \emph{xts} time series.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
sim_vwap <- function(ohlc, look_back=333, bid_offer=0.001, trend=1, lagg=1) {
  close <- log(quantmod::Cl(ohlc))
  volumes <- quantmod::Vo(ohlc)
  returns <- rutils::diffit(close)
  nrows <- NROW(ohlc)
  # Calculate VWAP prices
  vwap <- roll::roll_sum(closep*volumes, width=look_back, min_obs=1)
  volume_roll <- roll::roll_sum(volumes, width=look_back, min_obs=1)
  vwap <- vwap/volume_roll
  # Calculate the indicator
  indic <- trend*sign(close - vwap)
  if (lagg > 1) {
    indic <- roll::roll_sum(indic, width=lagg, min_obs=1)
    indic[1:lagg] <- 0
  }  # end if
  # Calculate positions, either: -1, 0, or 1
  pos <- rep(NA_integer_, nrows)
  pos[1] <- 0
  pos <- ifelse(indic == lagg, 1, pos)
  pos <- ifelse(indic == (-lagg), -1, pos)
  pos <- zoo::na.locf(pos, na.rm=FALSE)
  pos <- xts::xts(pos, order.by=index(close))
  # Lag the positions to trade on next day
  pos <- rutils::lagit(pos, lagg=1)
  # Calculate PnLs of strategy
  pnls <- returns*pos
  costs <- 0.5*bid_offer*abs(rutils::diffit(pos))*close
  pnls <- (pnls - costs)
  # Calculate strategy returns
  pnls <- cbind(pos, pnls)
  colnames(pnls) <- c("positions", "pnls")
  pnls
}  # end sim_vwap
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Multiple Trend Following VWAP Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{VWAP} strategies can be simulated by calling the function \texttt{sim\_vwap()} in a loop over a vector of $\lambda$ parameters.
      \vskip1ex
      But \texttt{sim\_vwap()} returns an \emph{xts} time series, and \texttt{sapply()} cannot merge \emph{xts} time series together.
      \vskip1ex
      So instead the loop is performed using \texttt{lapply()} which returns a list of \emph{xts}, and the list is merged into a single \emph{xts} using the functions \texttt{do.call()} and \texttt{cbind()}.
      <<echo=TRUE,eval=FALSE>>=
source("/Users/jerzy/Develop/lecture_slides/scripts/ewma_model.R")
look_backs <- seq(70, 200, 10)
# Perform lapply() loop over lambdas
pnls <- lapply(look_backs, function(look_back) {
  # Simulate VWAP strategy and calculate returns
  sim_vwap(ohlc=ohlc, look_back=look_back, bid_offer=0, lagg=2)[, "pnls"]
})  # end lapply
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("look_back=", look_backs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vwap_trend_returns.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of multiple VWAP strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls["2007/"]), main="Cumulative Returns of Trend Following VWAP Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot VWAP strategies with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(cumsum(pnls), theme=plot_theme,
  name="Cumulative Returns of VWAP Strategies")
legend("topleft", legend=colnames(pnls), inset=0.1,
  bg="white", cex=0.8, lwd=rep(6, NCOL(pnls)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Autoregressive Strategies}


%%%%%%%%%%%%%%%
\subsection{In-sample Forecasting Using Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{response} variable are the daily stock returns, and the \emph{predictor} matrix are the lagged returns.  Given \emph{n} lags, the forecasting model is an \emph{AR(n)} autoregressive model.
      \vskip1ex
      \emph{In-sample forecasting} consists of first fitting an \emph{AR(n)} model to the data, and calculating its coefficients.
      \vskip1ex
      The \emph{in-sample} forecasts are calculated by multiplying the response vector of returns by the fitted coefficients.
      \vskip1ex
      The mean squared errors (\emph{MSE}) of the \emph{in-sample} forecasts decrease steadily with the increasing order parameter \emph{n} of the \emph{AR(n)} model.
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of daily VTI log returns
vtis <- na.omit(rutils::etfenv$returns$VTI)
dates <- index(vtis)
vtis <- as.numeric(vtis)
nrows <- NROW(vtis)
# Define predictor matrix for forecasting
order_max <- 5
predictor <- sapply(1:order_max, rutils::lagit, input=vtis)
predictor <- cbind(rep(1, nrows), predictor)
colnames(predictor) <- paste0("pred_", 1:NCOL(predictor))
response <- vtis
# Calculate forecasts as function of the AR order
forecasts <- lapply(2:NCOL(predictor), function(ordern) {
  # Calculate fitted coefficients
  inverse <- MASS::ginv(predictor[, 1:ordern])
  coeff <- drop(inverse %*% response)
  # Calculate in-sample forecasts of vtis
  drop(predictor[, 1:ordern] %*% coeff)
})  # end lapply
names(forecasts) <- paste0("n=", 2:NCOL(predictor))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_forecast_insample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate mean squared errors
mse <- sapply(forecasts, function(x) {
  c(mse=mean((vtis - x)^2), cor=cor(vtis, x))
})  # end sapply
mse <- t(mse)
rownames(mse) <- names(forecasts)
# Plot forecasting MSE
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(x=2:NCOL(predictor), y=mse[, 1],
  xlab="AR(n) order", ylab="MSE", type="l", lwd=2,
  main="MSE of In-sample AR(n) Forecasting Model for VTI")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Out-of-sample Forecasting Using Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Out-of-sample forecasting} consists of first fitting an \emph{AR(n)} model to the training data, and calculating its coefficients.
      \vskip1ex
      The \emph{out-of-sample} forecasts are calculated by multiplying the \emph{out-of-sample} response vector of returns by the fitted coefficients.
      \vskip1ex
      The mean squared errors (\emph{MSE}) of the \emph{out-of-sample} forecasts increase steadily with the increasing order parameter \emph{n} of the \emph{AR(n)} model.
      \vskip1ex
      The reason for the increasing out-of-sample MSE is \emph{overfitting} of the coefficients to the training data.
      <<echo=TRUE,eval=FALSE>>=
in_sample <- 1:(nrows %/% 2)
out_sample <- (nrows %/% 2 + 1):nrows
# Calculate forecasts as function of the AR order
forecasts <- lapply(2:NCOL(predictor), function(ordern) {
  # Calculate fitted coefficients
  inverse <- MASS::ginv(predictor[in_sample, 1:ordern])
  coeff <- drop(inverse %*% response[in_sample])
  # Calculate out-of-sample forecasts of vtis
  drop(predictor[out_sample, 1:ordern] %*% coeff)
})  # end lapply
names(forecasts) <- paste0("n=", 2:NCOL(predictor))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_forecast_outofsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate mean squared errors
mse <- sapply(forecasts, function(x) {
  c(mse=mean((vtis[out_sample] - x)^2), cor=cor(vtis[out_sample], x))
})  # end sapply
mse <- t(mse)
rownames(mse) <- names(forecasts)
# Plot forecasting MSE
plot(x=2:NCOL(predictor), y=mse[, 1],
  xlab="AR(n) order", ylab="MSE", type="l", lwd=2,
  main="MSE of Out-of-sample AR(n) Forecasting Model for VTI")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive Strategy Out-of-sample Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The autoregressive strategy invests a dollar amount of \emph{VTI} equal to the sign of the forecasts. 
      \vskip1ex
      The performance of the autoregressive strategy is better with a smaller order parameter \emph{n} of the \emph{AR(n)} model.
      \vskip1ex
      Decreasing the order parameter of the autoregressive model is a form of \emph{shrinkage} because it reduces the number of predictive variables.
      <<echo=TRUE,eval=FALSE>>=
# Calculate out-of-sample PnLs
pnls <- sapply(forecasts, function(x) {
  cumsum(sign(x)*vtis[out_sample])
})  # end sapply
colnames(pnls) <- names(forecasts)
pnls <- xts::xts(pnls, dates[out_sample])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_pnl_order.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample PnLs
colorv <- colorRampPalette(c("red", "blue"))(NCOL(pnls[, 1:4]))
colnamev <- colnames(pnls[, 1:4])
dygraphs::dygraph(pnls[, 1:4],
  main="Autoregressive Strategies Performance With Different Order Parameters") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive Strategy Using Rolling Average Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} forecasts can be improved by using the rolling average of the returns as a predictor.
      \vskip1ex
      This is because the average of returns has a lower \emph{variance}.
      \vskip1ex
      But the average also has a higher \emph{bias} because it includes returns that may be unrelated to the present.
      \vskip1ex
      Using the rolling average of returns as a predictor reduces the forecast variance at the expense of increasing its bias (known as the \emph{bias-variance tradeoff}).
      <<echo=TRUE,eval=FALSE>>=
# Define predictor as a rolling mean
nagg <- 5
predictor <- roll::roll_mean(vtis, width=nagg, min_obs=1)
response <- vtis
# Define predictor matrix for forecasting
predictor <- sapply(1+nagg*(0:order_max), rutils::lagit,
                     input=predictor)
predictor <- cbind(rep(1, nrows), predictor)
# Calculate forecasts as function of the AR order
forecasts <- lapply(2:NCOL(predictor), function(ordern) {
  inverse <- MASS::ginv(predictor[in_sample, 1:ordern])
  coeff <- drop(inverse %*% response[in_sample])
  drop(predictor[out_sample, 1:ordern] %*% coeff)
})  # end lapply
names(forecasts) <- paste0("n=", 2:NCOL(predictor))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_pnl_rolling_sum.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate out-of-sample PnLs
pnls <- sapply(forecasts, function(x) {
  cumsum(sign(x)*vtis[out_sample])
})  # end sapply
colnames(pnls) <- names(forecasts)
pnls <- xts::xts(pnls, dates[out_sample])
# Plot dygraph of out-of-sample PnLs
dygraphs::dygraph(pnls[, 1:4],
  main="Autoregressive Strategies Performance Using Rolling Average Predictor") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive Strategy Using Rolling Average Forecasts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} forecasts can be further improved by using the average of past forecasts.
      \vskip1ex
      This is because the average of forecasts has a lower \emph{variance}.
      \vskip1ex
      But the average also has a higher \emph{bias} because it includes past forecasts that may be unrelated to the present.
      \vskip1ex
      Using the rolling average of past forecasts reduces the forecast variance at the expense of increasing its bias (known as the \emph{bias-variance tradeoff}).
      <<echo=TRUE,eval=FALSE>>=
# Calculate PnLs using the average of past forecasts
nagg <- 5
pnls <- sapply(forecasts, function(x) {
  x <- roll::roll_mean(x, width=nagg, min_obs=1)
  cumsum(sign(x)*vtis[out_sample])
})  # end sapply
colnames(pnls) <- names(forecasts)
pnls <- xts::xts(pnls, dates[out_sample])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_pnl_rolling_forecasts.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample PnLs
dygraphs::dygraph(pnls[, 1:4],
  main="Autoregressive Strategies Performance Using Rolling Average Forecasts") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Forecasting Returns Using Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Forecasting using an autoregressive model is performed by first fitting an \emph{AR(n)} model to past data, and calculating its coefficients.
      \vskip1ex
      The fitted coefficients are then applied to calculating the \emph{out-of-sample} forecasts.
      \vskip1ex
      The forecasting model depends on two unknown \emph{meta-parameters}: the order \emph{n} of the \emph{AR(n)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)
# Calculate a vector of daily VTI log returns
prices <- log(quantmod::Cl(rutils::etfenv$VTI))
vtis <- rutils::diffit(prices)
vtis <- as.numeric(vtis)
nrows <- NROW(vtis)
# Define predictor matrix for forecasting
order_max <- 5
design <- sapply(1:order_max, rutils::lagit, input=vtis)
colnames(design) <- paste0("pred_", 1:NCOL(design))
# Add response equal to vtis
design <- cbind(vtis, design)
colnames(design)[1] <- "response"
# Specify length of look-back interval
look_back <- 100
# Invert the predictor matrix
rangev <- (nrows-look_back):(nrows-1)
design_inv <- MASS::ginv(design[rangev, -1])
# Calculate fitted coefficients
coeff <- drop(design_inv %*% design[rangev, 1])
# Calculate forecast of vtis for.n_rows
drop(design[nrows, -1] %*% coeff)
# Compare with actual value
design[nrows, 1]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Autoregressive Forecasting Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the simulation of a model on historical data to test its forecasting accuracy.
      \vskip1ex
      The autoregressive forecasting model can be \emph{backtested} by calculating forecasts over either a \emph{rolling} or an \emph{expanding} look-back interval.
      \vskip1ex
      If the start date is fixed at the first row then the look-back interval is \emph{expanding}.
      \vskip1ex
      The coefficients of the \emph{AR(n)} process are fitted to past data, and then applied to calculating out-of-sample forecasts.
      \vskip1ex
      The \emph{backtesting} procedure allows determining the optimal \emph{meta-parameters} of the forecasting model: the order \emph{n} of the \emph{AR(n)} model and the length of look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of daily VTI log returns
vtis <- na.omit(rutils::etfenv$returns$VTI)
dates <- index(vtis)
vtis <- as.numeric(vtis)
nrows <- NROW(vtis)
# Define response equal to vtis
response <- vtis
# Define predictor as a rolling sum
nagg <- 5
predictor <- rutils::roll_sum(vtis, look_back=nagg)
# Define predictor matrix for forecasting
order_max <- 5
predictor <- sapply(1+nagg*(0:order_max), rutils::lagit,
                     input=predictor)
predictor <- cbind(rep(1, nrows), predictor)
# Perform rolling forecasting
look_back <- 100
forecasts <- sapply((look_back+1)/nrows, function(endp) {
  # Define rolling look-back range
  startp <- max(1, endp-look_back)
  # Or expanding look-back range
  # startp <- 1
  rangev <- startp:(endp-1)
  # Invert the predictor matrix
  design_inv <- MASS::ginv(predictor[rangev, ])
  # Calculate fitted coefficients
  coeff <- drop(design_inv %*% response[rangev])
  # Calculate forecast
  drop(predictor[endp, ] %*% coeff)
})  # end sapply
# Add warmup period
forecasts <- c(rep(0, look_back), forecasts)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mean Squared Error of the Autoregressive Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of a forecasting model can be measured using the \emph{mean squared error} and the \emph{correlation}.
      \vskip1ex
      The mean squared error (\emph{MSE}) of a forecasting model is the average of the squared forecasting residuals $\varepsilon_i$, equal to the differences between the actual values $r_i$ minus the \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$:
      \begin{displaymath}
        \operatorname{MSE} = \frac{1}{n} \sum_{i=1}^n (r_i - f_i)^2
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Mean squared error
mean((vtis - forecasts)^2)
# Correlation
cor(forecasts, vtis)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_forecast_resid.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot forecasting series with legend
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0))
plot(vtis[(nrows-look_back):nrows], col="blue", 
     xlab="", ylab="", type="l", lwd=2,
     main="Rolling Forecasting Using AR Model")
lines(forecasts[(nrows-look_back):nrows], col="red", lwd=2)
legend(x="top", legend=c("returns", "forecasts"),
       col=c("blue", "red"), lty=1, lwd=6,
       cex=0.9, bg="white", bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Function for the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{meta-parameters} of the \emph{backtesting} function are the order \emph{n} of the \emph{AR(n)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtesting function
sim_forecasts <- function(response, nagg=5, 
                          ordern=5, look_back=100) {
  nrows <- NROW(response)
  # Define predictor as a rolling sum
  predictor <- rutils::roll_sum(response, look_back=nagg)
  # Define predictor matrix for forecasting
  predictor <- sapply(1+nagg*(0:ordern), rutils::lagit,
                       input=predictor)
  predictor <- cbind(rep(1, nrows), predictor)
  # Perform rolling forecasting
  forecasts <- sapply((look_back+1)/nrows, function(endp) {
    # Define rolling look-back range
    startp <- max(1, endp-look_back)
    # Or expanding look-back range
    # startp <- 1
    rangev <- startp:(endp-1)
    # Invert the predictor matrix
    design_inv <- MASS::ginv(predictor[rangev, ])
    # Calculate fitted coefficients
    coeff <- drop(design_inv %*% response[rangev])
    # Calculate forecast
    drop(predictor[endp, ] %*% coeff)
  })  # end sapply
  # Add warmup period
  forecasts <- c(rep(0, look_back), forecasts)
  # Aggregate the forecasts
  rutils::roll_sum(forecasts, look_back=nagg)
}  # end sim_forecasts
# Simulate the rolling autoregressive forecasts
forecasts <- sim_forecasts(response=vtis, ordern=5, look_back=100)
c(mse=mean((vtis - forecasts)^2), cor=cor(vtis, forecasts))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Dependence On the Look-back Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{backtesting} function can be used to find the optimal \emph{meta-parameters} of the autoregressive forecasting model.
      \vskip1ex
      The accuracy of the forecasting model depends on the order \emph{n} of the \emph{AR(n)} model and on the length of the look-back interval (\texttt{look\_back}).
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      \vskip1ex
      The accuracy of the forecasting model increases steadily with longer look-back intervals (\texttt{look\_back}), because more data improves the estimates of the autoregressive coefficients.
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
# Calculate number of available cores
numcores <- detectCores() - 1
# Initialize compute cluster under Windows
cluster <- makeCluster(numcores)
# clusterExport(cluster, c("startd", "barp"))
# Perform parallel loop under Windows
look_backs <- seq(20, 600, 40)
forecasts <- parLapply(cluster, look_backs, sim_forecasts, 
  response=vtis, nagg=5, ordern=5)
# Perform parallel bootstrap under Mac-OSX or Linux
forecasts <- mclapply(look_backs, sim_forecasts, response=vtis, 
  nagg=5, ordern=5, mc.cores=numcores)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_ar_vti_mse.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate mean squared errors
mse <- sapply(forecasts, function(x) {
  c(mse=mean((vtis - x)^2), cor=cor(vtis, x))
})  # end sapply
mse <- t(mse)
rownames(mse) <- look_backs
# Select optimal look_back interval
look_back <- look_backs[which.min(mse[, 1])]
# Plot forecasting MSE
plot(x=look_backs, y=mse[, 1],
  xlab="look-back", ylab="MSE", type="l", lwd=2,
  main="MSE of AR Forecasting Model As Function of Look-back")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Dependence On the Order Parameter}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{backtesting} function can be used to find the optimal \emph{meta-parameters} of the autoregressive forecasting model.
      \vskip1ex
      The accuracy of the forecasting model depends on the order \emph{n} of the \emph{AR(n)} model and on the length of the look-back interval (\texttt{look\_back}).
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      \vskip1ex
      The accuracy of the forecasting model increases steadily with longer look-back intervals (\texttt{look\_back}), because more data improves the estimates of the autoregressive coefficients.
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
# Calculate number of available cores
numcores <- detectCores() - 1
# Initialize compute cluster under Windows
cluster <- makeCluster(numcores)
# clusterExport(cluster, c("startd", "barp"))
# Perform parallel loop under Windows
forecasts <- parLapply(cluster, orders, sim_forecasts, response=vtis, 
  nagg=5, look_back=look_back)
stopCluster(cluster)  # Stop R processes over cluster under Windows
# Perform parallel bootstrap under Mac-OSX or Linux
orders <- 2:6
forecasts <- mclapply(orders, sim_forecasts, response=vtis, 
  nagg=5, look_back=look_back, mc.cores=numcores)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_ar_vti_mse_order.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate mean squared errors
mse <- sapply(forecasts, function(x) {
  c(mse=mean((vtis - x)^2), cor=cor(vtis, x))
})  # end sapply
mse <- t(mse)
rownames(mse) <- orders
# Select optimal order parameter
ordern <- orders[which.min(mse[, 1])]
# Plot forecasting MSE
plot(x=orders, y=mse[, 1],
  xlab="ordern", ylab="MSE", type="l", lwd=2,
  main="MSE of Forecasting Model As Function of AR Order")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of the Rolling Autoregressive Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The return forecasts are calculated just before the close of the markets, so that trades can be executed before the close.
      \vskip1ex
      The autoregressive strategy is dominated by a few periods with very large returns, without producing profits for the remaining periods.
      \vskip1ex
      Using the return forecasts as portfolio weights produces very large weights in periods of high volatility, and creates excessive risk.
      \vskip1ex
      To reduce excessive risk, a binary strategy uses portfolio weights equaly to the sign of the forecasts.
      <<echo=TRUE,eval=FALSE>>=
# Simulate the rolling autoregressive forecasts
forecasts <- sim_forecasts(vtis, ordern=ordern, look_back=look_back)
# Calculate strategy PnLs
pnls <- sign(forecasts)*vtis
pnls <- cbind(vtis, pnls, (vtis+pnls)/2)
colnames(pnls) <- c("VTI", "AR_Strategy", "Combined")
cor(pnls)
# Annualized Sharpe ratios of VTI and AR strategy
pnls <- xts::xts(pnls, dates)
sqrt(252)*sapply(pnls, function (x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_ar_vti_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the cumulative strategy PnLs
dygraphs::dygraph(cumsum(pnls), main="Rolling Autoregressive Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Dependence On the Order Parameter}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of the forecasting model depends on the order \emph{n} of the \emph{AR(n)} model.
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      \vskip1ex
      Longer look-back intervals (\texttt{look\_back}) are usually better for the autoregressive forecasting model.
      \vskip1ex
      The return forecasts are calculated just before the close of the markets, so that trades can be executed before the close.
      \vskip1ex
      The autoregressive strategy is dominated by a few periods with very large returns, without producing profits for the remaining periods.
      <<echo=TRUE,eval=FALSE>>=
# Calculate PnLs for ordern=5
forecasts <- sim_forecasts(vtis, ordern=5, look_back=look_back)
pnls5 <- cumsum(sign(forecasts)*vtis)
# Calculate PnLs for ordern=3
forecasts <- sim_forecasts(vtis, ordern=3, look_back=look_back)
pnls3 <- cumsum(sign(forecasts)*vtis)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.48\paperwidth]{figure/strat_ar_vti_order.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the cumulative strategy returns
wealth <- cbind(pnls5, pnls3)
wealth <- xts::xts(wealth, dates)
colnamev <- c("AR(5)_Strategy", "AR(3)_Strategy")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Autoregressive Strategies for Different Order Parameters") %>%
  dySeries(name=colnamev[1], label=colnamev[1], col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], label=colnamev[2], col="red", strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Autoregressive Strategy With an Expanding Look-back Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of the forecasting model depends on whether a \emph{rolling} or an \emph{expanding} look-back interval is used.
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
      \vskip1ex
      Longer look-back intervals (\texttt{look\_back}) are usually better for the autoregressive forecasting model.
      \vskip1ex
      The return forecasts are calculated just before the close of the markets, so that trades can be executed before the close.
      \vskip1ex
      The autoregressive strategy is dominated by a few periods with very large returns, without producing profits for the remaining periods.
      <<echo=TRUE,eval=FALSE>>=
# Calculate PnLs for rolling look-back
forecasts <- sim_forecasts(vtis, ordern=3, look_back=look_back, is_rolling=TRUE)
pnls_rolling <- cumsum(sign(forecasts)*vtis)
# Calculate PnLs for expanding look-back
forecasts <- sim_forecasts(vtis, ordern=3, look_back=look_back, is_rolling=FALSE)
pnls_expanding <- cumsum(sign(forecasts)*vtis)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.48\paperwidth]{figure/strat_ar_vti_expanding.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the cumulative strategy returns
wealth <- cbind(pnls_rolling, pnls_expanding)
wealth <- xts::xts(wealth, dates)
colnamev <- c("AR(3)_Rolling", "AR(3)_Expanding")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Autoregressive Strategies for Expanding Look-back Interval") %>%
  dySeries(name=colnamev[1], label=colnamev[1], col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], label=colnamev[2], col="red", strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Improved Autoregressive Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the autoregressive strategy can be improved by fitting its coefficients using the \emph{capped returns}, to reduce the leverage of very large returns.
      \vskip1ex
      The performance can be further improved by fitting the coefficients over an \emph{expanding look-back interval}, instead of a \emph{rolling look-back interval}.
      \vskip1ex
      A longer look-back interval (\texttt{look\_back}) also improves the performance.
      <<echo=TRUE,eval=FALSE>>=
# Cap the VTI returns
cutoff <- 0.03
capped <- ifelse(vtis > cutoff, cutoff, vtis)
capped <- ifelse(capped < (-cutoff), -cutoff, capped)
# Calculate PnLs for vtis
forecasts <- sim_forecasts(vtis, ordern=3, look_back=look_back, is_rolling=FALSE)
pnls <- cumsum(sign(forecasts)*vtis)
# Calculate PnLs for capped VTI returns
forecasts <- sim_forecasts(capped, ordern=3, look_back=look_back, is_rolling=FALSE)
pnls_capped <- cumsum(sign(forecasts)*vtis)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.48\paperwidth]{figure/strat_ar_vti_perf_capped.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the cumulative strategy returns
wealth <- cbind(pnls, pnls_capped)
wealth <- xts::xts(wealth, dates)
colnamev <- c("AR(3)_Rolling", "AR(3)_Expanding")
colnamev <- c("AR_Strategy", "AR_Strategy_Capped")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Improved Autoregressive Strategies") %>%
  dySeries(name=colnamev[1], label=colnamev[1], col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], label=colnamev[2], col="red", strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
