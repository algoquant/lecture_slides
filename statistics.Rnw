% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='tiny', fig.width=4, fig.height=4)
options(digits=3)
options(width=80, dev='pdf')
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[10pt]{beamer}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% tikz package for plotting and tables
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \usepackage{bookmark}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Probability and Statistics]{Probability and Statistics}
\subtitle{FRE6871 \& FRE7241, Spring 2021}
% \subject{Getting Started With R}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \author[Jerzy Pawlowski]{Jerzy Pawlowski \texorpdfstring{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}
% \email{jp3900@poly.edu}
% \date{January 27, 2014}
\date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Random Numbers and Estimators}


%%%%%%%%%%%%%%%
\subsection{Pseudo-Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Pseudo-random numbers are deterministic sequences of numbers which have some of the properties of random numbers, but they are not truly random numbers.
      \vskip1ex
      Pseudo-random number generators depend on a \emph{seed} value, and produce the same sequence of numbers for a given \emph{seed} value.
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value.
      \vskip1ex
      The choice of \emph{seed} value isn't important, and a given value is just good as any other one.
      \vskip1ex
      The function \texttt{runif()} produces random numbers from the \emph{uniform} distribution.
      \vskip1ex
      The function \texttt{rnorm()} produces random numbers from the \emph{normal} distribution.
      \vskip1ex
      The function \texttt{rt()} produces random numbers from the \emph{t-distribution} with $df$ degrees of freedom.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
runif(3)  # three numbers from uniform distribution
runif(3)  # Simulate another three numbers
set.seed(1121)  # Reset random number generator
runif(3)  # Simulate another three numbers
# Simulate random number from standard normal distribution
rnorm(1)
# Simulate five standard normal random numbers
rnorm(5)
# Simulate five non-standard normal random numbers
rnorm(n=5, mean=1, sd=2)  # Match arguments by name
# Simulate t-distribution with 2 degrees of freedom
rt(n=5, df=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} is a recurrence relation which produces a deterministic sequence of numbers:
      \begin{displaymath}
        x_n = r x_{n-1} (1 - x_{n-1})
      \end{displaymath}
      If the \emph{seed} value $x_0$ is in the interval $(0, 1)$ and if $r=4$, then the sequence $x_n$ is also contained in the interval $(0, 1)$.
      \vskip1ex
      The function \texttt{curve()} plots a function defined by its name.
      <<echo=TRUE,eval=FALSE>>=
# Define logistic map function
log_map <- function(x, r=4) r*x*(1-x)
log_map(0.25, 4)
# Plot logistic map
x11(width=6, height=5)
curve(expr=log_map, type="l", xlim=c(0, 1),
      xlab="x[n-1]", ylab="x[n]", lwd=2, col="blue",
      main="logistic map")
lines(x=c(0, 0.25), y=c(0.75, 0.75), lwd=2, col="orange")
lines(x=c(0.25, 0.25), y=c(0, 0.75), lwd=2, col="orange")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_map.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Pseudo-Random Numbers Using Logistic Map}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{logistic map} can be used to calculate sequences of pseudo-random numbers.
      \vskip1ex
      For most \emph{seed} values $x_0$ and $r=4$, the \emph{logistic map} produces a pseudo-random sequence, but it's not uniformly distributed.
      \vskip1ex
      The inverse cosine function \texttt{acos()} transforms a \emph{logistic map} sequence into a uniformly distributed sequence,
      \begin{displaymath}
        u_n = \arccos(1 - 2 x_n) / \pi
      \end{displaymath}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate uniformly distributed pseudo-random
# Sequence using logistic map function
uni_form <- function(see_d, n_rows=10) {
  # Pre-allocate vector instead of "growing" it
  out_put <- numeric(n_rows)
  # initialize
  out_put[1] <- see_d
  # Perform loop
  for (i in 2:n_rows) {
    out_put[i] <- 4*out_put[i-1]*(1-out_put[i-1])
  }  # end for
  acos(1-2*out_put)/pi
}  # end uni_form
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/logistic_map_density.png}\\
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
uni_form(see_d=0.1, n_rows=15)
plot(
  density(uni_form(see_d=runif(1), n_rows=1e5)),
  xlab="", ylab="", lwd=2, col="blue",
  main="uniform pseudo-random number density")
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Binomial Random Numbers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.4\textwidth}
      A \emph{binomial} trial is a coin flip, that results in either a success or failure.
      \vskip1ex
      The \emph{binomial} distribution specifies the probability of obtaining a certain number of successes in a sequence of independent \emph{binomial} trials.
      \vskip1ex
      Let $p$ be the probability of obtaining a success in a \emph{binomial} trial, and let $(1-p)$ be the probability of failure.
      \vskip1ex
      $p = 0.5$ corresponds to flipping an unbiased coin.
      \vskip1ex
      The probability of obtaining $k$ successes in $n$ independent \emph{binomial} trials is equal to:
      \begin{displaymath}
        {n \choose k} p^k (1-p)^{(n-k)}
      \end{displaymath}
      The function \texttt{rbinom()} produces random numbers from the \emph{binomial} distribution.
    \column{0.6\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Flip unbiased coin once, 20 times
rbinom(n=20, size=1, 0.5)
# Number of heads after flipping twice, 20 times
rbinom(n=20, size=2, 0.5)
# Number of heads after flipping thrice, 20 times
rbinom(n=20, size=3, 0.5)
# Number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.8)
# Number of heads after flipping biased coin thrice, 20 times
rbinom(n=20, size=3, 0.2)
# Flip unbiased coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)  # Fast
as.numeric(runif(20) < 0.5)  # Slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Generating Random Samples and Permutations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{sample} is a subset of elements taken from a set of data elements.
      \vskip1ex
      The function \texttt{sample()} selects a random sample from a vector of data elements.
      \vskip1ex
      By default the \emph{size} of the sample (the \texttt{size} argument) is equal to the number of elements in the data vector.
      \vskip1ex
      So the call \texttt{sample(da\_ta)} produces a random permutation of all the elements of \texttt{da\_ta}.
      \vskip1ex
      The function \texttt{sample()} with \texttt{replace=TRUE} selects samples with replacement (the default is \texttt{replace=FALSE}).
      \vskip1ex
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution.
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Permutation of five numbers
sample(x=5)
# Permutation of four strings
sample(x=c("apple", "grape", "orange", "peach"))
# Sample of size three
sample(x=5, size=3)
# Sample with replacement
sample(x=5, replace=TRUE)
sample(  # Sample of strings
  x=c("apple", "grape", "orange", "peach"),
  size=12,
  replace=TRUE)
# Binomial sample: flip coin once, 20 times
sample(x=0:1, size=20, replace=TRUE)
# Flip unbiased coin once, 20 times
as.numeric(runif(20) > 0.5)  # Slower
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Statistical Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A data \emph{sample} is a set of observations $\{x_1, \ldots, x_n\}$ of a \emph{random variable} $x$.
      \vskip1ex
      Let $x$ follow a probability distribution with population \emph{mean} equal to $\mu$ and population \emph{standard deviation} equal to $\sigma$.
      \vskip1ex
      A \emph{statistic} is a function of the data \emph{sample}:  $f( x_1, \ldots, x_n )$, so it is itself a \emph{random variable}.
      \vskip1ex
      A statistical \emph{estimator} is a \emph{statistic} that provides an estimate of a distribution \emph{parameter}.
      \vskip1ex
      For example:
      \begin{displaymath}
        \bar{x} = \frac{1}{n}{\sum_{i=1}^n x_i}
      \end{displaymath}
      Is an \emph{estimator} of the \emph{population mean} of the \emph{distribution}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2))>>=
rm(list=ls())
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
da_ta <- rnorm(1000)

mean(da_ta)  # Sample mean

median(da_ta)  # Sample median

sd(da_ta)  # Sample standard deviation
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Higher Moments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of the moments of a probability distribution, based on a \emph{sample} of data $x_i$, are given by:
      \vskip1ex
      Sample mean: $\bar{x} = \frac{1}{n} \sum_{i = 1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2 = \frac{1}{n-1} \sum_{i = 1}^n (x_i-\bar{x})^2$
      \vskip1ex
      Their expected values are equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}] = \mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma\mathbb{E}[\hat\sigma] = \sigma$
      \vskip1ex
      The third and fourth moments are equal to:
      \begin{flalign*}
        \mu_3 = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n (x_i-\bar{x})^3 \\
        \mu_4 = \frac{n}{(n-1)^2} \sum_{i=1}^n (x_i-\bar{x})^4
      \end{flalign*}
      The skewness and kurtosis are equal to the moments scaled by the standard deviation:
      \begin{displaymath}
        \varsigma = \frac{\mu_3}{\sigma^3}, \quad \kappa = \frac{\mu_4}{\sigma^4}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
rm(list=ls())
# VTI returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Number of observations
n_rows <- NROW(re_turns)
# Mean of VTI returns
mea_n <- mean(re_turns)
# Standard deviation of VTI returns
s_d <- sd(re_turns)
# Standardize returns
re_turns <- (re_turns - mea_n)/s_d
# Skewness of VTI returns
n_rows/((n_rows-1)*(n_rows-2))*sum(re_turns^3)
# Kurtosis of VTI returns
n_rows/(n_rows-1)^2*sum(re_turns^4)
# Random normal returns
re_turns <- rnorm(n_rows)
# Mean and standard deviation of random normal returns
mean(re_turns); sd(re_turns)
# Skewness and kurtosis of random normal returns
n_rows/((n_rows-1)*(n_rows-2))*sum(re_turns^3)
n_rows/(n_rows-1)^2*sum(re_turns^4)
      @
      The normal distribution has skewness equal to zero $\varsigma = 0$, and kurtosis equal to three $\kappa = 3$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimators of Quantiles}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{quantile} corresponding to a given \emph{probability} $p$, is the value of the \emph{random variable} $x$, such that the probability of obtaining values less than $x$ is equal to the \emph{probability} $p$.
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} $p$.
      \vskip1ex
	  The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order.
      \vskip1ex
      The function \texttt{pnorm()} calculates the cumulative \emph{normal} distribution, i.e. the cumulative probability for a given quantile value.
      \vskip1ex
      The function \texttt{qnorm()} calculates the inverse cumulative \emph{normal} distribution, i.e. the quantile for a given probability value.
      \vskip1ex
      The function \texttt{dnorm()} calculates the normal probability \emph{density}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative standard normal distribution
c(pnorm(-2), pnorm(2))
# Calculate inverse cumulative standard normal distribution
c(qnorm(0.75), qnorm(0.25))
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean - MC estimate
mean(da_ta)
# Sample standard deviation - MC estimate
sd(da_ta)
# Monte Carlo estimate of cumulative probability
c(pnorm(1), sum(da_ta < 1)/n_rows)
# Monte Carlo estimate of quantile
conf_level <- 0.99
qnorm(conf_level)
cut_off <- conf_level*n_rows
da_ta <- sort(da_ta)
c(da_ta[cut_off], quantile(da_ta, probs=conf_level))
# Read the source code of quantile()
stats:::quantile.default
# microbenchmark quantile
library(microbenchmark)
summary(microbenchmark(
  monte_carlo=da_ta[cut_off],
  quan_tile=quantile(da_ta, probs=conf_level),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_\mu = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
# Standard error of sample mean
sd(da_ta)/sqrt(n_rows)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Probability Distributions}


%%%%%%%%%%%%%%%
\subsection{The Characteristic Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{characteristic function} $\hat{f}(t)$ is equal to the \emph{Fourier transform} of the \emph{probability density function} $f(x)$:
      \begin{displaymath}
        \hat{f}(t) = \mathbb{E}[e^{i t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{i t x} \, \mathrm{d}x}
      \end{displaymath}
      The \emph{normal} probability density function:
      \begin{displaymath}
        \phi(x) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      Has the \emph{characteristic function} $\hat\phi(t)$ equal to:
      \begin{displaymath}
        \hat\phi(t) = e^{i \mu t} e^{-(\sigma t)^2/2}
      \end{displaymath}
      The \emph{probability function} $f(x)$ is equal to the \emph{inverse Fourier transform} of the \emph{characteristic function} $\hat{f}(t)$:
      \begin{displaymath}
        f(x) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} {\hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
      
    \column{0.5\textwidth}
      The \emph{characteristic function} of the first derivative of $f(x)$ is equal to $(-i t) \hat{f}(t)$, the \emph{characteristic function} multiplied by $(-i t)$:
      \begin{displaymath}
        \frac{d f(x)}{d x} = \frac{1}{2 \pi} \int_{-\infty}^{\infty} { -i t \, \hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
      The \emph{characteristic function} of the \emph{n}-th derivative of $f(x)$ is equal to $(-i t)^n \hat{f}(t)$, the \emph{characteristic function} multiplied by $(-i t)^n$:
      \begin{displaymath}
        \frac{d^n f(x)}{d x^n} = \frac{1}{2 \pi} \int_{-\infty}^{\infty} { (-i t)^n \, \hat{f}(t) \, e^{- i t x} \, \mathrm{d}t}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Moment Generating Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{moment generating function} $M_X(t)$ of a random variable $x$ with the probability density function $f(x)$ is equal to:
      \begin{displaymath}
        M_X(t) = \mathbb{E}[e^{t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{t x} \, \mathrm{d}x}
      \end{displaymath}
      The \emph{n}-th derivative of $M_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{moment} $\mu_n$:
      \begin{align*}
        \mu_n &= \frac{d^n M_X(t)}{d t^n} |_{t = 0} = \frac{d^n \mathbb{E}[e^{t x}]}{d t^n} |_{t = 0} &\\
        &= \mathbb{E}[x^n e^{t x}] |_{t = 0} = \mathbb{E}[x^n] &
      \end{align*}
      The \emph{moments} $\mu_n$ are related to the \emph{central moments} $\mathbb{E}[(x - \mu)^n]$ but they are not equal to them.
    \column{0.5\textwidth}
      The \emph{moment generating function} can be expressed as a series of its \emph{moments}:
      \begin{displaymath}
        M_X(t) = \sum_{n=0}^{\infty} {\frac{\mu_n t^n}{n!}}
      \end{displaymath}
      The moment generating function for the \emph{normal} distribution is equal to:
      \begin{displaymath}
        M_X(t) = \exp(\mu t + \frac{1}{2} {\sigma^2 t^2})
      \end{displaymath}
      The \emph{characteristic function} $\hat{f}(t)$ is equal to the \emph{moment generating function} with a purely \emph{imaginary} argument:
      \begin{displaymath}
        \hat{f}(t) = \mathbb{E}[e^{i t x}] = M_X(it)
        \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cumulants of Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{cumulant generating function} $K_X(t)$ is equal to the logarithm of the \emph{moment generating function}: 
      \begin{displaymath}
        K_X(t) = \log{M_X(t)}
      \end{displaymath}
      The \emph{n}-th derivative of $K_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{cumulant} $\kappa_n$:
      \begin{displaymath}
        \kappa_n = \frac{d^n K_X(t)}{d t^n} |_{t = 0}
      \end{displaymath}
      The \emph{cumulants} are related to the \emph{moments} of a distribution:
      with the first three cumulants being equal to the \emph{central moments} (mean, variance, and skewness), while the higher order \emph{cumulants} are polynomials of the \emph{moments}.
      \vskip1ex
      The \emph{cumulant generating function} $K_X(t)$ can be expanded into a power series of the \emph{cumulants}: 
      \begin{align*}
        K_X(t) = \sum_{n=1}^{\infty} {\frac{\kappa_n t^n}{n!}} = \mu t + \sigma^2 \frac{t^2}{2} + \sigma^3 s^3 \frac{t^3}{6} + \dots
      \end{align*}
      
    \column{0.5\textwidth}
      The cumulant generating function for the \emph{normal} distribution is equal to:
      \begin{displaymath}
        K_X(t) = \mu t + \frac{1}{2} {\sigma^2 t^2}
      \end{displaymath}
      So that its first two \emph{cumulants} are equal to the \emph{mean} $\mu$ and the \emph{variance} $\sigma^2$, and the \emph{cumulants} of order \texttt{3} and higher are all equal to zero.
      \vskip1ex
      The advantage of \emph{cumulants} over the \emph{moments} is that the \emph{cumulants} of the sum of independent random variables are equal to the sum of their \emph{cumulants}:
      \begin{flalign*}
        K_{(X+Y)}(t) = \log{\mathbb{E}[e^{t (X+Y)}]} = \log{(\mathbb{E}[e^{t X}] \mathbb{E}[e^{t Y}])} \\
        = \log{\mathbb{E}[e^{t X}]} \log{\mathbb{E}[e^{t Y}]} = K_X(t) + K_Y(t)
      \end{flalign*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Hermite Polynomials}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{n}-th derivative of the \emph{standard normal} distribution $\phi(x)$ is given by Rodrigues' formula:
      \begin{displaymath}
          \frac{d^n \phi(x)}{d x^n} = \frac{d^n}{d x^n} \frac{e^{-x^2/2}}{\sqrt{2 \pi}} = (-1)^n H_n(x) \phi(x)
      \end{displaymath}
      Where $H_n$ are the \emph{Hermite polynomials}.
      \vskip1ex
      The first four \emph{Hermite polynomials} are equal to:
      \begin{align*}
        H_0(x) &= 1 ; H_1(x) = x \\
        H_2(x) &= x^2 - 1 ; H_3(x) = x^3 - 3x
      \end{align*}
      The even order polynomials are \emph{symmetric} $H_{2n}(-x) = H_{2n}(x)$ while the odd order are \emph{antisymmetric} $H_{2n-1}(-x) = -H_{2n-1}(x)$.
      <<echo=TRUE,eval=FALSE>>=
# Define Hermite polynomials
her_mite <- function(x, n) {
    switch(n+1, 1, x, (x^2 - 1), (x^3 - 3*x), 0)
}  # end her_mite
      @
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_poly.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=her_mite(x, in_dex),
        xlim=c(-3, 3), ylim=c(-2.5, 2.5),
        xlab="", ylab="", lwd=4, col=col_ors[in_dex],
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Polynomials", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("top", inset=0.0, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
    
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Hermite Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Hermite functions} $\psi_n(x)$ are equal to:
      \begin{displaymath}
        \psi_n(x) = \frac{1}{\sqrt{n! \sqrt{2 \pi}}} e^{-x^2/4} H_n(x)
      \end{displaymath}
      The \emph{Hermite functions} form an orthonormal set:
      \begin{displaymath}
        \int_{-\infty}^{\infty} {\psi_n(x) \, \psi_m(x) \, \mathrm{d}x} =
        \begin{cases}
          1 & \text{if } n = m\\
          0 & \text{if } n \neq m
        \end{cases}
      \end{displaymath}
      The \emph{Hermite functions} of increasing order oscillate more frequently, with the function of order $n$ crossing zero $n$ times.
      <<echo=TRUE,eval=FALSE>>=
# Define Hermite functions
hermite_fun <- function(x, n) 
  exp(-x^2/4)*her_mite(x, n)/(2*pi)^(0.25)/sqrt(factorial(n))
# Integrate Hermite functions
integrate(function(x, n, m) 
  hermite_fun(x, n)*hermite_fun(x, m),
  lower=(-Inf), upper=Inf, n=2, m=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_func.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=hermite_fun(x, in_dex),
        xlim=c(-6, 6), ylim=c(-0.6, 0.6),
        xlab="", ylab="", lwd=4, col=col_ors[in_dex],
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Functions", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("topright", inset=0.0, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Hermite Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
% wippp
      To-do: expand a probability distribution into a series of \emph{Hermite functions}.
      \vskip1ex
      The \emph{Hermite functions} $\psi_i(x)$ form an orthonormal basis that can be used to expand a given probability distribution $f(x)$ into a series:
      \begin{displaymath}
        f(x) = \sum_{i=0}^n {f_i \, \psi_i(x)}
      \end{displaymath}
      The coefficients $f_i$ are equal to:
      \begin{displaymath}
        f_i = \int_{-\infty}^{\infty} {f(x) \, \psi_i(x) \, \mathrm{d}x}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
    \includegraphics[width=0.5\paperwidth]{figure/hermite_poly.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Integrate Hermite functions
integrate(her_mite, lower=(-Inf), upper=Inf, n=2)
integrate(function(x, n, m) her_mite(x, n)*her_mite(x, m), 
          lower=(-Inf), upper=Inf, n=2, m=3)
integrate(function(x, n, m) her_mite(x, n)*her_mite(x, m), 
          lower=(-Inf), upper=Inf, n=2, m=2)
      @
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
col_ors <- c("red", "blue", "green")
for (in_dex in 1:3) {  # Plot three curves
  curve(expr=her_mite(x, in_dex),
        xlim=c(-4, 4), ylim=c(-0.6, 0.6),
        xlab="", ylab="", lwd=3, col=col_ors,
        add=as.logical(in_dex-1))
}  # end for
# Add title and legend
title(main="Hermite Functions", line=0.5)
lab_els <- paste("Order", 1:3, sep=" = ")
legend("topright", inset=0.05, bty="n",
       title=NULL, lab_els, cex=0.8, lwd=6, lty=1, 
       col=col_ors)
      @
    
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Bell polynomials}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Bell polynomials} $B_n$ are the coefficients in the expansion of the exponent of a series:
      \begin{displaymath}
        \exp( \sum_{n=1}^{\infty} { \kappa_n \frac{t^n}{n!} } ) = \sum_{n=0}^{\infty} { B_n(\kappa_1, \ldots, \kappa_n) \frac{t^n}{n!} }
      \end{displaymath}
      The first four \emph{Bell polynomials} are equal to:
      \begin{align*}
        B_0 &= 1 \\
        B_1(\kappa_1) &= \kappa_1 \\
        B_2(\kappa_1, \kappa_2) &= \kappa_1^2 + \kappa_2 \\
        B_3(\kappa_1, \kappa_2, \kappa_3) &= \kappa_1^3 + 3 \kappa_1 \kappa_2 + \kappa_3 \\
      \end{align*}

    \column{0.5\textwidth}

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Gram-Charlier Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Gram-Charlier series} expresses the \emph{density function} $f(x)$ in terms of its \emph{cumulants} and a \emph{basis function} $\phi(x)$, with \emph{characteristic} $\hat{\phi}(t)$.
      \vskip1ex
      The \emph{characteristic functions} $\hat{f}(t)$ and $\hat{\phi}(t)$ can be expressed in terms of their \emph{cumulants} as:
      \begin{align*}
        \hat{f}(t) &= e^{K_X(i t)} = \exp(\sum_{n=1}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}) \\
        \hat{\phi}(t) &= \exp(\sum_{n=1}^{\infty} {\phi_n \frac{(i t)^n}{n!}})
      \end{align*}
      Then $\hat{f}(t)$ can be expressed in terms of $\hat{\phi}(t)$ as:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=1}^{\infty} {(\kappa_n - \phi_n) \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      The \emph{basis function} $\phi(x)$ can be chosen to be a \emph{normal distribution}, with \emph{mean} and \emph{standard deviation} equal to that of $f(x)$, and with all its \emph{normal cumulants} $\phi_n$ of order \texttt{3} and higher equal to zero.
      
    \column{0.5\textwidth}
      Then we get a series starting at \texttt{n=3}:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=3}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      We can expand the exponent and collect terms with the same power of $t$ using the \emph{Bell polynomials} $B_n$:
      \begin{displaymath}
        \hat{f}(t) = \sum_{n=0}^{\infty} { B_n(0, 0, \kappa_3, \ldots, \kappa_n) \, \frac{(i t)^n}{n!} \, \hat{\phi}(t) }
      \end{displaymath}
      The \emph{inverse Fourier transform} of the above equation gives the \emph{probability function} $f(x)$:
      \begin{displaymath}
        f(x) = \sum_{n=0}^{\infty} { B_n(0, 0, \kappa_3, \ldots, \kappa_n) \frac{(-1)^n}{n!} \frac{d^n \phi(x)}{d x^n} }
      \end{displaymath}
      The derivatives of the \emph{normal} distribution $\phi(x)$ can be expressed using the \emph{Hermite polynomials} $H_n$ so that the \emph{Gram-Charlier series} becomes:
      \begin{displaymath}
        f(x) = \phi(x) \sum_{n=0}^{\infty} { \frac{B_n(0, 0, \kappa_3, \ldots, \kappa_n)}{n! \sigma^n} H_n(\frac{x-\mu}{\sigma}) }
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Edgeworth Expansion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
% wippp
      The \emph{Edgeworth expansion} expresses the \emph{probability density function} $f(x)$ as a series of its \emph{cumulants} and a \emph{basis function} $\phi(x)$ (with \emph{characteristic function} $\hat{\phi}(t)$).
      \vskip1ex
      The \emph{characteristic functions} $\hat{f}(t)$ and $\hat{\phi}(t)$ can be expressed in terms of their corresponding \emph{cumulants} as:
      \begin{align*}
        \hat{f}(t) &= e^{K_X(i t)} = \exp(\sum_{n=1}^{\infty} {\kappa_n \frac{(i t)^n}{n!}}) \\
        \hat{\phi}(t) &= \exp(\sum_{n=1}^{\infty} {\phi_n \frac{(i t)^n}{n!}})
      \end{align*}
      Then $\hat{f}(t)$ can be expressed in terms of $\hat{\phi}(t)$ as:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=1}^{\infty} {(\kappa_n - \phi_n) \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      
    \column{0.5\textwidth}
      If the \emph{basis function} $\phi(x)$ is chosen to be the \emph{normal distribution}, with \emph{mean} and \emph{standard deviation} equal to that of $f(x)$, and since the \emph{normal cumulants} of order \texttt{3} and higher are all equal to zero, then we get:
      \begin{displaymath}
        \hat{f}(t) = \exp[\sum_{n=3}^{\infty} {\kappa_n (-1)^n \frac{(i t)^n}{n!}}] \, \hat{\phi}(t)
      \end{displaymath}
      The \emph{inverse Fourier transform} of the above equation gives the \emph{probability function} $f(x)$:
      \begin{displaymath}
        f(x) = \exp[\sum_{n=3}^{\infty} {\kappa_n (-1)^n \frac{d^n}{d x^n}}] \, \phi(x)
      \end{displaymath}
      Now expand the exponent in a series and collect terms with the same order of the derivative to obtain:
      \begin{displaymath}
        f(x) = \sum_{n=0}^{\infty} { {B_n(0, 0, \kappa_3, \ldots, \kappa_n) (-1)^n \frac{d^n \phi(x)}{d x^n}} }
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


% wippp
%%%%%%%%%%%%%%%
\subsection{draft: The Cornish-Fisher Expansion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Cornish-Fisher expansion} expresses the quantiles of a distribution as a series of its \emph{moments}.
      \vskip1ex
      The Edgeworth Expansion
      The \emph{cumulant generating function} $K_X(t)$ is equal to the logarithm of the \emph{moment generating function}: 
      \begin{displaymath}
        K_X(t) = \log{M_X(t)}
      \end{displaymath}
      The \emph{n}-th derivative of $K_X(t)$ with respect to \texttt{t}, at $t = 0$ is equal to the \emph{n}-th \emph{cumulant} $\kappa_n$:
      \begin{displaymath}
        \kappa_n = \frac{d^n K_X(t)}{d t^n} |_{t = 0}
      \end{displaymath}
      The \emph{cumulants} are related to the \emph{moments} of the distribution:
      the first three cumulants are equal to the \emph{central moments} (mean, variance, and skewness), while the higher order \emph{cumulants} can be expressed as polynomials of the \emph{central moments}.
      \vskip1ex
      The \emph{cumulant generating function} $K_X(t)$ can be expanded into a power series of the \emph{cumulants}: 
      \begin{displaymath}
        K_X(t) = \sum_{n=1}^n {\frac{\kappa_n t^n}{n!}} = \mu t + \sigma^2 t^2 / 2 + 
      \end{displaymath}

      The \emph{n}-th \emph{moment} $\mu_n$ is not equal to the \emph{central moment} $\mathbb{E}[(x - \mu)^n]$.
      
      The \emph{moment generating function} $M_X(t)$ of a random variable $x$ with the probability function $f(x)$ is equal to:
      \begin{displaymath}
        M_X(t) = \mathbb{E}[e^{t x}] = \int_{-\infty}^{\infty} {f(x) \, e^{t x} \, \mathrm{d}x}
      \end{displaymath}
      
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Hypothesis Testing}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Hypothesis Tests} are designed to test the validity of \emph{null hypotheses}, and they consist of:
      \begin{itemize}
        \item A \emph{null hypothesis},
        \item A test \emph{statistic} derived from the data sample,
        \item A \emph{p}-value: the conditional probability of observing the test statistic value, assuming the \emph{null hypothesis} is \texttt{TRUE},
        \item A \emph{significance level} $\alpha$ corresponding to a \emph{critical value}.
      \end{itemize}
      The \emph{p}-value is compared to the \emph{significance level} and if the \emph{p}-value is less than the \emph{significance level} $\alpha$, then the \emph{null hypothesis} is rejected.
      \vskip1ex
      It's possible for the \emph{null hypothesis} to be \texttt{TRUE}, but to obtain a very small \emph{p}-value purely by chance.
      \vskip1ex
      The \emph{p}-value is the probability of erroneously rejecting a \texttt{TRUE} \emph{null hypothesis}, due to the randomness of the data sample.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
### Perform two-tailed test that sample is
### from Standard Normal Distribution (mean=0, SD=1)
# generate vector of samples and store in data frame
test_frame <- data.frame(samples=rnorm(1e4))
# get p-values for all the samples
test_frame$p_values <- sapply(test_frame$samples,
              function(x) 2*pnorm(-abs(x)))
# Significance level, two-tailed test, critical value=2*SD
signif_level <- 2*(1-pnorm(2))
# Compare p_values to significance level
test_frame$result <-
  test_frame$p_values > signif_level
# Number of null rejections
sum(!test_frame$result) / NROW(test_frame)
# Show null rejections
head(test_frame[!test_frame$result, ])
      @
      The \emph{p}-value is a conditional probability, and is not equal to the un-conditional probability of the hypothesis being \texttt{TRUE}.
      \vskip1ex
      In statistics we cannot \emph{prove} that a hypothesis is \texttt{TRUE} or not, but we can attempt to invalidate it, and conclude that it's unlikely to be \texttt{TRUE}, given the test statistic value and its \emph{p}-value.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Two-tailed Hypothesis Tests}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      \vskip1ex
      Two-tailed hypothesis tests are applied for testing if the absolute value of a sample  exceeds the critical value.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot the Normal probability distribution
curve(expr=dnorm(x, sd=1), type="l", xlim=c(-4, 4),
      xlab="", ylab="", lwd=3, col="blue")
title(main="Two-tailed Test", line=0.5)
# Plot tails of the distribution using polygons
star_t <- 2; e_nd <- 4
# Plot right tail using polygon
x_var <- seq(star_t, e_nd, length=100)
y_var <- dnorm(x_var, sd=1)
y_var[1] <- (-1)
y_var[NROW(y_var)] <- (-1)
polygon(x=x_var, y=y_var, col="red")
# Plot left tail using polygon
y_var <- dnorm(-x_var, sd=1)
y_var[1] <- (-1)
y_var[NROW(y_var)] <- (-1)
polygon(x=(-x_var), y=y_var, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_tow_tail.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using Package \protect\emph{ggplot2}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      <<hyp_test_ggp2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
library(ggplot2)  # Load ggplot2

qplot(  # Simple ggplot2
    main="Standard Normal Distribution",
    c(-4, 4),
    stat="function",
    fun=dnorm,
    geom="line",
    xlab=NULL, ylab=NULL
    ) +  # end qplot

theme(  # Modify plot theme
    plot.title=element_text(vjust=-1.0),
    plot.background=element_blank()
    ) +  # end theme

geom_vline(  # Add vertical line
  aes(xintercept=c(-2.0, 2.0)),
  colour="red",
  linetype="dashed"
  )  # end geom_vline
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_ggp2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Visualizing Hypothesis Testing Using \protect\emph{ggplot2} (cont.)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In two-tailed hypothesis tests, both tails of the probability distribution contribute to the \emph{p}-value.
      <<hyp_test_ggp2_2,echo=(-(1:2)),eval=FALSE,fig.show='hide'>>=
rm(list=ls())
par(oma=c(1, 1, 1, 1), mgp=c(2, 0.5, 0), mar=c(5, 1, 1, 1), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
### Create ggplot2 with shaded area
x_var <- -400:400/100
norm_frame <- data.frame(x_var=x_var,
                       d.norm=dnorm(x_var))
norm_frame$shade <- ifelse(
                  abs(norm_frame$x_var) >= 2,
                  norm_frame$d.norm, NA)
ggplot(  # Main function
  data=norm_frame,
  mapping=aes(x=x_var, y=d.norm)
  ) +  # end ggplot
# Plot line
  geom_line() +
# Plot shaded area
  geom_ribbon(aes(ymin=0, ymax=shade), fill="red") +
# No axis labels
  xlab("") + ylab("") +
# Add title
  ggtitle("Standard Normal Distribution") +
# Modify plot theme
  theme(
        plot.title=element_text(vjust=-1.0),
        plot.background=element_blank()
  )  # end theme
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/hyp_test_ggp2_2-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Student's t-test} for the Distribution Mean}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Student's t-test} is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ was obtained from a normal distribution with a \emph{mean} equal to $\mu$.
      \vskip1ex
      The test statistic is equal to the \emph{t-ratio}:
      \begin{displaymath}
        t = \frac{\bar{x} - \mu}{\hat\sigma / \sqrt{n}}
      \end{displaymath}
      Where $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean and $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample variance.
      \vskip1ex
      Under the \emph{null hypothesis} the \emph{t-ratio} follows the \emph{t-distribution} with $n$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(x) = \frac{\Gamma((n+1)/2)}{\sqrt{\pi n} \, \Gamma(n/2)} \, (1 + x^2/n)^{-(n+1)/2}
      \end{displaymath}
      \emph{Student's t-test} can also be used to test if two different normally distributed samples have equal \emph{population means}.
      \vskip1ex
      \emph{Student's t-test} is not valid for random variables that do not follow the normal distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_norm.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# t-test for single sample
t.test(rnorm(100))
# t-test for two samples
t.test(rnorm(100),
       rnorm(100, mean=1))
# Plot the normal and t-distribution densities
x11(width=6, height=5)
par(mar=c(3, 3, 3, 1), oma=c(0, 0, 0, 0))
curve(expr=dnorm, xlim=c(-4, 4),
      xlab="", ylab="", lwd=3)
curve(expr=dt(x, df=3),
      xlab="", ylab="", lwd=3,
      col="red", add=TRUE)
# Add title
title(main="Normal and t-distribution densities", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title=NULL, c("normal", "t-dist"),
       cex=0.8, lwd=6, lty=1,
       col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Analysis of Variance (\protect\emph{ANOVA})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Analysis of Variance (\emph{ANOVA}) to test if the sub-samples of the data have the same mean.
      \vskip1ex
      ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means.
      \emph{Student's t-test}
      is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ was obtained from a normal distribution with a \emph{mean} equal to $\mu$.
      \vskip1ex
      For example, \emph{ANOVA} is widely used to study the effect of medical treatments, with the \emph{null hypothesis} being that the treatments have no effect and that differences are due to random chance.
      \vskip1ex
      The test statistic is equal to the \emph{t-ratio}:
      \begin{displaymath}
        t = \frac{\bar{x} - \mu}{\hat\sigma / \sqrt{n}}
      \end{displaymath}
      Where $\bar{x}=\frac{1}{n} \sum_{i=1}^n x_i$ is the sample mean and $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample variance.
      \vskip1ex
      Under the \emph{null hypothesis} the \emph{t-ratio} follows the \emph{t-distribution} with $n$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(x) = \frac{\Gamma((n+1)/2)}{\sqrt{\pi n} \, \Gamma(n/2)} \, (1 + x^2/n)^{-(n+1)/2}
      \end{displaymath}
      \emph{Student's t-test} can also be used to test if two different normally distributed samples have equal \emph{population means}.
      \vskip1ex
      \emph{Student's t-test} is not valid for random variables that do not follow the normal distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/t_dist_norm.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# t-test for single sample
t.test(rnorm(100))
# t-test for two samples
t.test(rnorm(100),
       rnorm(100, mean=1))
# Plot the normal and t-distribution densities
x11(width=6, height=5)
par(mar=c(3, 3, 3, 1), oma=c(0, 0, 0, 0))
curve(expr=dnorm, xlim=c(-4, 4),
      xlab="", ylab="", lwd=3)
curve(expr=dt(x, df=3),
      xlab="", ylab="", lwd=3,
      col="red", add=TRUE)
# Add title
title(main="Normal and t-distribution densities", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title=NULL, c("normal", "t-dist"),
       cex=0.8, lwd=6, lty=1,
       col=c("black", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Shapiro-Wilk} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        W = \frac {(\sum_{i=1}^n a_i x_{(i)})^2} {\sum_{i=1}^n (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1, \ldots, a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution.
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1, \ldots, x_n\}$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to $1$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to $1$ for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Calculate VTI percentage returns
library(rutils)
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Reset output digits
dig_its <- options(digits=5)
# Shapiro-Wilk test for normal distribution
shapiro.test(rnorm(NROW(re_turns)))
# Shapiro-Wilk test for VTI returns
shapiro.test(as.numeric(re_turns))
# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(NROW(re_turns)))
# Restore output digits
options(digits=dig_its$digits)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Jarque-Bera} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        JB = \frac{n}{6} (\varsigma^2 + \frac{1}{4} (\kappa - 3)^2)
      \end{displaymath}
      Where the \emph{skewness} and \emph{kurtosis} are defined as:
      \begin{align*}
        \varsigma = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \kappa = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom.
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(tseries)  # Load package tseries
# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(NROW(re_turns)))
# Jarque-Bera test for VTI returns
jarque.bera.test(re_turns)
# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(NROW(re_turns)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} performs the \emph{Kolmogorov-Smirnov} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The second argument is either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a probability distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# KS test for normal distribution
ks.test(rnorm(100), pnorm)
# KS test for uniform distribution
ks.test(runif(100), pnorm)
# KS test for two similar normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
# KS test for two different normal distributions
ks.test(rnorm(100), rnorm(100, mean=1.0))
# Fit t-dist into VTI returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
optim_fit <- MASS::fitdistr(re_turns, densfun="t", df=2)
lo_cation <- optim_fit$estimate[1]
scal_e <- optim_fit$estimate[2]
# Perform Kolmogorov-Smirnov test on VTI returns
da_ta <- lo_cation + scal_e*rt(NROW(re_turns), df=2)
ks.test(as.numeric(re_turns), da_ta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Chi-squared} Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_1, \ldots , z_k$ be independent standard \emph{Normal} random variables.
      \vskip1ex
      Then the random variable $X = \sum_{i=1}^k z_i^2$ is distributed according to the \emph{Chi-squared} distribution with $k$ degrees of freedom: $X \sim \chi_k^2$, and its probability density function is given by:
      \begin{displaymath}
        f(x) = \frac{x^{k/2-1}\,e^{-x/2}}{2^{k/2}\, \Gamma(k/2)}
      \end{displaymath}
      \vskip1ex
      The \emph{Chi-squared} distribution with $k$ degrees of freedom has mean equal to $k$ and variance equal to $2k$.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Degrees of freedom
deg_free <- c(2, 5, 8, 11)
# Plot four curves in loop
col_ors <- c("red", "black", "blue", "green")
for (in_dex in 1:4) {
  curve(expr=dchisq(x, df=deg_free[in_dex]),
        xlim=c(0, 20), ylim=c(0, 0.3),
        xlab="", ylab="", col=col_ors[in_dex],
        lwd=2, add=as.logical(in_dex-1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/chisq_dist_mult.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="Chi-squared Distributions", line=0.5)
# Add legend
lab_els <- paste("df", deg_free, sep="=")
legend("topright", inset=0.05, bty="n",
       title="Degrees of freedom", lab_els,
       cex=0.8, lwd=6, lty=1, col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Chi-squared} Test for the Goodness of Fit}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Goodness of Fit} tests are designed to test if a set of observations fits an assumed theoretical probability distribution.
      \vskip1ex
      The \emph{Chi-squared} test tests if a frequency of counts fits the specified distribution.
      \vskip1ex
      The \emph{Chi-squared} statistic is the sum of squared differences between the observed frequencies $o_i$ and the theoretical frequencies $p_i$:
      \begin{displaymath}
        \chi^2 = N \sum_{i=1}^{n} {\frac{(o_i - p_i )^2}{p_i}}
      \end{displaymath}
      Where $N$ is the total number of observations.
      \vskip1ex
      The \emph{null hypothesis} is that the observed frequencies are consistent with the theoretical distribution.
      \vskip1ex
      The function \texttt{chisq.test()} performs the \emph{Chi-squared} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} in the function \texttt{hist()} should be chosen large enough to capture the shape of the frequency distribution.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Observed frequencies from random normal data
histo_gram <- hist(rnorm(1e3, mean=0), breaks=100, plot=FALSE)
freq_o <- histo_gram$counts
# Theoretical frequencies
freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
# Perform Chi-squared test for normal data
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
# Return p-value
chisq_test <- chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
chisq_test$p.value
# Observed frequencies from shifted normal data
histo_gram <- hist(rnorm(1e3, mean=2), breaks=100, plot=FALSE)
freq_o <- histo_gram$counts/sum(histo_gram$counts)
# Theoretical frequencies
freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
# Perform Chi-squared test for shifted normal data
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
# Calculate histogram of VTI returns
histo_gram <- hist(re_turns, breaks=100, plot=FALSE)
freq_o <- histo_gram$counts
# Calculate cumulative probabilities and then difference them
freq_t <- pt((histo_gram$breaks-lo_cation)/scal_e, df=2)
freq_t <- rutils::diff_it(freq_t)
# Perform Chi-squared test for VTI returns
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Nonparametric Estimators}


%%%%%%%%%%%%%%%
\subsection{Sorting and Ranking Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sort()} returns a vector sorted into ascending order, from smallest to largest.
      \vskip1ex
      A permutation is a re-ordering of the elements of a vector.
      \vskip1ex
      The permutation index specifies how the elements are re-ordered in a permutation.
      \vskip1ex
      The function \texttt{order()} calculates the permutation index to sort a given vector into ascending order.
      \vskip1ex
      Applying the function \texttt{order()} twice: \texttt{order(order())}, calculates the permutation index to sort the vector from ascending order into its original unsorted order.
      \vskip1ex
      The permutation index produced by: \texttt{order(order())} is the reverse of the permutation index produced by: \texttt{order()}.
      \vskip1ex
      The function \texttt{rank()} calculates the ranks of the elements, according to their magnitude, from smallest to largest.
      \vskip1ex
      The ranks of the elements are equal to the reverse permutation index.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Sort a vector into ascending order
da_ta <- round(runif(7), 3)
sort_ed <- sort(da_ta)
da_ta  # original data
sort_ed  # sorted data
# Calculate index to sort into ascending order
in_dex <- order(da_ta)
in_dex  # permutation index to sort
all.equal(sort_ed, da_ta[in_dex])
# Sort the ordered vector back to its original unsorted order
in_dex <- order(order(da_ta))
in_dex  # permutation index to unsort
all.equal(da_ta, sort_ed[in_dex])
# Calculate ranks of the vector elements
rank(da_ta)
all.equal(rank(da_ta), in_dex)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Mean and Median Estimators of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{mean} and the \emph{median} are both estimators of the \emph{location} (centrality) of a distribution.
      \vskip1ex
      For \emph{normally} distributed data, the \emph{mean} has a smaller standard error than the \emph{median} (it is more \emph{efficient}).
      \vskip1ex
      But for distributions with very large \emph{kurtosis} (fat tails), the \emph{median} may be more \emph{efficient} than the \emph{mean}, because it's less sensitive to data outliers.
      \vskip1ex
      In addition, the \emph{median} is often defined even for distributions for which the \emph{mean} and \emph{variance} are infinite.
      \vskip1ex
      For \emph{symmetric} distributions, the expected values of the sample \emph{mean} and the \emph{median} are equal to each other, and equal to the population mean.
      \vskip1ex
      But for \emph{skewed} distributions (for example asset returns), the \emph{median} estimator is \emph{biased} (its expected value is not equal to the population mean).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI returns
re_turns <- as.numeric(na.omit(rutils::etf_env$re_turns[, "VTI"]))
n_rows <- NROW(re_turns)
re_turns <- 100*(re_turns-mean(re_turns))/sd(re_turns)
# Simulate normal random data
n_data <- rnorm(n_rows, sd=100)
# Bootstrap the mean and median estimators
boot_data <- sapply(1:1e3, function(x) {
  # Simulate data
  n_data <- rnorm(n_rows, sd=100)
  re_turns <- re_turns[sample.int(n_rows, replace=TRUE)]
  c(n_mean=mean(n_data), 
    n_median=median(n_data), 
    vti_mean=mean(re_turns), 
    vti_median=median(re_turns))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped data
head(boot_data)
sum(is.na(boot_data))
# Means and medians from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:1e4,
  function(x, n_rows, re_turns) {
    # Simulate data
    n_data <- rnorm(n_rows, sd=100)
    re_turns <- re_turns[sample.int(n_rows, replace=TRUE)]
    c(n_mean=mean(n_data), 
      n_median=median(n_data), 
      vti_mean=mean(re_turns), 
      vti_median=median(re_turns))
  }, n_rows, re_turns)  # end parLapply
stopCluster(clus_ter)  # Stop R processes over cluster
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:1e4, function(x) {
  # Simulate data
  n_data <- rnorm(n_rows)
  t_data <- rt(n_rows, df=2)
  c(n_mean=mean(n_data), 
    n_median=median(n_data), 
    vti_mean=mean(t_data), 
    vti_median=median(t_data))
}, mc.cores=n_cores)  # end mclapply
# Means and medians from bootstrap
boot_data <- rutils::do_call(rbind, boot_data)
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Bias-Variance Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The tradeoff between \emph{unbiased} estimators but with \emph{higher variance}, and efficient estimators with \emph{lower variance} but with some \emph{bias} is called the \emph{bias-variance tradeoff}.
      \vskip1ex
      Let $\hat\theta$ be an estimator of the parameter $\theta$, with expected value $\mathbb{E}[\hat\theta] = \bar\theta$ (which may not necessarily be equal to $\theta$).
      \vskip1ex
      The \emph{accuracy} of the estimator $\hat\theta$ can be measured by its \emph{mean squared error} (MSE), equal to the expected value of the squared difference $(\hat\theta - \theta)^2$:
      \begin{align*}
        \operatorname{MSE} = \mathbb{E}[(\hat\theta - \theta)^2] = \mathbb{E}[(\hat\theta - \bar\theta + \bar\theta - \theta)^2] = \\
        \mathbb{E}[(\hat\theta - \bar\theta)^2 + 2 (\hat\theta - \bar\theta) (\bar\theta - \theta) + (\bar\theta - \theta)^2] = \\
        \mathbb{E}[(\hat\theta - \bar\theta)^2] + (\bar\theta - \theta)^2 = \operatorname{var}(\bar\theta) + \operatorname{bias}(\bar\theta)^2
      \end{align*}
      Since $\mathbb{E}[(\hat\theta - \bar\theta) (\bar\theta - \theta)] = (\bar\theta - \theta) \mathbb{E}[(\hat\theta - \bar\theta)] = 0$
      \vskip1ex
      The above formula shows that the $MSE$ is equal to the sum of the estimator \emph{variance} plus the square of the estimator \emph{bias}.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/bias_variance_tradeoff.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Bias and variance from bootstrap
bias_var <- apply(boot_data, MARGIN=2, 
  function(x) c(bias=mean(x), variance=var(x)))
# MSE of mean
bias_var[1, 3]^2 + bias_var[2, 3]
# MSE of median
bias_var[1, 4]^2 + bias_var[2, 4]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{Hodges-Lehmann} Estimator of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For distributions which are both \emph{leptokurtic} (fat tailed) and \emph{skewed}, the \emph{median} estimator is more \emph{efficient} but it's \emph{biased}, while the \emph{mean} is \emph{unbiased} but it's less \emph{efficient}.
      \vskip1ex
      The \emph{Hodges-Lehmann} estimator of location is a compromise between the \emph{mean} and the \emph{median} estimators.
      \vskip1ex
      The \emph{Hodges-Lehmann} estimator is the median of the means of all the possible one and two-element subsets.
      \vskip1ex
      For a dataset with $n$ measurements, the set of all possible one or two-element subsets of it has $n(n + 1)/2$ elements. For each such subset, the mean is computed. 
      Finally, the median of these $n(n + 1)/2$ averages is defined to be the \emph{Hodges-Lehmann} estimator of location.
      \vskip1ex
      A subset is obtained by removing a few elements, and the subset mean is the mean of this subset.
      \vskip1ex
      Nonparametric Estimators
      distribution free methods, which do not rely on assumptions that the data are drawn from a given parametric family of probability distributions. As such it is the opposite of parametric statistics.
      accuracy
      \vskip1ex
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      Explain breakdown point of estimators
      \href{https://en.wikipedia.org/wiki/Robust_statistics}{breakdown point of estimators}

      \vskip1ex

      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
re_turns <- as.numeric(na.omit(rutils::etf_env$re_turns[, "VTI"]))
n_rows <- NROW(re_turns)
re_turns <- 100*(re_turns-mean(re_turns))/sd(re_turns)
# Simulate normal random data
n_data <- rnorm(n_rows, sd=100)

# Hodges-Lehmann estimator


# Bootstrap the mean and median estimators
boot_data <- sapply(1:1e3, function(x) {
  # Simulate data
  n_data <- rnorm(n_rows, sd=100)
  re_turns <- re_turns[sample.int(n_rows, replace=TRUE)]
  c(n_mean=mean(n_data), 
    n_median=median(n_data), 
    vti_mean=mean(re_turns), 
    vti_median=median(re_turns))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped data
head(boot_data)
sum(is.na(boot_data))
# Means and medians from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Nonparametric Estimators of Location}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Robust 
      Order statistics, which are based on the ranks of observations, is one example of such statistics.
      \vskip1ex
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      Explain breakdown point of estimators
      \href{https://en.wikipedia.org/wiki/Robust_statistics}{breakdown point of estimators}

      \vskip1ex

      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:1e4, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:1e4,
  function(x, da_ta) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, da_ta=da_ta)  # end parLapply
stopCluster(clus_ter)  # Stop R processes over cluster
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:1e4, function(x) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
# Means and standard errors from bootstrap
boot_data <- rutils::do_call(rbind, boot_data)
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators and Influence Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The influence function measures the sensitivity of an estimator to changes in the values of individual data points.
      \vskip1ex
      But for distributions with very large kurtosis (fat tails), the \emph{median} may have a smaller standard error than the mean, because it's less sensitive to outliers.
      \vskip1ex
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1e3
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{Wilcoxon Signed Rank} Test for Distribution Similarity}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots, y_n\}$ be two samples of data, which form pairs of observations: $\{x_1, y_1\}, \ldots, \{x_n, y_n\}$.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Wilcoxon Signed Rank} test is that the two data samples, $x_i$ and $y_i$, were obtained from \emph{similar} probability distributions.
      \vskip1ex
      For \emph{symmetric} distributions, the \emph{Wilcoxon} test only requires that the distributions have equal \emph{medians}, but they can have different \emph{standard deviations}.
      \vskip1ex
      But for \emph{skewed} distributions, the \emph{Wilcoxon} test requires that the distributions be the same.
      \vskip1ex
      The function \texttt{wilcox.test()} with parameter \texttt{paired=TRUE} calculates the \emph{Wilcoxon Signed Rank} test statistic and its \emph{p}-value.
      \vskip1ex
      If a single argument is passed into \texttt{wilcox.test()} then it tests if the data has zero \emph{median}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI returns
re_turns <- as.numeric(na.omit(
  rutils::etf_env$re_turns[, "VTI"]))
# Wilcoxon test for normal distribution
da_ta <- rnorm(NROW(re_turns), sd=100)
wilcox.test(da_ta)
# Skewed distribution with mean=0
mean(re_turns); median(re_turns)
wilcox.test(re_turns-mean(re_turns))
# Same as
wilcox.test(re_turns-mean(re_turns), 
  rep(0, NROW(re_turns)), paired=TRUE)
# Skewed distribution with median=0
wilcox.test(re_turns-median(re_turns))
# Normal samples with different standard deviations
n_rows <- 1e3
sample1 <- rnorm(n_rows, sd=1)
sample2 <- rnorm(n_rows, sd=10)
wilcox.test(sample1, sample2, paired=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{W} Statistic of the \protect\emph{Wilcoxon Signed Rank} Test}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{W} statistic of the \emph{Wilcoxon Signed Rank} test is equal to the sum of the ranks of the absolute differences $r_i = \operatorname{rank}(|x_i - y_i|)$, weighted by their signs:
      \begin{displaymath}
        W = \sum_{i=1}^n \operatorname{sgn}(x_i - y_i) r_i
      \end{displaymath}
      The function \texttt{wilcox.test()} returns the \emph{V} statistic, not the the \emph{W} statistic:       \begin{displaymath}
        V = \sum_{i=1}^n \operatorname{H}(x_i - y_i) r_i
      \end{displaymath}
      Where $\operatorname{H}(x) = 1$ if $x > 0$, and $0$ otherwise.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wilcoxon test for random data around 0
da_ta <- (runif(n_rows) - 0.5)
wil_cox <- wilcox.test(da_ta)
# Calculate V statistic of Wilcoxon test
wil_cox$statistic
sum(rank(abs(da_ta))[da_ta>0])
# Calculate W statistic of Wilcoxon test
sum(sign(da_ta)*rank(abs(da_ta)))
# Two sets of normal data
sample1 <- rnorm(n_rows)
sample2 <- rnorm(n_rows, mean=0.1)
# Wilcoxon test
wil_cox <- wilcox.test(sample1, sample2, 
                       paired=TRUE)
wil_cox$statistic
# Calculate V statistic of Wilcoxon test
da_ta <- (sample1 - sample2)
sum(rank(abs(da_ta))[da_ta>0])
# Calculate W statistic of Wilcoxon test
sum(sign(da_ta)*rank(abs(da_ta)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Distribution of the \protect\emph{W} Statistic}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{W} statistic follows a distribution without a simple formula, which converges to the normal distribution for large sample size $n$, with an expected value equal to $0$ and a variance equal to $\frac{n(n+1)(2n+1)}{6}$.
        <<echo=TRUE,eval=FALSE>>=
# Calculate distributon of Wilcoxon W statistic
wilcox_w <- sapply(1:1e3, function(x) {
  da_ta <- (runif(n_rows) - 0.5)
  sum(sign(da_ta)*rank(abs(da_ta)))
})  # end sapply
wilcox_w <- wilcox_w/sqrt(n_rows*(n_rows+1)*(2*n_rows+1)/6)
var(wilcox_w)
      @
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)  # Plot in window
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
hist(wilcox_w, col="lightgrey",
     xlab="returns", breaks=50, xlim=c(-3, 3),
     ylab="frequency", freq=FALSE,
     main="Wilcoxon W Statistic Histogram")
lines(density(wilcox_w, bw=0.4), lwd=3, col="red")
curve(expr=dnorm, add=TRUE, lwd=3, col="blue")
# Add legend
legend("topright", inset=0.05, bty="n",
       leg=c("W density", "Normal"),
       lwd=6, lty=1, col=c("red", "blue"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/wilcoxon_hist.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{Wilcoxon Signed Rank} Test Versus \protect\emph{Student's t-test}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Wilcoxon} test can be considered to be a \emph{nonparametric} analogue of the \emph{Student's t-test}, but it tests for the \emph{medians}, rather than the \emph{means}.
      \vskip1ex
      The \emph{Wilcoxon} test is \emph{nonparametric} because it doesn't assume any type of sample distribution, unlike the \emph{Student's t-test} which assumes that the sample is taken from the \emph{normal} distribution.
      \vskip1ex
      The \emph{Wilcoxon} test is more \emph{robust} with respect to data outliers because it only depends on the ranks of the sample differences $(x_i - y_i)$, not the differences themselves.
      \vskip1ex
      Therefore the \emph{Wilcoxon} test reports fewer \emph{false positive} cases when there are outliers.
      \vskip1ex
      For many distributions, the \emph{Wilcoxon} test has greater \emph{sensitivity} than the \emph{Student's t-test}.
      \vskip1ex
      The \emph{sensitivity} of a statistical test is the ability to correctly identify \emph{true positive} cases (when the null hypothesis is \texttt{FALSE}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wilcoxon test for two normal distributions
sample1 <- rnorm(1e2)
sample2 <- rnorm(1e2, mean=0.1)
wilcox.test(sample1, sample2, 
            paired=TRUE)$p.value
t.test(sample1, sample2)$p.value
# Wilcoxon test with data outliers
sample2 <- rnorm(1e2)
sample2[1:3] <- sample2[1:3] + 1e3
wilcox.test(sample1, sample2, 
            paired=TRUE)$p.value
t.test(sample1, sample2)$p.value
      @
      \vspace{-1em}
      The \emph{sensitivity} of a statistical test is equal to the \emph{true positive} rate, i.e the fraction of \texttt{FALSE} null hypothesis cases that are correctly classified as \texttt{FALSE}.
      \vskip1ex
      The \emph{specificity} of a statistical test is the ability to correctly identify \emph{true negative} cases (when the null hypothesis is \texttt{TRUE}).
      \vskip1ex
      The \emph{specificity} of a statistical test is equal to the \emph{true negative} rate, i.e the fraction of \texttt{TRUE} null hypothesis cases that are correctly classified as \texttt{TRUE}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{Mann-Whitney} Test for Distribution Similarity}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $\{x^1_1, \ldots, x^1_{n_1}\}$ and $\{x^2_1, \ldots, x^2_{n_2}\}$ be two samples of data, and let $\{x_1, \ldots, x_n\}$ be the combined data sample, with $n = n_1 + n_2$.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Mann-Whitney} test is that the two data samples, $x^1$ and $x^2$, were obtained from \emph{similar} probability distributions.
      \vskip1ex
      In contrast to the \emph{Wilcoxon Signed Rank} test, the two samples don't have to be of equal length ($n_1 \neq n_2$).
      \vskip1ex
      The \emph{Mann-Whitney} test is also known as the \emph{Wilcoxon Rank Sum} test, or the \emph{Mann-Whitney-Wilcoxon} test.
      \vskip1ex
      The function \texttt{wilcox.test()} with parameter \texttt{paired=FALSE} (the default) calculates the \emph{Mann-Whitney} test statistic and its \emph{p}-value.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Data samples
da_ta <- sort(rnorm(38))
in_dex <- c(1:9, 20:29)

# Or
da_ta <- sort(rnorm(398))
in_dex <- c(1:99, 200:299)
sample1 <- da_ta[in_dex]
sample2 <- da_ta[-in_dex]

# Or
in_dex <- sample(1:NROW(re_turns), size=NROW(re_turns)/2)
sample1 <- re_turns[in_dex]
sample2 <- (-re_turns[-in_dex])

sample1 <- (sample1- median(sample1))
sample2 <- (sample2- median(sample2))
moments::moment(sample1, order=3)
moments::moment(sample2, order=3)

# Mann-Whitney test for normal distribution
wilcox.test(sample1, sample2, paired=FALSE)
wilcox.test(sample1, sample2, paired=TRUE)
blue <- rgb(0, 0, 1, alpha=0.5)
red <- rgb(1, 0, 0, alpha=0.5)
barplot(sample2, col=red)
barplot(sample1, col=blue, add=TRUE)
hist(sample1)


# Mann-Whitney test for normal distribution
da_ta <- rnorm(NROW(re_turns), sd=100)
wilcox.test(da_ta, paired=FALSE)
# Skewed distribution with mean=0
mean(re_turns); median(re_turns)
wilcox.test(re_turns-mean(re_turns), 
            paired=FALSE)
# Skewed distribution with median=0
wilcox.test(re_turns-median(re_turns), 
            paired=FALSE)
# Skewed distribution with median=0
wilcox.test(re_turns-median(re_turns), 
            da_ta, paired=FALSE)


sample1 <- sample(re_turns, size=NROW(re_turns))
sample2 <- (-sample1)
sample1 <- (sample1-median(sample1))
sample2 <- (sample2-median(sample2))
# Mann-Whitney-Wilcoxon rank sum test
wilcox.test(sample1, sample2, 
            paired=FALSE)$p.value


da_ta <- (-re_turns)
da_ta <- (da_ta-median(da_ta))
wilcox.test(re_turns-median(re_turns), 
            da_ta, paired=FALSE)
wilcox.test(re_turns-median(re_turns), 
            da_ta, paired=TRUE)

da_ta <- (-re_turns)
da_ta <- (da_ta-mean(da_ta))
wilcox.test(re_turns-mean(re_turns), 
            da_ta, paired=FALSE)
wilcox.test(re_turns-mean(re_turns), 
            da_ta, paired=TRUE)

foo <- sapply(1:100, function(x) {
  # Data samples
  sample1 <- sample(re_turns, size=NROW(re_turns)/2)
  sample2 <- sample(-re_turns, size=NROW(re_turns)/2)
  sample1 <- (sample1-median(sample1))
  sample2 <- (sample2-median(sample2))
  # Mann-Whitney-Wilcoxon rank sum test
  wilcox.test(sample1, sample2, paired=FALSE)$p.value
})
hist(foo)

# Skewed distribution with mean=0
mean(re_turns); median(re_turns)
wilcox.test(re_turns-mean(re_turns))
# Same as
wilcox.test(re_turns-mean(re_turns), 
            rep(0, NROW(re_turns)), 
            paired=TRUE)
# Skewed distribution with median=0
wilcox.test(re_turns-median(re_turns))

      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{U} Statistic of the \protect\emph{Mann-Whitney} Test}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $x_i$ be the combined data of two sub-samples: $x^1_i$ and $x^2_i$, with $n = n_1 + n_2$.
      \vskip1ex
      Let $r_i = \operatorname{rank}(x_i)$ be the ranks of the sample, and $r^1_i$ and $r^2_i$ be the ranks of the sub-samples.
      \vskip1ex
      The sum of all the ranks is equal to:
      \begin{displaymath}
        \sum_{i=1}^n r_i = \sum_{i=1}^{n_1} r^1_i + \sum_{i=1}^{n_2} r^2_i = \frac{n (n+1)}{2}
      \end{displaymath}
      \vskip1ex
      The \emph{Mann-Whitney} test statistics $U_1$ and $U_2$ are equal to the sum of the ranks minus a term accounting for the size of the samples:
      \begin{align*}
        U_1 = \sum_{i=1}^{n_1} r^1_i - \frac{n_1 (n_1+1)}{2} \\
        U_2 = \sum_{i=1}^{n_2} r^2_i - \frac{n_2 (n_2+1)}{2}
      \end{align*}
      So that $U_1 + U_2 = n_1 n_2$.
      \vskip1ex
      The \emph{U} statistic follows a distribution without a simple formula, which converges to the normal distribution for large sample size $n$.
      \vskip1ex
      In the extreme case when the ranks of the first sample are all greater than the second sample ($r^1_i > r^2_j$), then $U_2 = 0$ and $U_1 = n_1 n_2$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Data samples
sample1 <- rnorm(200)
sample2 <- rnorm(100, mean=0.1)
# Mann-Whitney-Wilcoxon rank sum test
wil_cox <- wilcox.test(sample1, sample2, 
                       paired=FALSE)
wil_cox$statistic
# Calculate U statistics of Mann-Whitney-Wilcoxon test
da_ta <- c(sample1, sample2)
rank_s <- rank(da_ta)
sum(rank_s[1:200]) - 100*201
sum(rank_s[201:300]) - 50*101
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The \protect\emph{U} Statistic of the \protect\emph{Wilcoxon Rank Sum} Test}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Wilcoxon Rank Sum} test statistic \emph{U} is equal to the sum of the ranks $r_i = \operatorname{rank}(|x_i - y_i|)$ of the absolute differences weighted by their signs:
      \begin{displaymath}
        U = \sum_{i=1}^n \operatorname{sgn}(x_i - y_i) r_i
      \end{displaymath}
      The statistic \emph{U} follows a distribution without a simple formula, which converges to the normal distribution for large sample size $n$, with an expected value equal to $0$ and a variance equal to $\frac{n(n+1)(2n+1)}{6}$.
      \vskip1ex
      The \emph{Wilcoxon Rank Sum} test is \emph{nonparametric} because it doesn't assume any type of sample distribution, unlike the \emph{Student's t-test} which assumes that the sample is taken from the \emph{normal} distribution.
      \vskip1ex
      The \emph{Wilcoxon Rank Sum} test is more \emph{robust} with respect to data outliers because it only depends on the ranks of the sample differences $(x_i - y_i)$, not the differences themselves.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wilcoxon test for random data around 0
n_rows <- 1e3
da_ta <- (runif(n_rows) - 0.5)
wil_cox <- wilcox.test(da_ta)
# Calculate V statistic of Wilcoxon test
wil_cox$statistic
sum(rank(abs(da_ta))[da_ta>0])
# Calculate W statistic of Wilcoxon test
sum(sign(da_ta)*rank(abs(da_ta)))
# Calculate distributon of Wilcoxon W statistic
wilcox_w <- sapply(1:1e3, function(x) {
  da_ta <- (runif(n_rows) - 0.5)
  sum(sign(da_ta)*rank(abs(da_ta)))
})  # end sapply
wilcox_w <- wilcox_w/sqrt(n_rows*(n_rows+1)*(2*n_rows+1)/6)
var(wilcox_w)
hist(wilcox_w)
      @
      \vspace{-1em}
      The function \texttt{wilcox.test()} returns the \emph{V} statistic, not the the \emph{W} statistic:       \begin{displaymath}
        V = \sum_{i=1}^n \operatorname{H}(x_i - y_i) r_i
      \end{displaymath}
      Where $\operatorname{H}(x) = 1$ if $x > 0$, and $0$ otherwise.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test for the Distribution Similarity}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kruskal-Wallis} test is designed to test the \emph{null hypothesis} that sub-samples of the data corresponding to different categories follow similar distributions.
      \vskip1ex
      The \emph{Kruskal-Wallis} test can be used as a type of \emph{nonparametric ANOVA} test, to test if the sub-samples of the data have the same mean.
      \vskip1ex
      For example, given the heights of several different species of trees, the \emph{Kruskal-Wallis} test can test if all the species have the same height.
      \vskip1ex
      The \emph{Kruskal-Wallis} test can also test if samples have different \emph{skewness}, even if they have the same \emph{means}.
      \vskip1ex
      The function \texttt{kruskal.test()} accepts a vector of sample data and a factor specifying the categories, and calculates the \emph{Kruskal-Wallis} statistic and its \emph{p}-value.
      \vskip1ex
      The function \texttt{kruskal.test()} can also accept the data as a formula combined with a matrix or data frame.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# iris data frame
aggregate(Sepal.Length ~ Species, data=iris,
  FUN=function(x) c(mean=mean(x), sd=sd(x)))
# Kruskal-Wallis test for iris data
k_test <- kruskal.test(Sepal.Length ~ Species, data=iris)
str(k_test)
k_test$statistic
# Kruskal-Wallis test for independent normal distributions
sample1 <- rnorm(1e3)
sample2 <- rnorm(1e3)
fac_tor <- c(rep(TRUE, 1e3), rep(FALSE, 1e3))
kruskal.test(x=c(sample1, sample2), g=fac_tor)
# Kruskal-Wallis test for shifted normal distributions
kruskal.test(x=c(sample1+1, sample2), g=fac_tor)
# Kruskal-Wallis test for beta distributions
sample1 <- rbeta(1e3, 2, 8) + 0.3
sample2 <- rbeta(1e3, 8, 2) - 0.3
mean(sample1); mean(sample2)
kruskal.test(x=c(sample1, sample2), g=fac_tor)
# Plot the beta distributions
x11()
plot(density(sample1), col="blue", lwd=3,
     xlim=range(c(sample1, sample2)), xlab="samples",
     main="Two samples from beta distributions with equal means")
lines(density(sample2), col="red", lwd=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test Statistic}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Given a data sample $x_i$ with $n$ elements, and a factor of $k$ categories, the sample can be divided into $k$ sub-samples $x^j_i$.
      \vskip1ex
      Let $r_i = \operatorname{rank}(x_i)$ be the ranks of the sample, and $r^j_i$ be the ranks of the sub-samples.
      \vskip1ex
      The \emph{Kruskal-Wallis} test statistic \emph{H} is proportional to the sum of squared differences between the average rank of the sample $\bar{r} = \frac{n+1}{2}$, minus the average ranks of the sub-samples $\bar{r}_j$:
      \begin{displaymath}
        H = \frac{12}{n(n+1)} \sum_{j=1}^k (\frac{n+1}{2} - \bar{r}_j)^2 n_j
      \end{displaymath}
      Where the sum is over all the $k$ categories, and $n_j$ is the number of elements in sub-sample $j$.
      \vskip1ex
      The \emph{H} statistic follows a distribution without a simple formula, which is approximately equal to the \emph{chi-squared} distribution with $k-1$ degrees of freedom.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Kruskal-Wallis test for iris data
k_test <- kruskal.test(Sepal.Length ~ Species, data=iris)
# Calculate Kruskal-Wallis test Statistic
n_rows <- NROW(iris)
iris_data <- data.frame(rank_s=rank(iris$Sepal.Length),
                        spe_cies=iris$Species)
kruskal_stat <- (12/n_rows/(n_rows+1))*sum(
  aggregate(rank_s ~ spe_cies,
            data=iris_data,
            FUN=function(x) {
              NROW(x)*((n_rows+1)/2 - mean(x))^2
            })[, 2])
c(k_test=unname(k_test$statistic),
  k_stat=kruskal_stat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kruskal-Wallis} Test with Data Outliers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kruskal-Wallis} test is \emph{nonparametric} because it doesn't assume any type of sample distribution.
      \vskip1ex
      The \emph{Kruskal-Wallis} test is also \emph{robust} with respect to data outliers, since it only depends on the ranks of the sample.
      \vskip1ex
      When a few data outliers are added to the data, \emph{Student's t-test} rejects the \emph{null hypothesis} that the means are equal, but the \emph{Kruskal-Wallis} test still accepts the \emph{null hypothesis} that the distributions are similar.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Kruskal-Wallis test with data outliers
sample1 <- rnorm(1e3)
sample2 <- rnorm(1e3)
sample2[1:11] <- sample2[1:11] + 50
fac_tor <- c(rep(TRUE, 1e3), rep(FALSE, 1e3))
kruskal.test(x=c(sample1, sample2), g=fac_tor)$p.value
t.test(sample1, sample2)$p.value
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:1e4, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2,
      function(x) c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:1e4,
  function(x, da_ta) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, da_ta=da_ta)  # end parLapply
stopCluster(clus_ter)  # Stop R processes over cluster
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:1e4,
  function(x) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
# Means and standard errors from bootstrap
boot_data <- rutils::do_call(rbind, boot_data)
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Robust Estimators of Skewness}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The medcouple robust skewness estimator
      % https://en.wikipedia.org/wiki/Skewness#Medcouple
      % https://en.wikipedia.org/wiki/Medcouple
      \vskip1ex
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the \emph{median} instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data, the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1e3
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:1e4, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # initialize compute cluster
boot_data <- parLapply(clus_ter, 1:1e4, 
  function(x, da_ta) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, da_ta=da_ta)  # end parLapply
stopCluster(clus_ter)  # Stop R processes over cluster
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:1e4, function(x) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
# Means and standard errors from bootstrap
boot_data <- rutils::do_call(rbind, boot_data)
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Pearson} Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{covariance} $\sigma_{xy}$ between two sets of data, $x_i$ and $y_i$, is defined as:
      \begin{displaymath}
        \sigma_{xy} = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{n-1}
      \end{displaymath}
      Where $\bar{x} = \frac{1}{n}{\sum_{i=1}^n x_i}$ and $\bar{y} = \frac{1}{n}{\sum_{i=1}^n y_i}$ are the \emph{mean} values.
      \vskip1ex
      The function \texttt{cov()} calculates the covariance between two numeric vectors.
      \vskip1ex
      The \emph{Pearson} correlation $\rho_P$ is equal to the \emph{covariance} divided by the standard deviations $\sigma_x$ and $\sigma_y$:
      \begin{displaymath}
        \rho_P = \frac{\sigma_{xy}}{\sigma_x \sigma_y}
      \end{displaymath}
      The function \texttt{cor()} calculates the correlation between two numeric vectors.
      \vskip1ex
      Depending on the argument \texttt{"method"}, it calculates either the \emph{Pearson} (default), \emph{Spearman}, or \emph{Kendall} correlations.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/cor_scatter_plot.png}
      \vspace{-2em}
        <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # initialize random number generator
# Define variables and calculate correlation
n_rows <- 100
x_var <- runif(n_rows); y_var <- runif(n_rows)
cor(x_var, y_var)
# Correlate the variables and calculate correlation
rh_o <- 0.5
y_var <- rh_o*x_var + (1-rh_o)*y_var
# Plot in x11 window
x11(width=5, height=4)
# Set plot parameters to reduce whitespace around plot
par(mar=c(4, 4, 2, 1), oma=c(0.5, 0.5, 0, 0))
# Plot scatterplot and exact regression line
plot(x_var, y_var, xlab="x_var", ylab="y_var")
title(main="Correlated Variables", line=0.5)
abline(a=0.25, b=rh_o, lwd=3, col="blue")
# Calculate regression
summary(lm(y_var ~ x_var))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Standard Error of \protect\emph{Pearson} Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      See my comment to Jean Paul comment: \\
      {\tiny{
      https://www.jstor.org/stable/2277400 \\
      https://www.tandfonline.com/doi/abs/10.1080/00220973.1956.11010555 \\
      % https://en.wikipedia.org/wiki/Pearson_correlation_coefficient \\
      % https://en.wikipedia.org/wiki/Fisher_transformation \\
      https://stats.stackexchange.com/questions/73621/standard-error-from-correlation-coefficient/262893 \\
      https://stats.stackexchange.com/questions/226380/derivation-of-the-standard-error-for-pearsons-correlation-coefficient \\
      https://stats.stackexchange.com/questions/154362/confidence-interval-on-point-biserial-correlation-coefficient \\
      }}
      \vskip1ex
      Let $x$ be standard normally distributed with mean zero $E[x] = 0$ and standard deviation equal to one: $E[x^2] = 1$.
      \vskip1ex
      Let $y = \rho x + \varepsilon \sqrt{1 - \rho^2}$, where $\varepsilon$ is standard normally distributed, with zero correlation to $x$: $E[x \varepsilon] = 0$.
      \vskip1ex
      Then $y$ is also standard normally distributed, with correlation to $x$ equal to $\rho$: $E[x y] = E[x (\rho x + \varepsilon \sqrt{1 - \rho^2})] = E[\rho x^2] = \rho$.
      \vskip1ex
      The variance of the correlation is equal to: $E[(x y - \rho)^2] = E[\rho^2 x^4 + 2 x^3 \varepsilon \rho \sqrt{1 - \rho^2} + x^2 \varepsilon^2 (1 - \rho^2) - 2 \rho x y + \rho^2] = 1 - \rho^2 + \rho^2 E[x^4] = 3 \rho^2 + 1 - \rho^2 - 2 \rho^2 + \rho^2$.
      \vskip1ex
      $E[\rho^2 x^4 + 2 x^3 \varepsilon \rho \sqrt{1 - \rho^2} + x^2 \varepsilon^2 (1 - \rho^2)] = 1 - \rho^2 + 3 \rho^2$
      \vskip1ex
      The standard error of
      
      the correlation estimator is equal to: $\frac{1}{\sqrt{n-2}}$, and slowly decreases as the square root of $n$ - the length of the data.
      \vskip1ex
      The function \texttt{cor.test()} performs a test of the statistical significance of the correlation coefficient, under the \emph{null hypothesis} of zero correlation.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Simulation of sample correlation
n_rows <- 1e4
rh_o <- 0.99
rho_2 <- sqrt(1-rh_o^2)
da_ta <- sapply(1:1000, function(x) {
  x_var <- rnorm(n_rows)
  y_var <- (rh_o*x_var + rho_2*rnorm(n_rows))
  cor(x_var, y_var)
})  # end sapply
sd(da_ta)
# Correct formula
(1-rh_o^2)/sqrt(n_rows-2)
# Incorrect formula
sqrt((1-rh_o^2)/(n_rows-2))


# Correlation
co_r <- cor(x_var, y_var)
# Standard error of correlation
std_error <- sqrt((1-co_r^2)/(n_rows-2))
# t-value of correlation
co_r/std_error
# 95% confidence intervals
co_r*c(1-qnorm(0.975)*std_error, 1+qnorm(0.975)*std_error)


# Test statistical significance of correlation
cor.test(x_var, y_var)

rh_o <- 0.9
rho_2 <- sqrt(1-rh_o^2)
set.seed(1121)
# Bootstrap of sample mean and median
boot_data <- sapply(1:1000, function(x) {
  x_var <- rnorm(n_rows)
  y_var <- (rh_o*x_var + rho_2*rnorm(n_rows))
  c(rho=mean(y_var*x_var), y_sd=sd(y_var), cor=cor(x_var, y_var))
})  # end sapply

# Means and standard errors from bootstrap
foo <- apply(boot_data, MARGIN=1, function(x) 
  c(mean=mean(x), std_error=sd(x)))
foo[2, ]
(1-rh_o^2)/sqrt(n_rows-2)
sqrt((1-rh_o^2)/(n_rows-2))

rho_2^2

rh_o^4


# Simulation of sample correlation
rh_o <- 0.99
rho_2 <- sqrt(1-rh_o^2)
da_ta <- sapply(1:10000, function(x) {
  x_var <- rnorm(n_rows)
  y_var <- (rh_o*x_var + rho_2*rnorm(n_rows))
  cor(x_var, y_var)
})  # end sapply
sd(da_ta)
# Correct formula
(1-rh_o^2)/sqrt(n_rows-2)
# Incorrect formula
sqrt((1-rh_o^2)/(n_rows-2))

da_ta <- sapply(1:10000, function(x) {
  rnorm(n_rows)^2 * rnorm(n_rows)^2
})  # end sapply
sd(da_ta)

foo <- (rnorm(n_rows)^2 * rnorm(n_rows)^2)
mean(rnorm(n_rows)^2 * rnorm(n_rows)^2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Spearman} Rank Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Spearman} correlation $\rho_S$ is equal to the \emph{Pearson} correlation between the ranks ${rx}_i$ and ${ry}_i$ of the variables $x_i$ and $y_i$:
      \begin{displaymath}
        \rho_S = \frac{\sum_{i=1}^n ({rx}_i - \bar{rx}) ({ry}_i - \bar{ry})}{(n-1) \sigma_{rx} \sigma_{ry}}
      \end{displaymath}
      If the ranks are all distinct integers, then the \emph{Spearman} correlation $\rho_S$ can be expressed as:
      \begin{displaymath}
        \rho_S = 1 - \frac{6 \sum_{i=1}^n {dr}_i^2}{n(n^2-1)}
      \end{displaymath}
      Where ${dr}_i = {rx}_i - {ry}_i$ are the differences between the ranks.
      \vskip1ex
      The \emph{Spearman} correlation is a \emph{robust} measure of association because it depends on the ranks, so it's not sensitive to the extreme values of the variables $x_i$ and $y_i$.
      \vskip1ex
      The \emph{Spearman} correlation is considered a \emph{nonparametric} estimator because it does not depend on the joint probability distribution of the variables $x_i$ and $y_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlations
cor(x_var, y_var, method="pearson")
cor(x_var, y_var, method="spearman")
# Test statistical significance of correlations
cor.test(x_var, y_var, method="pearson")
cor.test(x_var, y_var, method="spearman")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Kendall's} $\tau$ Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The pair of observations $\left\{ x_i, y_i \right\}$ is concordant with the pair $\left\{ x_j, y_j \right\}$ if the signs of the differences $({rx}_i-{rx}_j)$ and $({ry}_i-{ry}_j)$ are the same, i.e. if their ranks follow the same order.
      \vskip1ex
      The \emph{Kendall} correlation $\tau_K$ (\emph{Kendall's} $\tau$) is equal to the difference between the number of concordant pairs of observations, minus the number of discordant pairs:
      \begin{displaymath}
        \tau_K = \frac{2}{n(n-1)} \sum_{i<j}^n {\operatorname{sgn}({rx}_i-{rx}_j) \operatorname{sgn}({ry}_i-{ry}_j)}
      \end{displaymath}
      The \emph{Kendall} correlation $\tau_K$ is also a \emph{robust} and \emph{nonparametric} estimator of association, because it only depends on the ranks, so it's not sensitive to the extreme values of the variables $x_i$ and $y_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate correlations
cor(x_var, y_var, method="pearson")
cor(x_var, y_var, method="kendall")
# Test statistical significance of correlations
cor.test(x_var, y_var, method="pearson")
cor.test(x_var, y_var, method="kendall")
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
