% FRE7241_Lecture_6
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#6]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#6, Fall 2022}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{October 18, 2022}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Asset Pricing Models}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Alpha} and \protect\emph{Beta} of Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The daily stock returns $r_i - r_f$ in excess of the risk-free rate $r_f$, can be decomposed into \emph{systematic} returns $\beta (r_m - r_f)$ (where $r_m - r_f$ are the excess market returns) plus \emph{idiosyncratic} returns $\alpha + \varepsilon_i$ (which are uncorrelated to the market returns):
      \begin{displaymath}
        r_i - r_f = \alpha + \beta (r_m - r_f) + \varepsilon_i
      \end{displaymath}
      The \emph{alpha} $\alpha$ are the abnormal returns in excess of the risk premium, and $\varepsilon_i$ are the regression residuals with zero mean.
      \vskip1ex
      The \emph{idiosyncratic} risk (equal to $\varepsilon_i$) is uncorrelated to the \emph{systematic} risk, and can be reduced through portfolio diversification.
      <<echo=TRUE,eval=TRUE>>=
# Perform regression using formula
retp <- na.omit(rutils::etfenv$returns[, c("XLP", "VTI")])
riskfree <- 0.03/252
retp <- (retp - riskfree)
regmod <- lm(XLP ~ VTI, data=retp)
regmodsum <- summary(regmod)
# Get regression coefficients
coef(regmodsum)
# Get alpha and beta
coef(regmodsum)[, 1]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.4\paperwidth]{figure/reg_rets.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot scatterplot of returns with aspect ratio 1
plot(XLP ~ VTI, data=rutils::etfenv$returns,
     xlim=c(-0.1, 0.1), ylim=c(-0.1, 0.1), 
     asp=1, main="Regression XLP ~ VTI")
# Add regression line and perpendicular line
abline(regmod, lwd=2, col="red")
abline(a=0, b=-1/coef(regmodsum)[2, 1], lwd=2, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Statistical Significance of \protect\emph{Alpha} and \protect\emph{Beta}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The stock $\beta$ is independent of the risk-free rate $r_f$:
      \begin{displaymath}
        \beta = \frac{\mathrm{Cov}(r_i, r_m)}{\mathrm{Var}(r_m)}
      \end{displaymath}
      The \emph{t}-statistic (\emph{t}-value) is the ratio of the estimated value divided by its standard error.
      \vskip1ex
      The \emph{p}-value is the probability of obtaining values exceeding the \emph{t}-statistic, assuming the \emph{null hypothesis} is true.
      \vskip1ex
      A small \emph{p}-value means that the regression coefficients are very unlikely to be zero (given the data).
      \vskip1ex
      The \emph{beta} $\beta$ values of stock returns are very statistically significant, but the \emph{alpha} $\alpha$ values are mostly not significant.
      \vskip1ex
      The \emph{p}-value of the \emph{Durbin-Watson} test is large, which indicates that the regression residuals are not autocorrelated.
      \vskip1ex
      In practice, the $\alpha$, $\beta$, and the risk-free rate $r_f$, depend on the time interval of the data, so they're time dependent.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Get regression coefficients
coef(regmodsum)
# Calculate regression coefficients from scratch
betav <- drop(cov(retp$XLP, retp$VTI)/var(retp$VTI))
alpha <- drop(mean(retp$XLP) - betav*mean(retp$VTI))
c(alpha, betav)
# Calculate the residuals
residuals <- (retp$XLP - (alpha + betav*retp$VTI))
# Calculate the standard deviation of residuals
nrows <- NROW(residuals)
residsd <- sqrt(sum(residuals^2)/(nrows - 2))
# Calculate the standard errors of beta and alpha
sum2 <- sum((retp$VTI - mean(retp$VTI))^2)
betasd <- residsd/sqrt(sum2)
alphasd <- residsd*sqrt(1/nrows + mean(retp$VTI)^2/sum2)
c(alphasd, betasd)
# Perform the Durbin-Watson test of autocorrelation of residuals
lmtest::dwtest(regmod)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Alpha} and \protect\emph{Beta} of ETF Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{beta} $\beta$ values of ETF returns are very statistically significant, but the \emph{alpha} $\alpha$ values are mostly not significant.
      \vskip1ex
      Some of the ETFs with significant \emph{alpha} $\alpha$ values are the bond ETFs \emph{IEF} and \emph{TLT} (which have performed very well), and the natural resource ETFs \emph{USO} and \emph{DBC} (which have performed very poorly).
      <<echo=TRUE,eval=TRUE>>=
retp <- rutils::etfenv$returns
symbolv <- colnames(retp)
symbolv <- symbolv[symbolv != "VTI"]
# Perform regressions and collect statistics
betam <- sapply(symbolv, function(symbol) {
# Specify regression formula
  formulav <- as.formula(paste(symbol, "~ VTI"))
# Perform regression
  regmod <- lm(formulav, data=retp)
# Get regression summary
  regmodsum <- summary(regmod)
# Collect regression statistics
  with(regmodsum, 
    c(beta=coefficients[2, 1], 
      pbeta=coefficients[2, 4],
      alpha=coefficients[1, 1], 
      palpha=coefficients[1, 4], 
      pdw=lmtest::dwtest(regmod)$p.value))
})  # end sapply
betam <- t(betam)
# Sort by palpha
betam <- betam[order(betam[, "palpha"]), ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
betam
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Capital Asset Pricing Model (\protect\emph{CAPM})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The CAPM model states that the expected return for stock $n$: $\mathbbm{E}[R_n]$ is proportional to its beta $\beta_n$ times the expected excess return of the market $\mathbbm{E}[R_m] - r_f$:
      \begin{displaymath}
        \mathbbm{E}[R_n] = r_f + \beta_n (\mathbbm{E}[R_m] - r_f)
      \end{displaymath}
      The \emph{CAPM} model states that if a stock has a higher beta then it's also expected to earn higher returns.
      \vskip1ex
      According to the \emph{CAPM} model, assets are on average expected to earn only a \emph{systematic} return proportional to their \emph{systematic} risk.
      \vskip1ex
      The \emph{CAPM} model is not a regression model.
      \vskip1ex
      The \emph{CAPM} model depends on the choice of the risk-free rate $r_f$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(PerformanceAnalytics)
# Calculate XLP beta
PerformanceAnalytics::CAPM.beta(Ra=retp$XLP, Rb=retp$VTI)
# Or
retsxlp <- na.omit(retp[, c("XLP", "VTI")])
betav <- drop(cov(retsxlp$XLP, retsxlp$VTI)/var(retsxlp$VTI))
betav
# Calculate XLP alpha
PerformanceAnalytics::CAPM.alpha(Ra=retp$XLP, Rb=retp$VTI)
# Or
mean(retp$XLP - betav*retp$VTI)
# Calculate XLP bull beta
PerformanceAnalytics::CAPM.beta.bull(Ra=retp$XLP, Rb=retp$VTI)
# Calculate XLP bear beta
PerformanceAnalytics::CAPM.beta.bear(Ra=retp$XLP, Rb=retp$VTI)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Security Market Line for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Security Market Line} (SML) represents the linear relationship between expected stock returns and \emph{systematic} risk.
      \vskip1ex
      A scatterplot of asset returns versus their $\beta$ shows which assets earn a positive $\alpha$, and which don't.
      \vskip1ex
      If an asset lies on the \emph{SML}, then its returns are mostly \emph{systematic}, and its $\alpha$ is equal to zero.
      \vskip1ex
      Assets above the \emph{SML} have a positive $\alpha$, and those below have a negative $\alpha$.
      <<echo=TRUE,eval=FALSE>>=
symbolv <- rownames(betam)
betav <- betam[-match(c("VXX", "SVXY", "MTUM", "USMV", "QUAL"), symbolv), 1]
betav <- c(1, betav)
names(betav)[1] <- "VTI"
retsann <- sapply(retp[, names(betav)], PerformanceAnalytics::Return.annualized)
# Plot scatterplot of returns vs betas
minrets <- min(retsann)
plot(retsann ~ betav, xlab="betas", ylab="returns", 
     ylim=c(minrets, -minrets), main="Security Market Line for ETFs")
retsvti <- retsann["VTI"]
points(x=1, y=retsvti, col="red", lwd=3, pch=21)
# Plot Security Market Line
riskfree <- 0.01
abline(a=riskfree, b=(retsvti-riskfree), col="green", lwd=2)
# Add labels
text(x=betav, y=retsann, labels=names(betav), pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/capm_scatter.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Find optimal risk-free rate by minimizing residuals
rss <- function(riskfree) {
  sum((retsann - riskfree + betav*(retsvti-riskfree))^2)
}  # end rss
optimrss <- optimize(rss, c(-1, 1))
riskfree <- optimrss$minimum
abline(a=riskfree, b=(retsvti-riskfree), col="blue", lwd=2)
legend(x="top", bty="n", title="Security Market Line",
       legend=c("optimal fit", "riskfree=0.01"),
       y.intersp=0.5, cex=1.0, lwd=6, lty=1, col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Security Market Line for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best fitting \emph{Security Market Line} (SML) for stocks is almost flat, which shows that stocks with higher $\beta$ don't earn higher returns.
      \vskip1ex
      This is called the \emph{low beta anomaly}.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock returns
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
retsvti <- na.omit(rutils::etfenv$returns$VTI)
retp <- returns[index(retsvti), ]
nrows <- NROW(retp)
# Calculate stock betas
betav <- sapply(retp, function(x) {
  retp <- na.omit(cbind(x, retsvti))
  drop(cov(retp[, 1], retp[, 2])/var(retp[, 2]))
})  # end sapply
mean(betav)
# Calculate annual stock returns
retsann <- retp
retsann[1, ] <- 0
retsann <- zoo::na.locf(retsann, na.rm=FALSE)
retsann <- 252*sapply(retsann, sum)/nrows
# Remove stocks with zero returns
sum(retsann == 0)
betav <- betav[retsann > 0]
retsann <- retsann[retsann > 0]
retsvti <- 252*mean(retsvti)
# Plot scatterplot of returns vs betas
plot(retsann ~ betav, xlab="betas", ylab="returns", 
     main="Security Market Line for Stocks")
points(x=1, y=retsvti, col="red", lwd=3, pch=21)
# Plot Security Market Line
riskfree <- 0.01
abline(a=riskfree, b=(retsvti-riskfree), col="green", lwd=2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/capm_scatter_stocks.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Find optimal risk-free rate by minimizing residuals
rss <- function(riskfree) {
  sum((retsann - riskfree + betav*(retsvti-riskfree))^2)
}  # end rss
optimrss <- optimize(rss, c(-1, 1))
riskfree <- optimrss$minimum
abline(a=riskfree, b=(retsvti-riskfree), col="blue", lwd=2)
legend(x="top", bty="n", title="Security Market Line",
       legend=c("optimal fit", "riskfree=0.01"),
       y.intersp=0.5, cex=1.0, lwd=6, lty=1, col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Beta-adjusted Performance Measurement}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Treynor} ratio measures the excess returns per unit of the \emph{systematic} risk \emph{beta} $\beta$, and is equal to the excess returns (over a risk-free return) divided by the $\beta$:
      \begin{displaymath}
        T_r=\frac{E[R-r_f]}{\beta}
      \end{displaymath}
      The \emph{Treynor} ratio is similar to the \emph{Sharpe} ratio, with the difference that its denominator represents only \emph{systematic} risk, not total risk.
      \vskip1ex
      The \emph{Information} ratio is equal to the excess returns (over a benchmark) divided by the \emph{tracking error} (standard deviation of excess returns):
      \begin{displaymath}
        I_r = \frac{E[R-R_b]} {\sqrt{\sum_{i=1}^n (R_i-R_{i,b})^2}}
      \end{displaymath}
      The \emph{Information} ratio measures the amount of outperformance versus the benchmark, and the consistency of outperformance.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(PerformanceAnalytics)
# Calculate XLP Treynor ratio
TreynorRatio(Ra=retp$XLP, Rb=retp$VTI)
# Calculate XLP Information ratio
InformationRatio(Ra=retp$XLP, Rb=retp$VTI)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CAPM} Summary Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.55\textwidth}
      \texttt{PerformanceAnalytics::table.CAPM()} calculates the \emph{beta} $\beta$ and \emph{alpha} $\alpha$ values, the \emph{Treynor} ratio, and other performance statistics.
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
PerformanceAnalytics::table.CAPM(Ra=retp[, c("XLP", "XLF")], 
                                 Rb=retp$VTI, scale=252)
      @
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
capmstats <- table.CAPM(Ra=retp[, symbolv], 
              Rb=retp$VTI, scale=252)
colnamev <- strsplit(colnames(capmstats), split=" ")
colnamev <- do.call(cbind, colnamev)[1, ]
colnames(capmstats) <- colnamev
capmstats <- t(capmstats)
capmstats <- capmstats[, -1]
colnamev <- colnames(capmstats)
whichv <- match(c("Annualized Alpha", "Information Ratio", "Treynor Ratio"), colnamev)
colnamev[whichv] <- c("Alpha", "Information", "Treynor")
colnames(capmstats) <- colnamev
capmstats <- capmstats[order(capmstats[, "Alpha"], decreasing=TRUE), ]
# Copy capmstats into etfenv and save to .RData file
etfenv <- rutils::etfenv
etfenv$capmstats <- capmstats
save(etfenv, file="/Users/jerzy/Develop/lecture_slides/data/etf_data.RData")
      @
    \column{0.45\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
rutils::etfenv$capmstats[, c("Beta", "Alpha", "Information", "Treynor")]
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Stock Selection Strategies}


%%%%%%%%%%%%%%%
\subsection{Random Stock Selection}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A random portfolio is a sub-portfolio of stocks selected at random.
      \vskip1ex
      Random portfolios are used as a benchmark for stock pickers (portfolio managers).
      \vskip1ex
      If a portfolio manager outperforms the median of random portfolios, then they may have stock picking skill.
      <<echo=TRUE,eval=FALSE>>=
# Load the S&P500 stock prices
load("/Users/jerzy/Develop/lecture_slides/data/sp500_prices.RData")
# Subset (select) the prices after the start date of VTI
retvti <- na.omit(rutils::etfenv$returns$VTI)
colnames(retvti) <- "VTI"
pricev <- pricev[zoo::index(retvti)]
# Select columns with non-NA prices at start
pricev <- pricev[, !is.na(pricev[1, ])]
dim(pricev)
# Copy over NA prices using the function zoo::na.locf()
pricev <- zoo::na.locf(pricev, na.rm=FALSE)
sum(is.na(pricev))
datev <- zoo::index(pricev)
retvti <- retvti[datev]
nrows <- NROW(pricev)
nstocks <- NCOL(pricev)
# Normalize the prices so that they start at 1
pricesn <- lapply(pricev, function(x) x/as.numeric(x[1]))
pricesn <- rutils::do_call(cbind, pricesn)
head(pricevn[, 1:5])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_random.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the equal dollar-weighted average of all stock prices
indeks <- rowMeans(pricevn)
indeks <- xts::xts(indeks, order.by=datev)
colnames(indeks) <- "Index"
# Select a random, equal dollar-weighted portfolio of 5 stocks
set.seed(1121)
samplev <- sample.int(n=nstocks, size=5, replace=FALSE)
portf <- pricesn[, samplev]
portf <- rowMeans(portf)
portf <- xts::xts(portf, order.by=datev)
colnames(portf) <- "Random"
# Plot dygraph of stock index and random portfolio
wealthv <- cbind(indeks, portf)
colorv <- c("blue", "red")
endd <- rutils::calc_endpoints(pricev, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Stock Index and Random Portfolio") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Random Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most random portfolios underperform the index, so picking a portfolio which outperforms the stock index requires great skill.
      \vskip1ex
      An investor without skill, who selects stocks at random, has a high probability of underperforming the index, because they will most likely miss selecting the best performing stocks.
      \vskip1ex
      Therefore the proper benchmark for a stock picker is the median of random portfolios, not the stock index, which is the mean of all the stock prices.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
      <<echo=TRUE,eval=FALSE>>=
# Select 10 random equal dollar-weighted sub-portfolios
set.seed(1121)
nportf <- 10
portfs <- sapply(1:nportf, function(x) {
  pricev <- pricesn[, sample.int(n=nstocks, size=5, replace=FALSE)]
  rowMeans(pricev)
})  # end sapply
portfs <- xts::xts(portfs, order.by=datev)
colnames(portfs) <- paste0("portf", 1:nportf)
# Sort the sub-portfolios according to perfomance
portfs <- portfs[, order(portfs[nrows])]
round(head(portfs), 3)
round(tail(portfs), 3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_randomm.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and random portfolios
colorv <- colorRampPalette(c("red", "blue"))(nportf)
combined <- cbind(indeks, portfs)
colnames(combined)[1] <- "Index"
colnamev <- colnames(combined)
colorv <- c("green", colorv)
dygraphs::dygraph(log(combined[endd]), main="Stock Index and Random Portfolios") %>%
  dyOptions(colors=colorv, strokeWidth=1) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=3, col="green") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stock Portfolio Selection Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The strategy selects the \texttt{10} best performing stocks from the in-sample interval, and invests equal dollar amounts in the out-of-sample interval.
      \vskip1ex
      The out-of-sample performance of the best performing stocks in-sample, is not any better than the index.
      <<echo=TRUE,eval=FALSE>>=
# Define cutoff between in-sample and out-of-sample intervals
cutoff <- nrows %/% 2
datev[cutoff]
insample <- 1:cutoff
outsample <- (cutoff + 1):nrows
# Calculate the 10 best performing stocks in-sample
perfstat <- sort(drop(coredata(pricevn[cutoff, ])), decreasing=TRUE)
symbolv <- names(head(perfstat, 10))
# Calculate the in-sample portfolio
pricis <- pricesn[insample, symbolv]
# Normalize the prices so that they are 1 at cutoff+1
pricesn <- lapply(pricev, function(x) x/as.numeric(x[cutoff+1]))
pricesn <- rutils::do_call(cbind, pricesn)
# Calculate the out-of-sample portfolio
pricos <- pricesn[outsample, symbolv]
# Scale the prices to preserve the in-sample wealth
pricos <- sum(pricis[cutoff, ])*pricos/sum(pricos[1, ])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_stocks_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine indeks with out-of-sample stock portfolio returns
wealthv <- rbind(pricis, pricos)
wealthv <- xts::xts(rowMeans(wealthv), datev)
wealthv <- cbind(indeks, wealthv)
colnames(wealthv)[2] <- "Portfolio"
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv[outsample, ]), 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot out-of-sample stock portfolio returns
dygraphs::dygraph(log(wealthv[endd]), main="Out-of-sample Log Prices of Stock Portfolio") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(datev[cutoff], label="in-sample", strokePattern="solid", color="green") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Volatility Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by \href{https://www.paradoxinvesting.com}{Robeco}, \href{http://www.efalken.com/papers/efdiss94.pdf}{Eric Falkenstein}, and others has shown that low volatility stocks have outperformed high volatility stocks.
      \vskip1ex
      \emph{Betting against volatility} is a strategy which invests in low volatility stocks and shorts high volatility stocks.
      \vskip1ex
      \emph{USMV} is an \emph{ETF} that holds low volatility stocks, although it hasn't met expectations. 
      <<echo=TRUE,eval=FALSE>>=
# Calculate the stock volatilities, betas, and alphas
retp <- rutils::diffit(log(pricev))
varvti <- drop(var(retvti))
meanvti <- mean(retvti)
riskret <- sapply(retp, function(rets) {
  betav <- drop(cov(rets, retvti))/varvti
  resid <- rets - betav*retvti
  alphav <- mean(rets) - betav*meanvti
  c(alpha=alphav, beta=betav, vol=sd(rets), ivol=sd(resid))
})  # end sapply
riskret <- t(riskret)
tail(riskret)
# Sort stocks by their volatilities
riskret <- riskret[order(riskret[, "vol"]), ]
symbolv <- rownames(riskret)
# Calculate the cumulative returns of low and high volatility stocks
volow <- rowMeans(retp[, symbolv[1:(nstocks %/% 2)]])
volhigh <- rowMeans(retp[, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(volow, volhigh, volow - 0.3*volhigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_vol.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Volatility Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Volatility Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against volatility} strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
residv <- regmod$residuals
plot.default(x=retvti, y=residv, xlab="VTI", ylab="Low Volatility")
title(main="Treynor-Mazuy Market Timing Test\n for Low Volatility vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(residv), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_lowvol_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Volatility Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low volatility stocks selected in-sample also outperform the high volatility stocks in the out-of-sample period.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the in-sample stock volatilities, betas, and alphas
varvti <- drop(var(retvti[insample]))
meanvti <- mean(retvti[insample])
riskretis <- sapply(retp[insample], function(rets) {
  betav <- drop(cov(rets[insample], retvti[insample]))/varvti
  resid <- rets - betav*retvti[insample]
  alphav <- mean(rets[insample]) - betav*meanvti
  c(alpha=alphav, beta=betav, vol=sd(rets), ivol=sd(resid))
})  # end sapply
riskretis <- t(riskretis)
tail(riskretis)
# Sort stocks in-sample by their volatilities
riskretis <- riskretis[order(riskretis[, "vol"]), ]
head(riskretis)
symbolv <- rownames(riskretis)
# Calculate the out-of-sample returns of low and high volatility stocks
volow <- rowMeans(retp[outsample, symbolv[1:(nstocks %/% 2)]])
volhigh <- rowMeans(retp[outsample, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(volow, volhigh, volow - 0.3*volhigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_vol_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Volatility Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Idiosyncratic Volatility Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by \href{https://www.paradoxinvesting.com}{Robeco}, \href{http://www.efalken.com/papers/efdiss94.pdf}{Eric Falkenstein}, and others has shown that low idiosyncratic volatility stocks have outperformed high volatility stocks.
      \vskip1ex
      \emph{Betting against idiosyncratic volatility} is a strategy which invests in low idiosyncratic volatility stocks and shorts high volatility stocks.
      <<echo=TRUE,eval=FALSE>>=
# Sort stocks by their idiosyncratic volatilities
riskret <- riskret[order(riskret[, "ivol"]), ]
symbolv <- rownames(riskret)
# Calculate the cumulative returns of low and high volatility stocks
volow <- rowMeans(retp[, symbolv[1:(nstocks %/% 2)]])
volhigh <- rowMeans(retp[, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(volow, volhigh, volow - 0.3*volhigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_ivol.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Idiosyncratic Volatility Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Idiosyncratic Volatility Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against idiosyncratic volatility} strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
residv <- regmod$residuals
plot.default(x=retvti, y=residv, xlab="VTI", ylab="Low Volatility")
title(main="Treynor-Mazuy Market Timing Test\n for Low Idiosyncratic Volatility vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(residv), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_lowivol_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Idiosyncratic Volatility Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low idiosyncratic volatility stocks selected in-sample also outperform the high volatility stocks in the out-of-sample period.
      <<echo=TRUE,eval=FALSE>>=
# Sort stocks in-sample by their volatilities
riskretis <- riskretis[order(riskretis[, "ivol"]), ]
head(riskretis)
symbolv <- rownames(riskretis)
# Calculate the out-of-sample returns of low and high volatility stocks
volow <- rowMeans(retp[outsample, symbolv[1:(nstocks %/% 2)]])
volhigh <- rowMeans(retp[outsample, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(volow, volhigh, volow - 0.3*volhigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_voli_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Idiosyncratic Volatility Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Beta Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by NYU professors \href{https://pages.stern.nyu.edu/~afrazzin/}{Andrea Frazzini} and \href{https://www.lhpedersen.com}{Lasse Heje Pedersen} has shown that contrary to the \emph{CAPM} model, low beta stocks have outperformed high beta stocks.
      \vskip1ex
      The low beta stocks are mostly from defensive stock sectors, like consumer staples, healthcare, etc., which investors buy when they fear a market downturn.
      \vskip1ex
      The strategy of investing in low beta stocks and shorting high beta stocks is known as
\href{https://www.aqr.com/Insights/Datasets/Betting-Against-Beta-Equity-Factors-Monthly}{betting against beta.}
      <<echo=TRUE,eval=FALSE>>=
# Sort stocks by their betas
riskret <- riskret[order(riskret[, "beta"]), ]
head(riskret)
symbolv <- rownames(riskret)
# Calculate the cumulative returns of low and high beta stocks
betalow <- rowMeans(retp[, symbolv[1:(nstocks %/% 2)]])
betahigh <- rowMeans(retp[, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(betalow, betahigh, betalow - 0.3*betahigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_beta", "high_beta", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_beta.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high beta stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Beta Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Beta Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against beta} strategy does not have significant \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
residv <- regmod$residuals
plot.default(x=retvti, y=residv, xlab="VTI", ylab="Low Beta")
title(main="Treynor-Mazuy Market Timing Test\n for Low Beta vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(residv), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_lowbeta_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Beta Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low beta stocks selected in-sample also outperform the high beta stocks in the out-of-sample period.
      <<echo=TRUE,eval=FALSE>>=
# Sort stocks in-sample by their betas
riskretis <- riskretis[order(riskretis[, "beta"]), ]
head(riskretis)
symbolv <- rownames(riskretis)
# Calculate the out-of-sample returns of low and high beta stocks
betalow <- rowMeans(retp[outsample, symbolv[1:(nstocks %/% 2)]])
betahigh <- rowMeans(retp[outsample, symbolv[(nstocks %/% 2):nstocks]])
wealthv <- cbind(betalow, betahigh, betalow - 0.3*betahigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_beta", "high_beta", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_low_high_beta_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high beta stocks
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Beta Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Momentum Strategies}


%%%%%%%%%%%%%%%
\subsection{Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio weights of \emph{momentum} strategies can be calculated based on the past performance of the assets in many different ways:
      \begin{itemize}
        \item Invest equal dollar amounts in the top \texttt{n} best performing stocks and short the \texttt{n} worst performing stocks,
        \item Invest dollar amounts proportional to the past performance - purchase stocks with positive performance, and short stocks with negative performance,
        \item Subtract the weights mean so that their sum is equal to $0$: $\sum_{i=1}^n {w_i} = 0$, 
        \item Scale the weights so that the sum of squares is equal to $1$: $\sum_{i=1}^n {w^2_i} = 1$, 
      \end{itemize}
      \vskip1ex
      De-meaning the weights reduces the portfolio market \emph{beta}.
      \vskip1ex
      Scaling the weights reduces the portfolio \emph{leverage}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log percentage returns
retp <- rutils::diffit(log(pricev))
# Define performance objective function as sum of returns
objfun <- function(retp) sum(retp)
# Define performance objective function as Sharpe ratio
objfun <- function(rets) sum(rets)/sd(rets)
# Calculate performance statistics over look-back intervals
retsis <- retp[endd[1]:endd[2]]
perfstat <- sapply(retsis, objfun)
perfstat[!is.finite(perfstat)] <- 0
sum(is.na(perfstat))
# Calculate the best and worst performing stocks
perfstat <- sort(perfstat, decreasing=TRUE)
nstocks <- 10
symbolb <- names(head(perfstat, nstocks))
symbolw <- names(tail(perfstat, nstocks))
# Calculate equal weights for the best and worst performing stocks
weightv <- numeric(NCOL(retp))
names(weightv) <- colnames(retp)
weightv[symbolb] <- 1
weightv[symbolw] <- (-1)
# Calculate weights proportional to performance statistic
weightv <- perfstat
# Center weights so sum is equal to 0
weightv <- weightv - mean(weightv)
# Scale weights so sum of squares is equal to 1
weightv <- weightv/sqrt(sum(weightv^2))
# Calculate the momentum portfolio returns
retsportf <- retp %*% weightv
# Scale weights so in-sample portfolio volatility is same as equal weight
scalef <- sd(rowMeans(retsis))/sd(retsportf)
weightv <- scalef*weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a \emph{rolling momentum strategy}, the portfolio is rebalanced periodically and held out-of-sample.
      \vskip1ex
      \emph{Momentum strategies} can be \emph{backtested} by specifying the portfolio rebalancing frequency, the formation period, and the holding period:
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Specify a portfolio of stocks and their returns,
        \item Calculate the \emph{end points} for portfolio rebalancing,
        \item Define an objective function for calculating the past performance of the stocks,
        \item Calculate the past performance over the \emph{look-back} formation intervals,
        \item Calculate the portfolio weights from the past (in-sample) performance,
        \item Calculate the out-of-sample momentum strategy returns by applying the portfolio weights to the future returns,
        \item Apply a volatility scaling factor to the out-of-sample returns,
        \item Calculate the transaction costs and subtract them from the strategy returns.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retp, interval="months")
npts <- NROW(endd)
# Perform loop over the end points
nstocks <- 10
look_back <- 8
pnls <- lapply(2:(npts-1), function(ep) {
  # Select the look-back returns
  startp <- endd[max(1, ep-look_back)]
  retsis <- retp[startp:endd[ep], ]
  # Calculate the best and worst performing stocks in-sample
  perfstat <- sapply(retsis, objfun)
  perfstat[!is.finite(perfstat)] <- 0
  perfstat <- sort(perfstat, decreasing=TRUE)
  symbolb <- names(head(perfstat, nstocks))
  symbolw <- names(tail(perfstat, nstocks))
  # Calculate the momentum weights
  weightv <- numeric(NCOL(retp))
  names(weightv) <- colnames(retp)
  weightv[symbolb] <- 1
  # weightv[symbolw] <- (-1)
  # Calculate the in-sample portfolio returns
  retsportf <- retsis %*% weightv
  # Scale weights so in-sample portfolio volatility is same as equal weight
  weightv <- weightv*sd(rowMeans(retsis))/sd(retsportf)
  # Calculate the momentum portfolio returns
  retsportf <- retp[(endd[ep]+1):endd[ep+1], ] %*% weightv
  rowMeans(retsportf)
})  # end lapply
pnls <- rutils::do_call(c, pnls)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Momentum Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy for stocks produces a similar absolute return as the index, and also a similar Sharpe ratio.
      \vskip1ex
      The momentum strategy may be improved by a better choice of the model parameters: the length of look-back interval and the number of stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the average of all stock returns
indeks <- rowMeans(retp)
indeks <- xts::xts(indeks, order.by=datev)
colnames(indeks) <- "Index"
# Add initial startup interval returns
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Log Stock Index and Momentum Strategy") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomtop()} simulates (backtests) a \emph{momentum strategy} which buys equal dollar amounts of the best performing stocks.
      \vskip1ex
      The function \texttt{btmomtop()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
btmomtop <- function(rets,
  objfun=function(rets) (sum(rets)/sd(rets)),
  look_back=12, rfreq="months", nstocks=10, bid_offer=0.001,
  endd=rutils::calc_endpoints(rets, interval=rfreq), ...) {
  # Perform loop over end points
  npts <- NROW(endd)
  pnls <- lapply(2:(npts-1), function(ep) {
    # Select the look-back returns
    startp <- endd[max(1, ep-look_back)]
    retsis <- retp[startp:endd[ep], ]
    # Calculate the best and worst performing stocks in-sample
    perfstat <- sapply(retsis, objfun)
    perfstat[!is.finite(perfstat)] <- 0
    perfstat <- sort(perfstat, decreasing=TRUE)
    symbolb <- names(head(perfstat, nstocks))
    symbolw <- names(tail(perfstat, nstocks))
    # Calculate the momentum weights
    weightv <- numeric(NCOL(retp))
    names(weightv) <- colnames(retp)
    weightv[symbolb] <- 1
    # weightv[symbolw] <- (-1)
    # Calculate the in-sample portfolio returns
    retsportf <- retsis %*% weightv
    # Scale weights so in-sample portfolio volatility is same as equal weight
    weightv <- weightv*sd(rowMeans(retsis))/sd(retsportf)
    # Calculate the momentum portfolio returns
    retsportf <- retp[(endd[ep]+1):endd[ep+1], ] %*% weightv
    rowMeans(retsportf)
  })  # end lapply
  pnls <- rutils::do_call(c, pnls)
  pnls
}  # end btmomtop
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of Momentum Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Research indicates that the optimal length of the \emph{look-back interval} for momentum is about \texttt{8} to \texttt{12} months.
      \vskip1ex
      The dependence on the length of the \emph{look-back interval} is an example of the \emph{bias-variance tradeoff}.  If the \emph{look-back interval} is too short, the past performance estimates have high \emph{variance}, but if the \emph{look-back interval} is too long, the past estimates have high \emph{bias}.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (known as \emph{p-value hacking}).
      <<echo=TRUE,eval=FALSE>>=
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
endd <- rutils::calc_endpoints(retp, interval="months")
pnlsl <- lapply(look_backs, btmomtop, rets=retp, endd=endd, objfun=objfun)
# Or perform parallel loop under Mac-OSX or Linux
library(parallel)  # Load package parallel
ncores <- detectCores() - 1
pnlsl <- mclapply(look_backs, btmomtop, rets=retp, endd=endd, objfun=objfun, mc.cores=ncores)
profilev <- sapply(pnlsl, function(pnl) sum(pnl)/sd(pnl))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_stock_profile.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot Momentum profile
plot(x=look_backs, y=profilev, t="l",
  main="Momentum PnL as Function of Look-back Interval",
  xlab="look-back (months)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Momentum Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy for stocks produces a similar absolute return as the index, and also a similar Sharpe ratio.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      The \emph{backtesting} redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of momentum strategy
whichmax <- which.max(profilev)
look_backs[whichmax]
pnls <- pnlsl[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Optimal Momentum Strategy for Stocks") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Weighted Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomweight()} simulates (backtests) a \emph{momentum strategy} which buys dollar amounts proportional to the past performance of the stocks.
      \vskip1ex
      The function \texttt{btmomweight()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
btmomweight <- function(rets,
  objfun=function(rets) (sum(rets)/sd(rets)),
  look_back=12, rfreq="months", bid_offer=0.001,
  endd=rutils::calc_endpoints(rets, interval=rfreq), ...) {
  # Perform loop over end points
  npts <- NROW(endd)
  pnls <- lapply(2:(npts-1), function(ep) {
    # Select the look-back returns
    startp <- endd[max(1, ep-look_back)]
    retsis <- rets[startp:endd[ep], ]
    # Calculate weights proportional to performance
    perfstat <- sapply(retsis, objfun)
    perfstat[!is.finite(perfstat)] <- 0
    weightv <- perfstat
    # Calculate the in-sample portfolio returns
    retsportf <- retsis %*% weightv
    # Scale weights so in-sample portfolio volatility is same as equal weight
    weightv <- weightv*sd(rowMeans(retsis))/sd(retsportf)
    # Calculate the momentum portfolio returns
    rets[(endd[ep]+1):endd[ep+1], ] %*% weightv
  })  # end lapply
  rutils::do_call(c, pnls)
}  # end btmomweight
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Momentum Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy for stocks produces a similar absolute return as the index, and also a similar Sharpe ratio.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      The \emph{backtesting} redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
pnlsl <- lapply(look_backs, btmomweight, rets=retp, endd=endd, objfun=objfun)
# Or perform parallel loop under Mac-OSX or Linux
library(parallel)  # Load package parallel
ncores <- detectCores() - 1
pnlsl <- mclapply(look_backs, btmomweight, rets=retp, endd=endd, objfun=objfun, mc.cores=ncores)
profilev <- sapply(pnlsl, function(pnl) sum(pnl)/sd(pnl))
# Plot Momentum profile
plot(x=look_backs, y=profilev, t="l",
  main="Momentum PnL as Function of Look-back Interval",
  xlab="look-back (months)", ylab="pnl")
# Calculate best pnls of momentum strategy
whichmax <- which.max(profilev)
look_backs[whichmax]
pnls <- pnlsl[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_weighted_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Optimal Weighted Momentum Strategy for Stocks") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The MTUM Momentum ETF}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{MTUM} ETF is an actively managed ETF which follows a momentum strategy for stocks.
      \vskip1ex
      The \emph{MTUM} ETF has a slightly higher absolute return than the \emph{VTI} ETF, but it has a slightly lower Sharpe ratio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the scaled prices of VTI vs MTUM ETF
wealthv <- na.omit(rutils::etfenv$returns[, c("VTI", "MTUM")])
colnames(wealthv) <- c("VTI", "MTUM")
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the scaled prices of VTI vs MTUM ETF
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="VTI vs MTUM ETF") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_mtum.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Research indicates that the optimal length of the \emph{look-back interval} for momentum is about \texttt{4} to \texttt{10} months.
      \vskip1ex
      The dependence on the length of the \emph{look-back interval} is an example of the \emph{bias-variance tradeoff}.  If the \emph{look-back interval} is too short, the past performance estimates have high \emph{variance}, but if the \emph{look-back interval} is too long, the past estimates have high \emph{bias}.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (known as \emph{p-value hacking}).
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_profile.png}
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
retp <- rutils::etfenv$returns[, symbolv]
retp <- na.omit(retp)
datev <- zoo::index(retp)
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retp, interval="months")
npts <- NROW(endd)
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
objfun <- function(retp) sum(retp)/sd(retp)
pnlsl <- lapply(look_backs, btmomweight, rets=retp, endd=endd, objfun=objfun)
profilev <- sapply(pnlsl, function(pnl) sum(pnl)/sd(pnl))
# Plot Momentum PnLs
plot(x=look_backs, y=profilev, t="l",
  main="Momentum PnL as Function of Look-back Interval",
  xlab="look-back (months)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Momentum Strategy for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy for ETFs produces a higher absolute return and also a higher Sharpe ratio than the static \emph{All-Weather} portfolio.
      \vskip1ex
      The momentum strategy for ETFs also has a very low correlation to the static \emph{All-Weather} portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of momentum strategy
whichmax <- which.max(profilev)
look_backs[whichmax]
pnls <- pnlsl[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- retp %*% weightsaw
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(all_weather, pnls)
cor(wealthv)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("All-weather", "Strategy")
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Momentum Strategy and All-weather") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Series of Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the momentum weights
look_back <- look_backs[whichmax]
weightv <- lapply(1:(npts-1), function(ep) {
  # Select the look-back returns
  startp <- endd[max(1, ep-look_back)]
  retsis <- retp[startp:endd[ep], ]
  # Calculate weights proportional to performance
  perfstat <- sapply(retsis, objfun)
  weightv <- drop(perfstat)
  # Scale weights so in-sample portfolio volatility is same as equal weight
  retsportf <- retsis %*% weightv
  weightv*sd(rowMeans(retsis))/sd(retsportf)
})  # end lapply
weightv <- rutils::do_call(rbind, weightv)
# Plot the momentum weights
retvti <- cumsum(retp$VTI)
datav <- cbind(retvti[endd], weightv)
colnames(datav) <- c("VTI", paste0(colnames(retp), "weight"))
zoo::plot.zoo(datav, xlab=NULL, main="Momentum Weights")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF betas
betas_etf <- sapply(retp, function(x)
  cov(retp$VTI, x)/var(retp$VTI))
# Momentum beta is equal weights times ETF betas
betas <- weightv %*% betas_etf
betas <- xts::xts(betas, order.by=datev[endd])
colnames(betas) <- "momentum_beta"
datav <- cbind(betas, retvti[endd])
zoo::plot.zoo(datav, main="Momentum Beta & VTI Price", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{momentum} strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
retvti <- retp$VTI
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(pnls ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(pnls ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
residv <- regmod$residuals
plot.default(x=retvti, y=residv, xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy Market Timing Test\n for Momentum vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(residv), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness of Momentum Strategy Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most assets with \emph{positive returns} suffer from \emph{negative skewness}.
      \vskip1ex
      The \emph{momentum} strategy returns have more positive skewness compared to the negative skewness of \emph{VTI}.
      \vskip1ex
      The \emph{momentum} strategy is a genuine \emph{market anomaly}, because it has both positive returns and positive skewness.
      <<echo=TRUE,eval=FALSE>>=
# Standardize the returns
pnlsd <- (pnls-mean(pnls))/sd(pnls)
retvti <- (retvti-mean(retvti))/sd(retvti)
# Calculate skewness and kurtosis
apply(cbind(pnlsd, retvti), 2, function(x)
  sapply(c(skew=3, kurt=4),
    function(e) sum(x^e)))/NROW(retvti)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_distr.png}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
hist(pnlsd, breaks=80,
  main="Momentum and VTI Return Distributions (standardized",
  xlim=c(-4, 4), xlab="", ylab="", freq=FALSE)
# Draw kernel density of histogram
lines(density(pnlsd), col='red', lwd=2)
lines(density(retvti), col='blue', lwd=2)
# Add legend
legend("topright", inset=0.05, cex=1.0, title=NULL,
       leg=c("Momentum", "VTI"), bty="n",
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Momentum with the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the \emph{momentum} strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the \emph{momentum} strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Combine momentum strategy with all-weather
wealthv <- cbind(pnls, all_weather, 0.5*(pnls + all_weather))
colnames(wealthv) <- c("momentum", "all_weather", "combined")
wealthv <- xts::xts(wealthv, datev)
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Calculate strategy correlations
cor(wealthv)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_combined.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot ETF momentum strategy combined with All-Weather
dygraphs::dygraph(cumsum(wealthv)[endd], main="ETF Momentum Strategy Combined with All-Weather") %>%
  dyOptions(colors=c("red", "blue", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("green", "blue", "red")
quantmod::chart_Series(wealthv, theme=plot_theme,
             name="ETF Momentum Strategy Combined with All-Weather")
legend("topleft", legend=colnames(wealthv),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy With Daily Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a momentum strategy with \emph{daily rebalancing}, the weights are updated every day and the portfolio is rebalanced accordingly.
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions are preferred to \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling variance
look_back <- 152
variance <- roll::roll_var(retp, width=look_back, min_obs=1)
variance[1, ] <- variance[2, ]
variance[variance <= 0] <- 0
# Calculate rolling Sharpe
perfstat <- roll::roll_mean(retp, width=look_back, min_obs=1)
weightv <- perfstat/sqrt(variance)
weightv <- weightv/sqrt(rowSums(weightv^2))
weightv <- rutils::lagit(weightv)
sum(is.na(weightv))
# Calculate momentum profits and losses
pnls <- rowSums(weightv*retp)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_daily.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate transaction costs
bid_offer <- 0.0
costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
pnls <- (pnls - costs)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- retp %*% weightsaw
# Scale the momentum volatility to all_weather
pnls <- sd(all_weather)*pnls/sd(pnls)
# Calculate the wealth of momentum returns
wealthv <- xts::xts(cbind(all_weather, pnls), order.by=datev)
colnames(wealthv) <- c("All-Weather", "Momentum")
cor(wealthv)
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealthv)[endd], main="Daily Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{btmomdaily()} simulates a momentum strategy with \emph{daily rebalancing}.
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions are preferred to \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomweight()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomdaily <- function(rets, look_back=252, bid_offer=0.001, trend=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate rolling variance
  variance <- roll::roll_var(rets, width=look_back, min_obs=1)
  variance[1, ] <- 1
  variance[variance <= 0] <- 1
# Calculate rolling Sharpe
  perfstat <- roll::roll_mean(rets, width=look_back, min_obs=1)
  weights <- perfstat/sqrt(variance)
  weights <- weights/sqrt(rowSums(weights^2))
  weights <- rutils::lagit(weights)
  # Calculate momentum profits and losses
  pnls <- trend*rowSums(weights*rets)
  # Calculate transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weights)))
  (pnls - costs)
}  # end btmomdaily
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiple Daily ETF Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The best performing daily ETF \emph{momentum} strategies are with \emph{look-back} parameters between \texttt{100} and \texttt{120} days.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a long \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a daily ETF momentum strategy
source("/Users/jerzy/Develop/lecture_slides/scripts/back_test.R")
pnls <- btmomdaily(rets=retp, look_back=152,
  bid_offer=bid_offer)
# Perform sapply loop over look_backs
look_backs <- seq(90, 190, by=10)
pnls <- sapply(look_backs, btmomdaily,
  rets=retp, bid_offer=bid_offer)
# Scale the momentum volatility to all_weather
pnls <- apply(pnls, MARGIN=2, 
  function(pnl) sd(all_weather)*pnl/sd(pnl))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, zoo::index(retp))
tail(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_daily_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], main="Daily ETF Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot daily ETF momentum strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The daily ETF momentum strategy can be improved by introducing a \emph{holding period} for the portfolio.
      \vskip1ex
      Instead of holding the portfolio for only a day, its held for several days and then liquidated.  So several portfolios are held at the same time.
      \vskip1ex
      This is equivalent to averaging the portfolio weights over several days from the past.
      \vskip1ex
      The best length of the \emph{holding period} depends on the \emph{bias-variance tradeoff}.
      \vskip1ex
      If the \emph{holding period} is too short then the weights have too much day-over-day \emph{variance}.
      \vskip1ex
      If the \emph{holding period} is too long then the weights have too much \emph{bias} (they are stale).
      \vskip1ex
      The optimal length of the \emph{holding period} can be determined by cross-validation (backtesting).
      \vskip1ex
      The function \texttt{btmomdailyhold()} simulates a momentum strategy with \emph{daily rebalancing} with a holding period.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomdailyhold <- function(rets, look_back=252, holdp=5, bid_offer=0.001, trend=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate rolling variance
  variance <- roll::roll_var(rets, width=look_back, min_obs=1)
  variance[1, ] <- 1
  variance[variance <= 0] <- 1
  # Calculate rolling Sharpe
  perfstat <- roll::roll_mean(rets, width=look_back, min_obs=1)
  weightv <- perfstat/sqrt(variance)
  weightv <- weightv/sqrt(rowSums(weightv^2))
  # Average the weights over holding period
  weightv <- roll::roll_mean(weightv, width=holdp, min_obs=1)
  weightv <- rutils::lagit(weightv)
  # Calculate momentum profits and losses
  pnls <- trend*rowSums(weightv*rets)
  # Calculate transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
  (pnls - costs)
}  # end btmomdailyhold
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of holding periods.
      \vskip1ex
      The daily \emph{momentum} strategies with a holding period perform much better.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over holding periods
holdpv <- seq(2, 11, by=2)
pnls <- sapply(holdpv, btmomdailyhold, look_back=100,
                  rets=retp, bid_offer=bid_offer)
# Scale the momentum volatility to all_weather
pnls <- apply(pnls, MARGIN=2, 
  function(pnl) sd(all_weather)*pnl/sd(pnl))
colnames(pnls) <- paste0("holding=", holdpv)
pnls <- xts::xts(pnls, zoo::index(retp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_daily_hold.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], main="Daily ETF Momentum Strategies with Holding Period") %>%
  dyOptions(colors=colorv, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot daily ETF momentum strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple S\&P500 Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{momentum} strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The best performing daily \emph{S\&P500} \emph{momentum} strategies are with \emph{look-back} parameters between \texttt{120} and \texttt{160} days.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a short \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Load daily S&P500 percentage stock returns.
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns100
retp <- returns100["2000/"]
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)
# Simulate a daily S&P500 momentum strategy.
# Perform sapply loop over look_backs
look_backs <- seq(100, 170, by=10)
pnls <- sapply(look_backs, btmomdailyhold,
  holdp=5, rets=retp, bid_offer=0)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, zoo::index(retp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retp, interval="months")
# Plot dygraph of daily S&P500 momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], main="Daily S&P500 Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot daily S&P500 momentum strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Daily S&P500 Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple S\&P500 \protect\emph{Mean Reverting} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{mean reverting} strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies for the \emph{S\&P500} constituents perform the best for short \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies had their best performance prior to the 2008 financial crisis.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(3, 20, by=2)
pnls <- sapply(look_backs, btmomdaily,
  rets=retp, bid_offer=0, trend=(-1))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, zoo::index(retp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_revert.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily S&P500 momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], main="Daily S&P500 Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot daily S&P500 momentum strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Cumulative Returns of S&P500 Mean Reverting Strategies")
legend("topleft", legend=colnames(pnls),
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Factor Analysis}


%%%%%%%%%%%%%%%
\subsection{Rolling Beta Regressions Over Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling beta of \emph{XLP} versus \emph{VTI} changes over time, with lower beta in periods of \emph{VTI} selloffs.
      \vskip1ex
      The function \texttt{roll\_reg()} from package \emph{HighFreq} performs rolling regressions in \texttt{C++} (\emph{RcppArmadillo}), so it's therefore much faster than equivalent \texttt{R} code.
      <<echo=TRUE,eval=FALSE>>=
# Calculate XLP and VTI returns
retp <- na.omit(rutils::etfenv$returns[, c("XLP", "VTI")])
# Calculate monthly end points
endd <- xts::endpoints(retp, on="months")[-1]
# Calculate start points from look-back interval
look_back <- 12  # Look back 12 months
startp <- c(rep(1, look_back), endd[1:(NROW(endd)-look_back)])
head(cbind(endd, startp), look_back+2)
# Calculate rolling beta regressions every month in R
formulav <- XLP ~ VTI  # Specify regression formula
betar <- sapply(1:NROW(endd), FUN=function(ep) {
    datav <- retp[startp[ep]:endd[ep], ]
    # coef(lm(formulav, data=datav))[2]
    drop(cov(datav$XLP, datav$VTI)/var(datav$VTI))
  })  # end sapply
# Calculate rolling betas using RcppArmadillo
reg_stats <- HighFreq::roll_reg(respv=retp$XLP, retp=retp$VTI, endd=(endd-1), startp=(startp-1))
betas <- reg_stats$VTI
all.equal(betas, betar)
# Compare the speed of RcppArmadillo with R code
library(microbenchmark)
summary(microbenchmark(
  Rcpp=HighFreq::roll_reg(respv=retp$XLP, retp=retp$VTI, endd=(endd-1), startp=(startp-1)),
  Rcode=sapply(1:NROW(endd), FUN=function(ep) {
    datav <- retp[startp[ep]:endd[ep], ]
    drop(cov(datav$XLP, datav$VTI)/var(datav$VTI))
  }),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/rolling_betas.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of rolling XLP beta and VTI prices
dates <- zoo::index(retp[endd, ])
pricev <- rutils::etfenv$prices$VTI[dates]
datav <- cbind(pricev, betas)
colnamev <- colnames(datav)
dygraphs::dygraph(datav, main="XLP Rolling 12-month Beta and VTI Prices") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Engle-Granger Two-step Procedure for Cointegration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ADF} test can be applied to test for the cointegration of time series of prices.
      \vskip1ex
      The Engle-Granger two-step procedure for two time series consists of:
      \begin{itemize}
        \item Performing a regression to calculate the cointegrating factor $\beta$,
        \item Applying the \emph{ADF} test to the residuals of the regression to determine that they don't have a unit root (they are mean reverting).
      \end{itemize}
      The regression of prices is not statistically valid because they are not normally distributed.
      <<echo=TRUE,eval=FALSE>>=
# Calculate XLB and XLE prices
pricev <- na.omit(rutils::etfenv$pricev[, c("XLB", "XLE")])
cor(rutils::diffit(log(pricev)))
xlb <- drop(zoo::coredata(pricev$XLB))
xle <- drop(zoo::coredata(pricev$XLE))
# Calculate regression coefficients of XLB ~ XLE
betav <- cov(xlb, xle)/var(xle)
alpha <- (mean(xlb) - betav*mean(xle))
# Calculate regression residuals
fittedv <- (alpha + betav*xle)
residuals <- (xlb - fittedv)
# Perform ADF test on residuals
tseries::adf.test(residuals, k=1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/coint_prices.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot prices
dygraphs::dygraph(pricev, main="XLB and XLE Prices") %>%
  dyOptions(colors=c("blue", "red"))
# Plot cointegration residuals
residuals <- xts::xts(residuals, zoo::index(pricev))
dygraphs::dygraph(residuals, main="XLB and XLE Cointegration Residuals")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Principal Components} of \protect\emph{S\&P500} Stock Constituents}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{PCA} standard deviations are the volatilities of the \emph{principal component} time series.
      \vskip1ex
      The original time series of returns can be calculated approximately from the first few \emph{principal components} with the largest standard deviations.
      \vskip1ex
      The \emph{Kaiser-Guttman} rule uses only \emph{principal components} with \emph{variance} greater than $1$.
      \vskip1ex
      Another rule of thumb is to use the \emph{principal components} with the largest standard deviations which sum up to \texttt{80\%} of the total variance of returns.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock prices
load("/Users/jerzy/Develop/lecture_slides/data/sp500_prices.RData")
# Calculate stock prices and percentage returns
pricets <- zoo::na.locf(pricets, na.rm=FALSE)
pricets <- zoo::na.locf(pricets, fromLast=TRUE)
retp <- rutils::diffit(log(pricev))
# Standardize (de-mean and scale) the returns
retp <- lapply(retp, function(x) {(x - mean(x))/sd(x)})
retp <- rutils::do_call(cbind, retp)
# Perform principal component analysis PCA
pcad <- prcomp(retp, scale=TRUE)
# Find number of components with variance greater than 2
ncomp <- which(pcad$sdev^2 < 2)[1]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_scree.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot standard deviations of principal components
barplot(pcad$sdev[1:ncomp], 
        names.arg=colnames(pcad$rotation[, 1:ncomp]), 
        las=3, xlab="", ylab="", 
        main="Volatilities of S&P500 Principal Components")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Principal Component} Loadings (Weights)}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Principal component} loadings are the weights of \emph{principal component} portfolios.
      \vskip1ex
      The \emph{principal component} portfolios have mutually orthogonal returns
      represent the different orthogonal modes of the return variance.
      <<echo=TRUE,eval=FALSE>>=
# Calculate principal component loadings (weights)
# Plot barplots with PCA weights in multiple panels
ncomps <- 6
par(mfrow=c(ncomps/2, 2))
par(mar=c(4, 2, 2, 1), oma=c(0, 0, 0, 0))
# First principal component weights
weights <- sort(pcad$rotation[, 1], decreasing=TRUE)
barplot(weights[1:6], las=3, xlab="", ylab="", main="")
title(paste0("PC", 1), line=-2.0, col.main="red")
for (ordern in 2:ncomps) {
  weights <- sort(pcad$rotation[, ordern], decreasing=TRUE)
  barplot(weights[c(1:3, 498:500)], las=3, xlab="", ylab="", main="")
  title(paste0("PC", ordern), line=-2.0, col.main="red")
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_loadings.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Principal Component} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time series of the \emph{principal components} can be calculated by multiplying the loadings (weights) times the original data.
      \vskip1ex
      Higher order \emph{principal components} are gradually less volatile.
      <<echo=TRUE,eval=FALSE>>=
# Calculate principal component time series
retspca <- xts(retp %*% pcad$rotation[, 1:ncomps], 
                order.by=dates)
round(cov(retspca), 3)
pcacum <- cumsum(retspca)
# Plot principal component time series in multiple panels
par(mfrow=c(ncomps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
rangev <- range(pcacum)
for (ordern in 1:ncomps) {
  plot.zoo(pcacum[, ordern], ylim=rangev, xlab="", ylab="")
  title(paste0("PC", ordern), line=-2.0)
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_series.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Factor Model From \protect\emph{Principal Components}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      By inverting the \emph{PCA} analysis, the \emph{S\&P500} constituent returns can be calculated from the first \texttt{k} \emph{principal components} under a \emph{factor model}: 
      \begin{displaymath}
        \mathbf{r}_i = \alpha_i + \sum_{j=1}^k {\beta_{ji} \, \mathbf{F}_j} + \varepsilon_i
      \end{displaymath}
      The \emph{principal components} are interpreted as \emph{market factors}: $\mathbf{F}_j = \mathbf{pc}_j$.
      \vskip1ex
      The \emph{market betas} are the inverse of the \emph{principal component loadings}: $\beta_{ji} = w_{ij}$.
      \vskip1ex
      The $\varepsilon_i$ are the \emph{idiosyncratic} returns, which should be mutually independent and uncorrelated to the \emph{market factor} returns.
      <<echo=(-(1:2)),eval=FALSE>>=
par(mfrow=c(ncomps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
# Invert principal component time series
invmat <- solve(pcad$rotation)
all.equal(invmat, t(pcad$rotation))
solved <- retspca %*% invmat[1:ncomps, ]
solved <- xts::xts(solved, dates)
solved <- cumsum(solved)
retc <- cumsum(retp)
# Plot the solved returns
symbolv <- c("MSFT", "XOM", "JPM", "AAPL", "BRK_B", "JNJ")
for (symbol in symbolv) {
  plot.zoo(cbind(retc[, symbol], solved[, symbol]), 
    plot.type="single", col=c("black", "blue"), xlab="", ylab="")
  legend(x="topleft", bty="n",
         legend=paste0(symbol, c("", " solved")),
         title=NULL, inset=0.05, cex=1.0, lwd=6,
         lty=1, col=c("black", "blue"))
}  # end for
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_series_solved.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} \protect\emph{Factor Model} Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The original time series of returns can be calculated exactly from the time series of all the \emph{principal components}, by inverting the loadings matrix.
      \vskip1ex
      The original time series of returns can be calculated approximately from just the first few \emph{principal components}, which demonstrates that \emph{PCA} is a form of \emph{dimension reduction}.
      \vskip1ex
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices.
      <<echo=(-(1:2)),eval=FALSE>>=
par(mfrow=c(ncomps/2, 2))
par(mar=c(2, 2, 0, 1), oma=c(0, 0, 0, 0))
# Perform ADF unit root tests on original series and residuals
sapply(symbolv, function(symbol) {
  c(series=tseries::adf.test(retc[, symbol])$p.value,
    resid=tseries::adf.test(retc[, symbol] - solved[, symbol])$p.value)
})  # end sapply
# Plot the residuals
for (symbol in symbolv) {
  plot.zoo(retc[, symbol] - solved[, symbol], 
    plot.type="single", col="blue", xlab="", ylab="")
  legend(x="topleft", bty="n", legend=paste(symbol, "residuals"),
         title=NULL, inset=0.05, cex=1.0, lwd=6, lty=1, col="blue")
}  # end for
# Perform ADF unit root test on principal component time series
retspca <- xts(retp %*% pcad$rotation, order.by=dates)
pcacum <- cumsum(retspca)
adf_pvalues <- sapply(1:NCOL(pcacum), function(ordern)
  tseries::adf.test(pcacum[, ordern])$p.value)
# AdF unit root test on stationary time series
tseries::adf.test(rnorm(1e5))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/pca_sp500_residuals.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{Correlation and Factor Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<corr_plot,echo=(-(1:1)),eval=FALSE,fig.show='hide'>>=
library(quantmod)
### Perform pair-wise correlation analysis
# Calculate correlation matrix
cormat <- cor(retp)
colnames(cormat) <- colnames(retp)
rownames(cormat) <- colnames(retp)
# Reorder correlation matrix based on clusters
# Calculate permutation vector
library(corrplot)
ordern <- corrMatOrder(cormat, order="hclust", 
              hclust.method="complete")
# Apply permutation vector
cormat <- cormat[ordern, ordern]
# Plot the correlation matrix
colorv <- colorRampPalette(c("red", "white", "blue"))
corrplot(cormat, tl.col="black", tl.cex=0.8, 
    method="square", col=colorv(8), 
    cl.offset=0.75, cl.cex=0.7, 
    cl.align.text="l", cl.ratio=0.25)
# draw rectangles on the correlation matrix plot
corrRect.hclust(cormat, k=NROW(cormat) %/% 2, 
                method="complete", col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/corr_plot-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hierarchical Clustering Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{as.dist()} converts a matrix representing the \emph{distance} (dissimilarity) between elements, into a list of class \texttt{"dist"}.
      \vskip1ex
      For example, \texttt{as.dist()} converts \texttt{(1-correlation)} to distance.
      \vskip1ex
      The function \texttt{hclust()} recursively combines elements into clusters based on their mutual \emph{distance}.
      \vskip1ex
      First \texttt{hclust()} combines individual elements that are closest to each other.
      \vskip1ex
      Then it combines elements to the closest clusters, then clusters with other clusters, until all elements are combined into one cluster.
      \vskip1ex
      This process of recursive clustering can be represented as a \emph{dendrogram} (tree diagram).
      \vskip1ex
      Branches of a \emph{dendrogram} represent clusters.
      \vskip1ex
      Neighboring branches contain elements that are close to each other (have small distance).
      \vskip1ex
      Neighboring branches combine into larger branches, that then combine with their closest branches, etc.
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.5\paperwidth]{figure/cluster_plot-1}
      \vspace{-4em}
      <<cluster_plot,echo=TRUE,eval=FALSE,fig.width=6,fig.height=6,fig.show='hide'>>=
# Convert correlation matrix into distance object
distancev <- as.dist(1-cormat)
# Perform hierarchical clustering analysis
cluster <- hclust(distancev)
plot(cluster, ann=FALSE, xlab="", ylab="")
title("Dendrogram representing hierarchical clustering
      \nwith dissimilarity = 1-correlation", line=-0.5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  Study all the lecture slides in \texttt{FRE7241\_Lecture\_6.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_6.R}
\end{block}
\begin{block}{Recommended}
  \begin{itemize}[]
    \item Read about \emph{estimator shrinkage}:\\
    \emph{Aswani Regression Shrinkage Bias Variance Tradeoff.pdf}\\
    \emph{Blei Regression Lasso Shrinkage Bias Variance Tradeoff.pdf}\\
    \item Read about \emph{optimization methods}:\\
    \emph{Bolker Optimization Methods.pdf}\\
    \emph{Yollin Optimization.pdf}\\
    \emph{DEoptim Introduction.pdf}\\
    \emph{Ardia DEoptim Portfolio Optimization.pdf}\\
    \emph{Boudt DEoptim Portfolio Optimization.pdf}\\
    \emph{Boudt DEoptim Large Portfolio Optimization.pdf}\\
    \emph{Mullen Package DEoptim.pdf}\\
    \item Read about \emph{momentum}:\\
    \emph{Bouchaud Momentum Mean Reversion Equity Returns.pdf}\\
  \end{itemize}
\end{block}

\end{frame}


\end{document}
