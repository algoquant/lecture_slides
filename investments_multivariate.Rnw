% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% tikz package for plotting and tables
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% bbm and bbold packages for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Multivariate Investment Strategies]{Multivariate Investment Strategies}
\subtitle{FRE7241, Spring 2023}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Interest Rate Strategies}


%%%%%%%%%%%%%%%
\subsection{Interest Rate Yield Curve and Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Daily stock returns have insignificant correlations with the daily changes in interest rates, with the possible exception of the \texttt{10}-year bond yield. 
      \vskip1ex
      And these correlations change significantly over time.
      <<echo=TRUE,eval=FALSE>>=
# Load constant maturity Treasury rates
load(file="/Users/jerzy/Develop/lecture_slides/data/rates_data.RData")
# Combine rates into single xts series
rates <- do.call(cbind, as.list(ratesenv))
# Sort the columns of rates according bond maturity
namesv <- colnames(rates)
namesv <- substr(namesv, start=4, stop=10)
namesv <- as.numeric(names)
indeks <- order(names)
rates <- rates[, indeks]
# Align rates dates with VTI prices
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
colnames(closep) <- "VTI"
nrows <- NROW(closep)
datev <- zoo::index(closep)
rates <- na.omit(rates[datev])
closep <- closep[zoo::index(rates)]
datev <- zoo::index(closep)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns and IR changes
retp <- rutils::diffit(log(closep))
retr <- rutils::diffit(log(rates))
# Regress VTI returns versus the lagged rate differences
predv <- rutils::lagit(retr)
regmod <- lm(retp ~ predv)
summary(regmod)
# Regress VTI returns before and after 2012
summary(lm(retp["/2012"] ~ predv["/2012"]))
summary(lm(retp["2012/"] ~ predv["2012/"]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Components and Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The principal components of the interest rate yield curve can also be used as predictors of stock indices.
      \vskip1ex
      The second principal component describes the steepening and flattening of the yield curve, and it's an indicator of investor risk appetite.  So it's also related to bullish and bearish market periods.
      <<echo=TRUE,eval=FALSE>>=
# Calculate PCA of rates correlation matrix
eigend <- eigen(cor(retr))
pcar <- -(retr %*% eigend$vectors)
colnames(pcar) <- paste0("PC", 1:6)
# Define predictor as the YC PCAs
predv <- rutils::lagit(pcar)
regmod <- lm(retp ~ predv)
summary(regmod)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_steep.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot YC steepener principal component with VTI
datav <- cbind(retp, pcar[, 2])
colnames(datav) <- c("VTI", "Steepener")
colnamev <- colnames(datav)
dygraphs::dygraph(cumsum(datav), 
  main="VTI and Yield Curve Steepener") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For in-sample forecasts, the training set and the test set are the same.  The model is calibrated on the data that is used for forecasting. 
      \vskip1ex
      Yield Curve Strategy
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model works.
      \vskip1ex
      The in-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Define predictor with intercept term
predv <- rutils::lagit(retr)
predv <- cbind(rep(1, NROW(predv)), predv)
colnames(predv)[1] <- "intercept"
# Calculate inverse of predictor
invmat <- MASS::ginv(predv)
# Calculate coefficients from response and inverse of predictor
respv <- retp
coeff <- drop(invmat %*% respv)
# Calculate forecasts and pnls in-sample
fcast <- (predv %*% coeff)
pnls <- sign(fcast)*response
# Calculate in-sample factors
factors <- (predv*coeff)
apply(factors, 2, sd)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_insample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample IR strategy
wealthv <- cbind(retp, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Yield Curve Strategy In-sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
insample <- (datev < as.Date("2020-01-01"))
outsample <- (datev >= as.Date("2020-01-01"))
# Calculate inverse of predictor in-sample
invmat <- MASS::ginv(predv[insample, ])
# Calculate coefficients in-sample
coeff <- drop(invmat %*% respv[insample, ])
# Calculate forecasts and pnls out-of-sample
fcast <- (predv[outsample, ] %*% coeff)
pnls <- sign(fcast)*respv[outsample, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample IR PCA strategy
wealthv <- cbind(retp[outsample, ], pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Yield Curve Strategy Out-of-Sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Yearly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling yearly yield curve strategy, the model is recalibrated at the end of every year using a training set of data from the past year.
      The coefficients are applied to calculate out-of-sample forecasts in the following year.
      \vskip1ex
      The rolling yearly strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define yearly dates
format(datev[1], "%Y")
years <- paste0(seq(2001, 2022, 1), "-01-01")
years <- as.Date(years)
# Perform loop over yearly dates
pnls <- lapply(3:(NROW(years)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > years[ep-1]) & (datev < years[ep])
  outsample <- (datev >= years[ep]) & (datev < years[ep+1])
  # Calculate coefficients in-sample
  invmat <- MASS::ginv(predv[insample, ])
  coeff <- drop(invmat %*% respv[insample, ])
  # Calculate forecasts and pnls out-of-sample
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_yearly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling yearly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Yearly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Monthly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{11} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      \vskip1ex
      Research shows that looking back roughly a year provides the best out-of-sample forecasts.
      \vskip1ex
      The rolling monthly strategy performs better than the yearly strategy, but mostly in periods of high volatility, and otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(datev[1], "%m-%Y")
format(datev[NROW(datev)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
pnls <- lapply(12:(NROW(months)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > months[ep-11]) & (datev < months[ep])
  outsample <- (datev > months[ep]) & (datev < months[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- MASS::ginv(predv[insample, ])
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Monthly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Weekly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{10} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
pnls <- lapply(51:(NROW(weeks)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > weeks[ep-10]) & (datev < weeks[ep])
  outsample <- (datev > weeks[ep]) & (datev < weeks[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- MASS::ginv(predv[insample, ])
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_weekly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Weekly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularization of the Inverse Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{SVD} of a rectangular matrix $\mathbb{A}$ is defined as the factorization:
      \begin{displaymath}
        \mathbb{A} = \mathbb{U} \Sigma \mathbb{V}^T
      \end{displaymath}
      Where $\mathbb{U}$ and $\mathbb{V}$ are the \emph{singular matrices}, and $\Sigma$ is a diagonal matrix of \emph{singular values}.
      \vskip1ex
      The \emph{generalized inverse} matrix $\mathbb{A}^{-1}$ satisfies the inverse equation: $\mathbb{A} \mathbb{A}^{-1} \mathbb{A} = \mathbbm{A}$, and it can be expressed as a product of the \emph{SVD} matrices as follows:
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V} \, \Sigma^{-1} \, \mathbb{U}^T
      \end{displaymath}
      If any of the \emph{singular values} are zero then the \emph{generalized inverse} does not exist.
      \vskip1ex
      \emph{Regularization} is the removal of zero singular values, to make calculating the inverse matrix possible.
      \vskip1ex
      The \emph{regularized inverse} is obtained by removing the zero \emph{singular values}:
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V}_n \, \Sigma_n^{-1} \, \mathbb{U}_n^T
      \end{displaymath}
      Where $\mathbb{U}_n$, $\mathbb{V}_n$ and $\Sigma_n$ are the \emph{SVD} matrices without the zero \emph{singular values}.
      \vskip1ex
      The regularized inverse satisfies the inverse matrix equation: $\mathbb{A} \mathbb{A}^{-1} \mathbb{A} = \mathbbm{A}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.3\paperwidth]{figure/yc_pred_svd.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate singular value decomposition of the predictor matrix
svdec <- svd(predv)
barplot(svdec$d, main="Singular Values of YC Predictor Matrix")
# Calculate generalized inverse from SVD
invsvd <- svdec$v %*% (t(svdec$u) / svdec$d)
# Verify inverse property of inverse
all.equal(zoo::coredata(predv), predv %*% invsvd %*% predv)
# Calculate generalized inverse using MASS::ginv()
invmat <- MASS::ginv(predv)
all.equal(invmat, invsvd)
# Set tolerance for determining zero singular values
precv <- sqrt(.Machine$double.eps)
# Check for zero singular values
round(svdec$d, 12)
notzero <- (svdec$d > (precv*svdec$d[1]))
# Calculate regularized inverse from SVD
invreg <- svdec$v[, notzero] %*%
  (t(svdec$u[, notzero]) / svdec$d[notzero])
# Verify inverse property of invreg
all.equal(zoo::coredata(predv), predv %*% invreg %*% predv)
all.equal(invreg, invmat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shrinkage Inverse of the Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Regularization} is the removal of zero singular values, to make calculating the inverse matrix possible.
      \vskip1ex
      If the higher order singular values are very small then the inverse matrix will amplify the noise in the response matrix.
      \vskip1ex
      \emph{Dimension reduction} is achieved by the removal of small singular values, to improve the out-of-sample performance of the inverse matrix.
      \vskip1ex
      The \emph{shrinkage inverse} is obtained by removing the very small \emph{singular values}.
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V}_n \, \Sigma_n^{-1} \, \mathbb{U}_n^T
      \end{displaymath}
      This effectively reduces the number of parameters in the model.
      \vskip1ex
      The \emph{shrinkage inverse} satisfies the inverse equation only approximately (it is \emph{biased}), but it's often used in machine learning because it produces a lower \emph{variance} of the forecasts than the exact inverse.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate shrinkage inverse from SVD
dimax <- 3
invreg <- svdec$v[, 1:dimax] %*%
  (t(svdec$u[, 1:dimax]) / svdec$d[1:dimax])
# Inverse property fails for invreg
all.equal(zoo::coredata(predv), predv %*% invreg %*% predv)
# Calculate shrinkage inverse using RcppArmadillo
inverse_rcpp <- HighFreq::calc_inv(predv, dimax=dimax)
all.equal(invreg, inverse_rcpp, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy With Shrinkage In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The technique of \emph{regularization} is designed to reduce the number of parameters in a model, for example in portfolio optimization.
      \vskip1ex
      Regularization of the inverse predictor matrix improves the in-sample performance of the yield curve strategy.
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model can be improved.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample pnls for different dimax values
eigenvals <- 2:7
pnls <- lapply(eigenvals, function(dimax) {
  invmat <- HighFreq::calc_inv(predv, dimax=dimax)
  coeff <- drop(invmat %*% respv)
  fcast <- (predv %*% coeff)
  sign(fcast)*response
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigenvals)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_insample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample pnls
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="In-Sample Returns of Shrinkage YC Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy With Shrinkage Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
insample <- (datev < as.Date("2020-01-01"))
outsample <- (datev >= as.Date("2020-01-01"))
# Calculate in-sample pnls for different dimax values
eigenvals <- 2:7
pnls <- lapply(eigenvals, function(x) {
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=x)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigenvals)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_outsample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample pnls
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Out-of-Sample Returns of Shrinkage YC Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Monthly Yield Curve Strategy With Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling monthly strategy performs better than the standard strategy because regularization allows using shorter \texttt{look\_back} intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(datev[1], "%m-%Y")
format(datev[NROW(datev)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
look_back <- 6
dimax <- 3
pnls <- lapply((look_back+1):(NROW(months)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > months[ep-look_back]) & (datev < months[ep])
  outsample <- (datev > months[ep]) & (datev < months[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=dimax)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_monthly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Monthly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Weekly Yield Curve Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{4} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
look_back <- 4
dimax <- 4
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > weeks[ep-look_back]) & (datev < weeks[ep])
  outsample <- (datev > weeks[ep]) & (datev < weeks[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=dimax)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Weekly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A "kitchen sink" strategy combines many different predictors into a large predictor matrix with many columns.  
      \vskip1ex
      For example by combining the yield curve predictors with the lagged returns. 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load the yield curve data
load(file="/Users/jerzy/Develop/lecture_slides/data/rates_data.RData")
rates <- do.call(cbind, as.list(ratesenv))
namesv <- colnames(rates)
namesv <- substr(namesv, start=4, stop=10)
namesv <- as.numeric(names)
indeks <- order(names)
rates <- rates[, indeks]
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
colnames(closep) <- "VTI"
nrows <- NROW(closep)
datev <- zoo::index(closep)
rates <- na.omit(rates[datev])
closep <- closep[zoo::index(rates)]
datev <- zoo::index(closep)
retp <- rutils::diffit(log(closep))
retr <- rutils::diffit(log(rates))
# Create a combined predictor matrix
ordmax <- 5
predv <- sapply(1:ordmax, rutils::lagit, input=as.numeric(retp))
colnames(predv) <- paste0("retslag", 1:NCOL(predv))
predv <- cbind(predv, rutils::lagit(retr))
predv <- cbind(rep(1, NROW(predv)), predv)
colnames(predv)[1] <- "intercept"
respv <- retp
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Strategy With Shrinkage In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The technique of \emph{regularization} is designed to reduce the number of parameters in a model, for example in portfolio optimization.
      \vskip1ex
      Regularization of the inverse predictor matrix improves the in-sample performance of the yield curve strategy.
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model can be improved.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample pnls for different dimax values
eigenvals <- 2:11
pnls <- lapply(eigenvals, function(dimax) {
  invmat <- HighFreq::calc_inv(predv, dimax=dimax)
  coeff <- drop(invmat %*% respv)
  fcast <- (predv %*% coeff)
  sign(fcast)*response
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigenvals)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_combined_insample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample pnls
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="In-Sample Returns of Combined Strategies With Shrinkage") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Strategy With Shrinkage Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
insample <- (datev < as.Date("2020-01-01"))
outsample <- (datev >= as.Date("2020-01-01"))
# Calculate in-sample pnls for different dimax values
eigenvals <- 2:11
pnls <- lapply(eigenvals, function(x) {
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=x)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigenvals)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_combined_outsample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample pnls
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Out-of-Sample Returns of Combined Strategies With Shrinkage") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Monthly Combined Strategy With Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling monthly strategy performs better than the standard strategy because regularization allows using shorter \texttt{look\_back} intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(datev[1], "%m-%Y")
format(datev[NROW(datev)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
look_back <- 6
dimax <- 3
pnls <- lapply((look_back+1):(NROW(months)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > months[ep-look_back]) & (datev < months[ep])
  outsample <- (datev > months[ep]) & (datev < months[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=dimax)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_monthly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Monthly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Weekly Combined Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{4} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
look_back <- 8
dimax <- 4
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(ep) {
  # Define in-sample and out-of-sample intervals
  insample <- (datev > weeks[ep-look_back]) & (datev < weeks[ep])
  outsample <- (datev > weeks[ep]) & (datev < weeks[ep+1])
  # Calculate forecasts and pnls out-of-sample
  invmat <- HighFreq::calc_inv(predv[insample, ], dimax=dimax)
  coeff <- drop(invmat %*% respv[insample, ])
  fcast <- (predv[outsample, ] %*% coeff)
  sign(fcast)*respv[outsample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_roll_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vti <- rutils::diffit(closep[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Rolling Weekly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Forecasts Using Aggregated Predictor}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Needs more work to improve performance
      \vskip1ex
      Aggregating the predictor reduces its noise and increases the significance of correlations. 
      \vskip1ex
      The optimal aggregation number can be found by maximizing the regression t-values.
      <<echo=TRUE,eval=FALSE>>=
# Find optimal nagg for predictor
naggs <- 5:100
tvalues <- sapply(naggs, function(nagg) {
  predv <- roll::roll_mean(retr, width=nagg, min_obs=1)
  predv <- cbind(rep(1, NROW(predv)), predv)
  predv <- rutils::lagit(predv)
  regmod <- lm(respv ~ predv - 1)
  modelsum <- summary(regmod)
  max(abs(modelsum$coefficients[, 3][-1]))
})  # end sapply
naggs[which.max(tvalues)]
plot(naggs, tvalues, t="l", col="blue", lwd=2)
# Calculate aggregated predictor
nagg <- 53
predv <- roll::roll_mean(retr, width=nagg, min_obs=1)
predv <- rutils::lagit(predv)
predv <- cbind(rep(1, NROW(predv)), predv)
regmod <- lm(respv ~ predv - 1)
summary(regmod)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_insample_agg.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate forecasts and pnls in-sample
invmat <- MASS::ginv(predv)
coeff <- drop(invmat %*% respv)
fcast <- (predv %*% coeff)
pnls <- sign(fcast)*response
# Plot dygraph of in-sample IR strategy
wealthv <- cbind(retp, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Aggregated YC Strategy In-sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Aggregated Forecasts Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Needs more work to improve performance
      \vskip1ex
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
insample <- (datev < as.Date("2020-01-01"))
outsample <- (datev >= as.Date("2020-01-01"))
# Calculate forecasts and pnls out-of-sample
invmat <- MASS::ginv(predv[insample, ])
coeff <- drop(invmat %*% respv[insample, ])
fcast <- (predv[outsample, ] %*% coeff)
pnls <- sign(fcast)*respv[outsample, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_yc_outsample_agg.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample YC strategy
wealthv <- cbind(retp[outsample, ], pnls)
colnames(wealthv) <- c("VTI", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Aggregated YC Strategy Out-of-Sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Stock Selection Strategies}


%%%%%%%%%%%%%%%
\subsection{Stock Index Weighting Methods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stock market indices can be either capitalization-weighted, price-weighted, or equal-weighted.
      \vskip1ex
      The cap-weighted index is equal to the average of the market capitalizations of all its companies (stock price times number of shares).  The \emph{S\&P500} index is cap-weighted.
      \vskip1ex
      The price-weighted index is equal to the average of the stock prices.  The \emph{DJIA} index is price-weighted.
      \vskip1ex
      The equal-weighted index is equal to the value (wealth) of the equal-weighted portfolio of stocks.
      \vskip1ex
      The equal-weighted portfolio owns equal dollar amounts of each stock, and it rebalances its allocations as market prices change.
      \vskip1ex
      The cap-weighted and price-weighted indices are overweight large-cap stocks, compared to the equal-weight index which has larger weights for small-cap stocks.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load daily S&P500 log percentage stock returns
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Subset (select) the stock returns after the start date of VTI
retvti <- na.omit(rutils::etfenv$returns$VTI)
colnames(retvti) <- "VTI"
retp <- returns[zoo::index(retvti)]
datev <- zoo::index(retp)
retvti <- retvti[datev]
nrows <- NROW(retp)
nstocks <- NCOL(retp)
head(retp[, 1:5])
# Replace NA returns with zeros
retsna <- retp
retsna[is.na(retsna)] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Equal-Weight Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The equal-weight portfolio rebalances its allocations - it sells the stocks with higher returns and buys stocks with lower returns.  So it's a \emph{mean reverting} (contrarian) strategy.
      \vskip1ex
      The equal-weight portfolio underperforms the cap-weighted and price-weighted indices because it gradually overweights underperforming stocks, as it rebalances to maintain equal dollar allocations.
      \vskip1ex
      In periods when a small number of stocks dominate returns, the cap-weighted and price-weighted indices outperform the equal-weighted index.
      <<echo=TRUE,eval=FALSE>>=
# Calculate normalized log prices that start at 0
pricev <- cumsum(retsna)
head(pricev[, 1:5])
# Wealth of price-weighted (fixed shares) portfolio
wealthpw <- rowMeans(exp(pricev))
# Wealth of equal-weighted portfolio
wealthew <- exp(rowMeans(pricev))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_equal_weight.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate combined log wealth
wealthv <- cbind(wealthpw, wealthew)
wealthv <- log(wealthv)
wealthv <- xts::xts(wealthv, datev)
colnames(wealthv) <- c("Price-weighted", "Equal-weighted")
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv),
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the combined log wealth
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(wealthv[endd], 
  main="Wealth of Price-weighted and Equal-weighted Portfolios") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Random Stock Selection}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A random portfolio is a sub-portfolio of stocks selected at random.
      \vskip1ex
      Random portfolios are used as a benchmark for stock pickers (portfolio managers).
      \vskip1ex
      If a portfolio manager outperforms the median of random portfolios, then they may have stock picking skill.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the price-weighted average of all stock prices
wealthpw <- xts::xts(wealthpw, order.by=datev)
colnames(wealthpw) <- "Index"
# Select a random, price-weighted portfolio of 5 stocks
set.seed(1121)
samplev <- sample.int(n=nstocks, size=5, replace=FALSE)
portf <- pricev[, samplev]
portf <- rowMeans(exp(portf))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_random.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and random portfolio
wealthv <- cbind(wealthpw, portf)
wealthv <- log(wealthv)
colnames(wealthv)[2] <- "Random"
colorv <- c("blue", "red")
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(wealthv[endd], main="Stock Index and Random Portfolio") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Random Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most random portfolios underperform the index, so picking a portfolio which outperforms the stock index requires great skill.
      \vskip1ex
      An investor without skill, who selects stocks at random, has a high probability of underperforming the index, because they will most likely miss selecting the best performing stocks.
      \vskip1ex
      Therefore the proper benchmark for a stock picker is the median of random portfolios, not the stock index, which is the mean of all the stock prices.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
      <<echo=TRUE,eval=FALSE>>=
# Select 10 random price-weighted sub-portfolios
set.seed(1121)
nportf <- 10
portfs <- sapply(1:nportf, function(x) {
  portf <- pricev[, sample.int(n=nstocks, size=5, replace=FALSE)]
  rowMeans(exp(portf))
})  # end sapply
portfs <- xts::xts(portfs, order.by=datev)
colnames(portfs) <- paste0("portf", 1:nportf)
# Sort the sub-portfolios according to perfomance
portfs <- portfs[, order(portfs[nrows])]
round(head(portfs), 3)
round(tail(portfs), 3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_randomm.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and random portfolios
colorv <- colorRampPalette(c("red", "blue"))(nportf)
wealthv <- cbind(wealthpw, portfs)
wealthv <- log(wealthv)
colnames(wealthv)[1] <- "Index"
colnamev <- colnames(wealthv)
colorv <- c("green", colorv)
dygraphs::dygraph(wealthv[endd], main="Stock Index and Random Portfolios") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=3, col="green") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stock Portfolio Selection Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The strategy selects the \texttt{10} best performing stocks at the end of the in-sample interval, and invests in them in the out-of-sample interval.
      \vskip1ex
      The strategy buys equal and fixed number of shares of stocks, and at the end of the in-sample interval, selects the \texttt{10} best performing stocks. 
      It then invests the same number of shares in the out-of-sample interval.
      \vskip1ex
      The out-of-sample performance of the best performing stocks is not any better than the index.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
cutoff <- nrows %/% 2
datev[cutoff]
insample <- 1:cutoff
outsample <- (cutoff + 1):nrows
# Calculate the 10 best performing stocks in-sample
perfstat <- sort(drop(coredata(pricev[cutoff, ])), decreasing=TRUE)
symbolv <- names(head(perfstat, 10))
# Calculate the wealth of the 10 best performing stocks
wealthv <- pricev[, symbolv]
wealthv <- rowMeans(exp(wealthv))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_stocks_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine the price-weighted wealth with the 10 best performing stocks
wealthv <- cbind(wealthpw, wealthv)
wealthv <- log(wealthv)
colnames(wealthv)[2] <- "Portfolio"
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv[outsample, ]), 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot out-of-sample stock portfolio returns
dygraphs::dygraph(wealthv[endd], main="Out-of-sample Log Prices of Stock Portfolio") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(datev[cutoff], label="in-sample", strokePattern="solid", color="green") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Volatility Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by \href{https://www.paradoxinvesting.com}{Robeco}, \href{http://www.efalken.com/papers/efdiss94.pdf}{Eric Falkenstein}, and others has shown that low volatility stocks have outperformed high volatility stocks.
      \vskip1ex
      \emph{Betting against volatility} is a strategy which invests in low volatility stocks and shorts high volatility stocks.
      \vskip1ex
      \emph{USMV} is an \emph{ETF} that holds low volatility stocks, although it hasn't met expectations. 
      <<echo=TRUE,eval=FALSE>>=
# Calculate the stock volatilities, betas, and alphas
riskret <- sapply(retp, function(retp) {
  retp <- na.omit(retp)
  std <- sd(retp)
  retvti <- retvti[zoo::index(retp)]
  varvti <- drop(var(retvti))
  meanvti <- mean(retvti)
  betav <- drop(cov(retp, retvti))/varvti
  resid <- retp - betav*retvti
  alphav <- mean(retp) - betav*meanvti
  c(alpha=alphav, beta=betav, std=std, ivol=sd(resid))
})  # end sapply
riskret <- t(riskret)
tail(riskret)
# Calculate the median volatility
riskv <- riskret[, "std"]
medianv <- median(riskv)
# Calculate the returns of low and high volatility stocks
retlow <- rowMeans(retsna[, names(riskv[riskv<=medianv])])
rethigh <- rowMeans(retsna[, names(riskv[riskv>medianv])])
wealthv <- cbind(retlow, rethigh, retlow - 0.5*rethigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_vol.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Volatility Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Volatility Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against volatility} strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
resids <- regmod$residuals
plot.default(x=retvti, y=resids, xlab="VTI", ylab="Low Volatility")
title(main="Treynor-Mazuy Market Timing Test\n for Low Volatility vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(resids), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_lowvol_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Volatility Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low volatility stocks selected in-sample also have a higher \emph{Sharpe ratio} in the out-of-sample period than the high volatility stocks, although their absolute returns are similar.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the in-sample stock volatilities, betas, and alphas
riskretis <- sapply(retp[insample], function(retp) {
  combv <- na.omit(cbind(retp, retvti))
  if (NROW(combv) > 0) {
    retp <- na.omit(retp)
    std <- sd(retp)
    retvti <- retvti[zoo::index(retp)]
    varvti <- drop(var(retvti))
    meanvti <- mean(retvti)
    betav <- drop(cov(retp, retvti))/varvti
    resid <- retp - betav*retvti
    alphav <- mean(retp) - betav*meanvti
    return(c(alpha=alphav, beta=betav, std=std, ivol=sd(resid)))
  } else {
    return(c(alpha=0, beta=0, std=0, ivol=0))
  }  # end if
})  # end sapply
riskretis <- t(riskretis)
tail(riskretis)
# Calculate the median volatility
riskv <- riskretis[, "std"]
medianv <- median(riskv)
# Calculate the out-of-sample returns of low and high volatility stocks
retlow <- rowMeans(retsna[outsample, names(riskv[riskv<=medianv])])
rethigh <- rowMeans(retsna[outsample, names(riskv[riskv>medianv])])
wealthv <- cbind(retlow, rethigh, retlow - 0.5*rethigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_vol_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Volatility Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Idiosyncratic Volatility Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by \href{https://www.paradoxinvesting.com}{Robeco}, \href{http://www.efalken.com/papers/efdiss94.pdf}{Eric Falkenstein}, and others has shown that low idiosyncratic volatility stocks have outperformed high volatility stocks.
      \vskip1ex
      \emph{Betting against idiosyncratic volatility} is a strategy which invests in low idiosyncratic volatility stocks and shorts high volatility stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the median idiosyncratic volatility
riskv <- riskret[, "ivol"]
medianv <- median(riskv)
# Calculate the returns of low and high idiosyncratic volatility stocks
retlow <- rowMeans(retsna[, names(riskv[riskv<=medianv])])
rethigh <- rowMeans(retsna[, names(riskv[riskv>medianv])])
wealthv <- cbind(retlow, rethigh, retlow - 0.5*rethigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_ivol.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the returns of low and high idiosyncratic volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Idiosyncratic Volatility Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Idiosyncratic Volatility Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against idiosyncratic volatility} strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
resids <- regmod$residuals
plot.default(x=retvti, y=resids, xlab="VTI", ylab="Low Volatility")
title(main="Treynor-Mazuy Market Timing Test\n for Low Idiosyncratic Volatility vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(resids), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_lowivol_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Idiosyncratic Volatility Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low idiosyncratic volatility stocks selected in-sample also have a higher \emph{Sharpe ratio} in the out-of-sample period than the high idiosyncratic volatility stocks, although their absolute returns are similar.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the median in-sample idiosyncratic volatility
riskv <- riskretis[, "ivol"]
medianv <- median(riskv)
# Calculate the out-of-sample returns of low and high idiosyncratic volatility stocks
retlow <- rowMeans(retsna[outsample, names(riskv[riskv<=medianv])])
rethigh <- rowMeans(retsna[outsample, names(riskv[riskv>medianv])])
wealthv <- cbind(retlow, rethigh, retlow - 0.5*rethigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_vol", "high_vol", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_ivol_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the out-of-sample returns of low and high volatility stocks
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Idiosyncratic Volatility Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Beta Stock Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Research by NYU professors \href{https://pages.stern.nyu.edu/~afrazzin/}{Andrea Frazzini} and \href{https://www.lhpedersen.com}{Lasse Heje Pedersen} has shown that low beta stocks have outperformed high beta stocks, contrary to the \emph{CAPM} model.
      \vskip1ex
      The low beta stocks are mostly from defensive stock sectors, like consumer staples, healthcare, etc., which investors buy when they fear a market selloff.
      \vskip1ex
      The strategy of investing in low beta stocks and shorting high beta stocks is known as
\href{https://www.aqr.com/Insights/Datasets/Betting-Against-Beta-Equity-Factors-Monthly}{betting against beta.}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the median beta
riskv <- riskret[, "beta"]
medianv <- median(riskv)
# Calculate the returns of low and high beta stocks
betalow <- rowMeans(retsna[, names(riskv[riskv<=medianv])])
betahigh <- rowMeans(retsna[, names(riskv[riskv>medianv])])
wealthv <- cbind(betalow, betahigh, betalow - 0.5*betahigh)
wealthv <- xts::xts(wealthv, order.by=datev)
colnames(wealthv) <- c("low_beta", "high_beta", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_beta.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative returns of low and high beta stocks
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Beta Stocks In-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low Beta Stock Portfolio Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{betting against beta} strategy does not have significant \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(wealthv$long_short ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(wealthv$long_short ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
resids <- regmod$residuals
plot.default(x=retvti, y=resids, xlab="VTI", ylab="Low Beta")
title(main="Treynor-Mazuy Market Timing Test\n for Low Beta vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(resids), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_lowbeta_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Low and High Beta Stock Portfolios Out-Of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The low beta stocks selected in-sample also have a higher \emph{Sharpe ratio} in the out-of-sample period than the high beta stocks, although their absolute returns are similar.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the median beta
riskv <- riskretis[, "beta"]
medianv <- median(riskv)
# Calculate the out-of-sample returns of low and high beta stocks
betalow <- rowMeans(retsna[outsample, names(riskv[riskv<=medianv])])
betahigh <- rowMeans(retsna[outsample, names(riskv[riskv>medianv])])
wealthv <- cbind(betalow, betahigh, betalow - 0.5*betahigh)
wealthv <- xts::xts(wealthv, order.by=datev[outsample])
colnames(wealthv) <- c("low_beta", "high_beta", "long_short")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_beta_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the out-of-sample returns of low and high beta stocks
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Low and High Beta Stocks Out-Of-Sample") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Risk Parity} strategy the dollar portfolio allocations are rebalanced daily so that their dollar volatilities remain equal.
      \vskip1ex
      The dollar amount of stock that has unit dollar volatility is equal to the \emph{standardized prices} $\frac{p_i}{\sigma^d_i}$.  Where $\sigma^d_i$ is the dollar volatility.
      \vskip1ex
      So the allocations $a_i$ should be proportional to the \emph{standardized prices}:
      $a_i \propto \frac{p_i}{\sigma^d_i}$, 
      \vskip1ex
      But the \emph{standardized prices} are equal to the inverse of the percentage volatilities $\sigma_i$:
      $\frac{p_i}{\sigma^d_i} = \frac{1}{\sigma_i}$,
      so the allocations $a_i$ are proportional to the inverse of the percentage volatilities $a_i \propto \frac{1}{\sigma_i}$.
      \vskip1ex
      The function \texttt{HighFreq::run\_var()} calculates the trailing variance of a \emph{time series} of returns, by recursively weighting the past variance estimates $\sigma^2_{t-1}$, with the squared differences of the returns minus the trailing means $(r_t - \bar{r}_t)^2$, using the decay factor $\lambda$:
      \begin{flalign*}
        & \bar{r}_t = \lambda \bar{r}_{t-1} + (1-\lambda) r_t \\
        & \sigma^2_t = \lambda \sigma^2_{t-1} + (1-\lambda) (r_t - \bar{r}_t)^2
      \end{flalign*}
      Where $\sigma^2_t$ is the trailing variance at time $t$, and $r_t$ is the \emph{time series} of returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the trailing percentage volatilities
lambda <- 0.6
volat <- HighFreq::run_var(retsna, lambda=lambda)
volat <- sqrt(volat)
# Calculate the risk parity portfolio allocations
alloc <- ifelse(volat > 1e-4, 1/volat, 0)
# Scale allocations to 1 dollar total
allocs <- rowSums(alloc)
alloc <- ifelse(allocs > 0, alloc/allocs, 0)
# Lag the allocations
alloc <- rutils::lagit(alloc)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of the Risk Parity Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy does not perform well for stocks because their correlations are positive.
      \vskip1ex
      The risk parity strategy performs better when the correlations of asset returns are negative, and worse when the correlations are positive.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the wealth of risk parity
wealthrp <- exp(cumsum(rowSums(alloc*retp, na.rm=TRUE)))
# Combined wealth
wealthv <- cbind(wealthpw, wealthrp)
wealthv <- xts::xts(wealthv, datev)
colnames(wealthv) <- c("Price-weighted", "Risk parity")
wealthv <- log(wealthv)
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv),
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_risk_parity_stocks.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the log wealth
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(wealthv[endd], 
  main="Wealth of Price-weighted and Risk Parity Portfolios") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stocks With Low and High Trailing Volatilities}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing volatilities can be used to create low and high volatility portfolios, which are rebalanced daily.
      \vskip1ex
      The low volatility portfolio consists of stocks with trailing volatilities less than the median, and the high portfolio with trailing volatilities greater than the median.
      \vskip1ex
      The low volatility portfolio has higher risk-adjusted returns than the high volatility portfolio, which contradicts the \emph{CAPM} model.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the median volatilities
medianv <- matrixStats::rowMedians(volat)
# Calculate the wealth of low volatility stocks
alloc <- matrix(integer(nrows*nstocks), ncol=nstocks)
alloc[volat <= medianv] <- 1
alloc <- rutils::lagit(alloc)
retlow <- rowSums(alloc*retp, na.rm=TRUE)
wealth_lovol <- exp(cumsum(retlow))
# Calculate the wealth of high volatility stocks
alloc <- matrix(integer(nrows*nstocks), ncol=nstocks)
alloc[volat > medianv] <- 1
alloc <- rutils::lagit(alloc)
rethigh <- rowSums(alloc*retp, na.rm=TRUE)
wealth_hivol <- exp(cumsum(rethigh))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_low_high_vol_stocks.png}
      <<echo=TRUE,eval=FALSE>>=
# Combined wealth
wealthv <- cbind(wealth_lovol, wealth_hivol)
wealthv <- xts::xts(wealthv, datev)
colnames(wealthv) <- c("Low Volatility", "High Volatility")
wealthv <- log(wealthv)
# Calculate the Sharpe and Sortino ratios
retvol <- rutils::diffit(wealthv)
sqrt(252)*sapply(retvol, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the log wealth
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(wealthv[endd], 
  main="Wealth of Low and High Volatility Stocks") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Long-Short Stock Volatility Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Long-Short Volatility} strategy buys the low volatility stock portfolio and shorts the high volatility portfolio.
      \vskip1ex
      The high volatility portfolio returns are multiplied by a factor to compensate for their higher volatility.
      \vskip1ex
      The \emph{Long-Short Volatility} strategy has higher risk-adjusted returns than the price-weighted portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the volatilities of the low and high volatility stocks
volat <- HighFreq::run_var(retvol, lambda=lambda)
volat <- sqrt(volat)
volat[1:2, ] <- 1
colnames(volat) <- c("Low Volatility", "High Volatility")
# Multiply the high volatility portfolio returns by a factor
factv <- volat[, 1]/volat[, 2]
factv <- rutils::lagit(factv)
# Calculate the long-short volatility returns
retls <- (retlow - factv*rethigh)
wealthls <- exp(cumsum(retls))
# Combined wealth
wealthv <- cbind(wealthpw, wealthls)
wealthv <- xts::xts(wealthv, datev)
colnamev <- c("Price-weighted", "Long-Short Vol")
colnames(wealthv) <- colnamev
wealthv <- log(wealthv)
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv),
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_vol_long_short_stocks.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the log wealth
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(wealthv[endd], 
  main="Wealth of Price-weighted and Long-Short Vol Portfolios") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Momentum Strategies}


%%%%%%%%%%%%%%%
\subsection{Portfolio Weight Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Constraints on the portfolio weights are applied to satisfy investment objectives and risk limits.
      \vskip1ex
      Let $w_i$ be the portfolio weights produced by a model, which may not satisfy the constraints, so they must be transformed into new weights: $w^{\prime}_i$.
      \vskip1ex
      For example, the weights can be centered so their sum is equal to \texttt{0}: $\sum_{i=1}^n w^{\prime}_i = 0$, by shifting them by their mean value: 
      \begin{displaymath}
        w^{\prime}_i = w_i - \frac{1}{n} \sum_{i=1}^n w_i
      \end{displaymath}
      The advantage of centering is that it produces portfolios that are more risk neutral - less long or short risk.
      \vskip1ex
      The disadvantage is that it shifts the mean of the weights, and it allows highly leveraged portfolios, with very large positive and negative weights.
     \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load daily S&P500 percentage stock returns.
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns
retp <- returns100["2000/"]
retp[is.na(retp)] <- 0
# Remove stocks with very little data
datev <- zoo::index(retp) # dates
nrows <- NROW(retp) # number of rows
nzeros <- colSums(retp == 0)
sum(nzeros > nrows/2)
retp <- retp[, nzeros < nrows/2]
nstocks <- NCOL(retp) # number of stocks
# Objective function equal to Kelly ratio
objfun <- function(retp) {
  varv <- var(retp)
  if (varv > 0) mean(retp)/varv else 0
}  # end objfun
# Calculate performance statistics for all stocks
perfstat <- sapply(retp, objfun)
perfstat[!is.finite(perfstat)] <- 0
sum(is.na(perfstat))
sum(!is.finite(perfstat))
# Calculate weights proportional to performance statistic
weightm <- perfstat
hist(weightm)
# Center the weights
weightv <- weightm - mean(weightm)
sum(weightv)
sort(weightv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Quadratic Weight Constraint}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Another way of satisfying the constraints is by scaling (multiplying) the weights by a factor.
      \vskip1ex
      Under the \emph{quadratic} constraint, the sum of the \emph{squared} weights is equal to \texttt{1}: $\sum_{i=1}^n w^{\prime 2}_i = 1$, after they are scaled:
      \begin{displaymath}
        w^{\prime}_i = \frac{w_i}{\sqrt{\sum_{i=1}^n w^2_i}}
      \end{displaymath}
      Scaling the weights modifies the portfolio \emph{leverage} (the ratio of the portfolio risk divided by the capital), while maintaining the relative allocations.
      \vskip1ex
      The disadvantage of the \emph{quadratic} constraint is that it can produce portfolios with very low leverage.
      \vskip1ex
     \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Quadratic constraint
weightv <- weightm/sqrt(sum(weightm^2))
sum(weightv^2)
sum(weightv)
weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Linear Weight Constraint}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A widely used constraint is setting the sum of the weights equal to \texttt{1}: ${\sum_{i=1}^n w^{\prime}_i} = 1$, by dividing them by their sum:
      \begin{displaymath}
        w^{\prime}_i = \frac{w_i}{\sum_{i=1}^n w_i}
      \end{displaymath}
      The \emph{linear} constraint is equivalent to distributing a unit of capital among a stock portfolio.  
      \vskip1ex
      The disadvantage of the \emph{linear} constraint is that it has a long risk bias.  
      When the sum of the weights is negative, it switches their sign to positive.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Apply the linear constraint
weightv <- weightm/sum(weightm)
sum(weightv)
weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility Weight Constraint}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights can be scaled to satisfy a volatility target.
      \vskip1ex
      For example, they can be scaled so that the in-sample portfolio volatility $\sigma$ is the same as the volatility of the equal weight portfolio $\sigma_{ew}$:
      \begin{displaymath}
        w^{\prime}_i = \frac{\sigma_{ew}}{\sigma} w_i
      \end{displaymath}
      This produces portfolios with a leverage corresponding to the current market volatility.
      \vskip1ex
      Or the weights can be scaled so that the in-sample portfolio volatility $\sigma$ is equal to a target volatility $\sigma_t$:
      \begin{displaymath}
        w^{\prime}_i = \frac{\sigma_t}{\sigma} w_i
      \end{displaymath}
      This produces portfolios with a volatility close to the target, irrespective of the market volatility.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample portfolio volatility
volis <- sd(drop(retp %*% weightm))
# Calculate equal weight portfolio volatility
volew <- sd(rowMeans(retp))
# Apply the volatility constraint
weightv <- volew*weightm/volis
sqrt(var(drop(retp %*% weightv)))
# Apply the volatility target constraint
volt <- 1e-2
weightv <- volt*weightm/volis
sqrt(var(drop(retp %*% weightv)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Box Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Box constraints limit the individual weights, for example: $0 \leq w_i \leq 1$.
      \vskip1ex
      Box constraints are often applied when constructing long-only portfolios, or when limiting the exposure to certain stocks.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Box constraints
weightv[weightv > 1] <- 1
weightv[weightv < 0] <- 0
weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio weights of \emph{momentum} strategies can be calculated based on the past performance of the assets in many different ways:
      \begin{itemize}
        \item Invest equal dollar amounts in the top \texttt{n} best performing stocks and short the \texttt{n} worst performing stocks,
        \item Invest dollar amounts proportional to the past performance - purchase stocks with positive performance, and short stocks with negative performance,
        \item Apply the weight constraints. 
      \end{itemize}
      The \emph{momentum} weights can then be applied in the out-of-sample interval.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Objective function equal to sum of returns
objfun <- function(retp) sum(retp)
# Objective function equal to Sharpe ratio
objfun <- function(retp) mean(retp)/max(sd(retp), 1e-4)
# Objective function equal to Kelly ratio
objfun <- function(retp) {
  varv <- var(retp)
  if (varv > 0) mean(retp)/varv else 0
}  # end objfun
# Calculate performance statistics for all stocks
perfstat <- sapply(retp, objfun)
perfstat[!is.finite(perfstat)] <- 0
sum(is.na(perfstat))
# Calculate the best and worst performing stocks
perfstat <- sort(perfstat, decreasing=TRUE)
nstocks <- 10
symbolb <- names(head(perfstat, nstocks))
symbolw <- names(tail(perfstat, nstocks))
# Calculate equal weights for the best and worst performing stocks
weightv <- numeric(NCOL(retp))
names(weightv) <- colnames(retp)
weightv[symbolb] <- 1
weightv[symbolw] <- (-1)
# Calculate weights proportional to performance statistic
weightv <- perfstat
# Center weights so sum is equal to 0
weightv <- weightv - mean(weightv)
# Scale weights so sum of squares is equal to 1
weightv <- weightv/sqrt(sum(weightv^2))
# Calculate the momentum portfolio returns
retportf <- retp %*% weightv
# Scale weights so in-sample portfolio volatility is same as equal weight
scalev <- sd(rowMeans(retp))/sd(retportf)
weightv <- scalev*weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a \emph{rolling momentum strategy}, the portfolio is rebalanced periodically and held out-of-sample.
      \vskip1ex
      \emph{Momentum strategies} can be \emph{backtested} by specifying the portfolio rebalancing frequency, the formation interval, and the holding period:
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Specify a portfolio of stocks and their returns,
        \item Calculate the \emph{end points} for portfolio rebalancing,
        \item Define an objective function for calculating the past performance of the stocks,
        \item Calculate the past performance over the \emph{look-back} formation intervals,
        \item Calculate the portfolio weights from the past (in-sample) performance,
        \item Calculate the out-of-sample momentum strategy returns by applying the portfolio weights to the future returns,
        \item Apply a volatility scaling factor to the out-of-sample returns,
        \item Calculate the transaction costs and subtract them from the strategy returns.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Objective function equal to Kelly ratio
objfun <- function(retp) {
  varv <- var(retp)
  if (varv > 0) mean(retp)/varv else 0
}  # end objfun
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retp, interval="weeks")
npts <- NROW(endd)
# Perform loop over the end points
look_back <- 8
pnls <- lapply(2:(npts-1), function(ep) {
  # Select the look-back returns
  startp <- endd[max(1, ep-look_back)]
  retsis <- retp[startp:endd[ep], ]
  # Calculate the best and worst performing stocks in-sample
  perfstat <- sapply(retsis, objfun)
  perfstat[!is.finite(perfstat)] <- 0
  perfstat <- sort(perfstat, decreasing=TRUE)
  symbolb <- names(head(perfstat, nstocks))
  symbolw <- names(tail(perfstat, nstocks))
  # Calculate the momentum weights
  weightv <- numeric(NCOL(retp))
  names(weightv) <- colnames(retp)
  weightv[symbolb] <- 1
  weightv[symbolw] <- (-1)
  # Calculate the in-sample portfolio returns
  retportf <- retsis %*% weightv
  # Scale weights so in-sample portfolio volatility is same as equal weight
  weightv <- weightv*sd(rowMeans(retsis))/sd(retportf)
  # Calculate the momentum portfolio returns
  drop(retp[(endd[ep]+1):endd[ep+1], ] %*% weightv)
})  # end lapply
pnls <- rutils::do_call(c, pnls)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Stock Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The initial stock momentum strategy underperforms the index because of a poor choice of the model parameters.
      \vskip1ex
      The momentum strategy may be improved by a better choice of the model parameters: the length of look-back interval and the number of stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the average of all stock returns
indeks <- rowMeans(retp)
indeks <- xts::xts(indeks, order.by=datev)
colnames(indeks) <- "Index"
# Add initial startup interval to the momentum returns
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Log Stock Index and Momentum Strategy") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomtop()} simulates (backtests) a \emph{momentum strategy} which buys equal dollar amounts of the best performing stocks.
      \vskip1ex
      The function \texttt{btmomtop()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
btmomtop <- function(retp, objfun, look_back=12, rebalf="weeks", nstocks=10, 
  bid_offer=0.0, endd=rutils::calc_endpoints(retp, interval=rebalf), ...) {
  # Perform loop over end points
  npts <- NROW(endd)
  pnls <- lapply(2:(npts-1), function(ep) {
    # Select the look-back returns
    startp <- endd[max(1, ep-look_back)]
    retsis <- retp[startp:endd[ep], ]
    # Calculate the best and worst performing stocks in-sample
    perfstat <- sapply(retsis, objfun)
    perfstat[!is.finite(perfstat)] <- 0
    perfstat <- sort(perfstat, decreasing=TRUE)
    symbolb <- names(head(perfstat, nstocks))
    symbolw <- names(tail(perfstat, nstocks))
    # Calculate the momentum weights
    weightv <- numeric(NCOL(retp))
    names(weightv) <- colnames(retp)
    weightv[symbolb] <- 1
    weightv[symbolw] <- (-1)
    # Calculate the in-sample portfolio returns
    retportf <- retsis %*% weightv
    # Scale weights so in-sample portfolio volatility is same as equal weight
    weightv <- weightv*sd(rowMeans(retsis))/sd(retportf)
    # Calculate the momentum portfolio returns
    drop(retp[(endd[ep]+1):endd[ep+1], ] %*% weightv)
  })  # end lapply
  pnls <- rutils::do_call(c, pnls)
  pnls
}  # end btmomtop
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of Momentum Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Research indicates that the optimal length of the \emph{look-back interval} for momentum is about \texttt{8} to \texttt{12} months.
      \vskip1ex
      The dependence on the length of the \emph{look-back interval} is an example of the \emph{bias-variance tradeoff}.  If the \emph{look-back interval} is too short, the past performance estimates have high \emph{variance}, but if the \emph{look-back interval} is too long, the past estimates have high \emph{bias}.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (known as \emph{p-value hacking}).
      <<echo=TRUE,eval=FALSE>>=
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
endd <- rutils::calc_endpoints(retp, interval="weeks")
pnll <- lapply(look_backs, btmomtop, retp=retp, endd=endd, objfun=objfun)
# Perform parallel loop under Mac-OSX or Linux
library(parallel)  # Load package parallel
ncores <- detectCores() - 1
pnll <- mclapply(look_backs, btmomtop, retp=retp, endd=endd, objfun=objfun, mc.cores=ncores)
sharper <- sqrt(252)*sapply(pnll, function(pnl) mean(pnl)/sd(pnl))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_profile.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot Sharpe ratios of momentum strategies
plot(x=look_backs, y=sharper, t="l",
  main="Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The best stock momentum strategy underperforms the index because of a poor choice of the model type.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      The \emph{backtesting} redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of momentum strategy
whichmax <- which.max(sharper)
look_backs[whichmax]
pnls <- pnll[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Optimal Momentum Strategy for Stocks") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Weighted Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomweight()} simulates (backtests) a \emph{momentum strategy} which buys dollar amounts proportional to the past performance of the stocks.
      \vskip1ex
      The function \texttt{btmomweight()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
btmomweight <- function(retp, objfun, look_back=12, rebalf="weeks", 
  bid_offer=0.0, endd=rutils::calc_endpoints(retp, interval=rebalf), ...) {
  # Perform loop over end points
  npts <- NROW(endd)
  pnls <- lapply(2:(npts-1), function(ep) {
    # Select the look-back returns
    startp <- endd[max(1, ep-look_back)]
    retsis <- retp[startp:endd[ep], ]
    # Calculate weights proportional to performance
    perfstat <- sapply(retsis, objfun)
    perfstat[!is.finite(perfstat)] <- 0
    weightv <- perfstat
    # Calculate the in-sample portfolio returns
    retportf <- retsis %*% weightv
    # Scale weights so in-sample portfolio volatility is same as equal weight
    weightv <- weightv*sd(rowMeans(retsis))/sd(retportf)
    # Calculate the momentum portfolio returns
    retp[(endd[ep]+1):endd[ep+1], ] %*% weightv
  })  # end lapply
  rutils::do_call(c, pnls)
}  # end btmomweight
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Weighted Stock Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The stock momentum strategy produces a similar absolute return as the index, and also a similar Sharpe ratio.
      \vskip1ex
      The advantage of the momentum strategy is that it has a low correlation to stocks, so it can provide significant risk diversification when combined with stocks.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      The \emph{backtesting} redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
pnll <- lapply(look_backs, btmomweight, retp=retp, endd=endd, objfun=objfun)
# Or perform parallel loop under Mac-OSX or Linux
library(parallel)  # Load package parallel
ncores <- detectCores() - 1
pnll <- mclapply(look_backs, btmomweight, retp=retp, endd=endd, objfun=objfun, mc.cores=ncores)
sharper <- sqrt(252)*sapply(pnll, function(pnl) mean(pnl)/sd(pnl))
# Plot Sharpe ratios of momentum strategies
plot(x=look_backs, y=sharper, t="l",
  main="Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_weighted_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of momentum strategy
whichmax <- which.max(sharper)
look_backs[whichmax]
pnls <- pnll[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls, 0.5*(indeks+pnls))
colnames(wealthv)[3] <- "Combined"
cor(wealthv)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red", "green")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Optimal Weighted Momentum Strategy for Stocks") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy With Daily Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a momentum strategy with \emph{daily rebalancing}, the weights are updated every day and the portfolio is rebalanced accordingly.
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions must be used instead of \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the trailing variance
look_back <- 152
varm <- roll::roll_var(retp, width=look_back, min_obs=1)
head(varm)
varm[1, ] <- retp[1, ]^2
# Calculate the trailing Kelly ratio
perfstat <- roll::roll_mean(retp, width=look_back, min_obs=1)
weightv <- perfstat/varm
weightv[varm == 0] <- 0
sum(is.na(weightv))
weightv <- weightv/sqrt(rowSums(weightv^2))
weightv <- rutils::lagit(weightv)
# Calculate the momentum profits and losses
pnls <- rowSums(weightv*retp)
# Calculate the transaction costs
bid_offer <- 0.0
costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
pnls <- (pnls - costs)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_daily.png}
      <<echo=TRUE,eval=FALSE>>=
# Scale the momentum volatility to the equal weight index
indeksd <- sd(indeks)
pnls <- indeksd*pnls/sd(pnls)
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls, 0.5*(indeks+pnls))
colnames(wealthv)[2:3] <- c("Momentum", "Combined")
cor(wealthv)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red", "green")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Daily Momentum Strategy for Stocks") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{btmomdaily()} simulates a momentum strategy with \emph{daily rebalancing}.
      \vskip1ex
      If the argument \texttt{trend = -1} then it simulates a mean-reverting strategy (buys the worst performing stocks and sells the best performing).
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions are preferred to \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomdaily()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomdaily <- function(retp, look_back=252, trend=1, bid_offer=0.0, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate the trailing variance
  varm <- roll::roll_var(retp, width=look_back, min_obs=1)
  varm[1, ] <- retp[1, ]^2
# Calculate the trailing Kelly ratio
  perfstat <- roll::roll_mean(retp, width=look_back, min_obs=1)
  weightv <- perfstat/varm
  weightv[varm == 0] <- 0
  weightv <- weightv/sqrt(rowSums(weightv^2))
  weightv <- rutils::lagit(weightv)
  # Calculate the momentum profits and losses
  pnls <- trend*rowSums(weightv*retp)
  # Calculate the transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
  (pnls - costs)
}  # end btmomdaily
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiple Daily Stock Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily momentum strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The best performing daily stock momentum strategies are with \emph{look-back} parameters between \texttt{140} and \texttt{190} days.
      \vskip1ex
      The momentum strategies do not perform well, especially the ones with a long \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Simulate multiple daily stock momentum strategies
look_backs <- seq(90, 190, by=10)
pnls <- sapply(look_backs, btmomdaily, retp=retp)
# Scale the momentum volatility to the equal weight index
pnls <- apply(pnls, MARGIN=2, function(pnl) indeksd*pnl/sd(pnl))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev)
tail(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_daily_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily stock momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Daily Stock Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot daily stock momentum strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Daily Stock Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The daily ETF momentum strategy can be improved by introducing a \emph{holding period} for the portfolio.
      \vskip1ex
      Instead of holding the portfolio for only a day, its held for several days and then liquidated.  So several portfolios are held at the same time.
      \vskip1ex
      This is equivalent to averaging the portfolio weights over several days from the past.
      \vskip1ex
      The best length of the \emph{holding period} depends on the \emph{bias-variance tradeoff}.
      \vskip1ex
      If the \emph{holding period} is too short then the weights have too much day-over-day \emph{variance}.
      \vskip1ex
      If the \emph{holding period} is too long then the weights have too much \emph{bias} (they are stale).
      \vskip1ex
      The optimal length of the \emph{holding period} can be determined by cross-validation (backtesting).
      \vskip1ex
      The function \texttt{btmomdailyhold()} simulates a momentum strategy with \emph{daily rebalancing} with a holding period.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomdailyhold <- function(retp, look_back=252, holdp=5, trend=1, bid_offer=0.0, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate the trailing variance
  varm <- roll::roll_var(retp, width=look_back, min_obs=1)
  varm[1, ] <- retp[1, ]^2
  # Calculate the trailing Kelly ratio
  perfstat <- roll::roll_mean(retp, width=look_back, min_obs=1)
  weightv <- perfstat/varm
  weightv[varm == 0] <- 0
  weightv <- weightv/sqrt(rowSums(weightv^2))
  # Average the weights over holding period
  weightv <- roll::roll_mean(weightv, width=holdp, min_obs=1)
  weightv <- rutils::lagit(weightv)
  # Calculate the momentum profits and losses
  pnls <- trend*rowSums(weightv*retp)
  # Calculate the transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
  (pnls - costs)
}  # end btmomdailyhold
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiple Daily Momentum Strategies with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily momentum strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of holding periods.
      \vskip1ex
      The daily momentum strategies with a multi-period holding period perform better than with daily holding.
      \vskip1ex
      The daily momentum strategy with a longer holding period performs better than with a daily holding.
      \vskip1ex
      The reason is that a longer holding period averages the weights and reduces their variance.  But this also increases their bias, so there's an optimal holding period for an optimal bias-variance tradeoff.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over holding periods
holdpv <- seq(2, 11, by=2)
pnls <- sapply(holdpv, btmomdailyhold, look_back=180, retp=retp)
# Scale the momentum volatility to the equal weight index
pnls <- apply(pnls, MARGIN=2, function(pnl) indeksd*pnl/sd(pnl))
colnames(pnls) <- paste0("holding=", holdpv)
pnls <- xts::xts(pnls, datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_daily_hold.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph of daily stock momentum strategies with holding period
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Daily Stock Momentum Strategies with Holding Period") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot of daily stock momentum strategies with holding period
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Daily Stock Momentum Strategies with Holding Period")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Mean Reverting} Stock Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{mean reverting} stock momentum strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} momentum strategies for the stock constituents perform the best for short \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} momentum strategies had their best performance prior to and during the \texttt{2008} financial crisis.
      \vskip1ex
      If the argument \texttt{trend = -1} then the function \texttt{btmomdaily()} simulates a mean-reverting strategy (buys the worst performing stocks and sells the best performing).
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(3, 20, by=2)
pnls <- sapply(look_backs, btmomdaily, retp=retp, trend=(-1))
# Scale the momentum volatility to the equal weight index
pnls <- apply(pnls, MARGIN=2, function(pnl) indeksd*pnl/sd(pnl))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_stocks_revert.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of mean reverting daily stock momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Mean Reverting Daily Stock Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot mean reverting daily stock momentum strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Mean Reverting Daily Stock Momentum Strategies")
legend("topleft", legend=colnames(pnls),
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Online Daily Momentum Strategy Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{btmomrun()} simulates a momentum strategy with \emph{daily rebalancing}.
      \vskip1ex
      If the argument \texttt{trend = -1} then it simulates a mean-reverting strategy (buys the worst performing stocks and sells the best performing).
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions are preferred to \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      The function \texttt{btmomrun()} can be used to find the best choice of \emph{momentum strategy} parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomrun <- function(retp, lambda=0.9, trend=1, holdp=5, bid_offer=0.0, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate the trailing variance
  varm <- HighFreq::run_var(retp, lambda=lambda)
  # Calculate the trailing Kelly ratio
  perfstat <- HighFreq::run_mean(retp, lambda=lambda)
  weightv <- perfstat/varm
  weightv[varm == 0] <- 0
  weightv <- weightv/sqrt(rowSums(weightv^2))
  # Average the weights over holding period
  weightv <- HighFreq::roll_mean(weightv, look_back=holdp)
  weightv <- rutils::lagit(weightv)
  # Calculate the momentum profits and losses
  pnls <- trend*rowSums(weightv*retp)
  # Calculate the transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
  (pnls - costs)
}  # end btmomrun
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The MTUM Momentum ETF}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{MTUM} ETF is an actively managed ETF which follows a momentum strategy for stocks.
      \vskip1ex
      The \emph{MTUM} ETF has a slightly higher absolute return than the \emph{VTI} ETF, but it has a slightly lower Sharpe ratio.
      \vskip1ex
      The weak performance of the \emph{MTUM} ETF demonstrates that it's difficult to implement a successful momentum strategy for individual stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the scaled prices of VTI vs MTUM ETF
wealthv <- na.omit(rutils::etfenv$returns[, c("VTI", "MTUM")])
colnames(wealthv) <- c("VTI", "MTUM")
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_mtum.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the scaled prices of VTI vs MTUM ETF
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="VTI vs MTUM ETF") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Weights for PCA Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The principal components are portfolios of stocks and can be traded directly as if they were single stocks.
      \vskip1ex
      The returns of the PCA portfolios are orthogonal to each other - the correlations of returns are equal to zero.
      \vskip1ex
      If the returns are orthogonal and if the momentum weights are proportional to the \emph{Kelly ratios} (the returns divided by their variance):
      \begin{displaymath}
        w_i = \frac{\bar{r}_i}{\sigma^2_i}
      \end{displaymath}
      Then the momentum weights are equal to the \emph{maximum Sharpe} portfolio weights, equal to: $\mathbb{C}^{-1} \bar{r}$,  where $\mathbb{C}$ is the covariance matrix (which is diagonal in this case).
      \vskip1ex
      So the momentum strategy for assets with orthogonal returns is equivalent to an optimal portfolio strategy.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the PCA weights for scaled returns
retscaled <- lapply(retp, function(x) x/sd(x))
retscaled <- do.call(cbind, retscaled)
pcad <- eigen((t(retscaled) %*% retscaled)/(nrows-1))
pcaw <- pcad$vectors
rownames(pcaw) <- colnames(retp)
sort(-pcaw[, 1], decreasing=TRUE)
sort(pcaw[, 2], decreasing=TRUE)
round((t(pcaw) %*% pcaw)[1:5, 1:5], 4)
# Calculate the PCA time series from stock returns using PCA weights
retspca <- retscaled %*% pcaw
round((t(retspca) %*% retspca)[1:5, 1:5], 4)
retspca <- xts::xts(retspca, order.by=datev)
# Calculate the PCA using prcomp()
pcad <- prcomp(retscaled, center=FALSE, scale=FALSE)
all.equal(pcad$x, retspca, check.attributes=FALSE)
library(microbenchmark)
summary(microbenchmark(
  prcomp=prcomp(retscaled, center=FALSE, scale=FALSE),
  eigen=eigen((t(retscaled) %*% retscaled)/(NROW(retscaled)-1)),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy for PCA Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy can be improved by applying it to PCA portfolios.
      \vskip1ex
      The lowest order principal components exhibit greater trending (positive autocorrelations), so they have better momentum strategy performance than individual stocks.
      <<echo=TRUE,eval=FALSE>>=
# Simulate daily PCA momentum strategies for multiple look-backs
dimax <- 11
look_backs <- seq(80, 170, by=10)
pnll <- mclapply(look_backs, btmomdaily, retp=retspca[, 1:dimax], 
   mc.cores=ncores)
pnll <- lapply(pnll, function(pnl) indeksd*pnl/sd(pnl))
pnls <- do.call(cbind, pnll)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev)
# Plot Sharpe ratios of momentum strategies
sharper <- sqrt(252)*sapply(pnls, function(pnl) mean(pnl)/sd(pnl))
plot(x=look_backs, y=sharper, t="l",
  main="PCA Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_pca_daily_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retspca, interval="weeks")
# Plot dygraph of daily S&P500 momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Daily PCA Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal PCA Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The PCA momentum strategy using only the lowest order principal components outperforms the index.  
      \vskip1ex
      But this is thanks to using the in-sample principal components.
      \vskip1ex
      The best performing PCA momentum strategy has a relatively short look-back interval of only \texttt{100} days, so it's able to quickly adjust to changes in market direction.
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of PCA momentum strategy
whichmax <- which.max(sharper)
look_backs[whichmax]
pnls <- pnll[[whichmax]]
pnls <- xts::xts(pnls, order.by=datev)
colnames(pnls) <- "Strategy"
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_pca_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of stock index and PCA momentum strategy
colorv <- c("blue", "red")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Optimal PCA Momentum Strategy") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Mean Reverting} PCA Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{mean reverting} momentum strategy performs well for the higher order principal components.
      \vskip1ex
      This is because the higher order principal components exhibit greater mean reversion (negative autocorrelations) than individual stocks.
      \vskip1ex
      The \emph{mean reverting} PCA momentum strategies perform the best for short look-back intervals of about \texttt{5 - 10} days.
      \vskip1ex
      The \emph{mean reverting} momentum strategies had their best performance in periods of high volatility, especially prior to and during the \texttt{2008} financial crisis.
      \vskip1ex
      The backtest simulations don't account for transaction costs, so they should be interpreted only as an indication of potential performance.
      \vskip1ex
      If the argument \texttt{trend = -1} then the function \texttt{btmomdaily()} simulates a mean-reverting strategy (buys the worst performing stocks and sells the best performing).
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(3, 20, by=2)
pnls <- sapply(look_backs, btmomdaily, trend=(-1),
  retp=retspca[, (dimax+1):NCOL(retspca)])
colnames(pnls) <- paste0("look_back=", look_backs)
# Scale the momentum volatility to the equal weight index
pnls <- apply(pnls, MARGIN=2, function(pnl) indeksd*pnl/sd(pnl))
pnls <- xts::xts(pnls, datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_pca_revert.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily PCA momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Mean Reverting Daily PCA Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot daily PCA momentum strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Mean Reverting Daily Stock Momentum Strategies")
legend("topleft", legend=colnames(pnls),
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{PCA Momentum Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The principal component weights are calculated in-sample and applied out-of-sample.
      \vskip1ex
      The performance is much lower than in-sample, but it's still positive.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
cutoff <- nrows %/% 2
datev[cutoff]
insample <- 1:cutoff
outsample <- (cutoff + 1):nrows
# Calculate the PCA weights in-sample
pcad <- prcomp(retp[insample], center=FALSE, scale=TRUE)
# Calculate the out-of-sample PCA time series
retscaled <- lapply(retp, function(x) x[outsample]/sd(x[insample]))
retscaled <- do.call(cbind, retscaled)
retspca <- xts::xts(retscaled %*% pcad$vectors, order.by=datev[outsample])
# Simulate daily PCA momentum strategies for multiple look-backs
dimax <- 11
look_backs <- seq(60, 120, by=10)
pnll <- mclapply(look_backs, btmomdaily, 
   retp=retspca[, 1:dimax], mc.cores=ncores)
pnll <- lapply(pnll, function(pnl) indeksd*pnl/sd(pnl))
pnls <- do.call(cbind, pnll)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev[outsample])
# Plot Sharpe ratios of momentum strategies
sharper <- sqrt(252)*sapply(pnls, function(pnl) mean(pnl)/sd(pnl))
plot(x=look_backs, y=sharper, t="l",
  main="PCA Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_pca_daily_mult_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retspca, interval="weeks")
# Plot dygraph of daily out-of-sample PCA momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Daily Out-of-Sample PCA Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Mean Reverting} PCA Momentum Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The principal component weights are calculated in-sample and applied out-of-sample.
      \vskip1ex
      The performance is much lower than in-sample, but it's still positive.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(3, 20, by=2)
pnls <- sapply(look_backs, btmomdaily, trend=(-1),
  retp=retspca[, (dimax+1):NCOL(retspca)])
colnames(pnls) <- paste0("look_back=", look_backs)
# Scale the momentum volatility to the equal weight index
pnls <- apply(pnls, MARGIN=2, function(pnl) indeksd*pnl/sd(pnl))
pnls <- xts::xts(pnls, datev[outsample])
# Plot Sharpe ratios of momentum strategies
sharper <- sqrt(252)*sapply(pnls, function(pnl) mean(pnl)/sd(pnl))
plot(x=look_backs, y=sharper, t="l",
  main="PCA Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_pca_daily_mult_outsample_revert.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retspca, interval="weeks")
# Plot dygraph of daily S&P500 momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Mean Reverting Daily Out-of-Sample PCA Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Momentum Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the momentum strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Research indicates that the optimal length of the \emph{look-back interval} for momentum is about \texttt{4} to \texttt{10} months.
      \vskip1ex
      The dependence on the length of the \emph{look-back interval} is an example of the \emph{bias-variance tradeoff}.  If the \emph{look-back interval} is too short, the past performance estimates have high \emph{variance}, but if the \emph{look-back interval} is too long, the past estimates have high \emph{bias}.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (known as \emph{p-value hacking}).
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf_profile.png}
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
retp <- na.omit(rutils::etfenv$returns[, symbolv])
datev <- zoo::index(retp)
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retp, interval="weeks")
npts <- NROW(endd)
# Perform backtests for vector of look-back intervals
look_backs <- seq(3, 15, by=1)
pnll <- lapply(look_backs, btmomweight, retp=retp, endd=endd, objfun=objfun)
sharper <- sqrt(252)*sapply(pnll, function(pnl) mean(pnl)/sd(pnl))
# Plot Sharpe ratios of momentum strategies
plot(x=look_backs, y=sharper, t="l",
  main="Momentum Sharpe as Function of Look-back Interval",
  xlab="look-back (months)", ylab="Sharpe")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Momentum Strategy for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy for ETFs produces a higher absolute return and also a higher Sharpe ratio than the static \emph{All-Weather} portfolio.
      \vskip1ex
      The momentum strategy for ETFs also has a very low correlation to the static \emph{All-Weather} portfolio.
      \vskip1ex
      The momentum strategy works better for assets that are not correlated or are even anti-correlated.  
      \vskip1ex
      The momentum strategy also works better for portfolios than for individual stocks because of risk diversification.  
      \vskip1ex
      Portfolios of stocks can also be selected so that they are more autocorrelated - more trending - they have higher signal-to-noise ratios - larger Hurst exponents.
      <<echo=TRUE,eval=FALSE>>=
# Calculate best pnls of momentum strategy
whichmax <- which.max(sharper)
look_backs[whichmax]
pnls <- pnll[[whichmax]]
pnls <- c(rowMeans(retp[endd[1]:endd[2], ]), pnls)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- retp %*% weightsaw
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(all_weather, pnls, 0.5*(all_weather+pnls))
colnames(wealthv) <- c("All-weather", "Strategy", "Combined")
cor(wealthv)
wealthv <- xts::xts(wealthv, order.by=datev)
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of stock index and momentum strategy
colorv <- c("blue", "red", "green")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Momentum Strategy and All-weather for ETFs") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Series of Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In momentum strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way momentum strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the momentum weights
look_back <- look_backs[whichmax]
weightv <- lapply(1:(npts-1), function(ep) {
  # Select the look-back returns
  startp <- endd[max(1, ep-look_back)]
  retsis <- retp[startp:endd[ep], ]
  # Calculate weights proportional to performance
  perfstat <- sapply(retsis, objfun)
  weightv <- drop(perfstat)
  # Scale weights so in-sample portfolio volatility is same as equal weight
  retportf <- retsis %*% weightv
  weightv*sd(rowMeans(retsis))/sd(retportf)
})  # end lapply
weightv <- rutils::do_call(rbind, weightv)
# Plot the momentum weights
retvti <- cumsum(retp$VTI)
datav <- cbind(retvti[endd], weightv)
colnames(datav) <- c("VTI", paste0(colnames(retp), "_weight"))
zoo::plot.zoo(datav, xlab=NULL, main="Momentum Weights")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF betas
betasetf <- sapply(retp, function(x)
  cov(retp$VTI, x)/var(retp$VTI))
# Momentum beta is equal weights times ETF betas
betav <- weightv %*% betasetf
betav <- xts::xts(betav, order.by=datev[endd])
colnames(betav) <- "momentum_beta"
datav <- cbind(retvti[endd], betav)
zoo::plot.zoo(datav, main="Momentum Beta & VTI Price", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the momentum strategy has some \emph{market timing} skill.
      <<echo=TRUE,eval=FALSE>>=
# Merton-Henriksson test
retvti <- retp$VTI
predv <- cbind(VTI=retvti, 0.5*(retvti+abs(retvti)), retvti^2)
colnames(predv)[2:3] <- c("merton", "treynor")
regmod <- lm(pnls ~ VTI + merton, data=predv); summary(regmod)
# Treynor-Mazuy test
regmod <- lm(pnls ~ VTI + treynor, data=predv); summary(regmod)
# Plot residual scatterplot
resids <- regmod$residuals
plot.default(x=retvti, y=resids, xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy Market Timing Test\n for Momentum vs VTI", line=0.5)
# Plot fitted (predicted) response values
coefreg <- summary(regmod)$coeff
fittedv <- regmod$fitted.values - coefreg["VTI", "Estimate"]*retvti
tvalue <- round(coefreg["treynor", "t value"], 2)
points.default(x=retvti, y=fittedv, pch=16, col="red")
text(x=0.0, y=max(resids), paste("Treynor test t-value =", tvalue))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness of Momentum Strategy Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most assets with \emph{positive returns} suffer from \emph{negative skewness}.
      \vskip1ex
      The momentum strategy returns have more positive skewness compared to the negative skewness of \emph{VTI}.
      \vskip1ex
      The momentum strategy is a genuine \emph{market anomaly}, because it has both positive returns and positive skewness.
      <<echo=TRUE,eval=FALSE>>=
# Standardize the returns
pnlsd <- (pnls-mean(pnls))/sd(pnls)
retvti <- (retvti-mean(retvti))/sd(retvti)
# Calculate skewness and kurtosis
apply(cbind(pnlsd, retvti), 2, function(x)
  sapply(c(skew=3, kurt=4),
    function(e) sum(x^e)))/NROW(retvti)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_distr.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate kernel density of VTI
densvti <- density(retvti)
# Plot histogram of momentum returns
hist(pnlsd, breaks=80,
  main="Momentum and VTI Return Distributions (standardized)",
  xlim=c(-4, 4), ylim=range(densvti$y), xlab="", ylab="", freq=FALSE)
# Draw kernel density of histogram
lines(density(pnlsd), col='red', lwd=2)
lines(densvti, col='blue', lwd=2)
# Add legend
legend("topright", inset=0.0, cex=1.0, title=NULL,
       leg=c("Momentum", "VTI"), bty="n", y.intersp=0.5, 
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Momentum with the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The momentum strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the momentum strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the momentum strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Combine momentum strategy with all-weather
wealthv <- cbind(pnls, all_weather, 0.5*(pnls + all_weather))
colnames(wealthv) <- c("momentum", "all_weather", "combined")
wealthv <- xts::xts(wealthv, datev)
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Calculate strategy correlations
cor(wealthv)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_combined.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot ETF momentum strategy combined with All-Weather
dygraphs::dygraph(cumsum(wealthv)[endd], main="ETF Momentum Strategy Combined with All-Weather") %>%
  dyOptions(colors=c("red", "blue", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("green", "blue", "red")
quantmod::chart_Series(wealthv, theme=plot_theme,
             name="ETF Momentum Strategy Combined with All-Weather")
legend("topleft", legend=colnames(wealthv),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy for ETFs With Daily Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a momentum strategy with \emph{daily rebalancing}, the weights are updated every day and the portfolio is rebalanced accordingly.
      \vskip1ex
      A momentum strategy with \emph{daily rebalancing} requires more computations so compiled \texttt{C++} functions must be used instead of \texttt{apply()} loops.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily rebalancing} performs worse than the strategy with \emph{monthly rebalancing} because of the daily variance of the weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the trailing variance
look_back <- 152
varm <- roll::roll_var(retp, width=look_back, min_obs=1)
head(varm)
varm[1, ] <- retp[1, ]^2
# Calculate the trailing Kelly ratio
perfstat <- roll::roll_mean(retp, width=look_back, min_obs=1)
weightv <- perfstat/varm
weightv[varm == 0] <- 0
sum(is.na(weightv))
weightv <- weightv/sqrt(rowSums(weightv^2))
weightv <- rutils::lagit(weightv)
# Calculate the momentum profits and losses
pnls <- rowSums(weightv*retp)
# Calculate the transaction costs
bid_offer <- 0.0
costs <- 0.5*bid_offer*rowSums(abs(rutils::diffit(weightv)))
pnls <- (pnls - costs)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf_daily.png}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- retp %*% weightsaw
# Scale the momentum volatility to all_weather
pnls <- sd(all_weather)*pnls/sd(pnls)
# Calculate the wealth of momentum returns
wealthv <- xts::xts(cbind(all_weather, pnls), order.by=datev)
colnames(wealthv) <- c("All-Weather", "Momentum")
cor(wealthv)
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealthv)[endd], main="Daily Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiple Daily ETF Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The best performing daily ETF \emph{momentum} strategies are with \emph{look-back} parameters between \texttt{100} and \texttt{120} days.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a long \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a daily ETF momentum strategy
source("/Users/jerzy/Develop/lecture_slides/scripts/back_test.R")
pnls <- btmomdaily(retp=retp, look_back=152, bid_offer=bid_offer)
# Perform sapply loop over look_backs
look_backs <- seq(90, 190, by=10)
pnls <- sapply(look_backs, btmomdaily,
  retp=retp, bid_offer=bid_offer)
# Scale the momentum volatility to all_weather
pnls <- apply(pnls, MARGIN=2, 
  function(pnl) sd(all_weather)*pnl/sd(pnl))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev)
tail(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf_daily_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Daily ETF Momentum Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot daily ETF momentum strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls)[endd],
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Daily rank simple Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      The daily ETF momentum strategy can be improved by introducing a holding period for the portfolio.
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
btmomdaily <- function(retp, look_back=252, holdp=5, trend=1, bid_offer=0.0, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  
  posit <- matrixStats::rowRanks(retp)
  posit <- (posit - rowMeans(posit))
  posit <- HighFreq::lagit(posit, lagg=1)
  trend*rowMeans(posit*retp)
  
}  # end btmomdaily

# Load ETF data
symbolv <- rutils::etfenv$symbolv
symbolv <- symbolv[!(symbolv %in% c("TLT", "IEF", "MTUM", "QUAL", "VLUE", "USMV"))]
retp <- rutils::etfenv$returns[, symbolv]
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)

# Load S&P500 data
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
retp <- retp["2000/"]
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)
nstocks <- NCOL(retp)
retp <- retp[, !(retp[nstocks %/% 10, ] == 0)]


pnls <- btmomdaily(retp=retp, trend=(-1))
pnls <- xts::xts(pnls, datev)
colnames(pnls) <- "PnL"
dygraphs::dygraph(cumsum(pnls), main="Daily Momentum Strategy") %>%
  dyOptions(colors="blue", strokeWidth=2) %>%
  dyLegend(show="always", width=500)

@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Backtesting Daily rank simple Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF momentum strategies can be backtested by calling the function \texttt{btmomdaily()} in a loop over a vector of holding periods.
      \vskip1ex
      The daily momentum strategies with a holding period perform much better.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(50, 300, by=50)
pnls <- sapply(look_backs, btmomdaily,
  retp=retp, bid_offer=bid_offer)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, datev)

# Perform sapply loop over holding periods
holdpv <- seq(2, 11, by=2)
pnls <- sapply(holdpv, btmomdaily, look_back=120, retp=retp)
colnames(pnls) <- paste0("holding=", holdpv)
pnls <- xts::xts(pnls, datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf_daily_hold.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily ETF Momentum Strategies with Holding Period") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot daily ETF momentum strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtesting the Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is simulating the performance of a investment strategy on historical data.
      \vskip1ex
      \emph{Backtesting} is a type of \emph{cross-validation} applied to investment strategies.
      \vskip1ex
      \emph{Backtesting} is performed by \emph{training} the model on past data and \emph{testing} it on future out-of-sample data.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{past}), and the model forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{future}).
      \vskip1ex
      The out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{future} returns by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The momentum returns are lagged so that they are attached to the end of the future interval, instead of at its beginning.
      <<echo=TRUE,eval=FALSE>>=
# Calculate future out-of-sample performance
retsos <- apply(look_fwds, 1, function(ep) {
  sapply(retp[ep[1]:ep[2]], sum)
})  # end sapply
retsos <- t(retsos)
retsos[is.na(retsos)] <- 0
tail(retsos)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the momentum pnls
pnls <- rowSums(weightv*retsos)
# Lag the future and momentum returns to proper dates
retsos <- rutils::lagit(retsos)
pnls <- rutils::lagit(pnls)
# The momentum strategy has low correlation to stocks
cor(pnls, retsos)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- retsos %*% weightsaw
# Calculate the wealth of momentum returns
wealthv <- xts::xts(cbind(all_weather, pnls), order.by=datev)
colnames(wealthv) <- c("All-Weather", "Momentum")
cor(wealthv)
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealthv)[endd], main="Monthly Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Momentum Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Momentum} strategies can be \emph{backtested} by specifying the portfolio rebalancing frequency, the formation interval, and the holding period:
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Specify a portfolio of \emph{ETFs}, stocks, or other assets, and a time series of their returns,
        \item Specify \emph{end points} for the portfolio rebalancing frequency,
        \item Specify \emph{look-back} intervals for portfolio formation, and \emph{look-forward} intervals for portfolio holding, 
        \item Specify a performance function to calculate the past performance of the assets,
        \item Calculate the past performance over the \emph{look-back} formation intervals,
        \item Calculate the portfolio weights from the past (in-sample) performance,
        \item Calculate the future returns over the \emph{look-forward} holding intervals,
        \item Calculate the out-of-sample momentum strategy returns by applying the portfolio weights to the future returns,
        \item Calculate the transaction costs and subtract them from the strategy returns.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
retp <- rutils::etfenv$returns[, symbolv]
retp <- na.omit(retp)
# Or, select rows with IEF data
# retp <- retp[zoo::index(rutils::etfenv$IEF)]
# Copy over NA values
# retp[1, is.na(retp[1, ])] <- 0
# retp <- zoo::na.locf(retp, na.rm=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Look-back and Look-forward Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performance aggregations are be calculated over a vector of overlapping in-sample \emph{look-back} intervals attached at \emph{end points}.
      \vskip1ex
      For example, aggregations at monthly \emph{end points} over overlapping 12-month \emph{look-back} intervals.
      \vskip1ex
      An example of a data aggregation are the cumulative past returns at each \emph{end point}.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of \emph{end points} in the \emph{look-back} interval.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the length of the \emph{look-back} interval.
      \vskip1ex
      The \emph{look-back} intervals are spanned by the vectors of \emph{start points} and \emph{end points}.
      \vskip1ex
      Performance aggregations are also be calculated over non-overlapping out-of-sample \emph{look-forward} intervals.
      \vskip1ex
      The \emph{look-forward} intervals should not overlap with the \emph{look-back} intervals, in order to avoid data snooping.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end of month end points
endd <- rutils::calc_endpoints(retp, interval="weeks")
endd <- endd[-1]
npts <- NROW(endd)
datev <- zoo::index(retp)[endd]
# Start points equal end points lagged by 12-month look-back interval
look_back <- 12
startp <- c(rep_len(1, look_back),
  endd[1:(npts - look_back + 1)])
# Calculate matrix of look-back intervals
look_backs <- cbind(startp, endd)
colnames(look_backs) <- c("start", "end")
# Calculate matrix of look-forward intervals
look_fwds <- cbind(endd + 1, rutils::lagit(endd, -1))
look_fwds[npts, ] <- endd[npts]
colnames(look_fwds) <- c("start", "end")
# Inspect the intervals
head(cbind(look_backs, look_fwds))
tail(cbind(look_backs, look_fwds))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtest of the Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} momentum strategy returns are calculated by multiplying the weights times the future returns.
      \vskip1ex
      The \emph{transaction costs} are equal to half the \emph{bid-offer spread} $\delta$ times the absolute value of the traded dollar amounts of the \emph{risky assets}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the momentum profits and losses (pnls)
pnls <- rowSums(weightv*retsos)
# Lag the momentum returns and weights
# to correspond with end of future interval
pnls <- rutils::lagit(pnls)
weightv <- rutils::lagit(weightv)
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate the transaction costs
wealthv <- cumsum(pnls)
costs <- 0.5*bid_offer*wealth*rowSums(abs(rutils::diffit(weightv)))
wealthv <- cumsum(pnls - costs)
datev <- zoo::index(retp[endd])
wealthv <- xts::xts(wealthv, datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/momentum_etf.png}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
retsaw <- retp %*% weightsaw
wealthaw <- cumsum(retsaw)
wealthaw <- xts::xts(wealthaw[endd], datev)
# Plot the Momentum strategy and benchmark
wealthv <- cbind(wealthv, wealthaw)
colnames(wealthv) <- c("Momentum Strategy", "Benchmark")
dygraphs::dygraph(wealthv, main="Momentum Strategy") %>%
  dyAxis("y", label="Benchmark", independentTicks=TRUE) %>%
  dyAxis("y2", label="Momentum Strategy", independentTicks=TRUE) %>%
  dySeries(name="Momentum Strategy", axis="y2", label="Momentum Strategy", strokeWidth=2, col="red") %>%
  dySeries(name="Benchmark", axis="y", label="Benchmark", strokeWidth=2, col="blue") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtesting Functional for ETF Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional
btmomweight <- function(retp,
                      objfun=function(retp) (sum(retp)/sd(retp)),
                      look_back=12, rebalf="weeks", bid_offer=0.0,
                      endd=rutils::calc_endpoints(retp, interval=rebalf)[-1],
                      with_weights=FALSE, ...) {
  stopifnot("package:rutils" %in% search() || require("rutils", quietly=TRUE))
  # Define look-back and look-forward intervals
  npts <- NROW(endd)
  startp <- c(rep_len(1, look_back), endd[1:(npts-look_back)])
  # Calculate look-back intervals
  look_backs <- cbind(startp, endd)
  # Calculate look-forward intervals
  look_fwds <- cbind(endd + 1, rutils::lagit(endd, -1))
  look_fwds[npts, ] <- endd[npts]
  # Calculate past performance over look-back intervals
  perfstat <- t(apply(look_backs, 1, function(ep) sapply(retp[ep[1]:ep[2]], objfun)))
  perfstat[is.na(perfstat)] <- 0
  # Calculate future performance
  retsos <- t(apply(look_fwds, 1, function(ep) sapply(retp[ep[1]:ep[2]], sum)))
  retsos[is.na(retsos)] <- 0
  # Scale weights so sum of squares is equal to 1
  weightv <- perfstat
  weightv <- weightv/sqrt(rowSums(weightv^2))
  weightv[is.na(weightv)] <- 0  # Set NA values to zero
  # Calculate the momentum profits and losses
  pnls <- rowSums(weightv*retsos)
  # Calculate the transaction costs
  costs <- 0.5*bid_offer*cumsum(pnls)*rowSums(abs(rutils::diffit(weightv)))
  pnls <- (pnls - costs)
  if (with_weights)
    rutils::lagit(cbind(pnls, weightv))
  else
    rutils::lagit(pnls)
}  # end btmomweight
      @
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Portfolio Optimization Strategies}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy invests in the best performing portfolio in the past \emph{in-sample} interval, expecting that it will continue performing well \emph{out-of-sample}.
      \vskip1ex
      The \emph{portfolio optimization} strategy consists of:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculating the maximum Sharpe ratio portfolio weights in the \emph{in-sample} interval,
        \item Applying the weights and calculating the portfolio returns in the \emph{out-of-sample} interval.
      \end{enumerate}
      The optimal portfolio weights $\mathbf{w}$ are equal to the past in-sample excess returns $\mu = \mathbf{r} - r_f$ (in excess of the risk-free rate $r_f$) multiplied by the inverse of the covariance matrix $\mathbb{C}$:
      \begin{displaymath}
        \mathbf{w} = \mathbb{C}^{-1} \mu
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Select all the ETF symbols except "VXX", "SVXY" "MTUM", "QUAL", "VLUE", and "USMV"
symbolv <- colnames(rutils::etfenv$returns)
symbolv <- symbolv[!(symbolv %in% c("VXX", "SVXY", "MTUM", "QUAL", "VLUE", "USMV"))]
# Extract columns of rutils::etfenv$returns and overwrite NA values
retp <- rutils::etfenv$returns[, symbolv]
nstocks <- NCOL(retp)
# retp <- na.omit(retp)
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)
datev <- zoo::index(retp)
# Returns in excess of risk-free rate
riskf <- 0.03/252
retx <- (retp - riskf)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etf_weights_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
retsis <- retp["/2014"]
invmat <- MASS::ginv(cov(retsis))
weightv <- invmat %*% colMeans(retx["/2014"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retp)
# Plot portfolio weights
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
barplot(sort(weightv), main="Maximum Sharpe Weights", cex.names=0.7)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The in-sample performance of the optimal portfolio is much better than the equal weight portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
indeks <- xts::xts(rowMeans(retsis), zoo::index(retsis))
insample <- insample*sd(indeks)/sd(insample)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etf_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- cbind(indeks, insample)
colnames(pnls) <- c("Equal Weight", "Optimal")
endd <- rutils::calc_endpoints(pnls, interval="weeks")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="In-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The out-of-sample performance of the optimal portfolio is not nearly as good as in-sample.
      \vskip1ex
      Combining the optimal portfolio with the equal weight portfolio produces and even better performing portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate out-of-sample portfolio returns
retsos <- retp["2015/"]
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retsos), zoo::index(retsos))
outsample <- outsample*sd(indeks)/sd(outsample)
pnls <- cbind(indeks, outsample, (outsample + indeks)/2)
colnames(pnls) <- c("Equal Weight", "Optimal", "Combined")
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(pnls, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etf_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
endd <- rutils::calc_endpoints(pnls, interval="weeks")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Out-of-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for ETFs is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy underperforms in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
invmat <- MASS::ginv(cov(retsis))
weightv <- invmat %*% colMeans(retx["/2014"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retp)
# Calculate in-sample portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
# Calculate out-of-sample portfolio returns
retsos <- retp["2015/"]
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etfs_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
indeks <- xts::xts(rowMeans(retp), datev)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
endd <- rutils::calc_endpoints(pnls, interval="weeks")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Out-of-sample Optimal Portfolio Returns for ETFs") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularized Inverse of Singular Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The inverse of the covariance matrix of returns $\mathbb{C}$ can be calculated from its \emph{eigenvalues} $\mathbb{D}$ and its \emph{eigenvectors} $\mathbb{O}$:
      \begin{displaymath}
        \mathbb{C}^{-1} = \mathbb{O} \, \mathbb{D}^{-1} \, \mathbb{O}^T
      \end{displaymath}
      If the number of time periods of returns (rows) is less than the number of stocks (columns), then some of the higher order eigenvalues are zero, and the above covariance matrix inverse is singular.
      \vskip1ex
      The \emph{regularized inverse} $\mathbb{C}_n^{-1}$ is calculated by removing the zero eigenvalues, and keeping only the first $n$ \emph{eigenvalues}:
      \begin{displaymath}
        \mathbb{C}_n^{-1} = \mathbb{O}_n \, \mathbb{D}_n^{-1} \, \mathbb{O}_n^T
      \end{displaymath}
      Where $\mathbb{D}_n$ and $\mathbb{O}_n$ are matrices with the higher order eigenvalues and eigenvectors removed.
      \vskip1ex
      The function \texttt{MASS::ginv()} calculates the \emph{regularized} inverse of a matrix.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Create rectangular matrix with collinear columns
matrixv <- matrix(rnorm(10*8), nc=10)
# Calculate covariance matrix
covmat <- cov(matrixv)
# Calculate inverse of covmat - error
invmat <- solve(covmat)
# Perform eigen decomposition
eigend <- eigen(covmat)
eigenvec <- eigend$vectors
eigenval <- eigend$values
# Set tolerance for determining zero singular values
precv <- sqrt(.Machine$double.eps)
# Calculate regularized inverse matrix
notzero <- (eigenval > (precv*eigenval[1]))
invreg <- eigenvec[, notzero] %*%
  (t(eigenvec[, notzero]) / eigenval[notzero])
# Verify inverse property of invreg
all.equal(covmat, covmat %*% invreg %*% covmat)
# Calculate regularized inverse of covmat
invmat <- MASS::ginv(covmat)
# Verify that invmat is same as invreg
all.equal(invmat, invreg)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Dimension Reduction of the Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the higher order singular values are very small then the inverse matrix amplifies the statistical noise in the response matrix.
      \vskip1ex
      The technique of \emph{dimension reduction} calculates the inverse of a covariance matrix by removing the very small, higher order eigenvalues, to reduce the propagation of statistical noise and improve the signal-to-noise ratio:
      \begin{displaymath}
        \mathbb{C}^{-1}_{DR} = \mathbb{O}_{dimax} \, \mathbb{D}^{-1}_{dimax} \, \mathbb{O}_{dimax}^T
      \end{displaymath}
      The parameter \texttt{dimax} specifies the number of eigenvalues used for calculating the \emph{dimension reduction inverse} of the covariance matrix of returns.
      \vskip1ex
      Even though the \emph{dimension reduction inverse} $\mathbb{C}^{-1}_{DR}$ does not satisfy the matrix inverse property (so it's biased), its out-of-sample forecasts are usually more accurate than those using the actual inverse matrix.
      \vskip1ex
      But removing a larger number of eigenvalues increases the bias of the covariance matrix, which is an example of the \emph{bias-variance tradeoff}.
      \vskip1ex
      The optimal value of the parameter \texttt{dimax} can be determined using \emph{backtesting} (\emph{cross-validation}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample covariance matrix
covmat <- cov(retsis)
eigend <- eigen(covmat)
eigenvec <- eigend$vectors
eigenval <- eigend$values
# Calculate dimension reduction inverse of covariance matrix
dimax <- 3
covinv <- eigenvec[, 1:dimax] %*%
  (t(eigenvec[, 1:dimax]) / eigenval[1:dimax])
# Verify inverse property of inverse
all.equal(covmat, covmat %*% covinv %*% covmat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization for ETFs with Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retsis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etfs_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Optimal Portfolio Returns With Dimension Reduction") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r^{\prime}_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
retxm <- rowMeans(retx["/2014"])
retxis <- (1-alpha)*retx["/2014"] + alpha*retxm
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retxis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_etfs_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Optimal Portfolio Returns With Eigen and Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a \emph{rolling portfolio optimization strategy}, the portfolio is optimized periodically and held out-of-sample.
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Calculate the \emph{end points} for portfolio rebalancing,
        \item Define an objective function for optimizing the portfolio weights,
        \item Calculate the optimal portfolio weights from the past (in-sample) performance,
        \item Calculate the out-of-sample returns by applying the portfolio weights to the future returns.
      \end{itemize}
      <<echo=TRUE,eval=FALSE>>=
# Define monthly end points
endd <- rutils::calc_endpoints(retp, interval="weeks")
endd <- endd[endd > (nstocks+1)]
npts <- NROW(endd)
look_back <- 3
startp <- c(rep_len(0, look_back), endd[1:(npts-look_back)])
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate the portfolio weights
    insample <- retx[startp[ep-1]:endd[ep-1], ]
    invmat <- MASS::ginv(cov(insample))
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retp[(endd[ep-1]+1):endd[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls)
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Monthly ETF Rolling Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Strategy With Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling portfolio optimization strategy with dimension reduction performs better than the standard strategy because dimension reduction suppresses the data noise.
      \vskip1ex
      The strategy performs especially well during sharp market selloffs, like in the years \texttt{2008} and \texttt{2020}.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly end points
look_back <- 3; dimax <- 9
startp <- c(rep_len(0, look_back), endd[1:(npts-look_back)])
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- retx[startp[ep-1]:endd[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:dimax] %*%
      (t(eigenvec[, 1:dimax]) / eigenval[1:dimax])
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retp[(endd[ep-1]+1):endd[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly_shrinkeigen.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls)
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Rolling Portfolio Strategy With Dimension Reduction") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Strategy With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling portfolio optimization strategy with return shrinkage performs better than the standard strategy because return shrinkage suppresses the data noise.
      \vskip1ex
      The strategy performs especially well during sharp market selloffs, like in the years \texttt{2008} and \texttt{2020}.
      <<echo=TRUE,eval=FALSE>>=
# Define the return shrinkage intensity
alpha <- 0.7
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- retx[startp[ep-1]:endd[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:dimax] %*%
      (t(eigenvec[, 1:dimax]) / eigenval[1:dimax])
    # Shrink the in-sample returns to their mean
    insample <- (1-alpha)*insample + alpha*rowMeans(insample)
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retp[(endd[ep-1]+1):endd[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly_shrinkrets.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls)
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Rolling Portfolio Strategy With Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Weekly ETF Rolling Portfolio Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling weekly strategy performs better than the standard strategy because dimension reduction allows using shorter \texttt{look\_back} intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-01-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over monthly dates
look_back <- 21
dimax <- 3
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(ep) {
  # Define in-sample and out-of-sample returns
  insample <- (datev > weeks[ep-look_back]) & (datev < weeks[ep])
  outsample <- (datev > weeks[ep]) & (datev < weeks[ep+1])
  retsis <- retp[insample]
  retsos <- retp[outsample]
  # Calculate regularized inverse of covariance matrix
  # invmat <- MASS::ginv(cov(retsis))  # if VXX and SVXY are included then no shrinkage is better
  invmat <- HighFreq::calc_inv(cov(retsis), dimax=dimax)
  weightv <- invmat %*% colMeans(retsis - riskf)
  weightv <- drop(weightv/sqrt(sum(weightv^2)))
  # Calculate portfolio pnls out-of-sample
  xts::xts(retsos %*% weightv, zoo::index(retsos))
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of weekly rolling ETF portfolio strategy
vti <- rutils::diffit(cumsum(indeks)[zoo::index(pnls),])
wealthv <- cbind(vti, pnls)
colnames(wealthv) <- c("Index", "Strategy")
colnamev <- colnames(wealthv)
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Weekly ETF Rolling Portfolio Strategy With Shrinkage") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Function for Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for rolling portfolio strategy
roll_portf <- function(excess, # Excess returns
                       returns, # Stock returns
                       endd, # End points
                       look_back=12, # Look-back interval
                       dimax=3, # Dimension reduction intensity
                       alpha=0.0, # Return shrinkage intensity
                       bid_offer=0.0, # Bid-offer spread
                       ...) {
  npts <- NROW(endd)
  startp <- c(rep_len(0, look_back), endd[1:(npts-look_back)])
  pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- excess[startp[ep-1]:endd[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:dimax] %*%
      (t(eigenvec[, 1:dimax]) / eigenval[1:dimax])
    # Shrink the in-sample returns to their mean
    insample <- (1-alpha)*insample + alpha*rowMeans(insample)
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- returns[(endd[ep-1]+1):endd[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
  })  # end lapply
  pnls <- do.call(rbind, pnls)
  # Add warmup period to pnls
  rbind(indeks[paste0("/", start(pnls)-1)], pnls)
}  # end roll_portf
@
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Look-backs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of \emph{look-back} parameters.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a monthly ETF momentum strategy
pnls <- roll_portf(excess=retx, returns=retp, endd=endd, 
  look_back=look_back, dimax=dimax)
# Perform sapply loop over look_backs
look_backs <- seq(2, 15, by=1)
pnls <- lapply(look_backs, roll_portf,
  returns=retp, excess=retx, endd=endd, dimax=dimax)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("look_back=", look_backs)
pnlsums <- sapply(pnls, sum)
look_back <- look_backs[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly_multlb.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Rolling Portfolio Strategies") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of the dimension reduction parameter.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest for different dimax values
eigenvals <- 2:11
pnls <- lapply(eigenvals, roll_portf, excess=retx, 
  returns=retp, endd=endd, look_back=look_back)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigenval=", eigenvals)
pnlsums <- sapply(pnls, sum)
dimax <- eigenvals[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly_multeigen.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Rolling Portfolio Strategies With Dimension Reduction") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of return shrinkage parameters.
      \vskip1ex
      The best return shrinkage parameter for ETFs is equal to \texttt{0}, which means no return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over vector of return shrinkage intensities
alphav <- seq(from=0.0, to=0.9, by=0.1)
pnls <- lapply(alphav, roll_portf, excess=retx, 
  returns=retp, endd=endd, look_back=look_back, dimax=dimax)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("alpha=", alphav)
pnlsums <- sapply(pnls, sum)
alpha <- alphav[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_etf_monthly_multalpha.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colorv <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Rolling Portfolio Strategies With Return Shrinkage") %>%
  dyOptions(colors=colorv, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies using quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for stocks is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy completely fails in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns
retp <- returns["2000/"]
nstocks <- NCOL(retp)
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)
datev <- zoo::index(retp)
riskf <- 0.03/252
retx <- (retp - riskf)
retsis <- retp["/2010"]
retsos <- retp["2011/"]
# Maximum Sharpe weights in-sample interval
covmat <- cov(retsis)
invmat <- MASS::ginv(covmat)
weightv <- invmat %*% colMeans(retx["/2010"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retp), datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_optim_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
endd <- rutils::calc_endpoints(pnls, interval="weeks")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Out-of-sample Optimal Portfolio Returns for Stocks") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization for Stocks with Dimension Reduction}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate regularized inverse of covariance matrix
look_back <- 8; dimax <- 21
eigend <- eigen(cov(retsis))
eigenvec <- eigend$vectors
eigenval <- eigend$values
invmat <- eigenvec[, 1:dimax] %*%
  (t(eigenvec[, 1:dimax]) / eigenval[1:dimax])
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retx["/2010"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retp), datev)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_optim_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
endd <- rutils::calc_endpoints(pnls, interval="weeks")
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Out-of-sample Returns for Stocks with Dimension Reduction") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock Portfolio Weights With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r^{\prime}_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
retxm <- rowMeans(retx["/2010"])
retxis <- (1-alpha)*retx["/2010"] + alpha*retxm
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retxis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_optim_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
dygraphs::dygraph(cumsum(pnls)[endd], 
  main="Out-of-sample Returns for Stocks with Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Covariance Matrix Inverse Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
  \column{0.5\textwidth}
      \emph{RcppArmadillo} can be used to quickly calculate the regularized inverse of a covariance matrix.
      <<echo=TRUE,eval=FALSE>>=
library(RcppArmadillo)
# Source Rcpp functions from file
Rcpp::sourceCpp("/Users/jerzy/Develop/lecture_slides/scripts/back_test.cpp")
# Create random matrix of returns
matrixv <- matrix(rnorm(300), nc=5)
# Regularized inverse of covariance matrix
dimax <- 4
eigend <- eigen(cov(matrixv))
covinv <- eigend$vectors[, 1:dimax] %*%
  (t(eigend$vectors[, 1:dimax]) / eigend$values[1:dimax])
# Regularized inverse using RcppArmadillo
covinv_arma <- calc_inv(matrixv, dimax)
all.equal(covinv, covinv_arma)
# Microbenchmark RcppArmadillo code
library(microbenchmark)
summary(microbenchmark(
  rcode={eigend <- eigen(cov(matrixv))
    eigend$vectors[, 1:dimax] %*%
      (t(eigend$vectors[, 1:dimax]) / eigend$values[1:dimax])
  },
  cppcode=calc_inv(matrixv, dimax),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \column{0.5\textwidth}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat calc_inv(const arma::mat& matrixv, 
                   arma::uword dimax = 0, // Max number of PCA for dimension reduction
                   double eigen_thresh = 0.01) { // Threshold for discarding small singular values

  if (dimax == 0) {
    // Calculate the inverse using arma::pinv()
    return arma::pinv(tseries, eigen_thresh);
  } else {
    // Calculate the regularized inverse using SVD decomposition
    
    // Allocate SVD
    arma::vec svdval;
    arma::mat svdu, svdv;
    
    // Calculate the SVD
    arma::svd(svdu, svdval, svdv, tseries);
    
    // Subset the SVD
    dimax = dimax - 1;
    // For no regularization: dimax = tseries.n_cols
    svdu = svdu.cols(0, dimax);
    svdv = svdv.cols(0, dimax);
    svdval = svdval.subvec(0, dimax);
    
    // Calculate the inverse from the SVD
    return svdv*arma::diagmat(1/svdval)*svdu.t();
    
  }  // end if
  
}  // end calc_inv
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast portfolio optimization using matrix algebra can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::vec calc_weights(const arma::mat& returns, // Portfolio returns
                       std::string method = "ranksharpe",
                       double eigen_thresh = 0.001,
                       arma::uword dimax = 0,
                       double confi = 0.1,
                       double alpha = 0.0,
                       bool scale = true,
                       double vol_target = 0.01) {
  // Initialize
  arma::vec weightv(returns[ncols, fill::zeros);
  if (dimax == 0)  dimax = returns[ncols;
  
  // Switch for the different methods for weights
  switch(calc_method(method)) {
  case method::ranksharpe: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end ranksharpe
  case method::max_sharpe: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Shrink meancols to the mean of returns
    meancols = ((1-alpha)*meancols + alpha*arma::mean(meancols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), dimax);
    // weightv = calc_inv(cov(returns), dimax)*meancols;
    weightv = calc_inv(cov(returns), dimax, eigen_thresh)*meancols;
    break;
  }  // end max_sharpe
  case method::max_sharpe_median: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::median(returns, 0));
    // Shrink meancols to the mean of returns
    meancols = ((1-alpha)*meancols + alpha*arma::median(meancols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), dimax);
    weightv = calc_inv(cov(returns), dimax, eigen_thresh)*meancols;
    break;
  }  // end max_sharpe_median
  case method::min_var: {
    // Apply regularized inverse to unit vector
    weightv = calc_inv(cov(returns), dimax, eigen_thresh)*arma::ones(returns[ncols);
    break;
  }  // end min_var
  case method::min_varpca: {
    // Calculate highest order principal component
    arma::vec eigenval;
    arma::mat eigenvec;
    arma::eig_sym(eigenval, eigenvec, arma::cov(returns));
    weightv = eigenvec.col(0);
    break;
  }  // end min_varpca
  case method::rank: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end rank
  case method::rankrob: {
    // Median returns by columns
    arma::vec meancols = arma::trans(arma::median(returns, 0));
    // meancols = ((1-alpha)*meancols + alpha*arma::mean(meancols));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Apply regularized inverse
    // arma::mat invmat = calc_inv(cov(returns), dimax);
    // weightv = calc_inv(cov(returns), dimax)*meancols;
    // weightv = calc_inv(cov(returns), dimax)*meancols;
    // // Standard deviation by columns
    // arma::vec sd_cols = meancols;
    // for (arma::uword it=0; it < returns[ncols; it++) {
    //   sd_cols(it) = arma::median(arma::abs((returns.col(it) - sd_cols)));
    // }  // end for
    // sd_cols.replace(0, 1);
    // meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    // level;
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end rankrob
  case method::quantile: {
    // Sum of quantiles for columns
    arma::vec levels = {confi, 1-confi};
    weightv = conv_to<vec>::from(arma::sum(arma::quantile(returns, levels, 0), 0));
    // Weights equal to ranks
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(weightv)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end quantile
  default : {
    cout << "Warning: Invalid method parameter: " << method << endl;
    return arma::ones(returns[ncols);
  }  // end default
  }  // end switch
  
  if (scale == TRUE) {
    // return weightv/std::sqrt(sum(square(weightv)));
    // return weightv/sum(weightv);
    // Returns of equally weighted portfolio
    // arma::vec meanrows = arma::mean(returns, 1);
    // Returns of weighted portfolio
    // arma::vec returns_portf = returns*weightv;
    // Scale weights to equally weighted portfolio and return them
    // return weightv*arma::stddev(arma::mean(returns, 1))/arma::stddev(returns*weightv);
    // Scale weights so the resulting portfolio has a volatility equal to vol_target
    return weightv*vol_target/arma::stddev(returns*weightv);
  }  // end if
  
  return weightv;
  
}  // end calc_weights
    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Strategy Backtesting Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast backtesting of strategies can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat back_test(const arma::mat& retx, // Portfolio excess returns
                    const arma::mat& returns, // Portfolio returns
                    arma::uvec startp, 
                    arma::uvec endd, 
                    std::string method = "ranksharpe",
                    double eigen_thresh = 0.001,
                    arma::uword dimax = 0,
                    double confi = 0.1,
                    double alpha = 0.0,
                    bool scale = true,
                    double vol_target = 0.01,
                    double coeff = 1.0,
                    double bid_offer = 0.0) {
  
  arma::vec weightv(returns[ncols, fill::zeros);
  arma::vec weights_past = zeros(returns[ncols);
  arma::mat pnls = zeros(returns*nrows, 1);
  
  // Perform loop over the end points
  for (arma::uword it = 1; it < endd.size(); it++) {
    // cout << "it: " << it << endl;
    // Calculate portfolio weights
    weightv = coeff*calc_weights(retx.rows(startp(it-1), endd(it-1)), method, dimax, eigen_thresh, confi, alpha, scale, vol_target);
    // Calculate out-of-sample returns
    pnls.rows(endd(it-1)+1, endd(it)) = returns.rows(endd(it-1)+1, endd(it))*weightv;
    // Add transaction costs
    pnls.row(endd(it-1)+1) -= bid_offer*sum(abs(weightv - weights_past))/2;
    weights_past = weightv;
  }  // end for
  
  // Return the strategy pnls
  return pnls;
  
}  // end back_test

    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy for S\&P500 Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling portfolio optimization} strategy consists of rebalancing a portfolio over the end points:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculate the maximum Sharpe ratio portfolio weights at each end point,
        \item Apply the weights in the next interval and calculate the out-of-sample portfolio returns.
      \end{enumerate}
      The strategy parameters are: the rebalancing frequency (annual, monthly, etc.), and the length of look-back interval.
      <<echo=TRUE,eval=FALSE>>=
# Overwrite NA values in returns
retp <- returns100
retp[1, is.na(retp[1, ])] <- 0
retp <- zoo::na.locf(retp, na.rm=FALSE)
retx <- (retp - riskf)
nstocks <- NCOL(retp) ; datev <- zoo::index(retp)
# Define monthly end points
endd <- rutils::calc_endpoints(retp, interval="weeks")
endd <- endd[endd > (nstocks+1)]
npts <- NROW(endd) ; look_back <- 12
startp <- c(rep_len(0, look_back), endd[1:(npts-look_back)])
# Perform loop over end points - takes very long !!!
pnls <- lapply(2:npts, function(ep) {
    # Subset the excess returns
    insample <- retx[startp[ep-1]:endd[ep-1], ]
    invmat <- MASS::ginv(cov(insample))
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retp[(endd[ep-1]+1):endd[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- rutils::do_call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_sp500.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate returns of equal weight portfolio
indeks <- xts::xts(rowMeans(retp), datev)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls*sd(indeks)/sd(pnls))
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(indeks, pnls)
colnames(wealthv) <- c("Equal Weight", "Strategy")
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot cumulative strategy returns
dygraphs::dygraph(cumsum(wealthv)[endd], main="Rolling Portfolio Optimization Strategy for S&P500 Stocks") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{rolling portfolio optimization} strategy can be improved by applying both dimension reduction and return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Shift end points to C++ convention
endd <- (endd - 1)
endd[endd < 0] <- 0
startp <- (startp - 1)
startp[startp < 0] <- 0
# Specify dimension reduction and return shrinkage using list of portfolio optimization parameters
controlv <- HighFreq::param_portf(method="maxsharpe", dimax=21, alpha=0.7)
# Perform backtest in Rcpp
pnls <- HighFreq::back_test(excess=retx, returns=retp,
  startp=startp, endd=endd, controlv=controlv)
pnls <- pnls*sd(indeks)/sd(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_sp500_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Rolling S&P500 Portfolio Optimization Strategy With Shrinkage") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Shrinkage Parameters Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal values of the dimension reduction parameter \texttt{dimax} and the return shrinkage intensity parameter $\alpha$ can be determined using \emph{backtesting}.
      \vskip1ex
      The best dimension reduction parameter for this portfolio of stocks is equal to \texttt{dimax=33}, which means relatively weak dimension reduction.
      \vskip1ex
      The best return shrinkage parameter for this portfolio of stocks is equal to $\alpha=0.81$, which means strong return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over vector of return shrinkage intensities
alphav <- seq(from=0.01, to=0.91, by=0.1)
pnls <- lapply(alphav, function(alpha) {
  HighFreq::back_test(excess=retx, returns=retp,
  startp=startp, endd=endd, controlv=controlv)
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=alphav, y=profilev, t="l",
  main="Rolling Strategy as Function of Return Shrinkage",
  xlab="Shrinkage Intensity Alpha", ylab="pnl")
whichmax <- which.max(profilev)
alpha <- alphav[whichmax]
pnls <- pnls[[whichmax]]
# Perform backtest over vector of dimension reduction eigenvals
eigenvals <- seq(from=3, to=40, by=2)
pnls <- lapply(eigenvals, function(dimax) {
  HighFreq::back_test(excess=retx, returns=retp,
    startp=startp, endd=endd, controlv=controlv)
})  # end lapply
profilev <- sapply(pnls, sum)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_sp500_shrink_optim.png}
      <<echo=TRUE,eval=FALSE>>=
plot(x=eigenvals, y=profilev, t="l", 
  main="Strategy PnL as Function of dimax",
  xlab="dimax", ylab="pnl")
whichmax <- which.max(profilev)
dimax <- eigenvals[whichmax]
pnls <- pnls[[whichmax]]
pnls <- pnls*sd(indeks)/sd(pnls)
# Plot cumulative strategy returns
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Look-back Interval Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal value of the look-back interval can be determined using \emph{backtesting}.
      \vskip1ex
      The optimal value of the look-back interval for this portfolio of stocks is equal to \texttt{look\_back=9} months, which roughly agrees with the research literature on momentum strategies.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over look-backs
look_backs <- seq(from=3, to=12, by=1)
pnls <- lapply(look_backs, function(look_back) {
  startp <- c(rep_len(0, look_back), endd[1:(npts-look_back)])
  startp <- (startp - 1)
  startp[startp < 0] <- 0
  HighFreq::back_test(excess=retx, returns=retp,
    startp=startp, endd=endd, controlv=controlv)
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=look_backs, y=profilev, t="l", main="Strategy PnL as Function of Look-back Interval",
  xlab="Look-back Interval", ylab="pnl")
whichmax <- which.max(profilev)
look_back <- look_backs[whichmax]
pnls <- pnls[[whichmax]]
pnls <- pnls*sd(indeks)/sd(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_rolling_sp500_shrink_optim_lookback.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealthv <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealthv) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, function(x) 
  c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealthv)[endd], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Statistical Arbitrage}


%%%%%%%%%%%%%%%
\subsection{Cointegration of Stocks Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A group of stocks is \emph{cointegrated} if there is a portfolio of the stocks whose price is range-bound.
      \vskip1ex
      For two stocks, the cointegrating factor $\beta$ can be obtained from the regression of the stock prices.  The cointegrated portfolio $R$ is the residual of the regression:
      \begin{displaymath}
        R = p_1 - \beta p_2 - \alpha
      \end{displaymath}
      Regressing the stock \emph{prices} produces a \emph{cointegrated} portfolio, with a small variance of its \emph{prices}.
      Regressing the stock \emph{returns} produces a \emph{correlated} portfolio, with a small variance of its \emph{returns}.
      \vskip1ex
      The standard confidence intervals are not valid for the regression of prices, because prices are not stationary and are not normally distributed.
      <<echo=TRUE,eval=FALSE>>=
# Load daily S&P500 stock prices
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_prices.RData")
# Subset (select) the stock prices after the start date of XLK
pricev <- cbind(rutils::etfenv$prices$XLK, prices$MSFT)
pricev <- na.omit(pricev)
colnames(pricev) <- c("XLK", "MSFT")
datev <- zoo::index(pricev)
# Calculate the regression coefficients of MSFT ~ XLK
betav <- drop(cov(pricev$MSFT, pricev$XLK)/var(pricev$XLK))
alphav <- mean(pricev$MSFT) - betav*mean(pricev$XLK)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/coint_scatter.png}
      <<echo=TRUE,eval=FALSE>>=
# Scatterplot of prices
pricem <- zoo::coredata(pricev) # plot() doesn't like xts time series
plot(pricem[, "XLK"], pricem[, "MSFT"], main="MSFT and XLK Prices", 
     xlab="XLK", ylab="MSFT", pch=1, col="blue")
abline(a=alphav, b=betav, col="red", lwd=2)
# Plot the prices
endd <- rutils::calc_endpoints(pricev, interval="months")
dygraphs::dygraph(log(pricev)[endd], main="MSFT and XLK Prices") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Cointegrated Portfolio Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Engle-Granger two-step procedure can be used to test the cointegrated portfolio of two stocks:
      \begin{itemize}
        \item Perform a regression of the stock prices to calculate the cointegrating factor $\beta$,
        \item Apply the \emph{ADF} test to the regression residual (the portfolio price) to determine if they have a unit root (the portfolio price diverges), or if they are mean reverting.
      \end{itemize}
      The \emph{p}-value of the \emph{ADF} test for the cointegrated portfolio of \emph{MSFT} and \emph{XLK} is not small, so the \emph{null hypothesis} that it has a \emph{unit root} (it diverges) cannot be rejected.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{ADF} test is that the time series has a \emph{unit root} (it diverges).  So a small \emph{p}-value suggests that the \emph{null hypothesis} is \texttt{FALSE} and that the time series is range-bound.
      \vskip1ex
      The \emph{ADF} test statistic for the cointegrated portfolio is smaller than for either \emph{MSFT} or \emph{XLK} alone, which indicates that it's more mean-reverting.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the cointegrated portfolio prices (regression residual)
resids <- (pricev$MSFT - betav*pricev$XLK - alphav)
# Perform ADF test on the individual stocks
sapply(pricev, tseries::adf.test, k=1)
# Perform ADF test on the cointegrated portfolio
tseries::adf.test(resids, k=1)
# Perform ADF test for vector of cointegrating factors
betas <- seq(1.5, 2.5, 0.1)
adfstat <- sapply(betas, function(betav) {
  resids <- (pricev$MSFT - betav*pricev$XLK)
  tseries::adf.test(resids, k=1)$statistic
})  # end sapply
plot(x=betas, y=adfstat, type="l", xlab="cointegrating factor", ylab="statistic",
 main="ADF Test Statistic as Function of Cointegrating Factor")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/coint_prices.png}
      <<echo=TRUE,eval=FALSE>>=
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Trading Cointegrated Pairs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cointegrated portfolio has negative autocorrelations, so it can be traded using a mean-reverting strategy.
      \vskip1ex
      The mean-reverting strategy buys when the portfolio is cheap, sells it short when it's rich (expensive), and goes flat if the portfolio reaches a fair (mean) price.
      \vskip1ex
      The portfolio is cheap if its price is below its mean price minus the portfolio standard deviation, and it's rich (expensive) if its price is above the mean plus the standard deviation.  The portfolio price is fair if it's equal to the mean price.
      \vskip1ex
      Explain that the cointegration portfolio is not stationary (it has a time-dependent volatility), so it's hard to trade.
      We need to make the volatility less time-dependent.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of the cointegrated portfolio prices
hist(resids, breaks=100, xlab="Prices", 
  main="Histogram of Cointegrated Portfolio Prices")
# Plot PACF of the cointegrated portfolio returns
pacf(rutils::diffit(as.numeric(resids)), lag=10, xlab=NA, ylab=NA,
     main="PACF of Cointegrated Portfolio Returns")


# Plot the cointegration residuals
dygraphs::dygraph(resids[endd], main="MSFT and XLK Cointegration Residual") %>%
  dyOptions(colors=c("blue"), strokeWidth=2)
# Trade the cointegration residuals
threshold <- drop(sqrt(var(resids)))
meanv <- mean(resids)
posv <- 0
nrows <- NROW(resids)
pnls <- numeric(nrows)
# resids <- as.numeric(resids)
for (it in 1:nrows) {
  if ((posv > (-1)) && (resids[it] > (meanv + threshold))) {
    # Enter short
    pnls[it] <- resids[it]
    posv <- (-1)
  } else if ((posv < 1) && (resids[it] < (meanv - threshold))) {
    # Enter long
    pnls[it] <- (-resids[it])
    posv <- 1
  } else if ((posv < 0) && (resids[it] < meanv)) {
    # Unwind short
    pnls[it] <- (-resids[it])
    posv <- 0
  } else if ((posv > 0) && (resids[it] > meanv)) {
    # Unwind long
    pnls[it] <- resids[it]
    posv <- 0
  }  # end if
}  # end for
pnls <- cumsum(pnls)
plot(pnls, t="l")

# Dynamic
lambda <- 0.9
vars <- HighFreq::run_var(resids, lambda=lambda)
threshd <- HighFreq::lagit(sqrt(vars))
meand <- HighFreq::lagit(HighFreq::run_mean(resids, lambda=lambda))

# Plot YC steepener principal component with VTI
datav <- cbind(resids, meand+threshd, meand-threshd)
colnames(datav) <- c("Resids", "Upper", "Lower")
dygraphs::dygraph(datav[endd], main="Resids") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2)


posv <- 0
pnls <- numeric(nrows)
# resids <- as.numeric(resids)
for (it in 1:nrows) {
  if ((posv > (-1)) && (resids[it] > (meand[it] + threshd[it]))) {
    # Enter short
    pnls[it] <- resids[it]
    posv <- (-1)
  } else if ((posv < 1) && (resids[it] < (meand[it] - threshd[it]))) {
    # Enter long
    pnls[it] <- (-resids[it])
    posv <- 1
  } else if ((posv < 0) && (resids[it] < meand[it])) {
    # Unwind short
    pnls[it] <- (-resids[it])
    posv <- 0
  } else if ((posv > 0) && (resids[it] > meand[it])) {
    # Unwind long
    pnls[it] <- resids[it]
    posv <- 0
  }  # end if
}  # end for
pnls <- cumsum(pnls)
plot(pnls, t="l")


retres <- rutils::diffit(resids)
lambda <- 0.6
volat <- HighFreq::run_var(retres, lambda=lambda)
volat <- sqrt(volat)
volat <- xts::xts(volat, datev)
dygraphs::dygraph(volat[endd], main="Residual Volatility") %>%
  dyOptions(colors=c("blue"), strokeWidth=2)

# Calculate the regression residual
resids <- (pricev$MSFT - betav*pricev$XLK)
# Perform ADF test on residuals
retp <- rutils::diffit(pricev$XLK)
retres <- rutils::diffit(resids)
resids <- retres/volat
resids[1] <- 0
resids <- HighFreq::run_mean(resids, lambda=lambda)
resids <- xts::xts(resids, datev)
# resids <- cumsum(retres)
# Or
# resids <- resids/volat
# resids[1] <- 0
range(resids)
mad(resids)
tseries::adf.test(resids, k=1)
dygraphs::dygraph(resids[endd], main="MSFT and XLK Cointegration Residual") %>%
  dyOptions(colors=c("blue"), strokeWidth=2)

# Simulate in-sample VTI strategy
posit <- rep(NA_integer_, NROW(resids))
posit[1] <- 0
posit[resids > 2] <- (-1)
posit[resids <= 2] <- (1)
posit <- zoo::na.locf(posit)
posit <- rutils::lagit(posit)
pnls <- retres*posit


# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(retp, pnls)
colnames(wealthv) <- c("XLK", "Strategy")
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot dygraph of in-sample VTI strategy
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd],
  main="Logistic Strategy Using Top and Bottom Labels") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)


# Simulate in-sample VTI strategy
negav <- HighFreq::roll_sum(matrix(forecastneg), lagg)/lagg
posav <- HighFreq::roll_sum(matrix(forecastpos), lagg)/lagg
posit <- (negav - posav)
# posit <- ifelse(forecastpos, 1, 0)
# posit <- ifelse(forecastneg, -1, posit)
posit <- rutils::lagit(posit)
pnls <- retvti*posit
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(retvti, pnls)
colnames(wealthv) <- c("VTI", "Strategy")
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_logistic_rets_insample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
endd <- rutils::calc_endpoints(wealthv, interval="weeks")
dygraphs::dygraph(cumsum(wealthv)[endd],
  main="Logistic Forecasting Returns") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
