% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% tikz package for plotting and tables
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% bbm and bbold packages for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Multivariate Investment Strategies]{Multivariate Investment Strategies}
\subtitle{FRE7241, Spring 2022}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Interest Rate Strategies}


%%%%%%%%%%%%%%%
\subsection{Interest Rate Yield Curve and Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Daily stock returns have insignificant correlations with the daily changes in interest rates, with the possible exception of the 10-year bond yield. 
      \vskip1ex
      And these correlations change significantly over time.
      <<echo=TRUE,eval=FALSE>>=
# Load constant maturity Treasury rates
load(file="/Users/jerzy/Develop/lecture_slides/data/rates_data.RData")
# Combine rates into single xts series
rates <- do.call(cbind, as.list(rates_env))
# Sort the columns of rates according bond maturity
namesv <- colnames(rates)
namesv <- substr(namesv, start=4, stop=10)
namesv <- as.numeric(names)
indeks <- order(names)
rates <- rates[, indeks]
# Align rates dates with VTI prices
closep <- log(quantmod::Cl(rutils::etf_env$VTI))
colnames(closep) <- "VTI"
nrows <- NROW(closep)
dates <- zoo::index(closep)
rates <- na.omit(rates[dates])
closep <- closep[zoo::index(rates)]
dates <- zoo::index(closep)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns and IR changes
returns <- rutils::diff_it(closep)
rates_diff <- rutils::diff_it(log(rates))
# Regress VTI returns versus the lagged rate differences
predictor <- rutils::lagit(rates_diff)
model <- lm(returns ~ predictor)
summary(model)
# Regress VTI returns before and after 2012
summary(lm(returns["/2012"] ~ predictor["/2012"]))
summary(lm(returns["2012/"] ~ predictor["2012/"]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Principal Components and Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The principal components of the interest rate yield curve can also be used as predictors of stock indices.
      \vskip1ex
      The second principal component describes the steepening and flattening of the yield curve, and it's an indicator of investor risk appetite.  So it's also related to bullish and bearish market periods.
      <<echo=TRUE,eval=FALSE>>=
# Calculate PCA of rates correlation matrix
eigend <- eigen(cor(rates_diff))
rates_pca <- -rates_diff %*% eigend$vectors
colnames(rates_pca) <- paste0("PC", 1:6)
# Define predictor as the YC PCAs
predictor <- rutils::lagit(rates_pca)
model <- lm(returns ~ predictor)
summary(model)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_steep.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot YC steepener principal component with VTI
datav <- cbind(returns, rates_pca[, 2])
colnames(datav) <- c("VTI", "Steepener")
colnamev <- colnames(datav)
dygraphs::dygraph(cumsum(datav), main="VTI and Yield Curve Steepener") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For in-sample forecasts, the training set and the test set are the same.  The model is calibrated on the data that is used for forecasting. 
      \vskip1ex
      Yield Curve Strategy
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model works.
      \vskip1ex
      The in-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Define predictor with intercept term
predictor <- rutils::lagit(rates_diff)
predictor <- cbind(rep(1, NROW(predictor)), predictor)
colnames(predictor)[1] <- "intercept"
# Calculate inverse of predictor
inverse <- MASS::ginv(predictor)
# Calculate coefficients from response and inverse of predictor
response <- returns
coeff <- drop(inverse %*% response)
# Calculate forecasts and pnls in-sample
forecasts <- (predictor %*% coeff)
pnls <- sign(forecasts)*response
# Calculate in-sample factors
factors <- (predictor * coeff)
apply(factors, 2, sd)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_insample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample IR strategy
wealth <- cbind(returns, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Yield Curve Strategy In-sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
in_sample <- (dates < as.Date("2020-01-01"))
out_sample <- (dates >= as.Date("2020-01-01"))
# Calculate inverse of predictor in-sample
inverse <- MASS::ginv(predictor[in_sample, ])
# Calculate coefficients in-sample
coeff <- drop(inverse %*% response[in_sample, ])
# Calculate forecasts and pnls out-of-sample
forecasts <- (predictor[out_sample, ] %*% coeff)
pnls <- sign(forecasts)*response[out_sample, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_outsample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample IR PCA strategy
wealth <- cbind(returns[out_sample, ], pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Yield Curve Strategy Out-of-Sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Yearly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling yearly yield curve strategy, the model is recalibrated at the end of every year using a training set of data from the past year.
      The coefficients are applied to calculate out-of-sample forecasts in the following year.
      \vskip1ex
      The rolling yearly strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define yearly dates
format(dates[1], "%Y")
years <- paste0(seq(2001, 2022, 1), "-01-01")
years <- as.Date(years)
# Perform loop over yearly dates
pnls <- lapply(3:(NROW(years)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > years[i-1]) & (dates < years[i])
  out_sample <- (dates >= years[i]) & (dates < years[i+1])
  # Calculate coefficients in-sample
  inverse <- MASS::ginv(predictor[in_sample, ])
  coeff <- drop(inverse %*% response[in_sample, ])
  # Calculate forecasts and pnls out-of-sample
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_yearly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling yearly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Yearly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Monthly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{11} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      \vskip1ex
      Research shows that looking back roughly a year provides the best out-of-sample forecasts.
      \vskip1ex
      The rolling monthly strategy performs better than the yearly strategy, but mostly in periods of high volatility, and otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(dates[1], "%m-%Y")
format(dates[NROW(dates)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
pnls <- lapply(12:(NROW(months)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > months[i-11]) & (dates < months[i])
  out_sample <- (dates > months[i]) & (dates < months[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- MASS::ginv(predictor[in_sample, ])
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Monthly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Weekly Yield Curve Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{10} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
pnls <- lapply(51:(NROW(weeks)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > weeks[i-10]) & (dates < weeks[i])
  out_sample <- (dates > weeks[i]) & (dates < weeks[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- MASS::ginv(predictor[in_sample, ])
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_weekly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Weekly Yield Curve Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularization of the Inverse Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{SVD} of a rectangular matrix $\mathbb{A}$ is defined as the factorization:
      \begin{displaymath}
        \mathbb{A} = \mathbb{U} \Sigma \mathbb{V}^T
      \end{displaymath}
      Where $\mathbb{U}$ and $\mathbb{V}$ are the \emph{singular matrices}, and $\Sigma$ is a diagonal matrix of \emph{singular values}.
      \vskip1ex
      The \emph{generalized inverse} matrix $\mathbb{A}^{-1}$ satisfies the inverse equation: $\mathbb{A} \mathbb{A}^{-1} \mathbb{A} = \mathbbm{A}$, and it can be expressed as a product of the \emph{SVD} matrices as follows:
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V} \, \Sigma^{-1} \, \mathbb{U}^T
      \end{displaymath}
      If any of the \emph{singular values} are zero then the \emph{generalized inverse} does not exist.
      \vskip1ex
      \emph{Regularization} is the removal of zero singular values, to make calculating the inverse matrix possible.
      \vskip1ex
      The \emph{regularized inverse} is obtained by removing the zero \emph{singular values}:
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V}_n \, \Sigma_n^{-1} \, \mathbb{U}_n^T
      \end{displaymath}
      Where $\mathbb{U}_n$, $\mathbb{V}_n$ and $\Sigma_n$ are the \emph{SVD} matrices without the zero \emph{singular values}.
      \vskip1ex
      The regularized inverse satisfies the inverse matrix equation: $\mathbb{A} \mathbb{A}^{-1} \mathbb{A} = \mathbbm{A}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.3\paperwidth]{figure/yc_pred_svd.png}
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate singular value decomposition of the predictor matrix
svdec <- svd(predictor)
barplot(svdec$d, main="Singular Values of YC Predictor Matrix")
# Calculate generalized inverse from SVD
inverse <- svdec$v %*% (t(svdec$u) / svdec$d)
# Verify inverse property of inverse
all.equal(zoo::coredata(predictor), 
          predictor %*% inverse %*% predictor)
# Calculate generalized inverse using MASS::ginv()
inverse_ginv <- MASS::ginv(predictor)
all.equal(inverse_ginv, inverse)
# Set tolerance for determining zero singular values
precision <- sqrt(.Machine$double.eps)
# Check for zero singular values
round(svdec$d, 12)
not_zero <- (svdec$d > (precision * svdec$d[1]))
# Calculate regularized inverse from SVD
inv_reg <- svdec$v[, not_zero] %*%
  (t(svdec$u[, not_zero]) / svdec$d[not_zero])
# Verify inverse property of inv_reg
all.equal(zoo::coredata(predictor), 
          predictor %*% inv_reg %*% predictor)
all.equal(inv_reg, inverse)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shrinkage of the Inverse Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Regularization} is the removal of zero singular values, to make calculating the inverse matrix possible.
      \vskip1ex
      If the higher order singular values are very small then the inverse matrix will amplify the noise in the response matrix.
      \vskip1ex
      \emph{Shrinkage} is the removal of small singular values, to improve the out-of-sample performance of the inverse matrix.
      \vskip1ex
      The \emph{shrinkage inverse} is obtained by removing the very small \emph{singular values}.
      \begin{displaymath}
        \mathbb{A}^{-1} = \mathbb{V}_n \, \Sigma_n^{-1} \, \mathbb{U}_n^T
      \end{displaymath}
      This effectively reduces the number of parameters in the model.
      \vskip1ex
      The \emph{shrinkage inverse} satisfies the inverse equation only approximately (it is \emph{biased}), but it's often used in machine learning because it produces a lower \emph{variance} of the forecasts than the exact inverse.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate shrinkage inverse from SVD
eigen_max <- 3
inv_shrink <- svdec$v[, 1:eigen_max] %*%
  (t(svdec$u[, 1:eigen_max]) / svdec$d[1:eigen_max])
# Inverse property fails for inv_shrink
all.equal(zoo::coredata(predictor), 
          predictor %*% inv_shrink %*% predictor)
# Calculate shrinkage inverse using RcppArmadillo
inverse_rcpp <- HighFreq::calc_inv(predictor, eigen_max=eigen_max)
all.equal(inv_shrink, inverse_rcpp, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy With Shrinkage In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The technique of \emph{regularization} is designed to reduce the number of parameters in a model, for example in portfolio optimization.
      \vskip1ex
      Regularization of the inverse predictor matrix improves the in-sample performance of the yield curve strategy.
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model can be improved.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample pnls for different eigen_max values
eigen_maxs <- 2:7
pnls <- lapply(eigen_maxs, function(eigen_max) {
  inverse <- HighFreq::calc_inv(predictor, eigen_max=eigen_max)
  coeff <- drop(inverse %*% response)
  forecasts <- (predictor %*% coeff)
  sign(forecasts)*response
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigen_maxs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_insample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample pnls
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="In-Sample Returns of Shrinkage YC Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Yield Curve Strategy With Shrinkage Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
in_sample <- (dates < as.Date("2020-01-01"))
out_sample <- (dates >= as.Date("2020-01-01"))
# Calculate in-sample pnls for different eigen_max values
eigen_maxs <- 2:7
pnls <- lapply(eigen_maxs, function(x) {
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=x)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigen_maxs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_outsample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample pnls
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Out-of-Sample Returns of Shrinkage YC Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Monthly Yield Curve Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling monthly strategy performs better than the standard strategy because regularization allows using shorter look\_back intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(dates[1], "%m-%Y")
format(dates[NROW(dates)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
look_back <- 6
eigen_max <- 3
pnls <- lapply((look_back+1):(NROW(months)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > months[i-look_back]) & (dates < months[i])
  out_sample <- (dates > months[i]) & (dates < months[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=eigen_max)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_monthly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Monthly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Weekly Yield Curve Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{4} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
look_back <- 4
eigen_max <- 4
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > weeks[i-look_back]) & (dates < weeks[i])
  out_sample <- (dates > weeks[i]) & (dates < weeks[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=eigen_max)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Weekly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Predictor Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A "kitchen sink" strategy combines many different predictors into a large predictor matrix with many columns.  
      \vskip1ex
      For example by combining the yield curve predictors with the lagged returns. 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load the yield curve data
load(file="/Users/jerzy/Develop/lecture_slides/data/rates_data.RData")
rates <- do.call(cbind, as.list(rates_env))
namesv <- colnames(rates)
namesv <- substr(namesv, start=4, stop=10)
namesv <- as.numeric(names)
indeks <- order(names)
rates <- rates[, indeks]
closep <- log(quantmod::Cl(rutils::etf_env$VTI))
colnames(closep) <- "VTI"
nrows <- NROW(closep)
dates <- zoo::index(closep)
rates <- na.omit(rates[dates])
closep <- closep[zoo::index(rates)]
dates <- zoo::index(closep)
returns <- rutils::diff_it(closep)
rates_diff <- rutils::diff_it(log(rates))
# Create a combined predictor matrix
order_max <- 5
predictor <- sapply(1:order_max, rutils::lagit, input=as.numeric(returns))
colnames(predictor) <- paste0("retslag", 1:NCOL(predictor))
predictor <- cbind(predictor, rutils::lagit(rates_diff))
predictor <- cbind(rep(1, NROW(predictor)), predictor)
colnames(predictor)[1] <- "intercept"
response <- returns
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Strategy With Shrinkage In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The technique of \emph{regularization} is designed to reduce the number of parameters in a model, for example in portfolio optimization.
      \vskip1ex
      Regularization of the inverse predictor matrix improves the in-sample performance of the yield curve strategy.
      \vskip1ex
      Although it's not realistic to achieve the in-sample performance, it's useful because it provides insights into how the model can be improved.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample pnls for different eigen_max values
eigen_maxs <- 2:11
pnls <- lapply(eigen_maxs, function(eigen_max) {
  inverse <- HighFreq::calc_inv(predictor, eigen_max=eigen_max)
  coeff <- drop(inverse %*% response)
  forecasts <- (predictor %*% coeff)
  sign(forecasts)*response
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigen_maxs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_combined_insample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample pnls
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="In-Sample Returns of Combined Strategies With Shrinkage") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Combined Strategy With Shrinkage Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
in_sample <- (dates < as.Date("2020-01-01"))
out_sample <- (dates >= as.Date("2020-01-01"))
# Calculate in-sample pnls for different eigen_max values
eigen_maxs <- 2:11
pnls <- lapply(eigen_maxs, function(x) {
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=x)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigen", eigen_maxs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_combined_outsample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample pnls
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Out-of-Sample Returns of Combined Strategies With Shrinkage") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Monthly Combined Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling monthly strategy performs better than the standard strategy because regularization allows using shorter look\_back intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(dates[1], "%m-%Y")
format(dates[NROW(dates)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
look_back <- 6
eigen_max <- 3
pnls <- lapply((look_back+1):(NROW(months)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > months[i-look_back]) & (dates < months[i])
  out_sample <- (dates > months[i]) & (dates < months[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=eigen_max)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_monthly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Monthly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Rolling Weekly Combined Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling weekly yield curve strategy, the model is recalibrated at the end of every week using a training set of the past \texttt{4} weeks.
      The coefficients are applied to perform out-of-sample forecasts in the following week.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-05-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over weekly dates
look_back <- 8
eigen_max <- 4
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(i) {
  # Define in-sample and out-of-sample intervals
  in_sample <- (dates > weeks[i-look_back]) & (dates < weeks[i])
  out_sample <- (dates > weeks[i]) & (dates < weeks[i+1])
  # Calculate forecasts and pnls out-of-sample
  inverse <- HighFreq::calc_inv(predictor[in_sample, ], eigen_max=eigen_max)
  coeff <- drop(inverse %*% response[in_sample, ])
  forecasts <- (predictor[out_sample, ] %*% coeff)
  sign(forecasts)*response[out_sample, ]
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_roll_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling weekly IR strategy
vtis <- rutils::diff_it(closep[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Rolling Weekly Shrinkage YC Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Forecasts Using Aggregated Predictor}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Needs more work to improve performance
      \vskip1ex
      Aggregating the predictor reduces its noise and increases the significance of correlations. 
      \vskip1ex
      The optimal aggregation number can be found by maximizing the regression t-values.
      <<echo=TRUE,eval=FALSE>>=
# Find optimal nagg for predictor
naggs <- 5:100
tvalues <- sapply(naggs, function(nagg) {
  predictor <- roll::roll_mean(rates_diff, width=nagg, min_obs=1)
  predictor <- cbind(rep(1, NROW(predictor)), predictor)
  predictor <- rutils::lagit(predictor)
  model <- lm(response ~ predictor - 1)
  model_sum <- summary(model)
  max(abs(model_sum$coefficients[, 3][-1]))
})  # end sapply
naggs[which.max(tvalues)]
plot(naggs, tvalues, t="l", col="blue", lwd=2)
# Calculate aggregated predictor
nagg <- 53
predictor <- roll::roll_mean(rates_diff, width=nagg, min_obs=1)
predictor <- rutils::lagit(predictor)
predictor <- cbind(rep(1, NROW(predictor)), predictor)
model <- lm(response ~ predictor - 1)
summary(model)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_insample_agg.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate forecasts and pnls in-sample
inverse <- MASS::ginv(predictor)
coeff <- drop(inverse %*% response)
forecasts <- (predictor %*% coeff)
pnls <- sign(forecasts)*response
# Plot dygraph of in-sample IR strategy
wealth <- cbind(returns, pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Aggregated YC Strategy In-sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Aggregated Forecasts Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Needs more work to improve performance
      \vskip1ex
      For out-of-sample forecasts, the training set and the test set are separate.  The model is calibrated on the training data, and forecasts are calculated using the test data. 
      \vskip1ex
      The out-of-sample strategy performs well in periods of high volatility, but otherwise it's flat.
      <<echo=TRUE,eval=FALSE>>=
# Define in-sample and out-of-sample intervals
in_sample <- (dates < as.Date("2020-01-01"))
out_sample <- (dates >= as.Date("2020-01-01"))
# Calculate forecasts and pnls out-of-sample
inverse <- MASS::ginv(predictor[in_sample, ])
coeff <- drop(inverse %*% response[in_sample, ])
forecasts <- (predictor[out_sample, ] %*% coeff)
pnls <- sign(forecasts)*response[out_sample, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_yc_outsample_agg.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of out-of-sample YC strategy
wealth <- cbind(returns[out_sample, ], pnls)
colnames(wealth) <- c("VTI", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Aggregated YC Strategy Out-of-Sample") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Momentum Strategies}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy for an \protect\emph{ETF} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Momentum} strategies can be \emph{backtested} by specifying the portfolio rebalancing frequency, the formation period, and the holding period:
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Specify a portfolio of \emph{ETFs}, stocks, or other assets, and a time series of their returns,
        \item Specify \emph{end points} for the portfolio rebalancing frequency,
        \item Specify \emph{look-back} intervals for portfolio formation, and \emph{look-forward} intervals for portfolio holding, 
        \item Specify a performance function to calculate the past performance of the assets,
        \item Calculate the past performance over the \emph{look-back} formation intervals,
        \item Calculate the portfolio weights from the past performance,
        \item Calculate the future returns over the \emph{look-forward} holding intervals,
        \item Calculate the out-of-sample momentum strategy returns by applying the portfolio weights to the future returns,
        \item Calculate the transaction costs and subtract them from the strategy returns.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
returns <- rutils::etf_env$returns[, symbolv]
returns <- na.omit(returns)
# Or, select rows with IEF data
# returns <- returns[index(rutils::etf_env$IEF)]
# Copy over NA values
# returns[1, is.na(returns[1, ])] <- 0
# returns <- zoo::na.locf(returns, na.rm=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Look-back and Look-forward Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Performance aggregations are be calculated over a vector of overlapping in-sample \emph{look-back} intervals attached at \emph{end points}.
      \vskip1ex
      For example, aggregations at monthly \emph{end points} over overlapping 12-month \emph{look-back} intervals.
      \vskip1ex
      An example of a data aggregation are the cumulative past returns at each \emph{end point}.
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of \emph{end points} in the \emph{look-back} interval.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the length of the \emph{look-back} interval.
      \vskip1ex
      The \emph{look-back} intervals are spanned by the vectors of \emph{start points} and \emph{end points}.
      \vskip1ex
      Performance aggregations are also be calculated over non-overlapping out-of-sample \emph{look-forward} intervals.
      \vskip1ex
      The \emph{look-forward} intervals should not overlap with the \emph{look-back} intervals, in order to avoid data snooping.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define end of month end points
endp <- rutils::calc_endpoints(returns, interval="months")
endp <- endp[-1]
nrows <- NROW(endp)
dates <- zoo::index(returns)[endp]
# Start points equal end points lagged by 12-month look-back interval
look_back <- 12
startp <- c(rep_len(1, look_back-1),
  endp[1:(nrows - look_back + 1)])
# Calculate matrix of look-back intervals
look_backs <- cbind(startp, endp)
colnames(look_backs) <- c("start", "end")
# Calculate matrix of look-forward intervals
look_fwds <- cbind(endp + 1, rutils::lagit(endp, -1))
look_fwds[nrows, ] <- endp[nrows]
colnames(look_fwds) <- c("start", "end")
# Inspect the intervals
head(cbind(look_backs, look_fwds))
tail(cbind(look_backs, look_fwds))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio weights of \emph{momentum} strategies are calculated based on the past performance of the assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      \vskip1ex
      The portfolio weights of \emph{momentum} strategies can be scaled in several different ways.
      \vskip1ex
      To limit the portfolio leverage, the weights can be scaled so that the sum of squares is equal to $1$: $\sum_{i=1}^n {w^2_i} = 1$.
      \vskip1ex
      The weights can also be de-meaned, so that their sum is equal to zero, to create long-short portfolios with small betas: $\sum_{i=1}^n {w_i} = 0$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define performance function as Sharpe ratio
objfunc <- function(returns) sum(returns)/sd(returns)
# Calculate past performance over look-back intervals
past <- apply(look_backs, 1, function(ep) {
  sapply(returns[ep[1]:ep[2]], objfunc)
})  # end sapply
past <- t(past)
past[is.na(past)] <- 0
# Weights are proportional to past performance
weights <- past
# weights[weights < 0] <- 0
# Scale weights so sum of squares is equal to 1.
weights <- weights/sqrt(rowSums(weights^2))
# Or scale weights so sum is equal to 1
# weights <- weights/rowSums(weights)
# Set NA values to zero
weights[is.na(weights)] <- 0
sum(is.na(weights))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting the Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the testing of the accuracy of a forecasting model using simulation on historical data.
      \vskip1ex
      \emph{Backtesting} is a type of \emph{cross-validation} applied to time series data.
      \vskip1ex
      \emph{Backtesting} is performed by \emph{training} the model on past data and \emph{testing} it on future out-of-sample data.
      \vskip1ex
      The \emph{training} data is specified by the \emph{look-back} intervals (\texttt{pas\_t}), and the model forecasts are applied to the future data defined by the \emph{look-forward} intervals (\texttt{fu\_ture}).
      \vskip1ex
      The out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fu\_ture} returns by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The momentum returns are lagged so that they are attached to the end of the future interval, instead of at its beginning.
      <<echo=TRUE,eval=FALSE>>=
# Calculate future out-of-sample performance
future <- apply(look_fwds, 1, function(ep) {
  sapply(returns[ep[1]:ep[2]], sum)
})  # end sapply
future <- t(future)
future[is.na(future)] <- 0
tail(future)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the momentum pnls
pnls <- rowSums(weights*future)
# Lag the future and momentum returns to proper dates
future <- rutils::lagit(future)
pnls <- rutils::lagit(pnls)
# The momentum strategy has low correlation to stocks
cor(pnls, future)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- future %*% weightsaw
# Calculate the wealth of momentum returns
wealth <- xts::xts(cbind(all_weather, pnls), order.by=dates)
colnames(wealth) <- c("All-Weather", "Momentum")
cor(wealth)
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealth), main="Monthly Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Backtest of the Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} momentum strategy returns are calculated by multiplying the weights times the future returns.
      \vskip1ex
      The \emph{transaction costs} are equal to half the \emph{bid-offer spread} $\delta$ times the absolute value of the traded dollar amounts of the \emph{risky assets}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate momentum profits and losses (returns)
pnls <- rowSums(weights*future)
# Lag the momentum returns and weights
# to correspond with end of future interval
pnls <- rutils::lagit(pnls)
weights <- rutils::lagit(weights)
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Calculate transaction costs
wealth <- cumsum(pnls)
costs <- 0.5*bid_offer*wealth*rowSums(abs(rutils::diff_it(weights)))
wealth <- cumsum(pnls - costs)
dates <- index(returns[endp])
wealth <- xts::xts(wealth, dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf.png}
      <<echo=TRUE,eval=FALSE>>=
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
retsaw <- returns %*% weightsaw
wealthaw <- cumsum(retsaw)
wealthaw <- xts::xts(wealthaw[endp], dates)
# Plot the Momentum strategy and benchmark
wealth <- cbind(wealth, wealthaw)
colnames(wealth) <- c("Momentum Strategy", "Benchmark")
dygraphs::dygraph(wealth, main="Momentum Strategy") %>%
  dyAxis("y", label="Benchmark", independentTicks=TRUE) %>%
  dyAxis("y2", label="Momentum Strategy", independentTicks=TRUE) %>%
  dySeries(name="Momentum Strategy", axis="y2", label="Momentum Strategy", strokeWidth=2, col="red") %>%
  dySeries(name="Benchmark", axis="y", label="Benchmark", strokeWidth=2, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional
backtestmomentum <- function(returns,
                      objfunc=function(returns) (sum(returns)/sd(returns)),
                      look_back=12, re_balance="months", bid_offer=0.001,
                      endp=rutils::calc_endpoints(returns, interval=re_balance)[-1],
                      with_weights=FALSE, ...) {
  stopifnot("package:rutils" %in% search() || require("rutils", quietly=TRUE))
  # Define look-back and look-forward intervals
  nrows <- NROW(endp)
  startp <- c(rep_len(1, look_back-1), endp[1:(nrows-look_back+1)])
  # Calculate look-back intervals
  look_backs <- cbind(startp, endp)
  # Calculate look-forward intervals
  look_fwds <- cbind(endp + 1, rutils::lagit(endp, -1))
  look_fwds[nrows, ] <- endp[nrows]
  # Calculate past performance over look-back intervals
  past <- t(apply(look_backs, 1, function(ep) sapply(returns[ep[1]:ep[2]], objfunc)))
  past[is.na(past)] <- 0
  # Calculate future performance
  future <- t(apply(look_fwds, 1, function(ep) sapply(returns[ep[1]:ep[2]], sum)))
  future[is.na(future)] <- 0
  # Scale weights so sum of squares is equal to 1
  weights <- past
  weights <- weights/sqrt(rowSums(weights^2))
  weights[is.na(weights)] <- 0  # Set NA values to zero
  # Calculate momentum profits and losses
  pnls <- rowSums(weights*future)
  # Calculate transaction costs
  costs <- 0.5*bid_offer*cumprod(1 + pnls)*rowSums(abs(rutils::diff_it(weights)))
  pnls <- (pnls - costs)
  if (with_weights)
    rutils::lagit(cbind(pnls, weights))
  else
    rutils::lagit(pnls)
}  # end backtestmomentum
      @
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimization of Momentum Strategy Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The performance of the \emph{momentum} strategy depends on the length of the \emph{look-back interval} used for calculating the past performance.
      \vskip1ex
      Performing a \emph{backtest} allows finding the optimal \emph{momentum} (trading) strategy parameters, such as the \emph{look-back interval}.
      \vskip1ex
      But using a different rebalancing frequency in the \emph{backtest} can produce different values for the optimal trading strategy parameters.
      \vskip1ex
      So \emph{backtesting} just redefines the problem of finding (tuning) the optimal trading strategy parameters, into the problem of finding the optimal \emph{backtest} (meta-model) parameters.
      \vskip1ex
      But the advantage of using the \emph{backtest} meta-model is that it can reduce the number of parameters that need to be optimized.
      \vskip1ex
      Performing many \emph{backtests} on multiple trading strategies risks identifying inherently unprofitable trading strategies as profitable, purely by chance (\emph{p-value hacking}).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_profile.png}
      <<echo=TRUE,eval=FALSE>>=
source("/Users/jerzy/Develop/lecture_slides/scripts/back_test.R")
look_backs <- seq(3, 15, by=1)
objfunc <- function(returns) sum(returns)/sd(returns)
profilev <- sapply(look_backs, function(look_back) {
  pnls <- backtestmomentum(returns=returns, endp=endp,
    look_back=look_back, objfunc=objfunc)
  sum(pnls)
})  # end sapply
# Plot momemntum PnLs
x11(width=6, height=5)
plot(x=look_backs, y=profilev, t="l",
  main="Momemntum PnL as function of look_back",
  xlab="look_back (months)", ylab="pnl")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal ETF Momentum Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The hypothetical out-of-sample \emph{momentum} strategy returns can be calculated by multiplying the \texttt{fu\_ture} returns by the forecast \emph{ETF} portfolio weights.
      \vskip1ex
      The \emph{training} data is specified over the \emph{look-back} intervals, and the forecast weights are applied to the future data defined by the \emph{look-forward} intervals.
      \vskip1ex
      The momentum strategy has performed well for \emph{ETF} portfolios because of the consistent performance of bond ETFs, like \emph{TLT} and \emph{VYM}.
      <<echo=TRUE,eval=FALSE>>=
# Optimal look_back
look_back <- look_backs[which.max(profilev)]
pnls <- backtestmomentum(returns=returns,
  look_back=look_back, endp=endp,
  objfunc=objfunc, with_weights=TRUE)
tail(pnls)
# Calculate the wealth of momentum returns
retsmom <- pnls[, 1]
wealth <- xts::xts(cbind(all_weather, retsmom), order.by=dates)
colnames(wealth) <- c("All-Weather", "Momentum")
cor(wealth)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_etf_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealth), main="Monthly Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue")
quantmod::chart_Series(cumsum(wealth), theme=plot_theme, lwd=2,
             name="Momentum PnL")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Series of Momentum Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In \emph{momentum} strategies, the portfolio weights are adjusted over time to be proportional to the past performance of the assets.
      \vskip1ex
      This way \emph{momentum} strategies switch their weights to the best performing assets.
      \vskip1ex
      The weights are scaled to limit the portfolio \emph{leverage} and its market \emph{beta}.
      <<echo=TRUE,eval=FALSE>>=
# Plot the momentum portfolio weights
weights <- pnls[, -1]
vtis <- log(quantmod::Cl(rutils::etf_env$VTI[dates]))
colnames(vtis) <- "VTI"
datav <- cbind(vtis, weights)
datav <- na.omit(datav)
colnames(datav)[2:NCOL(pnls)] <- paste0(colnames(weights), "_weight")
zoo::plot.zoo(datav, xlab=NULL, main="Momentum Weights")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_weights.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Beta}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy market beta can be calculated by multiplying the \emph{ETF} betas by the \emph{ETF} portfolio weights.
      <<echo=TRUE,eval=FALSE>>=
# Calculate ETF betas
betas_etf <- sapply(returns, function(x)
  cov(returns$VTI, x)/var(x))
# Momentum beta is equal weights times ETF betas
betas <- weights %*% betas_etf
betas <- xts::xts(betas, order.by=dates)
colnames(betas) <- "momentum_beta"
datav <- cbind(betas, vtis)
zoo::plot.zoo(datav,
  oma = c(3, 1, 3, 0), mar = c(0, 4, 0, 1),
  main="Momentum Beta & VTI Price", xlab="")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_betas.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Market timing} skill is the ability to forecast the direction and magnitude of market returns.
      \vskip1ex
      The \emph{Treynor-Mazuy} test shows that the \emph{momentum} strategy has significant \emph{market timing} skill.
      <<echo=(-(1:3)),eval=FALSE>>=
# Open x11 for plotting and set parameters to reduce whitespace around plot
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
# Merton-Henriksson test
vtis <- rutils::diff_it(vtis)
design <- cbind(VTI=vtis, 0.5*(vtis+abs(vtis)), vtis^2)
colnames(design)[2:3] <- c("merton", "treynor")
model <- lm(retsmom ~ VTI + merton, data=design); summary(model)
# Treynor-Mazuy test
model <- lm(retsmom ~ VTI + treynor, data=design); summary(model)
# Plot residual scatterplot
plot.default(x=vtis, y=retsmom, xlab="VTI", ylab="momentum")
title(main="Treynor-Mazuy market timing test\n for Momentum vs VTI", line=0.5)
# Plot fitted (predicted) response values
points.default(x=vtis, y=model$fitted.values, pch=16, col="red")
residuals <- model$residuals
text(x=0.0, y=max(residuals), paste("Treynor test t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_timing.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness of Momentum Strategy Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most assets with \emph{positive returns} suffer from \emph{negative skewness}.
      \vskip1ex
      The \emph{momentum} strategy returns have more positive skewness compared to the negative skewness of \emph{VTI}.
      \vskip1ex
      The \emph{momentum} strategy is a genuine \emph{market anomaly}, because it has both positive returns and positive skewness.
      <<echo=TRUE,eval=FALSE>>=
# Standardize the returns
retsmom_std <- (retsmom-mean(retsmom))/sd(retsmom)
vtis <- (vtis-mean(vtis))/sd(vtis)
# Calculate skewness and kurtosis
apply(cbind(retsmom_std, vtis), 2, function(x)
  sapply(c(skew=3, kurt=4),
    function(e) sum(x^e)))/nrows
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_distr.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
hist(retsmom_std, breaks=30,
  main="Momentum and VTI Return Distributions (standardized",
  xlim=c(-4, 4),
  xlab="", ylab="", freq=FALSE)
# Draw kernel density of histogram
lines(density(retsmom_std), col='red', lwd=2)
lines(density(vtis), col='blue', lwd=2)
# Add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("Momentum", "VTI"),
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining Momentum with the \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{momentum} strategy has attractive returns compared to a static buy-and-hold strategy.
      \vskip1ex
      But the \emph{momentum} strategy suffers from draw-downs called \emph{momentum crashes}, especially after the market rallies from a sharp-sell-off.
      \vskip1ex
      This suggests that combining the \emph{momentum} strategy with a static buy-and-hold strategy can achieve significant diversification of risk.
      <<echo=TRUE,eval=FALSE>>=
# Combine momentum strategy with all-weather
all_weather <- sd(retsmom)*all_weather/sd(all_weather)
wealth <- cbind(retsmom, all_weather, 0.5*(retsmom + all_weather))
colnames(wealth) <- c("momentum", "all_weather", "combined")
# Calculate strategy annualized Sharpe ratios
apply(wealth, MARGIN=2, function(x) {
  sqrt(12)*sum(x)/sd(x)/NROW(x)
})  # end apply
# Calculate strategy correlations
cor(wealth)
# Calculate cumulative wealth
wealth <- xts::xts(wealth, dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_combined.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot ETF momentum strategy combined with All-Weather
dygraphs::dygraph(cumsum(wealth), main="ETF Momentum Strategy Combined with All-Weather") %>%
  dyOptions(colors=c("red", "blue", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Or
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("green", "blue", "red")
quantmod::chart_Series(wealth, theme=plot_theme,
             name="ETF Momentum Strategy Combined with All-Weather")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Momentum Strategy With Daily Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A momentum strategy with \emph{daily} rebalancing can't be practically backtested using \texttt{apply()} loops because they are too slow.
      \vskip1ex
      The package \emph{roll} contains extremely fast functions for calculating rolling aggregations using compiled \texttt{C++} code.
      \vskip1ex
      The momentum strategy with \emph{daily} rebalancing performs worse than the strategy with \emph{monthly} rebalancing.
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling variance
look_back <- 252
variance <- roll::roll_var(returns, width=look_back, min_obs=1)
variance[1, ] <- 1
# Calculate rolling Sharpe
past <- roll::roll_mean(returns, width=look_back, min_obs=1)
weights <- past/sqrt(variance)
weights <- weights/sqrt(rowSums(weights^2))
weights <- rutils::lagit(weights)
sum(is.na(weights))
# Calculate momentum profits and losses
pnls <- rowMeans(weights*response)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_daily_etf.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate transaction costs
bid_offer <- 0.001
costs <- 0.5*bid_offer*rowSums(abs(rutils::diff_it(weights)))
pnls <- (pnls - costs)
# Define all-weather benchmark
weightsaw <- c(0.30, 0.55, 0.15)
all_weather <- returns %*% weightsaw
# Calculate the wealth of momentum returns
wealth <- xts::xts(cbind(all_weather, pnls), order.by=index(returns))
colnames(wealth) <- c("All-Weather", "Momentum")
cor(wealth)
# Plot dygraph of the momentum strategy returns
dygraphs::dygraph(cumsum(wealth)[dates], main="Daily Momentum Strategy vs All-Weather") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Functional for Daily Momentum Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
momentum_daily <- function(returns, look_back=252, bid_offer=0.001, trend=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate rolling variance
  variance <- roll::roll_var(returns, width=look_back, min_obs=1)
  variance[1, ] <- 1
  variance[variance <= 0] <- 1
# Calculate rolling Sharpe
  past <- roll::roll_mean(returns, width=look_back, min_obs=1)
  weights <- past/sqrt(variance)
  weights <- weights/sqrt(rowSums(weights^2))
  weights <- rutils::lagit(weights)
  # Calculate momentum profits and losses
  pnls <- trend*rowMeans(weights*returns)
  # Calculate transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diff_it(weights)))
  (pnls - costs)
}  # end momentum_daily
@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Multiple Daily ETF Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a small \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a daily ETF momentum strategy
source("/Users/jerzy/Develop/lecture_slides/scripts/back_test.R")
pnls <- momentum_daily(returns=returns, look_back=252,
  bid_offer=bid_offer)
# Perform sapply loop over look_backs
look_backs <- seq(50, 300, by=50)
pnls <- sapply(look_backs, momentum_daily,
  returns=returns, bid_offer=bid_offer)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, index(returns))
tail(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_daily_etf_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily ETF Momentum Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      The daily ETF \emph{momentum} strategy can be improved by introducing a holding period for the portfolio.
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
momentum_daily <- function(returns, look_back=252, hold_period=5, bid_offer=0.001, trend=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  # Calculate rolling variance
  variance <- roll::roll_var(returns, width=look_back, min_obs=1)
  variance[1, ] <- 1
  variance[variance <= 0] <- 1
  # Calculate rolling Sharpe
  past <- roll::roll_mean(returns, width=look_back, min_obs=1)
  weights <- past/sqrt(variance)
  weights <- weights/sqrt(rowSums(weights^2))
  weights <- rutils::lagit(weights)
  # Average the weights over holding period
  weights <- roll::roll_mean(weights, width=hold_period, min_obs=1)
  # Calculate momentum profits and losses
  pnls <- trend*rowMeans(weights*returns)
  # Calculate transaction costs
  costs <- 0.5*bid_offer*rowSums(abs(rutils::diff_it(weights)))
  (pnls - costs)
}  # end momentum_daily
@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Daily Momentum Strategy with Holding Period}
\subsection{Multiple Daily ETF Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of holding periods.
      \vskip1ex
      The daily \emph{momentum} strategies with a holding period perform much better.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over holding periods
hold_periods <- seq(2, 11, by=2)
pnls <- sapply(hold_periods, momentum_daily, look_back=120,
                  returns=returns, bid_offer=bid_offer)
colnames(pnls) <- paste0("holding=", hold_periods)
pnls <- xts::xts(pnls, index(returns))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_daily_etf_hold.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily ETF Momentum Strategies with Holding Period") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{draft: Daily rank simple Momentum Strategy with Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
      The daily ETF \emph{momentum} strategy can be improved by introducing a holding period for the portfolio.
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
momentum_daily <- function(returns, look_back=252, hold_period=5, bid_offer=0.001, trend=1, ...) {
  stopifnot("package:quantmod" %in% search() || require("quantmod", quietly=TRUE))
  
  position_s <- matrixStats::rowRanks(returns)
  position_s <- (position_s - rowMeans(position_s))
  position_s <- HighFreq::lagit(position_s, lagg=1)
  trend*rowMeans(position_s*returns)
  
}  # end momentum_daily

# Load ETF data
symbolv <- rutils::etf_env$symbolv
symbolv <- symbolv[!(symbolv %in% c("TLT", "IEF", "MTUM", "QUAL", "VLUE", "USMV"))]
returns <- rutils::etf_env$returns[, symbolv]
returns[1, is.na(returns[1, ])] <- 0
returns <- zoo::na.locf(returns, na.rm=FALSE)

# Load S&P500 data
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
returns <- returns["2000/"]
returns[1, is.na(returns[1, ])] <- 0
returns <- zoo::na.locf(returns, na.rm=FALSE)
ncols <- NCOL(returns)
returns <- returns[, !(returns[ncols %/% 10, ] == 0)]


pnls <- momentum_daily(returns=returns, trend=(-1))
pnls <- xts::xts(pnls, index(returns))
colnames(pnls) <- "PnL"
dygraphs::dygraph(cumsum(pnls), main="Daily Momentum Strategy") %>%
  dyOptions(colors="blue", strokeWidth=1) %>%
  dyLegend(show="always", width=500)

@
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Backtesting Daily rank simple Momentum Strategy with Holding Period}
\subsection{Multiple Daily ETF Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of holding periods.
      \vskip1ex
      The daily \emph{momentum} strategies with a holding period perform much better.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(50, 300, by=50)
pnls <- sapply(look_backs, momentum_daily,
  returns=returns, bid_offer=bid_offer)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, index(returns))

# Perform sapply loop over holding periods
hold_periods <- seq(2, 11, by=2)
pnls <- sapply(hold_periods, momentum_daily, look_back=120, returns=returns)
colnames(pnls) <- paste0("holding=", hold_periods)
pnls <- xts::xts(pnls, index(returns))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_daily_etf_hold.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily ETF Momentum Strategies with Holding Period") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}





%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple S\&P500 Momentum Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{momentum} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a small \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Load daily S&P500 percentage stock returns.
load(file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns100
returns100 <- returns100["2000/"]
returns100[1, is.na(returns100[1, ])] <- 0
returns100 <- zoo::na.locf(returns100, na.rm=FALSE)
# Simulate a daily S&P500 momentum strategy.
# Perform sapply loop over look_backs
look_backs <- seq(100, 300, by=20)
pnls <- sapply(look_backs, momentum_daily,
  hold_period=5, returns=returns100, bid_offer=0)
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, index(returns100))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily S&P500 Momentum Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot daily S&P500 momentum strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Daily S&P500 Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Multiple S\&P500 \protect\emph{Mean Reverting} Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{S\&P500} \emph{mean reverting} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies for the \emph{S\&P500} constituents perform the best for short \emph{look-back} parameters.
      \vskip1ex
      The \emph{mean reverting} strategies had their best performance prior to the 2008 financial crisis.
      <<echo=TRUE,eval=FALSE>>=
# Perform sapply loop over look_backs
look_backs <- seq(3, 20, by=2)
pnls <- sapply(look_backs, momentum_daily,
  hold_period=5, returns=returns100, bid_offer=0, trend=(-1))
colnames(pnls) <- paste0("look_back=", look_backs)
pnls <- xts::xts(pnls, index(returns100))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_sp500_revert.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily S&P500 Momentum Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of S&P500 Mean Reverting Strategies")
legend("topleft", legend=colnames(pnls),
  inset=0.05, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The MTUM Momentum ETF}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{MTUM} ETF is an actively managed ETF following a momentum strategy.
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative returns of VTI vs MTUM ETF
wealth <- log(na.omit(rutils::etf_env$prices[, c("VTI", "MTUM")]))
colnames(wealth) <- c("VTI", "MTUM")
wealth <- rutils::diff_it(wealth)
dygraphs::dygraph(cumsum(wealth), main="VTI vs MTUM ETF") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_mtum.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Portfolio Optimization Strategies}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy invests in the best performing portfolio in the past \emph{in-sample} interval, expecting that it will continue performing well \emph{out-of-sample}.
      \vskip1ex
      The \emph{portfolio optimization} strategy consists of:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculating the maximum Sharpe ratio portfolio weights in the \emph{in-sample} interval,
        \item Applying the weights and calculating the portfolio returns in the \emph{out-of-sample} interval.
      \end{enumerate}
      The optimal portfolio weights $\mathbf{w}$ are equal to the past in-sample excess returns $\mu = \mathbf{r} - r_f$ (in excess of the risk-free rate $r_f$) multiplied by the inverse of the covariance matrix $\mathbb{C}$:
      \begin{displaymath}
        \mathbf{w} = \mathbb{C}^{-1} \mu
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Select all the ETF symbols except "VXX", "SVXY" "MTUM", "QUAL", "VLUE", and "USMV"
symbolv <- colnames(rutils::etf_env$returns)
symbolv <- symbolv[!(symbolv %in% c("VXX", "SVXY", "MTUM", "QUAL", "VLUE", "USMV"))]
# Extract columns of rutils::etf_env$returns and overwrite NA values
returns <- rutils::etf_env$returns[, symbolv]
nassets <- NCOL(returns)
# returns <- na.omit(returns)
returns[1, is.na(returns[1, ])] <- 0
returns <- zoo::na.locf(returns, na.rm=FALSE)
dates <- zoo::index(returns)
# Returns in excess of risk-free rate
riskf <- 0.03/252
excess <- (returns - riskf)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_weights_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
rets_is <- returns["/2014"]
inverse <- MASS::ginv(cov(rets_is))
weights <- inverse %*% colMeans(excess["/2014"])
weights <- drop(weights/sqrt(sum(weights^2)))
names(weights) <- colnames(returns)
# Plot portfolio weights
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
barplot(sort(weights), main="Maximum Sharpe Weights", cex.names=0.7)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The in-sample performance of the optimal portfolio is much better than the equal weight portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
indeks <- xts::xts(rowSums(rets_is)/sqrt(nassets), index(rets_is))
portf_is <- portf_is*sd(indeks)/sd(portf_is)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- cumsum(cbind(portf_is, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="In-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The out-of-sample performance of the optimal portfolio is not nearly as good as in-sample.
      \vskip1ex
      Combining the optimal portfolio with the equal weight portfolio produces and even better performing portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate out-of-sample portfolio returns
rets_os <- returns["2015/"]
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
indeks <- xts::xts(rowSums(rets_os)/sqrt(nassets), index(rets_os))
portf_os <- portf_os*sd(indeks)/sd(portf_os)
pnls <- cbind(portf_os, indeks, (portf_os + indeks)/2)
colnames(pnls) <- c("Optimal", "Equal Weight", "Combined")
sapply(pnls, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
dygraphs::dygraph(cumsum(pnls), main="Out-of-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("red", "blue", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Optimization Strategy for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for ETFs is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy underperforms in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
inverse <- MASS::ginv(cov(rets_is))
weights <- inverse %*% colMeans(excess["/2014"])
weights <- drop(weights/sqrt(sum(weights^2)))
names(weights) <- colnames(returns)
# Calculate in-sample portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
# Calculate out-of-sample portfolio returns
rets_os <- returns["2015/"]
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
indeks <- xts::xts(rowSums(returns)/sqrt(nassets), dates)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Out-of-sample Optimal Portfolio Returns for ETFs") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularized Inverse of Singular Covariance Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{regularization} technique allows calculating the inverse of \emph{singular} covariance matrices while reducing the effects of statistical noise.
      \vskip1ex
      If the number of time periods of returns is less than the number of assets (columns), then the covariance matrix of returns is \emph{singular}, and some of its \emph{eigenvalues} are zero, so it doesn't have an inverse.
      \vskip1ex
      The \emph{regularized} inverse $\mathbb{C}_n^{-1}$ is calculated by removing the higher order eigenvalues that are almost zero, and keeping only the first $n$ \emph{eigenvalues}:
      \begin{displaymath}
        \mathbb{C}_n^{-1} = \mathbb{O}_n \, \mathbb{D}_n^{-1} \, \mathbb{O}_n^T
      \end{displaymath}
      Where $\mathbb{D}_n$ and $\mathbb{O}_n$ are matrices with the higher order eigenvalues and eigenvectors removed.
      \vskip1ex
      The function \texttt{MASS::ginv()} calculates the \emph{regularized} inverse of a matrix.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Create rectangular matrix with collinear columns
ran_dom <- matrix(rnorm(10*8), nc=10)
# Calculate covariance matrix
covmat <- cov(ran_dom)
# Calculate inverse of covmat - error
inverse <- solve(covmat)
# Perform eigen decomposition
eigend <- eigen(covmat)
eigen_vec <- eigend$vectors
eigen_val <- eigend$values
# Set tolerance for determining zero singular values
precision <- sqrt(.Machine$double.eps)
# Calculate regularized inverse matrix
not_zero <- (eigen_val > (precision * eigen_val[1]))
inv_reg <- eigen_vec[, not_zero] %*%
  (t(eigen_vec[, not_zero]) / eigen_val[not_zero])
# Verify inverse property of inv_reg
all.equal(covmat, covmat %*% inv_reg %*% covmat)
# Calculate regularized inverse of covmat
inverse <- MASS::ginv(covmat)
# Verify inverse property of matrixv
all.equal(inverse, inv_reg)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shrinkage Inverse of the Covariance Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For a portfolio of \emph{S\&P500} stocks, the number return columns is very large, which may make the covariance matrix of returns simgular.
      \vskip1ex
      Removing the very small higher order eigenvalues also reduces the propagation of statistical noise and improves the signal-to-noise ratio.
      \vskip1ex
      But removing a larger number of eigenvalues increases the bias of the covariance matrix, which is an example of the \emph{bias-variance tradeoff}.
      \vskip1ex
      Even though the \emph{shrinkage inverse} $\mathbb{C}_n^{-1}$ does not satisfy the matrix inverse property, its out-of-sample forecasts may be more accurate than those using the actual inverse matrix.
      \vskip1ex
      The parameter \texttt{max\_eigen} specifies the number of eigenvalues used for calculating the \emph{regularized} inverse of the covariance matrix of returns.
      \vskip1ex
      The optimal value of the parameter \texttt{max\_eigen} can be determined using \emph{backtesting} (\emph{cross-validation}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample covariance matrix
covmat <- cov(rets_is)
eigend <- eigen(covmat)
eigen_vec <- eigend$vectors
eigen_val <- eigend$values
# Calculate shrinkage inverse of covariance matrix
eigen_max <- 3
inverse <- eigen_vec[, 1:eigen_max] %*%
  (t(eigen_vec[, 1:eigen_max]) / eigend$values[1:eigen_max])
# Verify inverse property of inverse
all.equal(covmat, covmat %*% inverse %*% covmat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Portfolio Optimization for ETFs with Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weights <- inverse %*% colMeans(excess["/2014"])
weights <- drop(weights/sqrt(sum(weights^2)))
names(weights) <- colnames(returns)
# Calculate portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Regularized Out-of-sample Optimal Portfolio Returns for ETFs") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Optimal ETF Portfolio Weights With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
excess_mean <- rowMeans(excess["/2014"])
excess_is <- (1 - alpha)*excess["/2014"] + alpha*excess_mean
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weights <- inverse %*% colMeans(excess_is)
weights <- drop(weights/sqrt(sum(weights^2)))
# Calculate portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Out-of-sample Returns for ETFs With Regularization and Shrinkage") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Monthly ETF Rolling Portfolio Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling monthly strategy performs better than the standard strategy because regularization allows using shorter look\_back intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly dates
format(dates[1], "%m-%Y")
format(dates[NROW(dates)], "%m-%Y")
months <- seq.Date(from=as.Date("2001-01-01"), to=as.Date("2021-04-01"), by="month")
# Perform loop over monthly dates
look_back <- 6
eigen_max <- 3
pnls <- lapply((look_back+1):(NROW(months)-1), function(i) {
  # Define in-sample and out-of-sample returns
  in_sample <- (dates > months[i-look_back]) & (dates < months[i])
  out_sample <- (dates > months[i]) & (dates < months[i+1])
  rets_is <- returns[in_sample]
  rets_os <- returns[out_sample]
  # Calculate shrinkage inverse of covariance matrix
  # inverse <- MASS::ginv(cov(rets_is))  # if VXX and SVXY are included then no shrinkage is better
  eigend <- eigen(cov(rets_is))
  eigen_vec <- eigend$vectors
  eigen_val <- eigend$values
  inverse <- eigen_vec[, 1:eigen_max] %*%
    (t(eigen_vec[, 1:eigen_max]) / eigend$values[1:eigen_max])
  weights <- inverse %*% colMeans(rets_is - riskf)
  weights <- drop(weights/sqrt(sum(weights^2)))
  # Calculate portfolio pnls out-of-sample
  xts::xts(rets_os %*% weights, index(rets_os))
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vtis <- rutils::diff_it(cumsum(indeks)[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("Index", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Monthly ETF Rolling Portfolio Strategy With Shrinkage") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Weekly ETF Rolling Portfolio Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The shrinkage rolling weekly strategy performs better than the standard strategy because regularization allows using shorter look\_back intervals since it suppresses the response noise.
      \vskip1ex
      In the rolling monthly yield curve strategy, the model is recalibrated at the end of every month using a training set of the past \texttt{6} months.
      The coefficients are applied to perform out-of-sample forecasts in the following month.
      <<echo=TRUE,eval=FALSE>>=
# Define weekly dates
weeks <- seq.Date(from=as.Date("2001-01-01"), to=as.Date("2021-04-01"), by="weeks")
# Perform loop over monthly dates
look_back <- 21
eigen_max <- 3
pnls <- lapply((look_back+1):(NROW(weeks)-1), function(i) {
  # Define in-sample and out-of-sample returns
  in_sample <- (dates > weeks[i-look_back]) & (dates < weeks[i])
  out_sample <- (dates > weeks[i]) & (dates < weeks[i+1])
  rets_is <- returns[in_sample]
  rets_os <- returns[out_sample]
  # Calculate shrinkage inverse of covariance matrix
  # inverse <- MASS::ginv(cov(rets_is))  # if VXX and SVXY are included then no shrinkage is better
  inverse <- HighFreq::calc_inv(cov(rets_is), eigen_max=eigen_max)
  weights <- inverse %*% colMeans(rets_is - riskf)
  weights <- drop(weights/sqrt(sum(weights^2)))
  # Calculate portfolio pnls out-of-sample
  xts::xts(rets_os %*% weights, index(rets_os))
})  # end lapply
pnls <- do.call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_weekly_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling monthly IR strategy
vtis <- rutils::diff_it(cumsum(indeks)[zoo::index(pnls),])
wealth <- cbind(vtis, pnls)
colnames(wealth) <- c("Index", "Strategy")
colnamev <- colnames(wealth)
dygraphs::dygraph(cumsum(wealth), main="Weekly ETF Rolling Portfolio Strategy With Shrinkage") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Functional ETF Rolling Portfolio Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weekly ETF \emph{momentum} strategy can be improved by introducing a holding period for the portfolio.
      <<echo=(-(1:1)),eval=FALSE>>=
# Define backtest functional for daily momentum strategy
# If trend=(-1) then it backtests a mean reverting strategy
roll_portf <- function(returns, look_back=252, eigen_max=3, hold_period=5, bid_offer=0.0, trend=1, ...) {
  cat("look_back=", look_back, "\n")
  pnls <- lapply((look_back+1):(NROW(weeks)-1), function(i) {
    # Define in-sample and out-of-sample returns
    in_sample <- (dates > weeks[i-look_back]) & (dates < weeks[i])
    out_sample <- (dates > weeks[i]) & (dates < weeks[i+1])
    rets_is <- returns[in_sample]
    rets_os <- returns[out_sample]
    # Calculate shrinkage inverse of covariance matrix
    # inverse <- MASS::ginv(cov(rets_is))  # if VXX and SVXY are included then no shrinkage is better
    # inverse <- HighFreq::calc_inv(cov(rets_is), eigen_max=eigen_max)
    # weights <- inverse %*% colMeans(rets_is - riskf)
    weights <- HighFreq::calc_weights(rets_is, method="max_sharpe", eigen_max=eigen_max, scale=FALSE)
    weights <- drop(weights/sqrt(sum(weights^2)))
    # Calculate portfolio pnls out-of-sample
    xts::xts(rets_os %*% weights, index(rets_os))
  })  # end lapply
  do.call(rbind, pnls)
}  # end roll_portf
@
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Multiple Weekly ETF Rolling Portfolio Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple daily ETF \emph{momentum} strategies can be backtested by calling the function \texttt{momentum\_daily()} in a loop over a vector of \emph{look-back} parameters.
      \vskip1ex
      The \emph{momentum} strategies do not perform well, especially the ones with a small \emph{look-back} parameter.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a daily ETF momentum strategy
pnls <- roll_portf(returns=returns, look_back=41, eigen_max=eigen_max)
# Perform sapply loop over look_backs
look_backs <- seq(5, 50, by=2)
pnls <- lapply(look_backs, roll_portf,
  returns=returns, eigen_max=eigen_max)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("look_back=", look_backs)
tail(pnls)
pnls[1, is.na(pnls[1, ])] <- 0
pnls <- zoo::na.locf(pnls)
foo <- sapply(pnls, sum)
look_backs[which.max(foo)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/momentum_daily_etf_mult.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls), main="Daily ETF Momentum Strategies") %>%
  dyOptions(colors=colors, strokeWidth=1) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Cumulative Returns of Daily ETF Momentum Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(returns)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for stocks is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy completely fails in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns
returns <- returns["2000/"]
nassets <- NCOL(returns)
returns[1, is.na(returns[1, ])] <- 0
returns <- zoo::na.locf(returns, na.rm=FALSE)
dates <- zoo::index(returns)
riskf <- 0.03/252
excess <- (returns - riskf)
rets_is <- returns["/2010"]
rets_os <- returns["2011/"]
# Maximum Sharpe weights in-sample interval
covmat <- cov(rets_is)
inverse <- MASS::ginv(covmat)
weights <- inverse %*% colMeans(excess["/2010"])
weights <- drop(weights/sqrt(sum(weights^2)))
names(weights) <- colnames(returns)
# Calculate portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
indeks <- xts::xts(rowSums(returns)/sqrt(nassets), dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_stocks_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Out-of-sample Optimal Portfolio Returns for Stocks") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization for Stocks with Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weights <- inverse %*% colMeans(excess["/2010"])
weights <- drop(weights/sqrt(sum(weights^2)))
names(weights) <- colnames(returns)
# Calculate portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
indeks <- xts::xts(rowSums(returns)/sqrt(nassets), dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_stocks_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Regularized Out-of-sample Optimal Portfolio Returns for Stocks") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock Portfolio Weights With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
excess_mean <- rowMeans(excess["/2010"])
excess_is <- (1 - alpha)*excess["/2010"] + alpha*excess_mean
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_stocks_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weights <- inverse %*% colMeans(excess_is)
weights <- drop(weights/sqrt(sum(weights^2)))
# Calculate portfolio returns
portf_is <- xts::xts(rets_is %*% weights, index(rets_is))
portf_os <- xts::xts(rets_os %*% weights, index(rets_os))
# Plot cumulative portfolio returns
pnls <- rbind(portf_is, portf_os)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cumsum(cbind(pnls, indeks))
colnames(pnls) <- c("Optimal Portfolio", "Equal Weight Portfolio")
dygraphs::dygraph(pnls, main="Out-of-sample Returns for Stocks With Regularization and Shrinkage") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyEvent(index(last(rets_is[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Covariance Matrix Inverse Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
  \column{0.5\textwidth}
      \emph{RcppArmadillo} can be used to quickly calculate the regularized inverse of a covariance matrix.
      <<echo=TRUE,eval=FALSE>>=
library(RcppArmadillo)
# Source Rcpp functions from file
Rcpp::sourceCpp("/Users/jerzy/Develop/lecture_slides/scripts/back_test.cpp")
# Create random matrix of returns
matrixv <- matrix(rnorm(300), nc=5)
# Regularized inverse of covariance matrix
eigen_max <- 4
eigend <- eigen(cov(matrixv))
covinv <- eigend$vectors[, 1:eigen_max] %*%
  (t(eigend$vectors[, 1:eigen_max]) / eigend$values[1:eigen_max])
# Regularized inverse using RcppArmadillo
covinv_arma <- calc_inv(matrixv, eigen_max)
all.equal(covinv, covinv_arma)
# Microbenchmark RcppArmadillo code
library(microbenchmark)
summary(microbenchmark(
  pure_r={eigend <- eigen(cov(matrixv))
    eigend$vectors[, 1:eigen_max] %*%
      (t(eigend$vectors[, 1:eigen_max]) / eigend$values[1:eigen_max])
  },
  r_cpp=calc_inv(matrixv, eigen_max),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \column{0.5\textwidth}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat calc_inv(const arma::mat& tseries,
                   double eigen_thresh = 0.001, 
                   arma::uword eigen_max = 0) {
  
  if (eigen_max == 0) {
    // Calculate the inverse using arma::pinv()
    return arma::pinv(tseries, eigen_thresh);
  } else {
    // Calculate the regularized inverse using SVD decomposition
    
    // Allocate SVD
    arma::vec svd_val;
    arma::mat svd_u, svd_v;
    
    // Calculate the SVD
    arma::svd(svd_u, svd_val, svd_v, tseries);
    
    // Subset the SVD
    eigen_max = eigen_max - 1;
    // For no regularization: eigen_max = tseries.n_cols
    svd_u = svd_u.cols(0, eigen_max);
    svd_v = svd_v.cols(0, eigen_max);
    svd_val = svd_val.subvec(0, eigen_max);
    
    // Calculate the inverse from the SVD
    return svd_v*arma::diagmat(1/svd_val)*svd_u.t();
    
  }  // end if
  
}  // end calc_inv
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast portfolio optimization using matrix algebra can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::vec calc_weights(const arma::mat& returns, // Portfolio returns
                       std::string method = "ranksharpe",
                       double eigen_thresh = 0.001,
                       arma::uword eigen_max = 0,
                       double confi = 0.1,
                       double alpha = 0.0,
                       bool scale = true,
                       double vol_target = 0.01) {
  // Initialize
  arma::vec weights(returns[ncols, fill::zeros);
  if (eigen_max == 0)  eigen_max = returns[ncols;
  
  // Switch for the different methods for weights
  switch(calc_method(method)) {
  case method::ranksharpe: {
    // Mean returns by columns
    arma::vec mean_cols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    mean_cols = mean_cols/sd_cols;
    // Weights equal to ranks of Sharpe
    weights = conv_to<vec>::from(arma::sort_index(arma::sort_index(mean_cols)));
    weights = (weights - arma::mean(weights));
    break;
  }  // end ranksharpe
  case method::max_sharpe: {
    // Mean returns by columns
    arma::vec mean_cols = arma::trans(arma::mean(returns, 0));
    // Shrink mean_cols to the mean of returns
    mean_cols = ((1-alpha)*mean_cols + alpha*arma::mean(mean_cols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), eigen_max);
    // weights = calc_inv(cov(returns), eigen_max)*mean_cols;
    weights = calc_inv(cov(returns), eigen_thresh, eigen_max)*mean_cols;
    break;
  }  // end max_sharpe
  case method::max_sharpe_median: {
    // Mean returns by columns
    arma::vec mean_cols = arma::trans(arma::median(returns, 0));
    // Shrink mean_cols to the mean of returns
    mean_cols = ((1-alpha)*mean_cols + alpha*arma::median(mean_cols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), eigen_max);
    weights = calc_inv(cov(returns), eigen_thresh, eigen_max)*mean_cols;
    break;
  }  // end max_sharpe_median
  case method::min_var: {
    // Apply regularized inverse to unit vector
    weights = calc_inv(cov(returns), eigen_thresh, eigen_max)*arma::ones(returns[ncols);
    break;
  }  // end min_var
  case method::min_varpca: {
    // Calculate highest order principal component
    arma::vec eigen_val;
    arma::mat eigen_vec;
    arma::eig_sym(eigen_val, eigen_vec, arma::cov(returns));
    weights = eigen_vec.col(0);
    break;
  }  // end min_varpca
  case method::rank: {
    // Mean returns by columns
    arma::vec mean_cols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    mean_cols = mean_cols/sd_cols;
    // Weights equal to ranks of Sharpe
    weights = conv_to<vec>::from(arma::sort_index(arma::sort_index(mean_cols)));
    weights = (weights - arma::mean(weights));
    break;
  }  // end rank
  case method::rankrob: {
    // Median returns by columns
    arma::vec mean_cols = arma::trans(arma::median(returns, 0));
    // mean_cols = ((1-alpha)*mean_cols + alpha*arma::mean(mean_cols));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    mean_cols = mean_cols/sd_cols;
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), eigen_max);
    // weights = calc_inv(cov(returns), eigen_max)*mean_cols;
    // weights = calc_inv(cov(returns), eigen_max)*mean_cols;
    // // Standard deviation by columns
    // arma::vec sd_cols = mean_cols;
    // for (arma::uword it=0; it < returns[ncols; it++) {
    //   sd_cols(it) = arma::median(arma::abs((returns.col(it) - sd_cols)));
    // }  // end for
    // sd_cols.replace(0, 1);
    // mean_cols = mean_cols/sd_cols;
    // Weights equal to ranks of Sharpe
    weights = conv_to<vec>::from(arma::sort_index(arma::sort_index(mean_cols)));
    // level;
    weights = (weights - arma::mean(weights));
    break;
  }  // end rankrob
  case method::quantile: {
    // Sum of quantiles for columns
    arma::vec levels = {confi, 1-confi};
    weights = conv_to<vec>::from(arma::sum(arma::quantile(returns, levels, 0), 0));
    // Weights equal to ranks
    weights = conv_to<vec>::from(arma::sort_index(arma::sort_index(weights)));
    weights = (weights - arma::mean(weights));
    break;
  }  // end quantile
  default : {
    cout << "Warning: Invalid method parameter: " << method << endl;
    return arma::ones(returns[ncols);
  }  // end default
  }  // end switch
  
  if (scale == TRUE) {
    // return weights/std::sqrt(sum(square(weights)));
    // return weights/sum(weights);
    // Returns of equally weighted portfolio
    // arma::vec mean_rows = arma::mean(returns, 1);
    // Returns of weighted portfolio
    // arma::vec returns_portf = returns*weights;
    // Scale weights to equally weighted portfolio and return them
    // return weights*arma::stddev(arma::mean(returns, 1))/arma::stddev(returns*weights);
    // Scale weights so the resulting portfolio has a volatility equal to vol_target
    return weights*vol_target/arma::stddev(returns*weights);
  }  // end if
  
  return weights;
  
}  // end calc_weights
    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Strategy Backtesting Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast backtesting of strategies can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat back_test(const arma::mat& excess, // Portfolio excess returns
                    const arma::mat& returns, // Portfolio returns
                    arma::uvec startp, 
                    arma::uvec endp, 
                    std::string method = "ranksharpe",
                    double eigen_thresh = 0.001,
                    arma::uword eigen_max = 0,
                    double confi = 0.1,
                    double alpha = 0.0,
                    bool scale = true,
                    double vol_target = 0.01,
                    double coeff = 1.0,
                    double bid_offer = 0.0) {
  
  arma::vec weights(returns[ncols, fill::zeros);
  arma::vec weights_past = zeros(returns[ncols);
  arma::mat pnls = zeros(returns*nrows, 1);
  
  // Perform loop over the end points
  for (arma::uword it = 1; it < endp.size(); it++) {
    // cout << "it: " << it << endl;
    // Calculate portfolio weights
    weights = coeff*calc_weights(excess.rows(startp(it-1), endp(it-1)), method, eigen_thresh, eigen_max, confi, alpha, scale, vol_target);
    // Calculate out-of-sample returns
    pnls.rows(endp(it-1)+1, endp(it)) = returns.rows(endp(it-1)+1, endp(it))*weights;
    // Add transaction costs
    pnls.row(endp(it-1)+1) -= bid_offer*sum(abs(weights - weights_past))/2;
    weights_past = weights;
  }  // end for
  
  // Return the strategy pnls
  return pnls;
  
}  // end back_test

    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy for S\&P500 Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling portfolio optimization} strategy consists of rebalancing a portfolio over the end points:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculate the maximum Sharpe ratio portfolio weights at each end point,
        \item Apply the weights in the next interval and calculate the out-of-sample portfolio returns.
      \end{enumerate}
      The parameters of this strategy are: the rebalancing frequency (annual, monthly, etc.), and the length of look-back interval.
      <<echo=TRUE,eval=FALSE>>=
# Overwrite NA values in returns100
returns100[1, is.na(returns100[1, ])] <- 0
returns100 <- zoo::na.locf(returns100, na.rm=FALSE)
excess <- (returns100 - riskf)
ncols <- NCOL(returns100) ; dates <- index(returns100)
nassets <- NCOL(returns100)
# Define monthly end points
endp <- rutils::calc_endpoints(returns100, interval="months")
endp <- endp[endp > (ncols+1)]
nrows <- NROW(endp) ; look_back <- 12
startp <- c(rep_len(0, look_back-1), endp[1:(nrows-look_back+1)])
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500.png}
      <<echo=TRUE,eval=FALSE>>=
# Perform loop over end points
pnls <- lapply(2:nrows, function(i) {
    # Subset the excess returns
    excess <- excess[startp[i-1]:endp[i-1], ]
    inverse <- MASS::ginv(cov(excess))
    # Calculate the maximum Sharpe ratio portfolio weights
    weights <- inverse %*% colMeans(excess)
    weights <- drop(weights/sqrt(sum(weights^2)))
    # Calculate the out-of-sample portfolio returns
    returns <- returns100[(endp[i-1]+1):endp[i], ]
    xts::xts(returns %*% weights, index(returns))
})  # end lapply
pnls <- rutils::do_call(rbind, pnls)
# Calculate returns of equal weight portfolio
indeks <- xts::xts(rowMeans(returns100), dates)
# Plot cumulative strategy returns
wealth <- na.omit(cbind(pnls, indeks*sd(rets_portf)/sd(indeks)))
colnames(wealth) <- c("Rolling Strategy", "Equal Weight")
dygraphs::dygraph(cumsum(wealth), main="Rolling Portfolio Optimization Strategy for S&P500 Stocks") %>%
  dyOptions(colors=c("red", "blue"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the rolling portfolio optimization strategy the portfolio weights are adjusted to their optimal values at every end point.
      \vskip1ex
      A portfolio optimization is performed using past data, and the optimal portfolio weights are applied out-of-sample in the next interval.
      \vskip1ex
      The weights are scaled to match the volatility of the equally weighted portfolio, and are kept constant until the next end point.
      <<echo=TRUE,eval=FALSE>>=
# Shift end points to C++ convention
endp <- (endp - 1)
endp[endp < 0] <- 0
startp <- (startp - 1)
startp[startp < 0] <- 0
# Specify shrinkage intensity
alpha <- 0.7
eigen_max <- 21
# Perform backtest in Rcpp
pnls <- HighFreq::back_test(excess=excess, returns=returns100,
  startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(pnls, indeks, (pnls+indeks)/2)
wealth <- cumsum(na.omit(wealth))
colnamev <- c("Strategy", "Index", "Combined")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth[endp], main="Rolling S&P500 Portfolio Optimization Strategy With Shrinkage") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=1) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=1) %>%
  dySeries(name=colnamev[3], axis="y2", col="green", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Strategy Parameters Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal values of the parameters \texttt{max\_eigen} and $\alpha$ can be determined using \emph{backtesting}.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over alphas
alpha_s <- seq(from=0.01, to=0.91, by=0.1)
pnls <- lapply(alpha_s, function(alpha) {
  HighFreq::back_test(excess=excess, returns=returns100,
  startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=alpha_s, y=profilev, t="l", main="Strategy PnL as Function of Shrinkage Intensity Alpha",
  xlab="Shrinkage Intensity Alpha", ylab="pnl")
alpha <- alpha_s[which.max(profilev)]
pnls <- pnls[[which.max(profilev)]]
# Perform backtest over eigen_maxs
eigen_maxs <- seq(from=3, to=40, by=2)
pnls <- lapply(eigen_maxs, function(eigen_max) {
  HighFreq::back_test(excess=excess, returns=returns100,
    startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=eigen_maxs, y=profilev, t="l", main="Strategy PnL as Function of eigen_max",
  xlab="eigen_max", ylab="pnl")
eigen_max <- eigen_maxs[which.max(profilev)]
pnls <- pnls[[which.max(profilev)]]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(pnls, indeks, (pnls+indeks)/2)
wealth <- cumsum(na.omit(wealth))
colnamev <- c("Strategy", "Index", "Combined")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth[endp], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=1) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=1) %>%
  dySeries(name=colnamev[3], axis="y2", col="green", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Look-back Interval Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal value of the look-back interval can be determined using \emph{backtesting}.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over look-backs
look_backs <- seq(from=3, to=24, by=1)
pnls <- lapply(look_backs, function(look_back) {
  startp <- c(rep_len(0, look_back-1), endp[1:(nrows-look_back+1)])
  startp <- (startp - 1)
  startp[startp < 0] <- 0
  HighFreq::back_test(excess=excess, returns=returns100,
    startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=look_backs, y=profilev, t="l", main="Strategy PnL as Function of Look-back Interval",
  xlab="Look-back Interval", ylab="pnl")
look_back <- look_backs[which.max(profilev)]
pnls <- pnls[[which.max(profilev)]]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink_optim_lookback.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(pnls, indeks, (pnls+indeks)/2)
wealth <- cumsum(na.omit(wealth))
colnamev <- c("Strategy", "Index", "Combined")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth[endp], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=1) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=1) %>%
  dySeries(name=colnamev[3], axis="y2", col="green", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


\end{document}
