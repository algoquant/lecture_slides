% FRE7241_Lecture2
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#2]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#2, Fall 2022}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{September 13, 2022}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Market Databases}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{ETF} Database}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Exchange-traded Funds (\emph{ETFs}) are funds which invest in portfolios of assets, such as stocks, commodities, or bonds.
      \vskip1ex
      \emph{ETFs} are shares in portfolios of assets, and they are traded just like stocks.
      \vskip1ex
      \emph{ETFs} provide investors with convenient, low cost, and liquid instruments to invest in various portfolios of assets.
      \vskip1ex
      The file \texttt{etf\_list.csv} contains a database of exchange-traded funds (\emph{ETFs}) and exchange traded notes (\emph{ETNs}).
      \vskip1ex
      We will select a portfolio of \emph{ETFs} for illustrating various investment strategies.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=TRUE,size="tiny">>=
# Select ETF symbols for asset allocation
symbolv <- c("VTI", "VEU", "EEM", "XLY", "XLP", "XLE", "XLF",
  "XLV", "XLI", "XLB", "XLK", "XLU", "VYM", "IVW", "IWB", "IWD",
  "IWF", "IEF", "TLT", "VNQ", "DBC", "GLD", "USO", "VXX", "SVXY",
  "MTUM", "IVE", "VLUE", "QUAL", "VTV", "USMV")
# Read etf database into data frame
etflist <- read.csv(file="/Users/jerzy/Develop/lecture_slides/data/etf_list.csv")
rownames(etflist) <- etflist$Symbol
# Select from etflist only those ETF's in symbolv
etflist <- etflist[symbolv, ]
# Shorten names
etfnames <- sapply(etflist$Name, function(name) {
  namesvplit <- strsplit(name, split=" ")[[1]]
  namesvplit <- namesvplit[c(-1, -NROW(namesvplit))]
  name_match <- match("Select", namesvplit)
  if (!is.na(name_match))
    namesvplit <- namesvplit[-name_match]
  paste(namesvplit, collapse=" ")
})  # end sapply
etflist$Name <- etfnames
etflist["IEF", "Name"] <- "10 year Treasury Bond Fund"
etflist["TLT", "Name"] <- "20 plus year Treasury Bond Fund"
etflist["XLY", "Name"] <- "Consumer Discr. Sector Fund"
etflist["EEM", "Name"] <- "Emerging Market Stock Fund"
etflist["MTUM", "Name"] <- "Momentum Factor Fund"
etflist["SVXY", "Name"] <- "Short VIX Futures"
etflist["VXX", "Name"] <- "Long VIX Futures"
etflist["DBC", "Name"] <- "Commodity Futures Fund"
etflist["USO", "Name"] <- "WTI Oil Futures Fund"
etflist["GLD", "Name"] <- "Physical Gold Fund"
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{ETF} Portfolio for Investment Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio contains \emph{ETFs} representing different \emph{industry sectors} and \emph{investment styles}.
      \vskip1ex
      The \emph{ETFs} with names \emph{X*} represent industry \emph{sector funds} (energy, financial, etc.)
      \vskip1ex
      The \emph{ETFs} with names \emph{I*} represent \emph{style funds} (value, growth, size).
      \vskip1ex
      \emph{IWB} is the Russell 1000 small-cap fund.
      \vskip1ex
      \emph{MTUM} is an \emph{ETF} which owns a stock portfolio representing the \emph{momentum factor}.
      \vskip1ex
      \emph{DBC} is an \emph{ETF} providing the total return on a portfolio of commodity futures.
      \vskip1ex
      \emph{VXX} is an \emph{ETN} providing the total return of \emph{long VIX} futures contracts (specifically the \emph{S\&P} VIX Short-Term Futures Index).
      \vskip1ex
      \emph{VXX} is \emph{bearish} because it's \emph{long} VIX futures, and the VIX \emph{rises} when stock prices \emph{drop}.
      \vskip1ex
      \emph{SVXY} is an \emph{ETF} providing the total return of \emph{short VIX} futures contracts.
      \vskip1ex
      \emph{SVXY} is \emph{bullish} because it's \emph{short} VIX futures, and the VIX \emph{drops} when stock prices \emph{rise}.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=FALSE,eval=TRUE,results='asis'>>=
print(xtable::xtable(etflist), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exchange Traded Notes (\protect\emph{ETNs})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{ETNs} are similar to \emph{ETFs}, with the difference that \emph{ETFs} are shares in a fund which owns the underlying assets, while \emph{ETNs} are notes from issuers which promise payouts according to a formula tied to the underlying asset.
      \vskip1ex
      \emph{ETFs} are similar to mutual funds, while \emph{ETNs} are similar to corporate bonds.
      \vskip1ex
      \emph{ETNs} are technically unsecured corporate debt, but instead of fixed coupons, they promise to provide returns on a market index or futures contract.
      \vskip1ex
      The \emph{ETN} issuer promises the payout and is responsible for tracking the index.
      \vskip1ex
      The \emph{ETN} investor has counterparty credit risk to the \emph{ETN} issuer.
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Downloading ETF Prices Using Package \protect\emph{quantmod}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{getSymbols()} downloads time series data into the specified \emph{environment}.
      \vskip1ex
      \texttt{getSymbols()} downloads the daily \emph{OHLC} prices and trading volume (Open, High, Low, Close, Adjusted, Volume).
      \vskip1ex
      \texttt{getSymbols()} creates objects in the specified \emph{environment} from the input strings (names), and assigns the data to those objects, without returning them as a function value, as a \emph{side effect}.
      \vskip1ex
      If the argument \texttt{"auto.assign"} is set to \texttt{FALSE}, then \texttt{getSymbols()} returns the data, instead of assigning it silently.
      \vskip1ex
      \emph{Yahoo} data quality deteriorated significantly in \texttt{2017}, and \emph{Google} data quality is also poor, leaving \emph{Tiingo} and \emph{Alpha Vantage} as the only major providers of free daily \emph{OHLC} stock prices.
      \vskip1ex
      But \href{https://www.quandl.com/}{Quandl} doesn't provide free \emph{ETF} pricev, leaving \emph{Alpha Vantage} as the best provider of free daily \emph{ETF} prices.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Select ETF symbols for asset allocation
symbolv <- c("VTI", "VEU", "EEM", "XLY", "XLP", "XLE", "XLF",
  "XLV", "XLI", "XLB", "XLK", "XLU", "VYM", "IVW", "IWB", "IWD",
  "IWF", "IEF", "TLT", "VNQ", "DBC", "GLD", "USO", "VXX", "SVXY",
  "MTUM", "IVE", "VLUE", "QUAL", "VTV", "USMV")
library(rutils)  # Load package rutils
etfenv <- new.env()  # New environment for data
# Boolean vector of symbols already downloaded
isdownloaded <- symbolv %in% ls(etfenv)
# Download data for symbolv using single command - creates pacing error
getSymbols.av(symbolv, adjust=TRUE, env=etfenv,
  output.size="full", api.key="T7JPW54ES8G75310")
# Download data from Alpha Vantage using while loop
nattempts <- 0  # number of download attempts
while (((sum(!isdownloaded)) > 0) & (nattempts<10)) {
  # Download data and copy it into environment
  nattempts <- nattempts + 1
  cat("Download attempt = ", nattempts, "\n")
  for (symbol in na.omit(symbolv[!isdownloaded][1:5])) {
    cat("Processing: ", symbol, "\n")
    tryCatch(  # With error handler
      quantmod::getSymbols.av(symbol, adjust=TRUE, env=etfenv, auto.assign=TRUE, output.size="full", api.key="T7JPW54ES8G75310"),
      # Error handler captures error condition
      error=function(error_cond) {
        print(paste("error handler: ", error_cond))
      },  # end error handler
      finally=print(paste("symbol=", symbol))
    )  # end tryCatch
  }  # end for
  # Update vector of symbols already downloaded
  isdownloaded <- symbolv %in% ls(etfenv)
  cat("Pausing 1 minute to avoid pacing...\n")
  Sys.sleep(65)
}  # end while
# Download all symbolv using single command - creates pacing error
# quantmod::getSymbols.av(symbolv, env=etfenv, adjust=TRUE, from="2005-01-03", output.size="full", api.key="T7NHW54ES8GG501C")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Inspecting ETF Prices in an Environment}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{get()} retrieves objects that are referenced using character strings, instead of their names.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
ls(etfenv)  # List files in etfenv
# Get class of object in etfenv
class(get(x=symbolv[1], envir=etfenv))
# Another way
class(etfenv$VTI)
colnames(etfenv$VTI)
# Get first 3 rows of data
head(etfenv$VTI, 3)
# Get last 11 rows of data
tail(etfenv$VTI, 11)
# Get class of all objects in etfenv
eapply(etfenv, class)
# Get class of all objects in R workspace
lapply(ls(), function(ob_ject) class(get(ob_ject)))
# Get end dates of all objects in etfenv
as.Date(sapply(etfenv, end))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Adjusting Stock Prices Using Package \protect\emph{quantmod}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Traded stock and bond prices experience jumps after splits and dividends, and must be adjusted to account for them.
      \vskip1ex
      The function \texttt{adjustOHLC()} adjusts \emph{OHLC} prices.
      \vskip1ex
      The function \texttt{get()} retrieves objects that are referenced using character strings, instead of their names.
      \vskip1ex
      The function \texttt{assign()} assigns a value to an object in a specified \emph{environment}, by referencing it using a character string (name).
      \vskip1ex
      The functions \texttt{get()} and \texttt{assign()} allow retrieving and assigning values to objects that are referenced using character strings.
      \vskip1ex
      The function \texttt{mget()} accepts a vector of strings and returns a list of the corresponding objects extracted from an \emph{environment}.
      \vskip1ex
      If the argument \texttt{"adjust"} in function \texttt{getSymbols()} is set to \texttt{TRUE}, then \texttt{getSymbols()} returns adjusted data.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# Check of object is an OHLC time series
is.OHLC(etfenv$VTI)
# Adjust single OHLC object using its name
etfenv$VTI <- adjustOHLC(etfenv$VTI, use.Adjusted=TRUE)

# Adjust OHLC object using string as name
assign(symbolv[1], adjustOHLC(
    get(x=symbolv[1], envir=etfenv), use.Adjusted=TRUE),
  envir=etfenv)

# Adjust objects in environment using vector of strings
for (symbol in ls(etfenv)) {
  assign(symbol,
         adjustOHLC(get(symbol, envir=etfenv), use.Adjusted=TRUE),
         envir=etfenv)
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extracting Time Series from Environments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{mget()} accepts a vector of strings and returns a list of the corresponding objects extracted from an \emph{environment}.
      \vskip1ex
      The extractor (accessor) functions from package \emph{quantmod}: \texttt{Cl()}, \texttt{Vo()}, etc., extract columns from \emph{OHLC} data.
      \vskip1ex
      A list of \emph{xts} series can be flattened into a single \emph{xts} series using the function \texttt{do.call()}.
      \vskip1ex
      The function \texttt{do.call()} executes a function call using a function name and a list of arguments.
      \vskip1ex
      \texttt{do.call()} passes the list elements individually, instead of passing the whole list as one argument.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
      \vskip1ex
      Time series can also be extracted from an \emph{environment} by coercing it into a \texttt{list}, and then subsetting and merging it into an \emph{xts} series using the function \texttt{do.call()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)  # Load package rutils
# Define ETF symbols
symbolv <- c("VTI", "VEU", "IEF", "VNQ")
# Extract symbolv from rutils::etfenv
pricev <- mget(symbolv, envir=rutils::etfenv)
# pricev is a list of xts series
class(pricev)
class(pricev[[1]])
tail(pricev[[1]])
# Extract close prices
pricev <- lapply(pricev, quantmod::Cl)
# Collapse list into time series the hard way
prices2 <- cbind(pricev[[1]], pricev[[2]], pricev[[3]], pricev[[4]])
class(pricev2)
dim(pricev2)
# Collapse list into time series using do.call()
pricev <- do.call(cbind, pricev)
all.equal(pricev2, pricev)
class(pricev)
dim(pricev)
# Or extract and cbind in single step
pricev <- do.call(cbind, lapply(
  mget(symbolv, envir=rutils::etfenv), quantmod::Cl))
# Or extract and bind all data, subset by symbolv
pricev <- lapply(symbolv, function(symbol) {
    quantmod::Cl(get(symbol, envir=rutils::etfenv))
})  # end lapply
# Or loop over etfenv without anonymous function
pricev <- do.call(cbind,
  lapply(as.list(rutils::etfenv)[symbolv], quantmod::Cl))
# Same, but works only for OHLC series - produces error
pricev <- do.call(cbind,
  eapply(rutils::etfenv, quantmod::Cl)[symbolv])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Managing Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time series columns can be renamed, and then saved into \texttt{.csv} files.
      \vskip1ex
      The function \texttt{strsplit()} splits the elements of a character vector.
      \vskip1ex
      The package \emph{zoo} contains functions \texttt{write.zoo()} and \texttt{read.zoo()} for writing and reading \emph{zoo} time series from \texttt{.txt} and \texttt{.csv} files.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
      \vskip1ex
      The function \texttt{assign()} assigns a value to an object in a specified \emph{environment}, by referencing it using a character string (name).
      \vskip1ex
      The function \texttt{save()} writes objects to compressed binary \texttt{.RData} files.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Column names end with ".Close"
colnames(pricev)
strsplit(colnames(pricev), split="[.]")
do.call(rbind, strsplit(colnames(pricev), split="[.]"))
do.call(rbind, strsplit(colnames(pricev), split="[.]"))[, 1]
# Drop ".Close" from colnames
colnames(pricev) <- rutils::get_name(colnames(pricev))
# Or
# colnames(pricev) <- do.call(rbind,
#   strsplit(colnames(pricev), split="[.]"))[, 1]
tail(pricev, 3)
# Which objects in global environment are class xts?
unlist(eapply(globalenv(), is.xts))
# Save xts to csv file
write.zoo(pricev,
  file="/Users/jerzy/Develop/lecture_slides/data/etf_series.csv", sep=",")
# Copy prices into etfenv
etfenv$pricev <- pricev
# Or
assign("pricev", pricev, envir=etfenv)
# Save to .RData file
save(etfenv, file="etf_data.RData")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating Percentage Returns from Close Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{quantmod::dailyReturn()} calculates the percentage daily returns from the \emph{Close} prices.
      \vskip1ex
      The \texttt{lapply()} and \texttt{sapply()} functionals perform a loop over the columns of \emph{zoo} and \emph{xts} series.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract VTI prices
pricev <- etfenv$pricev[ ,"VTI"]
pricev <- na.omit(pricev)
# Calculate percentage returns "by hand"
pricel <- as.numeric(pricev)
pricel <- c(pricel[1], pricel[-NROW(pricel)])
pricel <- xts(pricel, zoo::index(pricev))
retsp <- (pricev-pricel)/pricel
# Calculate percentage returns using dailyReturn()
retsd <- quantmod::dailyReturn(pricev)
head(cbind(retsd, retsp))
all.equal(retsd, retsp, check.attributes=FALSE)
# Calculate returns for all prices in etfenv$prices
retsp <- lapply(etfenv$pricev, function(xtsv) {
  retsd <- quantmod::dailyReturn(na.omit(xtsv))
  colnames(retsd) <- names(xtsv)
  retsd
})  # end lapply
# "retsp" is a list of xts
class(retsp)
class(retsp[[1]])
# Flatten list of xts into a single xts
retsp <- do.call(cbind, retsp)
class(retsp)
dim(retsp)
# Copy retsp into etfenv and save to .RData file
# assign("retsp", retsp, envir=etfenv)
etfenv$retsp <- retsp
save(etfenv, file="/Users/jerzy/Develop/lecture_slides/data/etf_data.RData")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Managing Data Inside Environments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{as.environment()} coerces objects (listv) into an environment.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
      \vskip1ex
      The function \texttt{mget()} accepts a vector of strings and returns a list of the corresponding objects extracted from an \emph{environment}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)
startd <- "2012-05-10"; endd <- "2013-11-20"
# Select all objects in environment and return as environment
new_env <- as.environment(eapply(etfenv, "[",
                  paste(startd, endd, sep="/")))
# Select only symbolv in environment and return as environment
new_env <- as.environment(
  lapply(as.list(etfenv)[symbolv], "[",
         paste(startd, endd, sep="/")))
# Extract and cbind Close prices and return to environment
assign("prices", rutils::do_call(cbind,
               lapply(ls(etfenv), function(symbol) {
                 xtsv <- quantmod::Cl(get(symbol, etfenv))
                 colnames(xtsv) <- symbol
                 xtsv
               })), envir=new_env)
# Get sizes of OHLC xts series in etfenv
sapply(mget(symbolv, envir=etfenv), object.size)
# Extract and cbind adjusted prices and return to environment
colname <- function(xtsv)
  strsplit(colnames(xtsv), split="[.]")[[1]][1]
assign("prices", rutils::do_call(cbind,
               lapply(mget(etfenv$symbolv, envir=etfenv),
                      function(xtsv) {
                        xtsv <- Ad(xtsv)
                        colnames(xtsv) <- colname(xtsv)
                        xtsv
               })), envir=new_env)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Loading Stock Tickers}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The file \texttt{sp500\_constituents.csv} contains a \emph{data frame} of \emph{S\&P500} constituents.
      \vskip1ex
      The stock tickers are stored in the column \texttt{"Ticker"}.
      \vskip1ex
      The \emph{data frame} contains duplicate tickers, which must be removed.
      \vskip1ex
      Some tickers (like "BRK.B" and "BF.B") are not valid symbols in \emph{Tiingo}, so they must be renamed.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load data frame of S&P500 constituents from CSV file
sp500 <- read.csv(file="/Users/jerzy/Develop/lecture_slides/data/sp500_constituents.csv")
# Inspect data frame of S&P500 constituents
dim(sp500)
colnames(sp500)
# Extract tickers from the column Ticker
symbolv <- sp500$Ticker
# Get duplicate tickers
tablev <- table(symbolv)
duplicates <- tablev[tablev>1]
duplicates <- names(duplicates)
# Get duplicate records (rows) of sp500
sp500[symbolv %in% duplicates, ]
# Get unique tickers
symbolv <- unique(symbolv)
# Find index of ticker "BRK.B"
which(symbolv=="BRK.B")
# Rename "BRK.B" to "BRK-B" and "BF.B" to "BF-B"
symbolv[which(symbolv=="BRK.B")] <- "BRK-B"
symbolv[which(symbolv=="BF.B")] <- "BF-B"
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Downloading Stock Time Series From \protect\emph{Tiingo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Yahoo} data quality deteriorated significantly in \texttt{2017}, and \emph{Google} data quality is also poor, leaving \emph{Tiingo}, \emph{Alpha Vantage}, and \href{https://www.quandl.com/}{Quandl} as the only major providers of free daily \emph{OHLC} stock prices.
      \vskip1ex
      But \href{https://www.quandl.com/}{Quandl} doesn't provide free \emph{ETF} pricev, while \emph{Tiingo} does.
      \vskip1ex
      The function \texttt{getSymbols()} has a \emph{method} for downloading time series data from \emph{Tiingo}, called \texttt{getSymbols.tiingo()}.
      \vskip1ex
      Users must first obtain a \emph{Tiingo} \emph{API key}, and then pass it in \texttt{getSymbols.tiingo()} calls:\\
      https://www.tiingo.com/
      \vskip1ex
      Note that the data are downloaded as \texttt{xts} time series, with a date-time index of class \texttt{POSIXct} (not \texttt{Date}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package rutils
library(rutils)
# Create new environment for data
sp500env <- new.env()
# Boolean vector of symbols already downloaded
isdownloaded <- symbolv %in% ls(sp500env)
# Download in while loop from Tiingo and copy into environment
nattempts <- 0  # Number of download attempts
while (((sum(!isdownloaded)) > 0) & (nattempts<3)) {
  # Download data and copy it into environment
  nattempts <- nattempts + 1
  cat("Download attempt = ", nattempts, "\n")
  for (symbol in symbolv[!isdownloaded]) {
    cat("processing: ", symbol, "\n")
    tryCatch(  # With error handler
      quantmod::getSymbols(symbol, src="tiingo", adjust=TRUE, auto.assign=TRUE,
                 from="1990-01-01", env=sp500env, api.key="j84ac2b9c5bde2d68e33034f65d838092c6c9f10"),
      # Error handler captures error condition
      error=function(error_cond) {
        print(paste("error handler: ", error_cond))
      },  # end error handler
      finally=print(paste("symbol=", symbol))
    )  # end tryCatch
  }  # end for
  # Update vector of symbols already downloaded
  isdownloaded <- symbolv %in% ls(sp500env)
  Sys.sleep(2)  # Wait 2 seconds until next attempt
}  # end while
class(sp500env$AAPL)
class(zoo::index(sp500env$AAPL))
tail(sp500env$AAPL)
symbolv[!isdownloaded]
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Coercing Date-time Indices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The date-time indices of the \emph{OHLC} stock prices are in the \texttt{POSIXct} format suitable for intraday pricev, not daily prices.
      \vskip1ex
      The function \texttt{as.Date()} coerces \texttt{POSIXct} objects into \texttt{Date} objects.
      \vskip1ex
      The function \texttt{get()} retrieves objects that are referenced using character strings, instead of their names.
      \vskip1ex
      The function \texttt{assign()} assigns a value to an object in a specified \emph{environment}, by referencing it using a character string (name).
      \vskip1ex
      The functions \texttt{get()} and \texttt{assign()} allow retrieving and assigning values to objects that are referenced using character strings.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# The date-time index of AAPL is POSIXct
class(zoo::index(sp500env$AAPL))
# Coerce the date-time index of AAPL to Date
zoo::index(sp500env$AAPL) <- as.Date(zoo::index(sp500env$AAPL))
# Coerce all the date-time indices to Date
for (symbol in ls(sp500env)) {
  ohlc <- get(symbol, envir=sp500env)
  zoo::index(ohlc) <- as.Date(zoo::index(ohlc))
  assign(symbol, ohlc, envir=sp500env)
}  # end for
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Managing Exceptions in Stock Symbols}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The column names for symbol \texttt{"LOW"} (Lowe's company) must be renamed for the extractor function \texttt{quantmod::Lo()} to work properly.
      \vskip1ex
      Tickers which contain a dot in their name (like "BRK.B") are not valid symbols in \texttt{R}, so they must be downloaded separately and renamed.
      <<echo=TRUE,eval=FALSE>>=
# "LOW.Low" is a bad column name
colnames(sp500env$LOW)
strsplit(colnames(sp500env$LOW), split="[.]")
do.call(cbind, strsplit(colnames(sp500env$LOW), split="[.]"))
do.call(cbind, strsplit(colnames(sp500env$LOW), split="[.]"))[2, ]
# Extract proper names from column names
namesv <- rutils::get_name(colnames(sp500env$LOW), field=2)
# Or
# namesv <- do.call(rbind, strsplit(colnames(sp500env$LOW),
#                                   split="[.]"))[, 2]
# Rename "LOW" colnames to "LOWES"
colnames(sp500env$LOW) <- paste("LO_WES", namesv, sep=".")
sp500env$LOWES <- sp500env$LOW
rm(LOW, envir=sp500env)
# Rename BF-B colnames to "BFB"
colnames(sp500env$"BF-B") <- paste("BFB", namesv, sep=".")
sp500env$BFB <- sp500env$"BF-B"
rm("BF-B", envir=sp500env)
# Rename BRK-B colnames
sp500env$BRKB <- sp500env$`BRK-B`
rm(`BRK-B`, envir=sp500env)
colnames(sp500env$BRKB) <- gsub("BRK-B", "BRKB", colnames(sp500env$BRKB))
# Save OHLC prices to .RData file
save(sp500env, file="/Users/jerzy/Develop/lecture_slides/data/sp500.RData")
# Download "BRK.B" separately with auto.assign=FALSE
# BRKB <- quantmod::getSymbols("BRK-B", auto.assign=FALSE, src="tiingo", adjust=TRUE, from="1990-01-01", api.key="j84ac2b9c5bde2d68e33034f65d838092c6c9f10")
# colnames(BRKB) <- paste("BRKB", namesv, sep=".")
# sp500env$BRKB <- BRKB
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/lowes_stock.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot OHLC candlestick chart for LOWES
chart_Series(x=sp500env$LOWES["2019-12/"],
  TA="add_Vo()", name="LOWES OHLC Stock Prices")
# Plot dygraph
dygraphs::dygraph(sp500env$LOWES["2019-12/", -5], main="LOWES OHLC Stock Prices") %>%
  dyCandlestick()
      @

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Index Constituent Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The file \texttt{sp500.RData} contains the \emph{environment} \texttt{sp500\_env} with \emph{OHLC} prices and trading volumes of \emph{S\&P500} stock index constituents.
      \vskip1ex
      The \emph{S\&P500} stock index constituent data is of poor quality before \texttt{2000}, so we'll mostly use the data after the year \texttt{2000}.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock prices
load("/Users/jerzy/Develop/lecture_slides/data/sp500.RData")
pricev <- eapply(sp500env, quantmod::Cl)
pricev <- rutils::do_call(cbind, pricev)
# Carry forward non-NA prices
pricev <- zoo::na.locf(pricev, na.rm=FALSE)
# Drop ".Close" from column names
colnames(pricev[, 1:4])
colnames(pricev) <- rutils::get_name(colnames(pricev))
# Or
# colnames(pricev) <- do.call(rbind,
#   strsplit(colnames(pricev), split="[.]"))[, 1]
# Calculate percentage returns of the S&P500 constituent stocks
# retsp <- xts::diff.xts(log(pricev))
retsp <- xts::diff.xts(pricev)/
  rutils::lagit(pricev, pad_zeros=FALSE)
set.seed(1121)
samplev <- sample(NCOL(retsp), s=100, replace=FALSE)
prices100 <- pricev[, samplev]
returns100 <- retsp[, samplev]
save(pricev, prices100,
  file="/Users/jerzy/Develop/lecture_slides/data/sp500_prices.RData")
save(retsp, returns100,
  file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_without_prices.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate number of constituents without prices
datav <- rowSums(is.na(pricev))
datav <- xts::xts(datav, order.by=zoo::index(pricev))
dygraphs::dygraph(datav, main="Number of S&P500 Constituents Without Prices") %>%
  dyOptions(colors="blue", strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Portfolio Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The price-weighted index of \emph{S\&P500} constituents closely follows the VTI \emph{ETF}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate price weighted index of constituent
ncols <- NCOL(pricev)
pricev <- zoo::na.locf(pricev, fromLast=TRUE)
indeks <- xts(rowSums(pricev)/ncols, zoo::index(pricev))
colnames(indeks) <- "index"
# Combine index with VTI
datav <- cbind(indeks[zoo::index(etfenv$VTI)], etfenv$VTI[, 4])
colnamev <- c("index", "VTI")
colnames(datav) <- colnamev
# Plot index with VTI
dygraphs::dygraph(datav,
  main="S&P 500 Price-weighted Index and VTI") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red") %>%
  dySeries(name=colnamev[2], axis="y2", col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_portfolio_index.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Writing Time Series To Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The data from \emph{Tiingo} is downloaded as \texttt{xts} time series, with a date-time index of class \texttt{POSIXct} (not \texttt{Date}).
      \vskip1ex
      The function \texttt{save()} writes objects to compressed binary \texttt{.RData} files.
      \vskip1ex
      The easiest way to share data between \texttt{R} and \texttt{Excel} is through \texttt{.csv} files.
      \vskip1ex
      The package \emph{zoo} contains functions \texttt{write.zoo()} and \texttt{read.zoo()} for writing and reading \emph{zoo} time series from \texttt{.txt} and \texttt{.csv} files.
      \vskip1ex
      The function \texttt{data.table::fread()} reads from \texttt{.csv} files over \texttt{6} times faster than the function \texttt{read.csv()}!
      \vskip1ex
      The function \texttt{data.table::fwrite()} writes to \texttt{.csv} files over \texttt{12} times faster than the function \texttt{write.csv()}, and \texttt{278} times faster than function \texttt{cat()}!
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Save the environment to compressed .RData file
dir_name <- "/Users/jerzy/Develop/lecture_slides/data/"
save(sp500env, file=paste0(dir_name, "sp500.RData"))
# Save the ETF prices into CSV files
dir_name <- "/Users/jerzy/Develop/lecture_slides/data/SP500/"
for (symbol in ls(sp500env)) {
  zoo::write.zoo(sp500env$symbol, file=paste0(dir_name, symbol, ".csv"))
}  # end for
# Or using lapply()
file_names <- lapply(ls(sp500env), function(symbol) {
  xtsv <- get(symbol, envir=sp500env)
  zoo::write.zoo(xtsv, file=paste0(dir_name, symbol, ".csv"))
  symbol
})  # end lapply
unlist(file_names)
# Or using eapply() and data.table::fwrite()
file_names <- eapply(sp500env , function(xtsv) {
  file_name <- rutils::get_name(colnames(xtsv)[1])
  data.table::fwrite(data.table::as.data.table(xtsv), file=paste0(dir_name, file_name, ".csv"))
  file_name
})  # end eapply
unlist(file_names)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading Time Series from Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{load()} reads data from \texttt{.RData} files, and \emph{invisibly} returns a vector of names of objects created in the workspace.
      \vskip1ex
      The function \texttt{Sys.glob()} listv files matching names obtained from wildcard expansion.
      \vskip1ex
      The easiest way to share data between \texttt{R} and \texttt{Excel} is through \texttt{.csv} files.
      \vskip1ex
      The function \texttt{as.Date()} parses \texttt{character} strings, and coerces \texttt{numeric} and \texttt{POSIXct} objects into \texttt{Date} objects.
      \vskip1ex
      The function \texttt{data.table::setDF()} coerces a \emph{data table} object into a \emph{data frame} using a \emph{side effect}, without making copies of data.
      \vskip1ex
      The function \texttt{data.table::fread()} reads from \texttt{.csv} files over \texttt{6} times faster than the function \texttt{read.csv()}!
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load the environment from compressed .RData file
dir_name <- "/Users/jerzy/Develop/lecture_slides/data/"
load(file=paste0(dir_name, "sp500.RData"))
# Get all the .csv file names in the directory
dir_name <- "/Users/jerzy/Develop/lecture_slides/data/SP500/"
file_names <- Sys.glob(paste0(dir_name, "*.csv"))
# Create new environment for data
sp500env <- new.env()
for (file_name in file_names) {
  xtsv <- xts::as.xts(zoo::read.csv.zoo(file_name))
  symbol <- rutils::get_name(colnames(xtsv)[1])
  # symbol <- strsplit(colnames(xtsv), split="[.]")[[1]][1]
  assign(symbol, xtsv, envir=sp500env)
}  # end for
# Or using fread()
for (file_name in file_names) {
  xtsv <- data.table::fread(file_name)
  data.table::setDF(xtsv)
  xtsv <- xts::xts(xtsv[, -1], as.Date(xtsv[, 1]))
  symbol <- rutils::get_name(colnames(xtsv)[1])
  assign(symbol, xtsv, envir=sp500env)
}  # end for
@
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Modeling and Fitting Asset Returns}


%%%%%%%%%%%%%%%
\subsection{Kernel Density of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The kernel density is proportional to the number of data points close to a given point.
      \vskip1ex
      The kernel density is analogous to a histogram, but it provides more detailed information about the distribution of the data.
      \vskip1ex
      The smoothing kernel $K(x)$ is a symmetric function which decreases with the distance $x$.
      \vskip1ex
      The kernel density $d_r$ at a point $r$ is equal to the sum over the kernel function $K(x)$:
      \begin{displaymath}
        d_r = \sum_{j=1}^n {K(r - r_j)}
      \end{displaymath}
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \vskip1ex
      The parameter \emph{smoothing bandwidth} is the standard deviation of the smoothing kernel $K(x)$. 
      \vskip1ex
      The function \texttt{density()} returns a vector of densities at equally spaced points, not for the original data points.
      \vskip1ex
      The function \texttt{approx()} interpolates a vector of data into another vector.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)  # Load package rutils
# Calculate VTI percentage returns
retsp <- rutils::etfenv$returns$VTI
retsp <- drop(coredata(na.omit(retsp)))
nrows <- NROW(retsp)
# Mean and standard deviation of returns
c(mean(retsp), sd(retsp))
# Calculate the smoothing bandwidth as the MAD of returns 10 points apart
retsp <- sort(retsp)
bwidth <- 10*mad(rutils::diffit(retsp, lagg=10))
# Calculate the kernel density
densityv <- sapply(1:nrows, function(it) {
  sum(dnorm(retsp-retsp[it], sd=bwidth))
})  # end sapply
madv <- mad(retsp)
plot(retsp, densityv, xlim=c(-5*madv, 5*madv),
     t="l", col="blue", lwd=3,
     xlab="returns", ylab="density",
     main="Density of VTI Returns")
# Calculate the kernel density using density()
densityv <- density(retsp, bw=bwidth)
NROW(densityv$y)
x11(width=6, height=5)
plot(densityv, xlim=c(-5*madv, 5*madv),
     xlab="returns", ylab="density",
     col="blue", lwd=3, main="Density of VTI Returns")
# Interpolate the densityv vector into returns
densityv <- approx(densityv$x, densityv$y, xout=retsp)
all.equal(densityv$x, retsp)
plot(densityv, xlim=c(-5*madv, 5*madv),
     xlab="returns", ylab="density",
     t="l", col="blue", lwd=3,
     main="Density of VTI Returns")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are usually not normally distributed and they exhibit \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
      \vskip1ex
      The function \texttt{lines()} draws a line through specified points.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hist_vti_dens.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
histp <- hist(retsp, breaks=100, freq=FALSE,
  xlim=c(-5*madv, 5*madv), xlab="", ylab="",
  main="VTI Return Distribution")
# Draw kernel density of histogram
lines(densityv, col="red", lwd=2)
# Add density of normal distribution
curve(expr=dnorm(x, mean=mean(retsp), sd=sd(retsp)),
      add=TRUE, lwd=2, col="blue")
# Add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("VTI", "Normal"), bty="n",
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Quantile-Quantile Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{Quantile-Quantile} (\emph{Q-Q}) plot is a plot of points with the same \emph{quantiles}, from two probability distributions.
      \vskip1ex
      If the two distributions are similar then all the points in the \emph{Q-Q} plot lie along the diagonal.
      \vskip1ex
      The \emph{VTI} \emph{Q-Q} plot shows that the \emph{VTI} return distribution has fat tails.
      \vskip1ex
      The \emph{p}-value of the \emph{Shapiro-Wilk} test is very close to zero, which shows that the \emph{VTI} returns are very unlikely to be normal.
      \vskip1ex
      The function \texttt{shapiro.test()} performs the \emph{Shapiro-Wilk} test of normality.
      \vskip1ex
      The function \texttt{qqnorm()} produces a normal \emph{Q-Q} plot.
      \vskip1ex
      The function \texttt{qqline()} fits a line to the normal quantiles.
      <<echo=TRUE,eval=FALSE>>=
# Create normal Q-Q plot
qqnorm(retsp, ylim=c(-0.1, 0.1), main="VTI Q-Q Plot",
       xlab="Normal Quantiles")
# Fit a line to the normal quantiles
qqline(retsp, col="red", lwd=2)
# Perform Shapiro-Wilk test
shapiro.test(retsp[1:4999])
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/qq_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Distributions of Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Box-and-whisker plots (\emph{boxplots}) are graphical representations of a distribution of values.
      \vskip1ex
      The bottom and top box edges (\emph{hinges}) are equal to the first and third quartiles, and the \emph{box} width is equal to the interquartile range (\emph{IQR}).
      \vskip1ex
      The nominal range is equal to 1.5 times the \emph{IQR} above and below the box \emph{hinges}.
      \vskip1ex
      The \emph{whiskers} are dashed vertical lines representing values beyond the first and third quartiles, but within the nominal range.
      \vskip1ex
      The \emph{whiskers} end at the last values within the nominal range, while the open circles represent outlier values beyond the nominal range.
      \vskip1ex
      The function \texttt{boxplot()} has two \texttt{methods}: one for \texttt{formula} objects (for categorical variables), and another for \texttt{data frames}.
      <<box_plots,eval=FALSE>>=
# Boxplot method for formula
boxplot(formula=mpg ~ cyl, data=mtcars,
        main="Mileage by number of cylinders",
        xlab="Cylinders", ylab="Miles per gallon")
# Boxplot method for data frame of EuStockMarkets percentage returns
boxplot(x=diff(log(EuStockMarkets)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/box_plots-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Moments of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution are given by:
      \vskip1ex
      Sample mean: $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}] = \mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma] = \sigma$
      \vskip1ex
      The sample skewness (third moment):
      \begin{displaymath}
        \varsigma = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment):
      \begin{displaymath}
        \kappa = \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has skewness equal to $0$ and kurtosis equal to $3$.
      \vskip1ex
      Stock returns typically have negative skewness and kurtosis much greater than $3$.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
# Number of observations
nrows <- NROW(retsp)
# Mean of VTI returns
retsm <- mean(retsp)
# Standard deviation of VTI returns
sdrets <- sd(retsp)
# Skewness of VTI returns
nrows/((nrows-1)*(nrows-2))*
  sum(((retsp - retsm)/sdrets)^3)
# Kurtosis of VTI returns
nrows*(nrows+1)/((nrows-1)^3)*
  sum(((retsp - retsm)/sdrets)^4)
# Random normal returns
retsp <- rnorm(nrows, sd=sdrets)
# Mean and standard deviation of random normal returns
retsm <- mean(retsp)
sdrets <- sd(retsp)
# Skewness of random normal returns
nrows/((nrows-1)*(nrows-2))*
  sum(((retsp - retsm)/sdrets)^3)
# Kurtosis of random normal returns
nrows*(nrows+1)/((nrows-1)^3)*
  sum(((retsp - retsm)/sdrets)^4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functions for Calculating Skew and Kurtosis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} provides an easy way for users to write functions.
      \vskip1ex
      The function \texttt{calc\_skew()} calculates the skew of returns, and \texttt{calc\_kurt()} calculates the kurtosis.
      \vskip1ex
      Functions return the value of the last expression that is evaluated.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# calc_skew() calculates skew of returns
calc_skew <- function(retsp) {
  retsp <- na.omit(retsp)
  sum(((retsp - mean(retsp))/sd(retsp))^3)/NROW(retsp)
}  # end calc_skew
# calc_kurt() calculates kurtosis of returns
calc_kurt <- function(retsp) {
  retsp <- na.omit(retsp)
  sum(((retsp - mean(retsp))/sd(retsp))^4)/NROW(retsp)
}  # end calc_kurt
# Calculate skew and kurtosis of VTI returns
calc_skew(retsp)
calc_kurt(retsp)
# calcmom() calculates the moments of returns
calcmom <- function(retsp, moment=3) {
  retsp <- na.omit(retsp)
  sum(((retsp - mean(retsp))/sd(retsp))^moment)/NROW(retsp)
}  # end calcmom
# Calculate skew and kurtosis of VTI returns
calcmom(retsp, moment=3)
calcmom(retsp, moment=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
nrows <- 1000
datav <- rnorm(nrows)
# Sample mean
mean(datav)
# Sample standard deviation
sd(datav)
# Standard error of sample mean
sd(datav)/sqrt(nrows)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Normal (Gaussian)} probability density function is given by:
      \begin{displaymath}
        \phi(x, \mu, \sigma) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      The \emph{Standard Normal} distribution $\phi(0, 1)$ is a special case of the \emph{Normal} $\phi(\mu, \sigma)$ with $\mu=0$ and $\sigma=1$.
      \vskip1ex
      The function \texttt{dnorm()} calculates the \emph{Normal} probability density.
      <<echo=TRUE,eval=FALSE>>=
xvar <- seq(-5, 7, length=100)
yvar <- dnorm(xvar, mean=1.0, sd=2.0)
plot(xvar, yvar, type="l", lty="solid", xlab="", ylab="")
title(main="Normal Density Function", line=0.5)
startp <- 3; endd <- 5  # Set lower and upper bounds
# Set polygon base
subv <- ((xvar >= startp) & (xvar <= endd))
polygon(c(startp, xvar[subv], endd),  # Draw polygon
        c(-1, yvar[subv], -1), col="red")
      @
    \column{0.5\textwidth}
    \includegraphics[width=0.45\paperwidth]{figure/norm_dist}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Plots of several \emph{Normal} distributions with different values of $\sigma$, using the function \texttt{curve()} for plotting functions given by their name.
      <<norm_dist_mult_curves,eval=FALSE,echo=(-(1:1)),fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
sigmavs <- c(0.5, 1, 1.5, 2)  # Sigma values
# Create plot colors
colorv <- c("red", "black", "blue", "green")
# Create legend labels
labelv <- paste("sigma", sigmavs, sep="=")
for (it in 1:4) {  # Plot four curves
  curve(expr=dnorm(x, sd=sigmavs[it]),
        xlim=c(-4, 4), xlab="", ylab="", lwd=2,
        col=colorv[it], add=as.logical(it-1))
}  # end for
# Add title
title(main="Normal Distributions", line=0.5)
# Add legend
legend("topright", inset=0.05, title="Sigmas",
       labelv, cex=0.8, lwd=2, lty=1, bty="n", col=colorv)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/norm_dist_mult_curves-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_{1},\ldots , z_{\nu}$ be independent standard normal random variables, with sample mean: $\bar{z}=\frac{1}{\nu} \sum_{i=1}^{\nu} z_i$ ($\mathbb{E}[\bar{z}]=\mu$) and sample variance: $\hat\sigma^2=\frac{1}{\nu-1} \sum_{i=1}^{\nu} (z_i-\bar{z})^2$
      \vskip1ex
      Then the random variable (\emph{t-ratio}):
      \begin{displaymath}
        t = \frac{\bar{z} - \mu}{\hat\sigma / \sqrt{\nu}}
      \end{displaymath}
      Follows the \emph{t-distribution} with $\nu$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu}\,\Gamma(\nu/2)}\, (1 + t^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
degf <- c(3, 6, 9)  # Df values
colorv <- c("black", "red", "blue", "green")
labelv <- c("normal", paste("df", degf, sep="="))
# Plot a Normal probability distribution
curve(expr=dnorm, xlim=c(-4, 4), xlab="", ylab="", lwd=2)
for (it in 1:3) {  # Plot three t-distributions
  curve(expr=dt(x, df=degf[it]), xlab="", ylab="",
        lwd=2, col=colorv[it+1], add=TRUE)
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_mult.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="t-distributions", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title="Degrees\n of freedom", labelv,
       cex=0.8, lwd=6, lty=1, col=colorv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mixture Models of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Mixture models} are produced by randomly sampling data from different distributions.
      \vskip1ex
      The mixture of two normal distributions with different variances produces a distribution with \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      Student's \emph{t-distribution} has fat tails because the sample variance in the denominator of the \emph{t-ratio} is variable.
      \vskip1ex
      The time-dependent volatility of asset returns is referred to as \emph{heteroskedasticity}.
      \vskip1ex
      Random processes with \emph{heteroskedasticity} can be considered a type of mixture model.
      \vskip1ex
      The \emph{heteroskedasticity} produces \emph{leptokurtosis} (large kurtosis, or fat tails).
      <<echo=TRUE,eval=FALSE>>=
# Mixture of two normal distributions with sd=1 and sd=2
nrows <- 1e5
retsp <- c(rnorm(nrows/2), 2*rnorm(nrows/2))
retsp <- (retsp-mean(retsp))/sd(retsp)
# Kurtosis of normal
calc_kurt(rnorm(nrows))
# Kurtosis of mixture
calc_kurt(retsp)
# Or
nrows*sum(retsp^4)/(nrows-1)^2
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/mix_normal.png}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot the distributions
plot(density(retsp), xlab="", ylab="",
  main="Mixture of Normal Returns",
  xlim=c(-3, 3), type="l", lwd=3, col="red")
curve(expr=dnorm, lwd=2, col="blue", add=TRUE)
curve(expr=dt(x, df=3), lwd=2, col="green", add=TRUE)
# Add legend
legend("topright", inset=0.05, lty=1, lwd=6, bty="n",
  legend=c("Mixture", "Normal", "t-distribution"),
  col=c("red", "blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Non-standard Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The non-standard Student's \emph{t-distribution} has the probability density function:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \sigma \, \Gamma(\nu/2)} \, (1 + (\frac{t - \mu}{\sigma})^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
       It has non-zero mean equal to the location parameter $\mu$, and a standard deviation proportional to the scale parameter $\sigma$.
        <<echo=TRUE,eval=FALSE>>=
dev.new(width=6, height=5, noRStudioGD=TRUE)
# x11(width=6, height=5)
# Define density of non-standard t-distribution
tdistr <- function(x, dfree, locv=0, scalev=1) {
  dt((x-locv)/scalev, df=dfree)/scalev
}  # end tdistr
# Or
tdistr <- function(x, dfree, locv=0, scalev=1) {
  gamma((dfree+1)/2)/(sqrt(pi*dfree)*gamma(dfree/2)*scalev)*
    (1+((x-locv)/scalev)^2/dfree)^(-(dfree+1)/2)
}  # end tdistr
# Calculate vector of scale values
scalev <- c(0.5, 1.0, 2.0)
colorv <- c("blue", "black", "red")
labelv <- paste("scale", format(scalev, digits=2), sep="=")
# Plot three t-distributions
for (it in 1:3) {
  curve(expr=tdistr(x, dfree=3, scalev=scalev[it]), xlim=c(-3, 3),
        xlab="", ylab="", lwd=2, col=colorv[it], add=(it>1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_scale.png}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="t-distributions with Different Scale Parameters", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n", title="Scale Parameters", labelv,
       cex=0.8, lwd=6, lty=1, col=colorv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Shapiro-Wilk} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        W = \frac {(\sum_{i=1}^n a_i x_{(i)})^2} {\sum_{i=1}^n (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1, \ldots, a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution.
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1, \ldots, x_n\}$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to $1$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to $1$ for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
      \vskip1ex
      The \emph{Shapiro-Wilk} test is not reliable for large sample sizes, so it's limited to less than \texttt{5000} sample size.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Calculate VTI percentage returns
library(rutils)
retsp <- as.numeric(na.omit(rutils::etfenv$returns$VTI))[1:4999]
# Reduce number of output digits
ndigits <- options(digits=5)
# Shapiro-Wilk test for normal distribution
nrows <- NROW(retsp)
shapiro.test(rnorm(nrows))
# Shapiro-Wilk test for VTI returns
shapiro.test(retsp)
# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(nrows))
# Restore output digits
options(digits=ndigits$digits)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Jarque-Bera} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        JB = \frac{n}{6} (\varsigma^2 + \frac{1}{4} (\kappa - 3)^2)
      \end{displaymath}
      Where the \emph{skewness} and \emph{kurtosis} are defined as:
      \begin{align*}
        \varsigma = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \kappa = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with  \texttt{2} degrees of freedom.
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(tseries)  # Load package tseries
# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(nrows))
# Jarque-Bera test for VTI returns
jarque.bera.test(retsp)
# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(NROW(retsp)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test \emph{null hypothesis} is that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic depends on the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} performs the \emph{Kolmogorov-Smirnov} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The second argument to \texttt{ks.test()} can be either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a probability distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# KS test for normal distribution
ks_test <- ks.test(rnorm(100), pnorm)
ks_test$p.value
# KS test for uniform distribution
ks.test(runif(100), pnorm)
# KS test for two shifted normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
ks.test(rnorm(100), rnorm(100, mean=1.0))
# KS test for two different normal distributions
ks.test(rnorm(100), rnorm(100, sd=2.0))
# KS test for VTI returns vs normal distribution
retsp <- as.numeric(na.omit(rutils::etfenv$returns$VTI))
retsp <- (retsp - mean(retsp))/sd(retsp)
ks.test(retsp, pnorm)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Chi-squared} Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_1, \ldots , z_k$ be independent standard \emph{Normal} random variables.
      \vskip1ex
      Then the random variable $X = \sum_{i=1}^k z^2_i$ is distributed according to the \emph{Chi-squared} distribution with $k$ degrees of freedom: $X \sim \chi_k^2$, and its probability density function is given by:
      \begin{displaymath}
        f(x) = \frac{x^{k/2-1}\,e^{-x/2}}{2^{k/2}\, \Gamma(k/2)}
      \end{displaymath}
      \vskip1ex
      The \emph{Chi-squared} distribution with $k$ degrees of freedom has mean equal to $k$ and variance equal to $2k$.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Degrees of freedom
degf <- c(2, 5, 8, 11)
# Plot four curves in loop
colorv <- c("red", "black", "blue", "green")
for (it in 1:4) {
  curve(expr=dchisq(x, df=degf[it]),
        xlim=c(0, 20), ylim=c(0, 0.3),
        xlab="", ylab="", col=colorv[it],
        lwd=2, add=as.logical(it-1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chisq_dist_mult.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="Chi-squared Distributions", line=0.5)
# Add legend
labelv <- paste("df", degf, sep="=")
legend("topright", inset=0.05, bty="n",
       title="Degrees of freedom", labelv,
       cex=0.8, lwd=6, lty=1, col=colorv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Chi-squared} Test for the Goodness of Fit}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Goodness of Fit} tests are designed to test if a set of observations fits an assumed theoretical probability distribution.
      \vskip1ex
      The \emph{Chi-squared} test tests if a frequency of counts fits the specified distribution.
      \vskip1ex
      The \emph{Chi-squared} statistic is the sum of squared differences between the observed frequencies $o_i$ and the theoretical frequencies $p_i$:
      \begin{displaymath}
        \chi^2 = N \sum_{i=1}^{n} {\frac{(o_i - p_i )^2}{p_i}}
      \end{displaymath}
      Where $N$ is the total number of observations.
      \vskip1ex
      The \emph{null hypothesis} is that the observed frequencies are consistent with the theoretical distribution.
      \vskip1ex
      The function \texttt{chisq.test()} performs the \emph{Chi-squared} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} in the function \texttt{hist()} should be chosen large enough to capture the shape of the frequency distribution.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Observed frequencies from random normal data
histp <- hist(rnorm(1e3, mean=0), breaks=100, plot=FALSE)
countsn <- histp$counts
# Theoretical frequencies
countst <- rutils::diffit(pnorm(histp$breaks))
# Perform Chi-squared test for normal data
chisq.test(x=countsn, p=countst, rescale.p=TRUE, simulate.p.value=TRUE)
# Return p-value
chisq_test <- chisq.test(x=countsn, p=countst, rescale.p=TRUE, simulate.p.value=TRUE)
chisq_test$p.value
# Observed frequencies from shifted normal data
histp <- hist(rnorm(1e3, mean=2), breaks=100, plot=FALSE)
countsn <- histp$counts/sum(histp$counts)
# Theoretical frequencies
countst <- rutils::diffit(pnorm(histp$breaks))
# Perform Chi-squared test for shifted normal data
chisq.test(x=countsn, p=countst, rescale.p=TRUE, simulate.p.value=TRUE)
# Calculate histogram of VTI returns
histp <- hist(retsp, breaks=100, plot=FALSE)
countsn <- histp$counts
# Calculate cumulative probabilities and then difference them
countst <- pt((histp$breaks-locv)/scalev, df=2)
countst <- rutils::diffit(countst)
# Perform Chi-squared test for VTI returns
chisq.test(x=countsn, p=countst, rescale.p=TRUE, simulate.p.value=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Likelihood Function of Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The non-standard Student's \emph{t-distribution} is:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \sigma \, \Gamma(\nu/2)} \, (1 + (\frac{t - \mu}{\sigma})^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
       It has non-zero mean equal to the location parameter $\mu$, and a standard deviation proportional to the scale parameter $\sigma$.
      \vskip1ex
      The negative logarithm of the probability density is equal to:
      \begin{multline*}
        -\log(f(t)) = -\log(\frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \Gamma(\nu/2)}) + \log(\sigma) + \\
        \frac{\nu+1}{2} \, \log(1 + (\frac{t - \mu}{\sigma})^2/\nu)
      \end{multline*}
      The \emph{likelihood} function $\mathcal{L}(\theta|\bar{x})$ is a function of the model parameters $\theta$, given the observed values $\bar{x}$, under the model's probability distribution $f(x|\theta)$:
      \begin{displaymath}
        \mathcal{L}(\theta|x) = \prod_{i=1}^{n} f(x_i|\theta)
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Objective function from function dt()
likefun <- function(par, dfree, data) {
  -sum(log(dt(x=(data-par[1])/par[2], df=dfree)/par[2]))
}  # end likefun
# Demonstrate equivalence with log(dt())
likefun(c(1, 0.5), 2, 2:5)
-sum(log(dt(x=(2:5-1)/0.5, df=2)/0.5))
# Objective function is negative log-likelihood
likefun <- function(par, dfree, data) {
  sum(-log(gamma((dfree+1)/2)/(sqrt(pi*dfree)*gamma(dfree/2))) +
    log(par[2]) + (dfree+1)/2*log(1+((data-par[1])/par[2])^2/dfree))
}  # end likefun
      @
      The \emph{likelihood} function measures how \emph{likely} are the parameters, given the observed values $\bar{x}$.
      \vskip1ex
      The \emph{maximum-likelihood} estimate (\emph{MLE}) of the parameters are those that maximize the \emph{likelihood} function:
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
      \end{displaymath}
      In practice the logarithm of the \emph{likelihood} $\log(\mathcal{L})$ is maximized, instead of the \emph{likelihood} itself.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Asset Returns into Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution to a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{fitdistr()} performs a \emph{maximum likelihood} optimization to find the non-standardized Student's \emph{t-distribution} location and scale parameters.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- as.numeric(na.omit(rutils::etfenv$returns$VTI))
# Fit VTI returns using MASS::fitdistr()
fitobj <- MASS::fitdistr(retsp, densfun="t", df=3)
summary(fitobj)
# Fitted parameters
fitobj$estimate
locv <- fitobj$estimate[1]
scalev <- fitobj$estimate[2]
locv; scalev
# Standard errors of parameters
fitobj$sd
# Log-likelihood value
fitobj$value
# Fit distribution using optim()
initp <- c(mean=0, scale=0.01)  # Initial parameters
fitobj <- optim(par=initp,
  fn=likefun, # Log-likelihood function
  data=retsp,
  dfree=3, # Degrees of freedom
  method="L-BFGS-B", # Quasi-Newton method
  upper=c(1, 0.1), # Upper constraint
  lower=c(-1, 1e-7)) # Lower constraint
# Optimal parameters
locv <- fitobj$par["mean"]
scalev <- fitobj$par["scale"]
locv; scalev
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Student's \protect\emph{t-distribution} Fitted to Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns typically exhibit \emph{negative skewness} and \emph{large kurtosis} (leptokurtosis), or fat tails.
      \vskip1ex
      Stock returns fit the non-standard \emph{t-distribution} with \texttt{3} degrees of freedom quite well.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
        <<echo=TRUE,eval=FALSE>>=
dev.new(width=6, height=5, noRStudioGD=TRUE)
# x11(width=6, height=5)
# Plot histogram of VTI returns
madv <- mad(retsp)
histp <- hist(retsp, col="lightgrey",
  xlab="returns", breaks=100, xlim=c(-5*madv, 5*madv),
  ylab="frequency", freq=FALSE, main="Histogram of VTI Returns")
lines(density(retsp, adjust=1.5), lwd=3, col="blue")
# Plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(retsp),
  sd=sd(retsp)), add=TRUE, lwd=3, col="green")
# Define non-standard t-distribution
tdistr <- function(x, dfree, locv=0, scalev=1) {
  dt((x-locv)/scalev, df=dfree)/scalev
}  # end tdistr
# Plot t-distribution function
curve(expr=tdistr(x, dfree=3, locv=locv, scalev=scalev), col="red", lwd=3, add=TRUE)
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("density", "t-distr", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_rets.png}
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Goodness of Fit of Student's \protect\emph{t-distribution} Fitted to Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Q-Q} plot illustrates the relative distributions of two samples of data.
      \vskip1ex
      The \emph{Q-Q} plot shows that stock returns fit the non-standard \emph{t-distribution} with \texttt{3} degrees of freedom quite well.
      \vskip1ex
      The function \texttt{qqplot()} produces a \emph{Q-Q} plot for two samples of data.
      \vskip1ex
      The function \texttt{ks.test()} performs the \emph{Kolmogorov-Smirnov} test for the similarity of two distributions.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Kolmogorov-Smirnov} test is that the two samples were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test rejects the \emph{null hypothesis} that stock returns follow closely the non-standard \emph{t-distribution} with \texttt{3} degrees of freedom.
        <<echo=TRUE,eval=FALSE>>=
# Calculate sample from non-standard t-distribution with df=3
tdata <- scalev*rt(NROW(retsp), df=3) + locv
# Q-Q plot of VTI Returns vs non-standard t-distribution
qqplot(tdata, retsp, xlab="t-Dist Quantiles", ylab="VTI Quantiles", 
       main="Q-Q plot of VTI Returns vs Student's t-distribution")
# Calculate quartiles of the distributions
probs <- c(0.25, 0.75)
qrets <- quantile(retsp, probs)
qtdata <- quantile(tdata, probs)
# Calculate slope and plot line connecting quartiles
slope <- diff(qrets)/diff(qtdata)
intercept <- qrets[1]-slope*qtdata[1]
abline(intercept, slope, lwd=2, col="red")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_qq.png}
        <<echo=TRUE,eval=FALSE>>=
# KS test for VTI returns vs t-distribution data
ks.test(retsp, tdata)
# Define cumulative distribution of non-standard t-distribution
ptdistr <- function(x, dfree, locv=0, scalev=1) {
  pt((x-locv)/scalev, df=dfree)
}  # end ptdistr
# KS test for VTI returns vs cumulative t-distribution
ks.test(sample(retsp, replace=TRUE), ptdistr, dfree=3, locv=locv, scalev=scalev)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Leptokurtosis Fat Tails of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability under the \emph{normal} distribution decreases exponentially for large values of $x$:
      \begin{displaymath}
        \phi(x) \propto e^{-{x^2/2\sigma^2}} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a normal variable can be thought of as the sum of a large number of independent binomial variables of equal size.
      \vskip1ex
      So large values are produced only when all the contributing binomial variables are of the same sign, which is very improbable, so it produces extremely low tail probabilities (thin tails),
      \vskip1ex
      But in reality, the probability of large negative asset returns decreases much slower, as the negative power of the returns (fat tails).
      \vskip1ex
      The probability under Student's \emph{t-distribution} decreases as a power for large values of $x$:
      \begin{displaymath}
        f(x) \propto {\left| x \right|}^{-(\nu+1)} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a \emph{t-variable} can be thought of as the sum of normal variables with different volatilities (different sizes).
    \column{0.5\textwidth}
      \includegraphics[width=0.4\paperwidth]{figure/stock_fat_tails.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot log density of VTI returns
plot(density(retsp, adjust=3), xlab="VTI Returns", ylab="Density",
     main="Fat Left Tail of VTI Returns (density in log scale)", 
     type="l", lwd=3, col="blue", xlim=c(min(retsp), -0.02), log="y")
# Plot t-distribution function
curve(expr=dt((x-locv)/scalev, df=3)/scalev, lwd=3, col="red", add=TRUE, log="y")
# Plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(retsp), sd=sd(retsp)), lwd=3, col="green", add=TRUE, log="y")
# Add legend
legend("topleft", inset=0.01, bty="n", 
  leg=c("density", "t-distr", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Volumes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling average trading volumes have increased significantly since the 2008 crisis, mostly because of high frequency trading (HFT).
      \vskip1ex
      Higher levels of volatility coincide with higher \emph{trading volumes}.
      \vskip1ex
      The time-dependent volatility of asset returns (\emph{heteroskedasticity}) produces their fat tails (\emph{leptokurtosis}).
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns and trading volumes
ohlc <- rutils::etfenv$VTI
closep <- drop(coredata(quantmod::Cl(ohlc)))
retsp <- rutils::diffit(log(closep))
volumes <- coredata(quantmod::Vo(ohlc))
# Calculate rolling variance
look_back <- 121
variance <- HighFreq::roll_var_ohlc(log(ohlc), method="close", look_back=look_back, scale=FALSE)
variance[1:look_back, ] <- variance[look_back+1, ]
# Calculate rolling average volume
volume_roll <- HighFreq::roll_vec(volumes, look_back=look_back)/look_back
# dygraph plot of VTI variance and trading volumes
datav <- xts::xts(cbind(variance, volume_roll), zoo::index(ohlc))
colnamev <- c("variance", "volume")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="VTI Variance and Trading Volumes") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], strokeWidth=2, axis="y", col="blue") %>%
  dySeries(name=colnamev[2], strokeWidth=2, axis="y2", col="red")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/volume_volat_dyg.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Returns in Trading Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time-dependent volatility of asset returns (\emph{heteroskedasticity}) produces their fat tails (\emph{leptokurtosis}).
      \vskip1ex
      If asset returns were measured at fixed intervals of \emph{trading volumes} (\emph{trading time} instead of clock time), then the volatility would be lower and less time-dependent.
      \vskip1ex
      The asset returns can be adjusted to \emph{trading time} by dividing them by the \emph{square root of the trading volumes}, to obtain scaled returns over equal trading volumes.
      \vskip1ex
      The scaled returns have a more positive \emph{skewness} and a smaller \emph{kurtosis} than unscaled returns.
      <<echo=TRUE,eval=FALSE>>=
# Scale returns using volume (volume clock)
rets_scaled <- ifelse(volumes > 0, 
  sqrt(volume_roll)*retsp/sqrt(volumes), 0)
rets_scaled <- sd(retsp)*rets_scaled/sd(rets_scaled)
# rets_scaled <- ifelse(volumes > 1e4, retsp/volumes, 0)
# Calculate moments of scaled returns
nrows <- NROW(retsp)
sapply(list(retsp=retsp, rets_scaled=rets_scaled),
  function(rets) {sapply(c(skew=3, kurt=4),
           function(x) sum((rets/sd(rets))^x)/nrows)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vti_scaled.png}
      <<echo=TRUE,eval=FALSE>>=
# x11(width=6, height=5)
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
# Plot densities of SPY returns
madv <- mad(retsp)
# bwidth <- mad(rutils::diffit(retsp))
plot(density(retsp, bw=madv/10), xlim=c(-5*madv, 5*madv),
     lwd=3, mgp=c(2, 1, 0), col="blue",
     xlab="returns (standardized)", ylab="frequency",
     main="Density of Volume-scaled VTI Returns")
lines(density(rets_scaled, bw=madv/10), lwd=3, col="red")
curve(expr=dnorm(x, mean=mean(retsp), sd=sd(retsp)),
      add=TRUE, lwd=3, col="green")
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("unscaled", "scaled", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
quartz.save("figure/vti_scaled.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Risk and Performance Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PerformanceAnalytics} for Risk and Performance Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package 
      \href{https://cran.r-project.org/web/packages/PerformanceAnalytics/index.html}{\emph{PerformanceAnalytics}} 
      contains functions for calculating risk and performance statistics, such as the variance, skewness, kurtosis, beta, alpha, etc.
      \vskip1ex
      The function \texttt{data()} loads external data or listv data sets in a package.
      \vskip1ex
      \texttt{managers} is an \emph{xts} time series containing monthly percentage returns of six asset managers (HAM1 through HAM6), the EDHEC Long-Short Equity hedge fund index, the \texttt{S\&P 500}, and US Treasury 10-year bond and 3-month bill total returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package PerformanceAnalytics
library(PerformanceAnalytics)
# Get documentation for package PerformanceAnalytics
# Get short description
packageDescription("PerformanceAnalytics")
# Load help page
help(package="PerformanceAnalytics")
# List all objects in PerformanceAnalytics
ls("package:PerformanceAnalytics")
# List all datasets in PerformanceAnalytics
data(package="PerformanceAnalytics")
# Remove PerformanceAnalytics from search path
detach("package:PerformanceAnalytics")
      @
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
perf_data <- unclass(data(
    package="PerformanceAnalytics"))$results[, -(1:2)]
apply(perf_data, 1, paste, collapse=" - ")
# Load "managers" data set
data(managers)
class(managers)
dim(managers)
head(managers, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plots of Cumulative Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.CumReturns()} from package \emph{PerformanceAnalytics} plots the cumulative returns of a time series of returns.
      <<echo=TRUE,eval=FALSE>>=
# Load package "PerformanceAnalytics"
library(PerformanceAnalytics)
# Calculate ETF returns
retsp <- rutils::etfenv$returns[, c("VTI", "DBC", "IEF")]
retsp <- na.omit(retsp)
# Plot cumulative ETF returns
x11(width=6, height=5)
chart.CumReturns(retsp, lwd=2, ylab="",
  legend.loc="topleft", main="ETF Cumulative Returns")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_cum_returns.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Histogram()} from package \emph{PerformanceAnalytics} plots the histogram (frequency distribution) and the density of returns.
      <<echo=TRUE,eval=FALSE>>=
retsp <- rutils::etfenv$returns$VTI
retsp <- na.omit(retsp)
x11(width=6, height=5)
chart.Histogram(retsp, xlim=c(-0.04, 0.04),
  colorset = c("lightgray", "red", "blue"), lwd=3,
  main=paste("Distribution of", colnames(retsp), "Returns"),
  methods = c("add.density", "add.normal"))
legend("topright", inset=0.05, bty="n",
       leg=c("VTI Density", "Normal"),
       lwd=6, lty=1, col=c("red", "blue"))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/returns_histogram.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Boxplot()} from package \emph{PerformanceAnalytics} plots a box-and-whisker plot for a distribution of returns.
      \vskip1ex
      The function \texttt{chart.Boxplot()} is a wrapper and calls the function \texttt{graphics::boxplot()} to plot the box plots.
      \vskip1ex
      A \emph{box plot} (box-and-whisker plot) is a graphical display of a distribution of data: \\
      The \emph{box} represents the upper and lower quartiles, \\
      The vertical lines (whiskers) represent values beyond the quartiles, \\
      Open circles represent values beyond the nominal range (outliers).
      <<echo=TRUE,eval=FALSE>>=
retsp <- rutils::etfenv$returns[,
  c("VTI", "IEF", "IVW", "VYM", "IWB", "DBC", "VXX")]
x11(width=6, height=5)
chart.Boxplot(names=FALSE, retsp)
par(cex.lab=0.8, cex.axis=0.8)
axis(side=2, at=(1:NCOL(retsp))/7.5-0.05,labels=colnames(retsp))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_box_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation Estimator of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate normally distributed data
nrows <- 1000
datav <- rnorm(nrows)
sd(datav)
mad(datav)
median(abs(datav - median(datav)))
median(abs(datav - median(datav)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
bootd <- sapply(1:10000, function(x) {
  samplev <- datav[sample.int(nrows, replace=TRUE)]
  c(sd=sd(samplev), mad=mad(samplev))
})  # end sapply
bootd <- t(bootd)
# Analyze bootstrapped variance
head(bootd)
sum(is.na(bootd))
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
ncores <- detectCores() - 1  # Number of cores
cluster <- makeCluster(ncores)  # Initialize compute cluster
bootd <- parLapply(cluster, 1:10000,
  function(x, datav) {
    samplev <- datav[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, datav=datav)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
bootd <- mclapply(1:10000, function(x) {
    samplev <- datav[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, mc.cores=ncores)  # end mclapply
stopCluster(cluster)  # Stop R processes over cluster
bootd <- rutils::do_call(rbind, bootd)
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
retsp <- rutils::etfenv$returns$VTI
retsp <- na.omit(retsp)
nrows <- NROW(retsp)
sd(retsp)
mad(retsp)
# Bootstrap of sd and mad estimators
bootd <- sapply(1:10000, function(x) {
  samplev <- retsp[sample.int(nrows, replace=TRUE)]
  c(sd=sd(samplev), mad=mad(samplev))
})  # end sapply
bootd <- t(bootd)
# Means and standard errors from bootstrap
100*apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
ncores <- detectCores() - 1  # Number of cores
cluster <- makeCluster(ncores)  # Initialize compute cluster
clusterExport(cluster, c("nrows", "returns"))
bootd <- parLapply(cluster, 1:10000,
  function(x) {
    samplev <- retsp[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  })  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
bootd <- mclapply(1:10000, function(x) {
    samplev <- retsp[sample.int(nrows, replace=TRUE)]
    c(sd=sd(samplev), mad=mad(samplev))
  }, mc.cores=ncores)  # end mclapply
stopCluster(cluster)  # Stop R processes over cluster
bootd <- rutils::do_call(rbind, bootd)
# Means and standard errors from bootstrap
apply(bootd, MARGIN=2, function(x)
  c(mean=mean(x), stderror=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Downside Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some investors argue that positive returns don't represent risk, only those returns less than the target rate of return $r_t$.
      \vskip1ex
      The \emph{Downside Deviation} (semi-deviation) $\sigma_{d}$ is equal to the standard deviation of returns less than the target rate of return $r_t$:
      \begin{displaymath}
        \sigma_{d} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ([r_i-r_t]_{-})^2}
      \end{displaymath}
      The function \texttt{DownsideDeviation()} from package \emph{PerformanceAnalytics} calculates the downside deviation, for either the full time series (\texttt{method="full"}) or only for the subseries less than the target rate of return $r_t$ (\texttt{method="subset"}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
# Define target rate of return of 50 bps
targetr <- 0.005
# Calculate the full downside returns
returns_sub <- (retsp - targetr)
returns_sub <- ifelse(returns_sub < 0, returns_sub, 0)
nrows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/nrows),
  drop(DownsideDeviation(retsp, MAR=targetr, method="full")))
# Calculate the subset downside returns
returns_sub <- (retsp - targetr)
returns_sub <- returns_sub[returns_sub < 0]
nrows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/nrows),
  drop(DownsideDeviation(retsp, MAR=targetr, method="subset")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{drawdown} is the drop in prices from their historical peak, and is equal to the difference between the prices minus the cumulative maximum of the prices.
      \vskip1ex
      \emph{Drawdown risk} determines the risk of liquidation due to stop loss limits.
      <<echo=TRUE,eval=FALSE>>=
# Calculate time series of VTI drawdowns
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
drawdns <- (closep - cummax(closep))
# Extract the date index from the time series closep 
dates <- zoo::index(closep)
# Calculate the maximum drawdown date and depth
indexmin <- which.min(drawdns)
datemin <- dates[indexmin]
maxdd <- drawdns[datemin]
# Calculate the drawdown start and end dates
startd <- max(dates[(dates < datemin) & (drawdns == 0)])
endd <- min(dates[(dates > datemin) & (drawdns == 0)])
# dygraph plot of VTI drawdowns
datav <- cbind(closep, drawdns)
colnamev <- c("VTI", "Drawdowns")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="VTI Drawdowns") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], 
         valueRange=(1.2*range(drawdns)+0.1), independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red") %>%
  dyEvent(startd, "start drawdown", col="blue") %>%
  dyEvent(datemin, "max drawdown", col="red") %>%
  dyEvent(endd, "end drawdown", col="green")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/drawdown_plot.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot VTI drawdowns using package quantmod
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
x11(width=6, height=5)
quantmod::chart_Series(x=closep, name="VTI Drawdowns", theme=plot_theme)
xval <- match(startd, dates)
yval <- max(closep)
abline(v=xval, col="blue")
text(x=xval, y=0.95*yval, "start drawdown", col="blue", cex=0.9)
xval <- match(datemin, dates)
abline(v=xval, col="red")
text(x=xval, y=0.9*yval, "max drawdown", col="red", cex=0.9)
xval <- match(endd, dates)
abline(v=xval, col="green")
text(x=xval, y=0.85*yval, "end drawdown", col="green", cex=0.9)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk Using \texttt{PerformanceAnalytics::table.Drawdowns()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Drawdowns()} from package \emph{PerformanceAnalytics} calculates a data frame of drawdowns.
      <<echo=TRUE,eval=FALSE>>=
library(xtable)
library(PerformanceAnalytics)
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
retsp <- rutils::diffit(closep)
# Calculate table of VTI drawdowns
tablev <- PerformanceAnalytics::table.Drawdowns(retsp, geometric=FALSE)
# Convert dates to strings
tablev <- cbind(sapply(tablev[, 1:3], as.character), tablev[, 4:7])
# Print table of VTI drawdowns
print(xtable(tablev), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
      <<echo=FALSE,eval=TRUE,size="tiny",results='asis'>>=
library(xtable)
library(PerformanceAnalytics)
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
retsp <- rutils::diffit(closep)
# Calculate table of VTI drawdowns
tablev <- PerformanceAnalytics::table.Drawdowns(retsp, geometric=FALSE)
# Convert dates to strings
tablev <- cbind(sapply(tablev[, 1:3], as.character), tablev[, 4:7])
# Print table of VTI drawdowns
print(xtable(tablev), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{PerformanceSummary} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{charts.PerformanceSummary()} from package \emph{PerformanceAnalytics} plots three charts: cumulative returns, return bars, and drawdowns, for time series of returns.
      <<echo=(-(1:1)),eval=FALSE>>=
# Load "managers" data set
data(managers)
charts.PerformanceSummary(ham1,
  main="", lwd=2, ylog=TRUE)
      @
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.45\paperwidth]{figure/performance_summary-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Loss Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns has a long left tail of negative returns representing the risk of loss.
      \vskip1ex
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$.
      \vskip1ex
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
confl <- 0.1
varisk <- quantile(retsp, confl)
cvar <- mean(retsp[retsp < varisk])
# Plot histogram of VTI returns
x11(width=6, height=5)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
histp <- hist(retsp, col="lightgrey",
  xlab="returns", ylab="frequency", breaks=100,
  xlim=c(-0.05, 0.01), freq=FALSE, main="VTI Returns Histogram")
# Calculate density
densv <- density(retsp, adjust=1.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_var.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot density
lines(densv, lwd=3, col="blue")
# Plot line for VaR
abline(v=varisk, col="red", lwd=3)
text(x=varisk, y=25, labels="VaR", lwd=2, pos=2)
# Plot polygon shading for CVaR
text(x=1.5*varisk, y=10, labels="CVaR", lwd=2, pos=2)
varmax <- -0.06
rangev <- (densv$x < varisk) &  (densv$x > varmax)
polygon(c(varmax, densv$x[rangev], varisk),
  c(0, densv$y[rangev], 0), col=rgb(1, 0, 0,0.5), border=NA)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk (\protect\emph{VaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$:
      \begin{displaymath}
        \alpha = \int_{-\infty}^{\mathrm{VaR}(\alpha)} \operatorname{f}(r) \, \mathrm{d}r
      \end{displaymath}
      Where $\operatorname{f}(r)$ is the probability density (distribution) of returns.
      \vskip1ex
      At a high confidence level, the value of $\mathrm{VaR}$ is subject to estimation error, and various numerical methods are used to approximate it.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      The function \texttt{VaR()} from package \emph{PerformanceAnalytics} calculates the \emph{Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
confl <- 0.05
# Calculate VaR as quantile
varisk <- quantile(retsp, probs=confl)
# Or by sorting
sortv <- sort(as.numeric(retsp))
indeks <- round(confl*NROW(retsp))
varisk <- sortv[indeks]
# PerformanceAnalytics VaR
PerformanceAnalytics::VaR(retsp,
  p=(1-confl), method="historical")
all.equal(unname(varisk),
  as.numeric(PerformanceAnalytics::VaR(retsp,
  p=(1-confl), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk (\protect\emph{CVaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the \emph{Expected Shortfall} (\emph{ES}), or the Expected Tail Loss (\emph{ETL}).
      \vskip1ex
      The function \texttt{ETL()} from package \emph{PerformanceAnalytics} calculates the \emph{Conditional Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
# Calculate VaR as quantile
varisk <- quantile(retsp, confl)
# Calculate CVaR as expected loss
cvar <- mean(retsp[retsp < varisk])
# Or by sorting
sortv <- sort(as.numeric(retsp))
indeks <- round(confl*NROW(retsp))
varisk <- sortv[indeks]
cvar <- mean(sortv[1:indeks])
# PerformanceAnalytics VaR
PerformanceAnalytics::ETL(retsp,
  p=(1-confl), method="historical")
all.equal(cvar,
  as.numeric(PerformanceAnalytics::ETL(retsp,
  p=(1-confl), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Return Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Stats()} from package \emph{PerformanceAnalytics} calculates a data frame of risk and return statistics of the return distributions.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the risk-return statistics
risk_ret <-
  PerformanceAnalytics::table.Stats(rutils::etfenv$returns)
class(risk_ret)
# Transpose the data frame
risk_ret <- as.data.frame(t(risk_ret))
# Add Name column
risk_ret$Name <- rownames(risk_ret)
# Add Sharpe ratio column
risk_ret$Sharpe <- risk_ret$"Arithmetic Mean"/risk_ret$Stdev
# Sort on Sharpe ratio
risk_ret <- risk_ret[order(risk_ret$Sharpe, decreasing=TRUE), ]
      @
    \column{0.5\textwidth}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Copy from rutils to save time
risk_ret <- rutils::etfenv$riskstats
# Add Sharpe ratio column
risk_ret$Sharpe <- risk_ret$"Arithmetic Mean"/risk_ret$Stdev
# Sort on Sharpe ratio
risk_ret <- risk_ret[order(risk_ret$Sharpe, decreasing=TRUE), ]
# Print data frame
knitr::kable(risk_ret[, c("Sharpe", "Skewness", "Kurtosis")])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Risk and Return Preferences}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investors typically prefer larger \emph{odd moments} of the return distribution (mean, skewness), and smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      But positive skewness is often associated with lower returns, which can be observed in the \emph{VIX} volatility ETFs, \emph{VXX} and \emph{SVXY}.
      \vskip1ex
      The \emph{VXX} ETF is long the \emph{VIX} index (effectively long an option), so it has positive skewness and small kurtosis, but negative returns (it's short market risk).
      \vskip1ex
      Since the \emph{VXX} is effectively long an option, it pays option premiums so it has negative returns most of the time, with isolated periods of positive returns when markets drop.
      \vskip1ex
      The \emph{SVXY} ETF is short the \emph{VIX} index, so it has negative skewness and large kurtosis, but positive returns (it's long market risk).
      \vskip1ex
      Since the \emph{SVXY} is effectively short an option, it earns option premiums so it has positive returns most of the time, but it suffers sharp losses when markets drop.
    \column{0.5\textwidth}
    \vspace{1em}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Print data frame
knitr::kable(risk_ret[c("VXX", "SVXY"), c("Sharpe", "Skewness", "Kurtosis")])
      @
      \includegraphics[width=0.45\paperwidth]{figure/vix_vxx_svxy.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of VXX versus SVXY
pricev <- na.omit(rutils::etfenv$pricev[, c("VXX", "SVXY")])
pricev <- pricev["2017/"]
colnamev <- c("VXX", "SVXY")
colnames(pricev) <- colnamev
dygraphs::dygraph(pricev, main="Prices of VXX and SVXY") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="green") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness and Return Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Similarly to the \emph{VXX} and \emph{SVXY}, for most other ETFs positive skewness is often associated with lower returns.
      \vskip1ex
      Some of the exceptions are bond ETFs (like \emph{IEF}), which have both non-negative skewness and positive returns.
      \vskip1ex
      Another exception are commodity ETFs (like \emph{USO} oil), which have both negative skewness and negative returns.
      <<echo=TRUE,eval=FALSE>>=
# Remove VIX volatility ETF data
risk_ret <- risk_ret[-match(c("VXX", "SVXY"), risk_ret$Name), ]
# Plot scatterplot of Sharpe vs Skewness
plot(Sharpe ~ Skewness, data=risk_ret,
     ylim=1.1*range(risk_ret$Sharpe),
     main="Sharpe vs Skewness")
# Add labels
text(x=risk_ret$Skewness, y=risk_ret$Sharpe,
          labels=risk_ret$Name, pos=3, cex=0.8)
# Plot scatterplot of Kurtosis vs Skewness
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
plot(Kurtosis ~ Skewness, data=risk_ret,
     ylim=c(1, max(risk_ret$Kurtosis)),
     main="Kurtosis vs Skewness")
# Add labels
text(x=risk_ret$Skewness, y=risk_ret$Kurtosis,
          labels=risk_ret$Name, pos=1, cex=0.8)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/etf_skew_sharp.png}
      % \includegraphics[width=0.45\paperwidth]{figure/etf_skew_kurtosis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Return Measures}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe ratio} $\mathrm{S_r}$ is equal to the excess returns (in excess of the risk-free return $r_f$) divided by the standard deviation $\sigma$ of the returns:
      \begin{displaymath}
        \mathrm{S_r} = \frac{E[r-r_f]}{\sigma}
      \end{displaymath}
      The \emph{Sortino ratio} $\mathrm{{So}_r}$ is equal to the excess returns divided by the \emph{downside deviation} $\sigma_{d}$ (standard deviation of returns that are less than a target rate of return $r_t$):
      \begin{displaymath}
        \mathrm{{So}_r} = \frac{E[r-r_t]}{\sigma_{d}}
      \end{displaymath}
      The \emph{Calmar ratio} $\mathrm{C_r}$ is equal to the excess returns divided by the \emph{maximum drawdown} $\mathrm{DD}$ of the returns:
      \begin{displaymath}
        \mathrm{C_r} = \frac{E[r-r_f]}{\mathrm{DD}}
      \end{displaymath}
      The \emph{Dowd ratio} $\mathrm{D_r}$ is equal to the excess returns divided by the \emph{Value at Risk} ($\mathrm{VaR}$) of the returns:
      \begin{displaymath}
        \mathrm{D_r} = \frac{E[r-r_f]}{\mathrm{VaR}}
      \end{displaymath}
      The \emph{Conditional Dowd ratio} $\mathrm{{Dc}_r}$ is equal to the excess returns divided by the \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) of the returns:
      \begin{displaymath}
        \mathrm{{Dc}_r} = \frac{E[r-r_f]}{\mathrm{CVaR}}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
retsp <- rutils::etfenv$returns[, c("VTI", "IEF")]
retsp <- na.omit(retsp)
# Calculate the Sharpe ratio
confl <- 0.05
PerformanceAnalytics::SharpeRatio(retsp, p=(1-confl), 
  method="historical")
# Calculate the Sortino ratio
PerformanceAnalytics::SortinoRatio(retsp)
# Calculate the Calmar ratio
PerformanceAnalytics::CalmarRatio(retsp)
# Calculate the Dowd ratio
PerformanceAnalytics::SharpeRatio(retsp, FUN="VaR", 
  p=(1-confl), method="historical")
# Calculate the Dowd ratio from scratch
varisk <- sapply(retsp, quantile, probs=confl)
-sapply(retsp, mean)/varisk
# Calculate the Conditional Dowd ratio
PerformanceAnalytics::SharpeRatio(retsp, FUN="ES", 
  p=(1-confl), method="historical")
# Calculate the Conditional Dowd ratio from scratch
cvar <- sapply(retsp, function(x) {
  mean(x[x < quantile(x, confl)])
})
-sapply(retsp, mean)/cvar
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Return of Compounded Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Compounded stock returns become closer to normally distributed, and their skewness, kurtosis, and tail risks decrease significantly compared to daily returns.
      \vskip1ex
      So stocks become less risky over longer holding periods, and investors may choose to own a higher percentage of stocks, provided they hold them for a longer period of time.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
retsp <- drop(zoo::coredata(retsp))
nrows <- NROW(retsp)
# Calculate compounded VTI returns
holdp <- 252
retc <- sqrt(holdp)*sapply(1:nrows, function(x) {
    mean(retsp[sample.int(nrows, size=holdp, replace=TRUE)])
})  # end sapply
# Calculate mean, standard deviation, skewness, and kurtosis
datav <- cbind(retsp, retc)
colnames(datav) <- c("VTI", "Agg")
apply(datav, MARGIN=2, function(x) {
  # Standardize the returns
  meanval <- mean(x); stddev <- sd(x); x <- (x - meanval)/stddev
  c(mean=meanval, stddev=stddev, skew=mean(x^3), kurt=mean(x^4))
})  # end sapply
# Calculate the Sharpe and Dowd ratios
confl <- 0.05
sapply(colnames(datav), function(name) {
  x <- datav[, name]; stddev <- sd(x)
  varisk <- unname(quantile(x, probs=confl))
  cvar <- mean(x[x < varisk])
  ratio <- 1
  if (name == colnames(datav)[2]) {ratio <- holdp}
  sqrt(252/ratio)*mean(x)/c(Sharpe=stddev, Dowd=-varisk, DowdC=-cvar)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/risk_compound.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the densities of returns
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot(density(retsp), t="l", lwd=3, col="blue",
     xlab="returns", ylab="density", xlim=c(-0.04, 0.04),
     main="Distribution of Compounded Stock Returns")
lines(density(retc), t="l", col="red", lwd=3)
curve(expr=dnorm(x, mean=mean(retc), sd=sd(retc)), col="green", lwd=3, add=TRUE)
legend("topright", legend=c("VTI Daily", "Compounded", "Normal"),
       inset=-0.1, bg="white", lty=1, lwd=6, col=c("blue", "red", "green"), bty="n")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Rebalancing Strategies}


%%%%%%%%%%%%%%%
\subsection{Calculating Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Given a time series of asset prices $p_i$, the dollar returns $r^d_i$, the percentage returns $r^p_i$, and the log returns $r^l_i$ are defined as:
      \begin{displaymath}
        r^d_i = p_i - p_{i-1} \quad r^p_i = \frac{p_i - p_{i-1}}{p_{i-1}} \quad r^l_i = \log(\frac{p_i}{p_{i-1}})
      \end{displaymath}
      The initial returns are all equal to zero.
      \vskip1ex
      If the log returns are small $r^l \ll 1$, then they are approximately equal to the percentage returns: $r^l \approx r^p$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)
# Extract ETF prices from rutils::etfenv$prices
pricev <- rutils::etfenv$prices
pricev <- zoo::na.locf(pricev, na.rm=FALSE)
pricev <- zoo::na.locf(pricev, fromLast=TRUE)
dates <- zoo::index(pricev)
# Calculate simple dollar returns
retsd <- rutils::diffit(pricev)
# Or
# retsd <- lapply(pricev, rutils::diffit)
# retsd <- rutils::do_call(cbind, retsd)
# Calculate percentage returns
retsp <- retsd/rutils::lagit(pricev, lagg=1, pad_zeros=FALSE)
# Calculate log returns
retsl <- rutils::diffit(log(pricev))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Compounding Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The sum of the dollar returns:
      $\sum_{i=1}^n r^d_i$
      represents the wealth path from owning a \emph{fixed number of shares}.
      \vskip1ex
      The compounded percentage returns:
      $\prod_{i=1}^n (1 + r^p_i$)
      also represent the wealth path from owning a \emph{fixed number of shares}, initially equal to \texttt{\$1} dollar.
      \vskip1ex
      The sum of the percentage returns (without compounding):
      $\sum_{i=1}^n r^p_i$
      represents the wealth path from owning a \emph{fixed dollar amount} of stock.
      \vskip1ex
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing} - selling shares when their price goes up, and vice versa.
      \vskip1ex
      This \emph{rebalancing} therefore acts as a mean reverting strategy.
      \vskip1ex
      Rebalancing requires borrowing from a \emph{margin account}, and it also incurs trading costs.
      \vskip1ex
      The logarithm of the wealth of a \emph{fixed number of shares} is often used to compare investments, and it's approximately equal to the sum of the percentage returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_log_vti.png}
      <<echo=TRUE,eval=FALSE>>=
# Set the initial dollar returns
retsd[1, ] <- pricev[1, ]
# Calculate prices from dollar returns
pricesn <- cumsum(retsd)
all.equal(pricevn, pricev)
# Compound the percentage returns
pricesn <- cumprod(1+retsp)
# Set the initial prices
pricesi <- as.numeric(pricev[1, ])
pricesn <- lapply(1:NCOL(pricevn), function (i)
    pricesi[i]*pricesn[, i])
pricesn <- rutils::do_call(cbind, pricesn)
# Or
# pricesn <- t(t(pricevn)*pricesi)
all.equal(pricevn, pricev, check.attributes=FALSE)
# Plot log VTI prices
endd <- rutils::calc_endpoints(rutils::etfenv$VTI, interval="months")
dygraphs::dygraph(log(quantmod::Cl(rutils::etfenv$VTI)[endd]),
  main="Logarithm of VTI Prices") %>%
  dyOptions(colors="blue", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Funding Costs of Single Asset Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The wealth accumulated from owning a \emph{fixed dollar amount} of stock is equal to the cash earned from rebalancing, which is proportional to the sum of the percentage returns, and it's kept in a \emph{margin account}: $m_t = \sum_{i=1}^t r^p_i$.
      \vskip1ex
      The cash in the \emph{margin account} can be positive (accumulated profits) or negative (losses).
      \vskip1ex
      The \emph{funding costs} $c^f_t$ are approximately equal to the \emph{margin account} $m_t$ times the \emph{funding rate} $f$: $c^f_t = f \, m_t = f \, \sum_{i=1}^t r^p_i$.
      \vskip1ex
      Positive \emph{funding costs} represent interest profits earned on the \emph{margin account}, while negative costs represent the interest paid for funding stock purchases.
      \vskip1ex
      The \emph{cumulative funding costs} $\sum_{i=1}^t c^f_i$ must be added to the \emph{margin account}: $m_t + \sum_{i=1}^t c^f_i$.
      <<echo=TRUE,eval=FALSE>>=
# Calculate percentage VTI returns
pricev <- rutils::etfenv$prices$VTI
pricev <- na.omit(pricev)
retsp <- rutils::diffit(pricev)/
  rutils::lagit(pricev, lagg=1, pad_zeros=FALSE)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_margin.png}
      <<echo=TRUE,eval=FALSE>>=
# Funding rate per day
frate <- 0.01/252
# Margin account
margin <- cumsum(retsp)
# Cumulative funding costs
fcosts <- cumsum(frate*margin)
# Add funding costs to margin account
margin <- (margin + fcosts)
# dygraph plot of margin and funding costs
datav <- cbind(margin, fcosts)
colnamev <- c("Margin", "Cumulative Funding")
colnames(datav) <- colnamev
endd <- rutils::calc_endpoints(datav, interval="months")
dygraphs::dygraph(datav[endd], main="VTI Margin Funding Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Trading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The total \emph{transaction costs} are the sum of the \emph{broker commissions}, the \emph{bid-offer spread} (for market orders), \emph{lost trades} (for limit orders), and \emph{market impact}.
      \vskip1ex
      Broker commissions depend on the broker, the size of the trades, and on the type of investors, with institutional investors usually enjoying smaller commissions.
      \vskip1ex
      The \emph{bid-offer spread} is the percentage difference between the \emph{offer} minus the \emph{bid} price, divided by the \emph{mid} price.
      \vskip1ex
      Market impact is the effect of large trades pushing the market prices (the limit order book) against the trades, making the filled price worse.
      \vskip1ex
      Limit orders are not subject to the bid-offer spread but they are exposed to \emph{lost trades}.
      \vskip1ex
      \emph{Lost trades} are limit orders that don't get executed, resulting in lost potential profits.
      \vskip1ex
      Limit orders may receive rebates from some exchanges, which may reduce transaction costs.
    \column{0.5\textwidth}
      The \emph{bid-offer spread} for liquid stocks can be assumed to be about \texttt{10} basis points (bps).
      \vskip1ex
      In reality the \emph{bid-offer spread} is not static and depends on many factors, such as market liquidity (trading volume), volatility, and the time of day.
      \vskip1ex
      The \emph{transaction costs} due to the \emph{bid-offer spread} are equal to the number of traded shares times their price, times half the \emph{bid-offer spread}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Single Asset Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing}, selling shares when their price goes up, and vice versa.
      \vskip1ex
      The dollar amount of stock that must be traded in a given period is equal to the absolute of the percentage returns: $\left| r_t \right|$.
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ due to rebalancing are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \left| r_t \right|$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{margin account} $m_t$: $m_t - \sum_{i=1}^t c^r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_single_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# bid_offer equal to 10 bps for liquid ETFs
bid_offer <- 0.001
# Cumulative transaction costs
costs <- bid_offer*cumsum(abs(retsp))/2
# Subtract transaction costs from margin account
margin <- cumsum(retsp)
margin <- (margin - costs)
# dygraph plot of margin and transaction costs
datav <- cbind(margin, costs)
colnamev <- c("Margin", "Cumulative Transaction Costs")
colnames(datav) <- colnamev
dygraphs::dygraph(datav[endd], main="VTI Transaction Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Combining the Returns of Multiple Assets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Adding the weighted dollar returns is equivalent to buying a \emph{fixed number of shares} (aka \emph{Fixed Share Allocation} or FSA) proportional to the weights.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} (aka \emph{Fixed Dollar Amount} or FDA) proportional to the weights.
      \vskip1ex
      The portfolio allocations must be periodically rebalanced to keep the dollar amounts of the stocks proportional to the weights.
      \vskip1ex
      This \emph{rebalancing} acts as a mean reverting strategy - selling shares when their price goes up, and vice versa.
      \vskip1ex
      The portfolio with fixed dollar amounts has a slightly higher Sharpe ratio than the portfolio with a fixed number of shares.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI and IEF dollar returns
pricev <- rutils::etfenv$pricev[, c("VTI", "IEF")]
pricev <- na.omit(pricev)
retsd <- rutils::diffit(pricev)
# Calculate VTI and IEF percentage returns
retsp <- retsd/rutils::lagit(pricev, lagg=1, pad_zeros=FALSE)
# Set the initial dollar returns
retsd[1, ] <- pricev[1, ]
# Wealth of fixed shares equal to $0.5 each (without rebalancing)
weightv <- c(0.5, 0.5)  # dollar weights
# Scale the dollar returns using the dollar weights
pricesi <- as.numeric(pricev[1, ])
wealth_fsa <- cumsum(retsd %*% (weightv/pricesi))
# Or using percentage returns
wealth_fsa2 <- cumprod(1+retsp) %*% weightv
all.equal(wealth_fsa, drop(wealth_fsa2))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_weighted_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed dollars (with rebalancing)
wealth_fda <- cumsum(retsp %*% weightv)
# Calculate the Sharpe and Sortino ratios
wealthv <- cbind(log(wealth_fsa), wealth_fda)
wealthv <- xts::xts(wealthv, zoo::index(pricev))
colnames(wealthv) <- c("Fixed shares", "Fixed dollars")
sqrt(252)*sapply(rutils::diffit(wealthv), 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the log wealth
colnamev <- colnames(wealthv)
endd <- rutils::calc_endpoints(retsp, interval="months")
dygraphs::dygraph(wealthv[endd], main="Wealth of Weighted Portfolios") %>%
  dySeries(name=colnamev[1], col="blue", strokeWidth=2) %>%
  dySeries(name=colnamev[2], col="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Weighted Portfolio Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing}, selling shares when their price goes up, and vice versa.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} proportional to the weights.
      \vskip1ex
      The dollar amount of stock that must be traded in a given period is equal to the weighted sum of the absolute percentage returns: $w1 \left| r^1_t \right| + w2 \left| r^2_t \right|$.
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ due to rebalancing are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} (w1 \left| r^1_t \right| + w2 \left| r^2_t \right|)$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{margin account} $m_t$: $m_t - \sum_{i=1}^t c^r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# Margin account for fixed dollars (with rebalancing)
margin <- cumsum(retsp %*% weightv)
# Cumulative transaction costs
costs <- bid_offer*cumsum(abs(retsp) %*% weightv)/2
# Subtract transaction costs from margin account
margin <- (margin - costs)
# dygraph plot of margin and transaction costs
datav <- cbind(margin, costs)
datav <- xts::xts(datav, zoo::index(pricev))
colnamev <- c("Margin", "Cumulative Transaction Costs")
colnames(datav) <- colnamev
dygraphs::dygraph(datav[endd], main="Fixed Dollar Portfolio Transaction Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{proportional dollar allocation} strategy (\emph{PDA}), the total wealth $w_t$ is allocated to the assets $w_i$ proportional to the portfolio weights $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      The total wealth $w_t$ is not fixed and is equal to the portfolio market value $w_t = \sum w_i$, so there's no margin account.
      \vskip1ex
      The portfolio is rebalanced daily to maintain the dollar allocations $w_i$ equal to the total wealth $w_t = \sum w_i$ times the portfolio weights: $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      Let $r_i$ be the percentage returns, $\omega_i$ be the portfolio weights, and $\bar{r}_t = \sum_{i=1}^n \omega_i r_i$ be the weighted percentage returns at time $t$.
      \vskip1ex
      The total portfolio wealth at time $t$ is equal to the wealth at time $t-1$ multiplied by the weighted returns: $w_t = w_{t-1} (1 + \bar{r}_t)$.
      \vskip1ex
      The dollar amount of stock $i$ at time $t$ increases by $\omega_i r_i$ so it's equal to $\omega_i w_{t-1} (1 + r_i)$, while the target amount is $\omega_i w_t = \omega_i w_{t-1} (1 + \bar{r}_t)$
      \vskip1ex
      The dollar amount of stock $i$ needed to trade to rebalance back to the target weight is equal to:
      \begin{flalign*}
        \varepsilon_i &= \left| \omega_i w_{t-1} (1 + \bar{r}_t) - \omega_i w_{t-1} (1 + r_i) \right|\\
        &= \omega_i w_{t-1} \left| \bar{r}_t - r_i \right|
      \end{flalign*}
      If $\bar{r}_t > r_i$ then an amount $\varepsilon_i$ of the stock $i$ needs to be bought, and if $\bar{r}_t < r_i$ then it needs to be sold.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_allocations.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
wealth_fsa <- cumsum(retsd %*% (weightv/pricesi))
# Or compound the percentage returns
wealth_fsa <- cumprod(1+retsp) %*% weightv
# Wealth of proportional allocations (with rebalancing)
wealth_pda <- cumprod(1 + retsp %*% weightv)
wealthv <- cbind(wealth_fsa, wealth_pda)
wealthv <- xts::xts(wealthv, zoo::index(pricev))
colnames(wealthv) <- c("Fixed shares", "Prop dollars")
wealthv <- log(wealthv)
# Calculate the Sharpe and Sortino ratios
sqrt(252)*sapply(rutils::diffit(wealthv),
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the log wealth
dygraphs::dygraph(wealthv[endd], 
  main="Wealth of Proportional Dollar Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In each period the stocks must be rebalanced to maintain the proportional dollar allocations.
      \vskip1ex
      The total dollar amount of stocks that need to be traded to rebalance back to the target weight is equal to: $\sum_{i=1}^n \varepsilon_i = w_{t-1} \sum_{i=1}^n \omega_i \left| \bar{r}_t - r_i \right|$
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \sum_{i=1}^n \varepsilon_i$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{wealth} $w_t$: $w_t - \sum_{i=1}^t c^r_i$.
      <<echo=TRUE,eval=FALSE>>=
# Returns in excess of weighted returns
retsw <- retsp %*% weightv
retsx <- lapply(retsp, function(x) (retsw - x))
retsx <- do.call(cbind, retsx)
sum(retsx %*% weightv)
# Calculate weighted sum of absolute excess returns
retsx <- abs(retsx) %*% weightv
# Total dollar amount of stocks that need to be traded
retsx <- retsx*rutils::lagit(wealth_pda)
# Cumulative transaction costs
costs <- bid_offer*cumsum(retsx)/2
# Subtract transaction costs from wealth
wealth_pda <- (wealth_pda - costs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_allocations_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of wealth and transaction costs
wealthv <- cbind(wealth_pda, costs)
wealthv <- xts::xts(wealthv, zoo::index(pricev))
colnamev <- c("Wealth", "Cumulative Transaction Costs")
colnames(wealthv) <- colnamev
dygraphs::dygraph(wealthv[endd], 
  main="Transaction Costs With Proportional Allocations") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Proportional Target Allocation Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{fixed share strategy} (\emph{FSA}), the number of shares is fixed, with their initial dollar value equal to the portfolio weights.
      \vskip1ex
      In the \emph{proportional dollar allocation} strategy (\emph{PDA}), the portfolio is rebalanced daily to maintain the dollar allocations $w_i$ equal to the total wealth $w_t = \sum w_i$ times the portfolio weights: $\omega_i$: $w_i = \omega_i w_t$.
      \vskip1ex
      In the \emph{proportional target allocation} strategy (\emph{PTA}), the portfolio is rebalanced only if the dollar allocations $w_i$ differ from their targets $\omega_i w_t$ more than the threshold value $\tau$: $\tau > \frac{\sum \left| w_i - \omega_i w_t \right|}{w_t}$.
      \vskip1ex
      The \emph{PTA} strategy is path-dependent so it must be simulated using an explicit loop. 
      \vskip1ex
      The \emph{PTA} strategy is contrarian, since it sells assets that have outperformed, and it buys assets that have underperformed.
      \vskip1ex
      If the threshold level is very small then the \emph{PTA} strategy rebalances daily and it's the same as the \emph{PDA}.
      \vskip1ex
      If the threshold level is very large then the \emph{PTA} strategy does not rebalance and it's the same as the \emph{FSA}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
wealth_fsa <- drop(apply(retsp, 2, function(x) cumprod(1+x)) %*% weightv)-1
# Wealth of proportional dollar allocations (with rebalancing)
wealth_pda <- cumprod(1 + retsp %*% weightv) - 1
# Wealth of proportional target allocation (with rebalancing)
retsp <- zoo::coredata(retsp)
threshold <- 0.05
wealthv <- matrix(nrow=NROW(retsp), ncol=2)
colnames(wealthv) <- colnames(retsp)
wealthv[1, ] <- weightv
for (it in 2:NROW(retsp)) {
  # Accrue wealth without rebalancing
  wealthv[it, ] <- wealthv[it-1, ]*(1 + retsp[it, ])
  # Rebalance if wealth allocations differ from weights
  if (sum(abs(wealthv[it, ] - sum(wealthv[it, ])*weightv))/sum(wealthv[it, ]) > threshold) {
    # cat("Rebalance at:", it, "\n")
    wealthv[it, ] <- sum(wealthv[it, ])*weightv
  } # end if
} # end for
wealthv <- rowSums(wealthv) - 1
wealthv <- cbind(wealth_pda, wealthv)
wealthv <- xts::xts(wealthv, zoo::index(pricev))
colnames(wealthv) <- c("Proportional Allocations", "Proportional Target")
dygraphs::dygraph(wealthv, main="Wealth of Proportional Target Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stock and Bond Portfolio With Proportional Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolios combining stocks and bonds can provide a much better risk versus return tradeoff than either of the assets separately, because the returns of stocks and bonds are usually negatively correlated, so they are natural hedges of each other.
      \vskip1ex
      The fixed portfolio weights represent the percentage dollar allocations to stocks and bonds, while the portfolio wealth grows over time.
      \vskip1ex
      The weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios.
      <<echo=TRUE,eval=FALSE>>=
# Calculate stock and bond returns
retsp <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
weightv <- c(0.4, 0.6)
retsp <- cbind(retsp, retsp %*% weightv)
colnames(retsp)[3] <- "Combined"
# Calculate correlations
cor(retsp)
# Calculate Sharpe ratios
sqrt(252)*sapply(retsp, function(x) mean(x)/sd(x))
# Calculate standard deviation, skewness, and kurtosis
sapply(retsp, function(x) {
  # Calculate standard deviation
  stddev <- sd(x)
  # Standardize the returns
  x <- (x - mean(x))/stddev
  c(stddev=stddev, skew=mean(x^3), kurt=mean(x^4))
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_proportional_stocks_bonds.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of proportional allocations
wealthv <- cumprod(1 + retsp)
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(retsp, interval="months")
# Plot cumulative log wealth
dygraphs::dygraph(log(wealthv[endd]), 
  main="Stocks and Bonds With Proportional Allocations") %>%
  dyOptions(colors=c("blue", "green", "blue", "red")) %>%
  dySeries("Combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock and Bond Portfolio Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal stock and bond weights can be calculated using optimization.
      \vskip1ex
      Using the past \texttt{20} years of data, the optimal \emph{VTI} weight is about \texttt{0.25}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the Sharpe ratios
sqrt(252)*sapply(retsp, function(x) mean(x)/sd(x))
# Calculate the Sharpe ratios for vector of weights
weightv <- seq(0.05, 0.95, 0.05)
sharpev <- sqrt(252)*sapply(weightv, function(weight) {
  weightv <- c(weight, 1-weight)
  retsp <- (retsp[, 1:2] %*% weightv)
  mean(retsp)/sd(retsp)
})  # end sapply
# Calculate the optimal VTI weight
weightm <- weightv[which.max(sharpev)]
# Calculate the optimal weight using optimization
calc_sharpe <- function(weight) {
  weightv <- c(weight, 1-weight)
  retsp <- (retsp[, 1:2] %*% weightv)
  -mean(retsp)/sd(retsp)
}  # end calc_sharpe
optv <- optimize(calc_sharpe, interval=c(0, 1))
weightm <- optv$minimum
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_sharpe_max.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot Sharpe ratios
plot(x=weightv, y=sharpev, 
     main="Sharpe Ratio as Function of VTI Weight",
     xlab="VTI weight", ylab="Sharpe Ratio", 
     t="l", lwd=3, col="blue")
abline(v=weightm, lty="dashed", lwd=1, col="blue")
text(x=weightm, y=0.7*max(sharpev), pos=4, cex=1.2, 
     labels=paste("optimal VTI weight =", round(weightm, 2)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distributions of Terminal Wealth From Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The past data represents only one possible future scenario.  We can generate more scenarios using bootstrap simulation.
      \vskip1ex
      The bootstrap data is a list of simulated \emph{VTI} and \emph{IEF} returns, which represent possible realizations of future returns, based on past history.
      \vskip1ex
      The distribution of \emph{VTI} wealth is much wider than \emph{IEF}, but it has a much greater mean value.
      <<echo=TRUE,eval=FALSE>>=
# Coerce the returns from xts time series to matrix 
retsp <- zoo::coredata(retsp[, 1:2])
nrows <- NROW(retsp)
# Bootstrap the returns and calculate a list of random returns
nboot <- 1e4
library(parallel)  # Load package parallel
ncores <- detectCores() - 1  # Number of cores
# Perform parallel bootstrap under Windows
cluster <- makeCluster(ncores)  # Initialize compute cluster under Windows
clusterSetRNGStream(cluster, 1121)  # Reset random number generator in all cores
clusterExport(cluster, c("retsp", "nrows"))
bootd <- parLapply(cluster, 1:nboot, function(x) {
  retsp[sample.int(nrows, replace=TRUE), ]
})  # end parLapply
# Stop R processes over cluster under Windows.
stopCluster(cluster)
# Perform parallel bootstrap under Mac-OSX or Linux
set.seed(1121)
bootd <- mclapply(1:nboot, function(x) {
  retsp[sample.int(nrows, replace=TRUE), ]
}, mc.cores=ncores)  # end mclapply
is.list(bootd); NROW(bootd); dim(bootd[[1]])
# Calculate the distribution of terminal wealths of VTI and IEF
wealthv <- sapply(bootd, function(retsp) {
  apply(retsp, 2, function(x) prod(1+x))
})  # end sapply
wealthv <- t(wealthv)
class(wealthv); dim(wealthv); tail(wealthv)
# Calculate the means and standard deviations of the terminal wealths
apply(wealthv, 2, mean)
apply(wealthv, 2, sd)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_wealth_vtiief.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the densities of the terminal wealths of VTI and IEF
wealthvti <- wealthv[, "VTI"]
wealthief <- wealthv[, "IEF"]
meanvti <- mean(wealthvti); meanief <- mean(wealthief)
densvti <- density(wealthvti); densief <- density(wealthief)
plot(densvti, col="blue", lwd=3, xlab="wealth",
     xlim=c(0, 2*max(densief$x)), ylim=c(0, max(densief$y)), 
     main="Terminal Wealth Distributions of VTI and IEF")
lines(densief, col="green", lwd=3)
abline(v=meanvti, col="blue", lwd=2, lty="dashed")
text(x=meanvti, y=0.5, labels="VTI mean", pos=4, cex=0.8)
abline(v=meanief, col="green", lwd=2, lty="dashed")
text(x=meanief, y=0.5, labels="IEF mean", pos=4, cex=0.8)
legend(x="topright", legend=c("VTI", "IEF"),
       inset=0.1, cex=1.0, bg="white", bty="n",
       lwd=6, lty=1, col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Stock Wealth and Holding Period}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of stock wealth for short holding periods is close to symmetric around par (\texttt{1}).
      \vskip1ex
      The distribution for long holding periods is highly positively skewed with a much larger mean.
      \vskip1ex
      U.S. stocks in the last 40 years have had higher risk-adjusted wealth for longer holding periods.
      \vskip1ex
      The downside risk is equal to the mean of the wealth below par (\texttt{1}).
      <<echo=TRUE,eval=FALSE>>=
# Calculate the distributions of portfolio wealth
holdv <- nrows*seq(0.1, 1.0, 0.1)
wealthm <- sapply(bootd, function(retsp) {
  sapply(holdv, function(holdp) {
    prod(1 + retsp[1:holdp, "VTI"])
  })  # end sapply
})  # end sapply
wealthm <- t(wealthm)
dim(wealthm)
# Define the risk-adjusted wealth measure
riskretfun <- function(wealthv) {
  riskv <- min(wealthv)
  if (min(wealthv) < 1)
    riskv <- mean((1-wealthv)[wealthv<1])
  mean(wealthv)/riskv
}  # end riskretfun
# Calculate the stock wealth risk-return ratios
riskrets <- apply(wealthm, 2, riskretfun)
# Plot the stock wealth risk-return ratios
plot(x=holdv, y=riskrets, 
     main="Stock Risk-Return Ratio as Function of Holding Period",
     xlab="Holding Period", ylab="Ratio", 
     t="l", lwd=3, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_wealth_longshort.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the stock wealth for long and short holding periods
wealth1 <- wealthm[, 9]
wealth2 <- wealthm[, 1]
mean1 <- mean(wealth1); mean2 <- mean(wealth2)
dens1 <- density(wealth1); dens2 <- density(wealth2)
plot(dens1, col="blue", lwd=3, xlab="wealth",
     xlim=c(0, 2*max(dens2$x)), ylim=c(0, max(dens2$y)), 
     main="Wealth Distributions for Long and Short Holding Periods")
lines(dens2, col="green", lwd=3)
abline(v=mean1, col="blue", lwd=2, lty="dashed")
text(x=mean1, y=0.5, labels="Long", pos=4, cex=0.8)
abline(v=mean2, col="green", lwd=2, lty="dashed")
text(x=mean2, y=0.5, labels="Short", pos=4, cex=0.8)
legend(x="top", legend=c("Long", "Short"),
       inset=0.1, cex=1.0, bg="white", bty="n",
       lwd=6, lty=1, col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock and Bond Portfolio Allocations From Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal stock and bond weights can be calculated using bootstrap simulation.
      \vskip1ex
      Bootstrapping the past \texttt{20} years of data, the optimal \emph{VTI} weight is about \texttt{0.35}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the distributions of portfolio wealth
weightv <- seq(0.05, 0.95, 0.05)
wealthm <- sapply(bootd, function(retsp) {
  sapply(weightv, function(weight) {
    prod(1 + retsp %*% c(weight, 1-weight))
  })  # end sapply
})  # end sapply
wealthm <- t(wealthm)
dim(wealthm)
# Calculate the portfolio risk-return ratios
riskrets <- apply(wealthm, 2, riskretfun)
# Calculate the optimal VTI weight
weightm <- weightv[which.max(riskrets)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_riskret_max.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the portfolio risk-return ratios
plot(x=weightv, y=riskrets, 
     main="Portfolio Risk-Return Ratio as Function of VTI Weight",
     xlab="VTI weight", ylab="Ratio", 
     t="l", lwd=3, col="blue")
abline(v=weightm, lty="dashed", lwd=1, col="blue")
text(x=weightm, y=0.7*max(riskrets), pos=4, cex=1.2, 
     labels=paste("optimal VTI weight =", round(weightm, 2)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a portfolio with proportional allocations of stocks ($30\%$), bonds ($55\%$), and commodities and precious metals ($15\%$) (approximately).
      \vskip1ex
      The \emph{All-Weather} portfolio was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      {\tiny
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/} \\
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511} \\
      }
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vskip1ex
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
retsp <- na.omit(rutils::etfenv$returns[, symbolv])
# Calculate all-weather portfolio wealth
weightsaw <- c(0.30, 0.55, 0.15)
retsp <- cbind(retsp, retsp %*% weightsaw)
colnames(retsp)[4] <- "All Weather"
# Calculate Sharpe ratios
sqrt(252)*sapply(retsp, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_all_weather.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative wealth from returns
wealthv <- cumprod(1+retsp)
# Calculate a vector of monthly end points
endd <- rutils::calc_endpoints(wealthv, interval="months")
# dygraph all-weather wealth
dygraphs::dygraph(wealthv[endd], main="All-Weather Portfolio") %>%
  dyOptions(colors=c("blue", "green", "orange", "red")) %>%
  dySeries("All Weather", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot all-weather wealth
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "red")
quantmod::chart_Series(wealthv, theme=plot_theme, lwd=c(2, 2, 2, 4),
             name="All-Weather Portfolio")
legend("topleft", legend=colnames(wealthv),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Proportion Portfolio Insurance Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Constant Proportion Portfolio Insurance} (CPPI) strategy the portfolio is rebalanced between stocks and zero-coupon bonds, to protect against the loss of principal.
      \vskip1ex
      A zero-coupon bond pays no coupon, but it's bought at a discount to par ($100\%$), and pays par at maturity.  The investor receives capital appreciation instead of coupons.
      \vskip1ex
      Let $P$ be the investor principal amount (total initial invested dollar amount), and let $F$ be the zero-coupon \emph{bond floor}.  
      The zero-coupon bond floor $F$ is set so that its value at maturity is equal to the principal $P$.  
      This guarantees that the investor is paid back at least the full principal $P$.
      \vskip1ex
      The stock investment is levered by the \emph{CPPI multiplier} $C$.  
      The initial dollar amount invested in stocks is equal to the \emph{cushion} $(P - F)$ times the \emph{multiplier} $C$: $C * (P - F)$.
      The remaining amount of the principal is invested in zero-coupon bonds and is equal to: $P - C * (P - F)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
retsp <- na.omit(rutils::etfenv$returns$VTI["2008/2009"])
dates <- zoo::index(retsp)
nrows <- NROW(retsp)
retsp <- drop(zoo::coredata(retsp))
# Bond floor
bfloor <- 60
# CPPI multiplier
coeff <- 2
# Portfolio market values
portfv <- numeric(nrows)
# Initial principal
portfv[1] <- 100
# Stock allocation
stockv <- numeric(nrows)
stockv[1] <- min(coeff*(portfv[1] - bfloor), portfv[1])
# Bond allocation
bondv <- numeric(nrows)
bondv[1] <- (portfv[1] - stockv[1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{CPPI Strategy Dynamics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the stock price changes and the portfolio value becomes $P_t$, then the dollar amount invested in stocks must be adjusted to: $C * (P_t - F)$.  
      The amount invested in stocks changes both because the stock price changes and because of rebalancing with the zero-coupon bonds. 
      \vskip1ex
      The amount invested in zero-coupon bonds is then equal to: $P_t - C * (P_t - F)$.
      If the portfolio value drops to the \emph{bond floor} $P_t = F$, then all the stocks must be sold, with only the zero-coupon bonds remaining.
      But if the stock price rises, more stocks must be purchased, and vice versa.
      \vskip1ex
      Therefore the \emph{CPPI} strategy is a \emph{trend following} strategy, buying stocks when their prices are rising, and selling when their prices are dropping.
      \vskip1ex
      The \emph{CPPI} strategy can be considered a dynamic replication of a portfolio with a zero-coupon bond and a stock call option.
      \vskip1ex
      The \emph{CPPI} strategy is exposed to \emph{gap risk}, if stock prices drop suddenly by a large amount.  
      The \emph{gap risk} is exacerbated by high leverage, when the \emph{multiplier} $C$ is large, say greater than $5$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_cppi.png}
      <<echo=TRUE,eval=FALSE>>=
# Simulate CPPI strategy
for (t in 2:nrows) {
  portfv[t] <- portfv[t-1] + stockv[t-1]*retsp[t]
  stockv[t] <- min(coeff*(portfv[t] - bfloor), portfv[t])
  bondv[t] <- (portfv[t] - stockv[t])
}  # end for
# dygraph plot of CPPI strategy
pricevti <- 100*cumprod(1+retsp)
datav <- xts::xts(cbind(stockv, bondv, portfv, pricevti), dates)
colnames(datav) <- c("stocks", "bonds", "CPPI", "VTI")
endd <- rutils::calc_endpoints(datav, interval="weeks")
dygraphs::dygraph(datav[endd], main="CPPI strategy") %>%
  dyOptions(colors=c("red", "green", "blue", "orange"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Risk Parity} strategy the dollar portfolio allocations are rebalanced daily so that their dollar volatilities remain the same.
      \vskip1ex
      This means that the allocations $a_i$ are proportional to the \emph{standardized prices} ($\frac{p_i}{\sigma^d_i}$ - the dollar amounts of stocks with unit dollar volatilities):
      $a_i \propto \frac{p_i}{\sigma^d_i}$, 
      where $\sigma^d_i$ is the dollar volatility.
      \vskip1ex
      But the \emph{standardized prices} are equal to the inverse of the percentage volatilities $\sigma_i$:
      $\frac{p_i}{\sigma^d_i} = \frac{1}{\sigma_i}$,
      so the allocations $a_i$ are proportional to the inverse of the percentage volatilities $a_i \propto \frac{1}{\sigma_i}$.
      \vskip1ex
      In general, the dollar allocations $a_i$ may be set proportional to some target weights $\omega_i$:
      \begin{displaymath}
        a_i \propto \frac{\omega_i}{\sigma_i}
      \end{displaymath}
      The risk parity strategy is also called the equal risk contributions (ERC) strategy.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate dollar and percentage returns for VTI and IEF.
pricev <- rutils::etfenv$pricev[, c("VTI", "IEF")]
pricev <- na.omit(pricev)
retsd <- rutils::diffit(pricev)
retsp <- retsd/rutils::lagit(pricev, lagg=1, pad_zeros=FALSE)
# Calculate wealth of proportional allocations.
weightv <- c(0.5, 0.5)
retsw <- retsp %*% weightv
wealth_pda <- cumprod(1 + retsw)
# Calculate rolling percentage volatility.
look_back <- 21
volat <- HighFreq::roll_var(retsp, look_back=look_back)
iszero <- (rowSums(volat) == 0)
volat[iszero, ] <- 1
# Calculate the risk parity portfolio allocations.
alloc <- lapply(1:NCOL(pricev), 
  function(x) weightv[x]/volat[, x])
alloc <- do.call(cbind, alloc)
# Scale allocations to 1 dollar total.
alloc <- alloc/rowSums(alloc)
# Lag the allocations
alloc <- rutils::lagit(alloc)
# Calculate wealth of risk parity.
retsw <- rowSums(retsp*alloc)
wealth_risk_parity <- cumprod(1 + retsw)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy for \emph{VTI} and \emph{IEF} has a higher \emph{Sharpe ratio} than the fixed ratio strategy because it's more overweight bonds, which is also why it has lower absolute returns. 
      \vskip1ex
      Risk parity works better for assets with low correlations and very different volatilities, like stocks and bonds.
      \vskip1ex
      The shiny app \texttt{app\_risk\_parity\_strat.R} allows users to study the performance of the risk parity strategy as a function of its weight parameters.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_risk_parity.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log wealths.
wealthv <- log(cbind(wealth_pda, wealth_risk_parity))
wealthv <- xts::xts(wealthv, zoo::index(pricev))
colnames(wealthv) <- c("Fixed Ratio", "Risk Parity")
# Calculate the Sharpe ratios.
sqrt(252)*sapply(rutils::diffit(wealthv), function (x) mean(x)/sd(x))
# Plot a dygraph of the log wealths.
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(wealthv[endd], 
  main="Log Wealth of Risk Parity vs Proportional Allocations") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy reduces allocations to assets with rising volatilities, which is often accompanied by negative returns.
      \vskip1ex
      This allows the risk parity strategy to better time the markets - selling when prices are about to drop and buying when prices are rising.
      \vskip1ex
      The t-value of the \emph{Treynor-Mazuy} test is slightly significant, indicating some market timing skill of the risk parity strategy for \emph{VTI} and \emph{IEF}. 
      <<echo=TRUE,eval=FALSE>>=
# Test risk parity market timing of VTI using Treynor-Mazuy test
retsrp <- rutils::diffit(wealthv)
vti <- retsp$VTI
desv <- cbind(retsrp, vti, vti^2)
desv <- na.omit(desv)
colnames(desv)[1:2] <- c("fixed", "risk_parity")
colnames(desv)[4] <- "treynor"
model <- lm(risk_parity ~ VTI + treynor, data=desv)
summary(model)
# Plot residual scatterplot
residuals <- (desv$risk_parity - model$coeff[2]*vti)
residuals <- model$residuals
x11(width=6, height=5)
plot.default(x=vti, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Risk Parity vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vti^2)
points.default(x=vti, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("Risk Parity t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_risk_parity_timing_skill.png}
      <<echo=TRUE,eval=FALSE>>=
# Test for fixed ratio market timing of VTI using Treynor-Mazuy test
model <- lm(fixed ~ VTI + treynor, data=desv)
summary(model)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vti^2)
points.default(x=vti, y=fittedv, pch=16, col="blue")
text(x=0.05, y=0.6*max(residuals), paste("Fixed Ratio t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calendar Strategies}


%%%%%%%%%%%%%%%
\subsection{Sell in May Calendar Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \href{https://en.wikipedia.org/wiki/Sell_in_May}{\emph{Sell in May}} is a \emph{market timing} \emph{calendar strategy}, in which stocks are sold at the beginning of May, and then bought back at the beginning of November.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions
retsp <- na.omit(rutils::etfenv$returns$VTI)
posit <- rep(NA_integer_, NROW(retsp))
dates <- zoo::index(retsp)
dates <- format(dates, "%m-%d")
posit[dates == "05-01"] <- 0
posit[dates == "05-03"] <- 0
posit[dates == "11-01"] <- 1
posit[dates == "11-03"] <- 1
# Carry forward and backward non-NA posit
posit <- zoo::na.locf(posit, na.rm=FALSE)
posit <- zoo::na.locf(posit, fromLast=TRUE)
# Calculate strategy returns
sell_inmay <- posit*retsp
wealthv <- cbind(retsp, sell_inmay)
colnames(wealthv) <- c("VTI", "sell_in_may")
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_sell_inmay.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth of Sell in May strategy
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], main="Sell in May Strategy") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# OR: Open x11 for plotting
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue", "red")
quantmod::chart_Series(wealthv, theme=plot_theme, name="Sell in May Strategy")
legend("topleft", legend=colnames(wealthv),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sell in May Strategy Market Timing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sell in May} strategy doesn't demonstrate any ability of \emph{timing} the \emph{VTI} ETF.
      <<echo=TRUE,eval=FALSE>>=
# Test if Sell in May strategy can time VTI
desv <- cbind(wealth$sell_in_may, 0.5*(retsp+abs(retsp)), retsp^2)
colnames(desv) <- c("VTI", "merton", "treynor")
# Perform Merton-Henriksson test
model <- lm(sell_inmay ~ VTI + merton, data=desv)
summary(model)
# Perform Treynor-Mazuy test
model <- lm(sell_inmay ~ VTI + treynor, data=desv)
summary(model)
# Plot Treynor-Mazuy residual scatterplot
residuals <- (sell_inmay - model$coeff[2]*retsp)
plot.default(x=retsp, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Sell in May vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*retsp^2)
points.default(x=retsp, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.8*max(residuals), paste("Treynor test t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/timing_skill_sell_inmay.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Seasonal Overnight Market Anomaly}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/market-sentiment-and-an-overnight-anomaly/}{\emph{Overnight Market Anomaly}}
      is the consistent outperformance of overnight returns relative to the daytime returns.
      \vskip1ex
      The Overnight Strategy consists of holding a long position only overnight (buying at the close and selling at the open the next day).
      \vskip1ex
      The Daytime Strategy consists of holding a long position only during the daytime (buying at the open and selling at the close the same day).
      \vskip1ex
      The \emph{Overnight Market Anomaly} has been observed for many decades for most stock market indices, but not always for all stock sectors.
      \vskip1ex
      The \emph{Overnight Market Anomaly} has mostly disappeared after the \texttt{2008-2009} financial crisis.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log of OHLC VTI prices
ohlc <- log(rutils::etfenv$VTI)
openp <- quantmod::Op(ohlc)
highp <- quantmod::Hi(ohlc)
lowp <- quantmod::Lo(ohlc)
closep <- quantmod::Cl(ohlc)
# Calculate the close-to-close log returns, the intraday 
# open-to-close returns and the overnight close-to-open returns.
close_close <- rutils::diffit(closep)
colnames(close_close) <- "close_close"
open_close <- (closep - openp)
colnames(open_close) <- "open_close"
close_open <- (openp - rutils::lagit(closep, lagg=1, pad_zeros=FALSE))
colnames(close_open) <- "close_open"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_overnight.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate Sharpe and Sortino ratios
wealthv <- cbind(close_close, close_open, open_close)
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot log wealth
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Wealth of Close-to-Close, Close-to-Open, and Open-to-Close Strategies") %>%
  dySeries(name="close_close", label="Close-to-Close (static)", strokeWidth=2, col="blue") %>%
  dySeries(name="close_open", label="Close-to-Open (overnight)", strokeWidth=2, col="red") %>%
  dySeries(name="open_close", label="Open-to-Close (daytime)", strokeWidth=2, col="green") %>%
  dyLegend(width=600)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Turn of the Month Effect}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/strategies/turn-of-the-month-in-equity-indexes/}{\emph{Turn of the Month} (TOM) effect}
      is the outperformance of stocks on the last trading day of the month and on the first three days of the following month.
      \vskip1ex
      The \emph{TOM} effect was observed for the period from 1928 to 1975, but it has been less pronounced since the year \texttt{2000}.
      \vskip1ex
      The \emph{TOM} effect has been attributed to the investment of funds deposited at the end of the month.
      \vskip1ex
      This would explain why the \emph{TOM} effect has been more pronounced for less liquid small-cap stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(retsp)
# Calculate first business day of every month
dayv <- as.numeric(format(dates, "%d"))
indeks <- which(rutils::diffit(dayv) < 0)
dates[head(indeks)]
# Calculate Turn of the Month dates
indeks <- lapply((-1):2, function(x) indeks + x)
indeks <- do.call(c, indeks)
sum(indeks > NROW(dates))
indeks <- sort(indeks)
dates[head(indeks, 11)]
# Calculate Turn of the Month pnls
pnls <- numeric(NROW(retsp))
pnls[indeks] <- retsp[indeks, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/strat_tom.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealthv <- cbind(retsp, pnls)
colnamev <- c("VTI", "Strategy")
colnames(wealthv) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI Turn of the Month strategy
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd], 
  main="Turn of the Month Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Stop-loss Rules}


%%%%%%%%%%%%%%%
\subsection{Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules are used to reduce losses in case of a significant drawdown in returns.
      \vskip1ex
      For example, a simple stop-loss rule is to sell the stock if its price drops by $5\%$ below the recent maximum price, and buy it back when the price recovers.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(retsp)
retsp <- drop(coredata(retsp))
nrows <- NROW(retsp)
# Simulate stop-loss strategy
stopl <- 0.05
maxp <- 0.0
retc <- 0.0
pnls <- retsp
for (i in 1:(nrows-1)) {
# Calculate drawdown
  retc <- retc + retsp[i]
  maxp <- max(maxp, retc)
  dd <- (retc - maxp)
# Check for stop-loss
  if (dd < -stopl*maxp)
    pnls[i+1] <- 0
}  # end for
# Same but without using explicit loops
cumsumv <- cumsum(retsp)
cummaxv <- cummax(cumsumv)
dd <- (cumsumv - cummaxv)
pnls2 <- retsp
isdd <- rutils::lagit(dd < -stopl*cummaxv)
pnls2 <- ifelse(isdd, 0, pnls2)
all.equal(pnls, pnls2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/stop_loss.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealthv <- xts::xts(cbind(retsp, pnls), dates)
colnamev <- c("VTI", "Strategy")
colnames(wealthv) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealthv, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI stop-loss strategy
endd <- rutils::calc_endpoints(wealthv, interval="months")
dygraphs::dygraph(cumsum(wealthv)[endd],
  main="VTI Stop-loss Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules can reduce the largest drawdowns but they also tend to reduce cumulative returns.
      <<echo=TRUE,eval=FALSE>>=
# Simulate multiple stop-loss strategies
cumsumv <- cumsum(retsp)
cummaxv <- cummax(cumsumv)
dd <- (cumsumv - cummaxv)
cum_pnls <- sapply(0.01*(1:20), function(stopl) {
  pnls <- retsp
  isdd <- rutils::lagit(dd < -stopl*cummaxv)
  pnls <- ifelse(isdd, 0, pnls)
  sum(pnls)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/stop_loss_cum.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative pnls for stop-loss strategies
plot(x=0.01*(1:20), y=cum_pnls, 
     main="Cumulative PnLs for Stop-loss Strategies",
     xlab="stop-loss level", ylab="cumulative pnl", 
     t="l", lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Study all the lecture slides in \texttt{FRE7241\_Lecture\_2.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_2.R},
    \item Study \emph{bootstrap simulation} from the files \emph{bootstrap\_technique.pdf} and \emph{doBootstrap\_primer.pdf},
    \item Study the following sections in the file \texttt{numerical\_analysis.pdf}: 
    \begin{itemize}[]
    {\scriptsize
      \item Numerical Calculations, 
      \item Optimizing \texttt{R} Code for Speed and Memory Usage, 
      \item Writing Fast \texttt{R} Code Using Vectorized Operations, 
      \item Simulation, 
      \item Parallel Computing in \texttt{R}, 
      \item Run the code corresponding to the above sections from \texttt{numerical\_analysis.R}
    }
    \end{itemize}
  \end{itemize}
\end{block}

\begin{block}{Recommended}
  Read the following sections in the file \texttt{R\_environment.pdf}: 
  \begin{itemize}[]
    \item \emph{Environments in \texttt{R}}, 
    \item \emph{Data Input and Output}, 
    \item Run the code corresponding to the above sections from \texttt{R\_environment.R}
  \end{itemize}
\end{block}

\end{frame}


\end{document}
