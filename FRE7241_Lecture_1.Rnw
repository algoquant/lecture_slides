% FRE7241_Lecture_1
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size='tiny', fig.width=4, fig.height=4)
options(width=80, dev='pdf')
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm and bbold packages for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{bbold}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#1]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#1, Fall 2021}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{September 7, 2021}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Introduction}


%%%%%%%%%%%%%%%
\subsection{Welcome Students!}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      My name is Jerzy Pawlowski \href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}
      \vskip1ex
      I'm an adjunct professor at NYU Tandon because I love teaching and I want to share my professional knowledge with young, enthusiastic students.
      \vskip1ex
      I'm interested in applications of \emph{machine learning} to \emph{systematic investing}.
      \vskip1ex
      I'm an advocate of \emph{open-source software}, and I share it on GitHub:\\
      \href{https://github.com/algoquant/}{\color{blue}{My GitHub account}} 
      \vskip1ex
      In my finance career, I have worked as a hedge fund \emph{portfolio manager}, \emph{CLO structurer} (banker), and \emph{quant analyst}.\\
      \href{https://www.linkedin.com/in/jerzypawlowski/}{\color{blue}{My LinkedIn profile}} 
    \column{0.5\textwidth}
    \includegraphics[width=0.5\paperwidth]{image/Jerzy_Pawlowski_linked.png}
    \includegraphics[width=0.5\paperwidth]{image/github_algoquant.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{FRE7241 Course Description and Objectives}
\begin{frame}[t]{\subsecname}
\vspace{-2em}

\begin{block}{Course Description}
The course will apply the \texttt{R} programming language to \emph{trend following}, \emph{momentum trading}, \emph{statistical arbitrage} (pairs trading), and other active portfolio management strategies.  The course will implement volatility and price \emph{forecasting models}, asset pricing and \emph{factor models}, and \emph{portfolio optimization}.  The course will apply \emph{machine learning} techniques, such as \emph{parameter regularization} (shrinkage), \emph{bagging} and \emph{backtesting} (cross-validation).
\end{block}
\pause

\begin{block}{Course Objectives}
Students will learn through \texttt{R} coding exercises how to:\\
\hskip1em - download data from external sources, and to scrub and format it.\\
\hskip1em - estimate time series parameters, and fit models such as \emph{ARIMA}, \emph{GARCH}, and factor models.\\
\hskip1em - optimize portfolios under different constraints and risk-return objectives.\\
\hskip1em - backtest active portfolio management strategies and evaluate their performance.\\
\end{block}
\pause

\begin{block}{Course Prerequisites}
  \emph{FRE6123 Financial Risk Management and Asset Pricing}.  The \texttt{R} language is considered to be challenging, so this course requires programming experience with other languages such as \texttt{C++} or \texttt{Python}.  Students with less programming experience are encouraged to first take \emph{FRE6871 R in Finance}, and also \emph{FRE6883 Financial Computing} by prof. Song Tang.  Students should also have knowledge of basic statistics (random variables, estimators, hypothesis testing, regression, etc.)
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Homeworks and Tests}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Homeworks and Tests}
Grading will be based on homeworks and in-class tests.  There will be no final exam.
      \vskip1ex
The tests will require writing code, which should run directly when pasted into an \texttt{R} session, and should produce the required output, without any modifications.
      \vskip1ex
Students will be allowed to consult lecture slides, and to copy code from them, and to copy from books or any online sources, but they will be required to provide references to those external sources (such as links or titles and page numbers).
      \vskip1ex
The tests will be closely based on code contained in the lecture slides, so students are encouraged to become very familiar with those slides.
      \vskip1ex
Students will submit their homework and test files only through \emph{Brightspace} (not emails).
      \vskip1ex
Students will be required to bring their laptop computers to class and run the \texttt{R} Interpreter, and the \texttt{RStudio} Integrated Development Environment (\emph{IDE}), during the lecture.
      \vskip1ex
Homeworks will also include reading assignments designed to help prepare for tests.
\end{block}
\pause

\begin{block}{Graduate Assistant}
The graduate assistant (GA) will be Shardha Koul \href{mailto:sk9225@nyu.edu}{sk9225@nyu.edu}.\\
The GA will answer questions during office hours, or via \emph{Brightspace} forums, not via emails.  Please send emails regarding lecture matters from \emph{Brightspace} (not personal emails).
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Tips for Solving Homeworks and Tests}
\begin{frame}[t]{\subsecname}
\vspace{-1em}
  \begin{block}{Tips for Solving Homeworks and Tests}
    The tests will require mostly copying code samples from the lecture slides, making some modifications to them, and combining them with other code samples.
    \vskip1ex
    Partial credit will be given even for code that doesn't produce the correct output, but that has elements of code that can be useful for producing the right answer.
    \vskip1ex
    So don't leave test assignments unanswered, and instead copy any code samples from the lecture slides that are related to the solution and make sense.
    \vskip1ex
    Contact the GA during office hours via text or phone, and submit questions to the GA or to me via \emph{Brightspace}.
  \end{block}
\pause
  \begin{block}{Please Submit \emph{Minimal Working Examples} With Your Questions}
    When submitting questions, please provide a \emph{minimal working example} that produces the error in \texttt{R}, with the following items:
      \begin{itemize}
        \item The \emph{complete} \texttt{R} code that produces the error, including the seed value for random numbers,
        \item The version of \texttt{R} (output of command: \texttt{sessionInfo()}), and the versions of \texttt{R} packages, 
        \item The type and version of your operating system (Windows or OSX),
        \item The dataset file used by the \texttt{R} code,
        \item The text or screenshots of error messages,
      \end{itemize}
    \vskip1ex
    You can read more about producing \emph{minimal working examples} here:
      \url{http://stackoverflow.com/help/mcve}\\
      \url{http://www.jaredknowles.com/journal/2013/5/27/writing-a-minimal-working-example-mwe-in-r}
  \end{block}
\end{frame}


%%%%%%%%%%%%%%%
\subsection{Course Grading Policies}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Numerical Scores}
  Tests will be graded and assigned numerical scores.  
  Each part of the tests will be graded separately and assigned a numerical score.\\
  Maximum scores will be given only for complete code, that produces the correct output when it's pasted into an \texttt{R} session, without any modifications. 
  As long as the \texttt{R} code uses the required functions and produces the correct output, it will be given full credit.\\
  Partial credit will be given even for code that doesn't produce the correct output, but that has elements of code that can be useful for producing the right answer.
\end{block}
\pause

\begin{block}{Letter Grades}
  Letter grades for the course will be derived from the cumulative scores obtained for all the tests.  Very high numerical scores close to the maximum won't guarantee an A letter grade, since grading will also depend on the difficulty of the assignments.
\end{block}
\pause

\begin{block}{Plagiarism}
  Plagiarism (copying from other students) and cheating will be punished.\\
  But copying code from lecture slides, books, or any online sources is allowed and encouraged.\\
  Students must provide references to any external sources from which they copy code (such as links or titles and page numbers).
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{FRE7241 Course Materials}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Lecture Slides}
The course will be mostly self-contained, using detailed lecture slides containing extensive, working \texttt{R} code examples.\\
The course will also utilize data and tutorials which are freely available on the internet.
\end{block}
\pause

\begin{block}{FRE7241 Recommended Textbooks}
\begin{itemize}[]
  \item \href{https://www.amazon.com/dp/331935731X/}{\emph{Financial Data and Models Using \texttt{R}}} by Clifford Ang, provides a good introduction to time series, portfolio optimization, and performance measures.
  \item \href{https://www.systematicmoney.org/systematic-trading}{\emph{Systematic Trading}} by 
  \href{https://github.com/robcarver17}{Rob Carver}, explains practical systematic trading rules.
  % \fullcite{carverbook}\\
  
  \item \href{https://chrisconlan.com/automated-trading-r-apressspringer/}{\emph{Automated Trading}} by 
  \href{https://chrisconlan.com}{Chris Conlan}, explains how to implement a practical computer trading system.
  
  \item \href{https://people.orie.cornell.edu/davidr/SDAFE2/index.html}{\emph{Statistics and Data Analysis for Financial Engineering}} by David Ruppert, introduces regression, cointegration, multivariate time series analysis, \emph{ARIMA}, \emph{GARCH}, \emph{CAPM}, and factor models, with examples in \texttt{R}.
  % \fullcite{ruppertbook}\\
  
  \item \href{https://www.pfaffikus.de/books/wiley/}{\emph{Financial Risk Modelling and Portfolio Optimization with \texttt{R}}} by Bernhard Pfaff, introduces volatility models, portfolio optimization, and tactical asset allocation, with a great review of \texttt{R} packages and examples in \texttt{R}.
  % \fullcite{pfaffbook}\\
\end{itemize}

Many textbooks can be downloaded in electronic format from the 
\href{http://library.nyu.edu/}{\emph{NYU Library}}.

\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{FRE7241 Supplementary Books}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{}
\begin{itemize}[]
  \item \href{http://www-bcf.usc.edu/~gareth/ISL/index.html}{\emph{Introduction to Statistical Learning}} by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, introduces machine learning techniques using \texttt{R}, but without deep learning.
  % \fullcite{islbook}
  \item \href{https://press.princeton.edu/books/hardcover/9780691166278/quantitative-risk-management}{\emph{Quantitative Risk Management}} by Alexander J. McNeil, Rudiger Frey, and Paul Embrechts: review of Value at Risk, factor models, ARMA and GARCH, extreme value theory, and credit risk models.
  \item \href{http://eeecon.uibk.ac.at/~zeileis/teaching/AER/index.html}{\emph{Applied Econometrics with \texttt{R}}} by Christian Kleiber and Achim Zeileis, introduces advanced statistical models and econometrics.
  % \fullcite{kleiberbook}
  \item \href{http://it-ebooks.info/book/1734/}{\emph{The Art of \texttt{R} Programming}} by Norman Matloff, contains a good introduction to \texttt{R} and to statistical models.
  % \fullcite{matloffbook}
  \item \href{http://adv-r.had.co.nz/}{\emph{Advanced \texttt{R}}} by Hadley Wickham, is the best book for learning the advanced features of \texttt{R}.
  % \fullcite{hadleybook}
  \item \href{http://www.nr.com/}{\emph{Numerical Recipes in \texttt{C++}}} by William Press, Saul Teukolsky, William Vetterling, and Brian Flannery, is a great reference for linear algebra and numerical methods, implemented in working \texttt{C++} code.
  % \fullcite{numrecipesbook}
  \item The books \href{http://www.statmethods.net/}{\emph{\texttt{R} in Action}} by Robert Kabacoff and \href{http://www.jaredlander.com/r-for-everyone/}{\emph{\texttt{R} for Everyone}} by Jared Lander, are good introductions to \texttt{R} and to statistical models.
  \item \href{https://smile.amazon.com/hz/wishlist/genericItemsPage/1C3R818RWTWNW}{\emph{Quant Finance books}} by Jerzy Pawlowski.
  \item \href{https://smile.amazon.com/hz/wishlist/genericItemsPage/1R44X8846DX3N}{\emph{Quant Trading books}} by Jerzy Pawlowski.
  % \fullcite{kabacoffbook}
  % \fullcite{landerbook}
\end{itemize}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{FRE7241 Supplementary Materials}
\begin{frame}[t]{\subsecname}

\begin{block}{Robert Carver's trading blog}
Great blog about practical systematic trading and investments, with Python code:
\hskip1em\url{http://qoppac.blogspot.com/}
\end{block}

\begin{block}{Introduction to Computational Finance with \texttt{R}}
Good course by prof. Eric Zivot, with lots of \texttt{R} examples:
\hskip1em\url{https://www.datacamp.com/community/open-courses/computational-finance-and-financial-econometrics-with-r}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      \texttt{Notepad++} is a free source code editor for \texttt{MS Windows}, that supports several programming languages, including \texttt{R}.
      \vskip1ex
      \texttt{Notepad++} has a very convenient and fast \emph{search and replace} function, that allows \emph{search and replace} in multiple files.\\
      \hskip1em\url{http://notepad-plus-plus.org/}
    \column{0.3\textwidth}
      \includegraphics[height=0.5\textwidth]{image/npp.jpg}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{\texttt{R} Help and Documentation}


%%%%%%%%%%%%%%%
\subsection{Internal \texttt{R} Help and Documentation}
\begin{frame}[fragile,t]{\subsecname}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{help()} displays documentation on a function or subject.\\
      \vskip1ex
      Preceding the keyword with a single \texttt{"?"} is equivalent to calling \texttt{help()}.
    \column{0.5\textwidth}
      \vspace{-1em}
% tidy=FALSE prevents translation of "?getwd" into "`?`(getwd)"
      <<echo=TRUE,eval=FALSE>>=
# Display documentation on function "getwd"
help(getwd)
# Equivalent to "help(getwd)"
?getwd
      @
  \end{columns}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{help.start()} displays a page with links to internal documentation.
      \vskip1ex
      \texttt{R} documentation is also available in \texttt{RGui} under the help tab.
      \vskip1ex
      The \emph{pdf} files with \texttt{R} documentation are also available directly under:\\
%      \texttt{\color{blue}{C:/Program Files/R/R-3.1.2/bin/x64}}\\
      \href{C:/Program Files/R/R-3.1.2/doc/manual/}{C:/Program Files/R/R-3.1.2/doc/manual/}\\
      (the exact path will depend on the \texttt{R} version.)
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Open the hypertext documentation
help.start()
      @
      \vskip1ex
      \includegraphics[height=0.2\textwidth]{image/Rlogo.png}
  \end{columns}
\end{block}

\begin{block}{}
  \href{http://cran.r-project.org/doc/manuals/r-release/R-intro.pdf}{Introduction to \texttt{R}}
  by Venables and \texttt{R} Core Team.
  % \fullcite{website:rintro}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Online Help and Documentation}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{\texttt{R} Cheat Sheets}
  \emph{Cheat Sheets} are a fast way to find what you want\\
  \hskip1em\url{https://www.rstudio.com/resources/cheatsheets/
}
\end{block}

\begin{block}{\texttt{R} Programming \texttt{Wikibook}}
  \texttt{Wikibooks} are crowdsourced textbooks\\
  \hskip1em\url{http://en.wikibooks.org/wiki/R_Programming/}\\
\end{block}

\begin{block}{\texttt{R FAQ}}
  Frequently Asked Questions about \texttt{R}\\
  \hskip1em\url{http://cran.r-project.org/doc/FAQ/R-FAQ.html}\\
\end{block}

\begin{block}{\texttt{R}-seek Online Search Tool}
  \texttt{R}-seek allows online searches specific to the \texttt{R} language\\
  \hskip1em\url{http://www.rseek.org/}
\end{block}

\begin{block}{\texttt{R}-help Mailing List}
  \texttt{R}-help is a very comprehensive Q\&A mailing list\\
  \hskip1em\url{https://stat.ethz.ch/mailman/listinfo/r-help}\\
  \texttt{R}-help has archives of past Q\&A - search it before you ask\\
  \hskip1em\url{https://stat.ethz.ch/pipermail/r-help/}\\
  GMANE allows searching the \texttt{R}-help archives using a usenet newsgroup style GUI\\
  \hskip1em\url{http://news.gmane.org/gmane.comp.lang.r.general}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Style Guides}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{DataCamp \texttt{R} style guide}
  The DataCamp \texttt{R} style guide is very close to what I have adopted:\\
  \hskip1em\href{https://www.datacamp.com/teach/documentation\#tab_style_guide_r}{DataCamp R style guide}
\end{block}

\begin{block}{Google \texttt{R} style guide}
  The Google \texttt{R} style guide is similar to DataCamp's:\\
  \hskip1em\href{https://google.github.io/styleguide/Rguide.xml}{Google R style guide}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stack Exchange}
\begin{frame}[t]{\subsecname}

\begin{columns}[T]
  \column{0.5\textwidth}
    \begin{block}{Stack Overflow}
      Stack Overflow is a Q\&A forum for computer programming, and is part of Stack Exchange\\
%  Stack Overflow is a Q\&A forum for programmers (covers many different languages)\\
      \hskip1em\url{http://stackoverflow.com}\\
      \hskip1em\url{http://stackoverflow.com/questions/tagged/r}\\
      \hskip1em\url{http://stackoverflow.com/tags/r/info}\\
    \end{block}

    \begin{block}{Stack Exchange}
      Stack Exchange is a family of Q\&A forums in a variety of fields\\
      \hskip1em\url{http://stackexchange.com/}\\
      \hskip1em\url{http://stackexchange.com/sites\#technology}\\
      \hskip1em\url{http://quant.stackexchange.com/}\\
    \end{block}
  \column{0.5\textwidth}
    \includegraphics[height=0.9\textwidth]{image/stack_exchange2.png}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{RStudio} Support}
\begin{frame}[t]{\subsecname}

\begin{block}{}
\emph{RStudio} has extensive online help, Q\&A database, and documentation\\
\hskip1em\url{https://support.rstudio.com/hc/en-us}\\
\vskip1ex
\hskip1em\url{https://support.rstudio.com/hc/en-us/sections/200107586-Using-RStudio}\\
\vskip1ex
\hskip1em\url{https://support.rstudio.com/hc/en-us/sections/200148796-Advanced-Topics}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Online Books and References}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}

\begin{block}{Hadley Wickham book \emph{Advanced \texttt{R}}}
The best book for learning the advanced features of \texttt{R}:
\hskip1em\url{http://adv-r.had.co.nz/}
\end{block}

\begin{block}{Cookbook for \texttt{R} by Winston Chang from \emph{RStudio}}
Good plotting, but not interactive:
\hskip1em\url{http://www.cookbook-r.com/}
\end{block}

\begin{block}{Efficient \texttt{R} programming by Colin Gillespie and Robin Lovelace}
Good tips for fast \texttt{R} programming:
\hskip1em\url{https://csgillespie.github.io/efficientR/programming.html}
\end{block}

\begin{block}{Endmemo web book}
Good, but not interactive:
\hskip1em\url{http://www.endmemo.com/program/R/}
\end{block}

\begin{block}{Quick-R by Robert Kabacoff}
Good, but not interactive:
\hskip1em\url{http://www.statmethods.net/}
\end{block}

\begin{block}{\texttt{R} for Beginners by Emmanuel Paradis}
Good, basic introduction to \texttt{R}:
\hskip1em\url{http://cran.r-project.org/doc/contrib/Paradis-rdebuts_en.pdf}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Online Interactive Courses}
\begin{frame}[fragile,t]{\subsecname}

\begin{block}{Datacamp Interactive Courses}
  Datacamp introduction to \texttt{R}:
  \hskip1em\url{https://www.datacamp.com/courses/introduction-to-r/}
  \vskip1ex
  Datacamp list of free courses:
  \hskip1em\url{https://www.datacamp.com/community/open-courses}
  \vskip1ex
  Datacamp basic statistics in \texttt{R}:
  \hskip1em\url{https://www.datacamp.com/community/open-courses/basic-statistics}
  \vskip1ex
  Datacamp computational finance in \texttt{R}:
  \hskip1em\url{https://www.datacamp.com/community/open-courses/computational-finance-and-financial-econometrics-with-r}
  \vskip1ex
  Datacamp machine learning in \texttt{R}:
  \hskip1em\url{https://www.datacamp.com/community/open-courses/kaggle-r-tutorial-on-machine-learning}
\end{block}

\begin{block}{Try \texttt{R}}
  Interactive \texttt{R} tutorial, but rather basic:
  \hskip1em\url{http://tryr.codeschool.com/}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Blogs and Experts}
\begin{frame}[fragile,t]{\subsecname}

\begin{block}{R-Bloggers}
R-Bloggers is an aggregator of blogs dedicated to \texttt{R}\\
\hskip1em\url{http://www.r-bloggers.com/}\\
Tal Galili is the author of R-Bloggers and has his own excellent blog\\
\hskip1em\url{http://www.r-statistics.com/}\\
\end{block}

\begin{block}{Dirk Eddelbuettel}
Dirk is a \emph{Top Answerer} for \texttt{R} questions on Stackoverflow, the author of the \texttt{Rcpp} package, and the CRAN Finance View\\
\hskip1em\url{http://dirk.eddelbuettel.com/}\\
\hskip1em\url{http://dirk.eddelbuettel.com/code/}\\
\hskip1em\url{http://dirk.eddelbuettel.com/blog/}\\
\hskip1em\url{http://www.rinfinance.com/}\\
\end{block}

\begin{block}{Romain Frangois}
Romain is an \texttt{R} Enthusiast and \texttt{Rcpp} Hero\\
\hskip1em\url{http://romainfrancois.blog.free.fr/}\\
\hskip1em\url{http://romainfrancois.blog.free.fr/index.php?tag/graphgallery}\\
\hskip1em\url{http://blog.r-enthusiasts.com/}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{More \subsecname}

\begin{block}{Revolution Analytics Blog}
\texttt{R} blog by Revolution Analytics software vendor\\
\hskip1em\url{http://blog.revolutionanalytics.com/}\\
\end{block}

\begin{block}{\emph{RStudio} Blog}
\texttt{R} blog by \emph{RStudio}\\
\hskip1em\url{http://blog.rstudio.org/}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GitHub} for Hosting Software Projects Online}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GitHub} is an internet-based online service for hosting repositories of software projects.
      \vskip1ex
      \emph{GitHub} provides version control using \emph{git} (designed by Linus Torvalds).
      \vskip1ex
      Most \texttt{R} projects are now hosted on \emph{GitHub}.
      \vskip1ex
      \emph{Google} uses \emph{GitHub} to host its \emph{tensorflow} library for machine learning:\\
      \hskip1em\url{https://github.com/tensorflow/tensorflow}
      \vskip1ex
      All the \emph{FRE-7241} and \emph{FRE-6871} lectures are hosted on \emph{GitHub}:\\
      \hskip1em\url{https://github.com/algoquant/lecture_slides}\\
      \hskip1em\url{https://github.com/algoquant}
      \vskip1ex
      Hosting projects on \emph{Google} is a great way to advertize your skills and network with experts.
    \column{0.5\textwidth}
    \includegraphics[width=0.5\paperwidth]{image/github_algoquant.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Getting Started With \texttt{R}}


%%%%%%%%%%%%%%%
\subsection{What is \texttt{R}?}
\begin{frame}[t]{\subsecname}
\vspace{-1em}
\begin{block}{}
\begin{columns}[T]
  \column{0.7\textwidth}
    \begin{itemize}
      \item An open-source software environment for statistical computing and graphics.
      \item An interpreted language, that allows interactive code development.
      \item A functional language where every operator is an \texttt{R} function.
      \item A very expressive language that can perform complex operations with very few lines of code.
      \item A language with metaprogramming facilities that allow programming on the language.
      \item A language written in \texttt{C/C++}, which can easily call other \texttt{C/C++} programs.
      \item Can be easily extended with \emph{packages} (function libraries), providing the latest developments like \emph{Machine Learning}.
      \item Supports object-oriented programming with \emph{classes} and \emph{methods}.
      \item Vectorized functions written in \texttt{C/C++}, allow very fast execution of loops over vector elements.
    \end{itemize}
  \column{0.3\textwidth}
    \href{http://www.r-project.org/}{\includegraphics[height=0.2\textwidth]{image/Rlogo.png} Project}
    \vskip1ex
    \href{http://en.wikipedia.org/wiki/R_(programming_language)}{\includegraphics[height=0.2\textwidth]{image/Rlogo.png} Wikipedia}
%    \hskip1em\url{http://blog.revolutionanalytics.com/2011/08/what-language-is-r-written-in.html}\\
\end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Why is \texttt{R} More Difficult Than Other Languages?}
\begin{frame}[t]{\subsecname}
\vspace{-1em}
\begin{block}{}
\begin{columns}[T]
  \column{0.7\textwidth}
    \texttt{R} is more difficult than other languages because:
    \begin{itemize}
      \item \texttt{R} is a \emph{functional} language, which makes its syntax unfamiliar to users of procedural languages like \texttt{C/C++}.
      \item The huge number of user-created \emph{packages} makes it difficult to tell which are the best for particular applications.
      \item \texttt{R} can produce very cryptic \emph{warning} and \emph{error} messages, because it's a programming environment, so it performs many operations quietly, but those can sometimes fail.
      \item Fixing errors usually requires analyzing the complex structure of the \texttt{R} programming environment.
    \end{itemize}
    This course is designed to teach the most useful elements of \texttt{R} for financial analysis, through case studies and examples,
  \column{0.3\textwidth}
    \includegraphics[height=0.2\textwidth]{image/Rlogo.png}
\end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{What are the Best Ways to Use \texttt{R}?}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If used properly, \texttt{R} can be fast and interactive:
      \begin{itemize}
        \item Use \texttt{R} as an interface to libraries written in \texttt{C++}, \texttt{Java}, and \texttt{JavaScript}.
        \item Avoid using too many \texttt{R} function calls (every command in \texttt{R} is a function).
        \item Avoid using \texttt{apply()} and \texttt{for()} loops for large datasets.
        \item Use \texttt{R} functions which are \emph{compiled} \texttt{C++} code, instead of using interpreted \texttt{R} code.
        \item Use package \emph{data.table} for high performance data management.
        \item Use package \emph{shiny} for interactive charts of live models running in \texttt{R}.
        \item Use package \emph{dygraphs} for interactive time series plots.
        \item Use package \emph{knitr} for \emph{RMarkdown} documents.
        \item Pre-allocate memory for new objects.
        \item Write \texttt{C++} functions in \emph{Rcpp} and \emph{RcppArmadillo}.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{image/Jeremy_Clarkson_Linus_Torvalds.jpg}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative sum of a vector
vec_tor <- runif(1e5)
# Use compiled function
cum_sum <- cumsum(vec_tor)
# Use for loop
cum_sum2 <- vec_tor
for (i in 2:NROW(vec_tor))
  cum_sum2[i] <- (vec_tor[i] + cum_sum2[i-1])
# Compare the two methods
all.equal(cum_sum, cum_sum2)
# Microbenchmark the two methods
library(microbenchmark)
summary(microbenchmark(
  cumsum=cumsum(vec_tor),
  loop_alloc={
    cum_sum2 <- vec_tor
    for (i in 2:NROW(vec_tor))
      cum_sum2[i] <- (vec_tor[i] + cum_sum2[i-1])
  },
  loop_nalloc={
    # Doesn't allocate memory to cum_sum3
    cum_sum3 <- vec_tor[1]
    for (i in 2:NROW(vec_tor))
      # This command adds an extra element to cum_sum3
      cum_sum3[i] <- (vec_tor[i] + cum_sum3[i-1])
  },
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{R} License}
\begin{frame}[t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      \texttt{R} is open-source software released under the GNU General Public License:
      \vskip1ex
      \hskip1em\url{http://www.r-project.org/Licenses}\\
    \column{0.3\textwidth}
      \includegraphics[height=0.2\textwidth]{image/GPLv3_Logo.png}
  \end{columns}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      Some other \texttt{R} packages are released under the Creative Commons Attribution-ShareAlike License:
      \vskip1ex
      \hskip1em\url{http://creativecommons.org}\\
    \column{0.3\textwidth}
      \includegraphics[height=0.1\textwidth]{image/CC_License.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Installing \texttt{R} and \protect\emph{RStudio}}
\begin{frame}[t]{\subsecname}

\begin{block}{}
  Students will be required to bring their laptop computers to all the lectures, and to run the \texttt{R} Interpreter and \textbf{RStudio} RStudio during the lecture.
  \vskip1ex
  Laptop computers will be necessary for following the lectures, and for performing tests.
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      Students will be required to install and to become proficient with the \texttt{R} Interpreter.\\
      Students can download the \texttt{R} Interpreter from \texttt{CRAN} (Comprehensive \texttt{R} Archive Network):\\
      \hskip1em\url{http://cran.r-project.org/}
      \vskip1ex
      To invoke the \texttt{RGui} interface, click on:\\
      \href{C:/Program Files/R/R-3.1.2/bin/x64/RGui.exe}{C:/Program Files/R/R-3.1.2/bin/x64/RGui.exe}\\
    \column{0.3\textwidth}
      \includegraphics[height=0.2\textwidth]{image/Rlogo.png}
  \end{columns}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.7\textwidth}
      Students will be required to install and to become proficient with the \emph{RStudio} Integrated Development Environment (\emph{IDE}),\\
      \hskip1em\url{http://www.rstudio.com/products/rstudio/}
    \column{0.3\textwidth}
      \includegraphics[height=0.2\textwidth]{image/RStudio.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Using \protect\emph{RStudio}}
\begin{frame}[t]{\subsecname}

% Snapshot of \emph{RStudio} GUI
\includegraphics[height=0.6\textwidth]{image/RStudio_screen.png}

\end{frame}



%%%%%%%%%%%%%%%
% \section{First Steps in \texttt{R}}
\section{The \texttt{R} Environment}


%%%%%%%%%%%%%%%
\subsection{A First \texttt{R} Session}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variables are created by an assignment operation, and they don't have to be declared.
      \vskip1ex
      The standard assignment operator in \texttt{R} is the arrow symbol \texttt{"<-"}.
      \vskip1ex
      \texttt{R} interprets text in quotes ("") as character strings.
      \vskip1ex
      Text that is not in quotes ("") is interpreted as a \emph{symbol} or \emph{expression}.
      \vskip1ex
      Typing a \emph{symbol} or \emph{expression} evaluates it.
      \vskip1ex
      \texttt{R} uses the hash "\texttt{\#}" sign to mark text as comments.
      \vskip1ex
      All text after the hash "\texttt{\#}" sign is treated as a comment, and is not executed as code.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# "<-" and "=" are valid assignment operators
my_var <- 3

# Typing a symbol or expression evaluates it
my_var

# Text in quotes is interpreted as a string
my_var <- "Hello World!"

# Typing a symbol or expression evaluates it
my_var

my_var  # Text after hash is treated as comment
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exploring an \texttt{R} Session}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{getwd()} returns a vector of length \texttt{1}, with the first element containing a string with the name of the current working directory (\texttt{cwd}).
      \vskip1ex
      The function \texttt{setwd()} accepts a character string as input (the name of the directory), and sets the working directory to that string.
      \vskip1ex
      \texttt{R} is a functional language, and \texttt{R} commands are functions, so they must be followed by parentheses \texttt{"()"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
getwd()  # Get cwd
setwd("C:/Develop/R")  # Set cwd
getwd()  # Get cwd
      @
  \end{columns}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Get system date and time
      \vskip4ex
      Just the date
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
Sys.time()  # Get date and time

Sys.Date()  # Get date only
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{R} Workspace}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The workspace is the current \texttt{R} working environment, which includes all user-defined objects and the command history.
      \vskip1ex
      The function \texttt{ls()} returns names of objects in the \texttt{R} workspace.
      \vskip1ex
      The function \texttt{rm()} removes objects from the \texttt{R} workspace.
      \vskip1ex
      The workspace can be saved into and loaded back from an \texttt{.RData} file (compressed binary file format).
      \vskip1ex
      The function \texttt{save.image()} saves the whole workspace.
      \vskip1ex
      The function \texttt{save()} saves just the selected objects.
      \vskip1ex
      The function \texttt{load()} reads data from \texttt{.RData} files, and \emph{invisibly} returns a vector of names of objects created in the workspace.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
rm(list=ls())
setwd("C:/Develop/lecture_slides/data")
var1 <- 3  # Define new object
ls()  # List all objects in workspace
# List objects starting with "v"
ls(pattern=glob2rx("v*"))
# Remove all objects starting with "v"
rm(list=ls(pattern=glob2rx("v*")))
save.image()  # Save workspace to file .RData in cwd
rm(var1)  # Remove object
ls()  # List objects
load(".RData")
ls()  # List objects
var2 <- 5  # Define another object
save(var1, var2,  # Save selected objects
     file="C:/Develop/lecture_slides/data/my_data.RData")
rm(list=ls())  # Remove all objects
ls()  # List objects
load_ed <- load(file="C:/Develop/lecture_slides/data/my_data.RData")
load_ed
ls()  # List objects
      @
  \end{columns}
\end{block}
% \pause

\end{frame}


%%%%%%%%%%%%%%%
\begin{frame}[fragile,t]{\subsecname \hskip0.5em (cont.)}
\vspace{-1em}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      When you quit \texttt{R} you'll be prompted "Save workspace image?"\\
      \vskip1ex
      If you answer $YES$ then the workspace will be saved into the \texttt{.RData} file in the \texttt{cwd}.\\
      \vskip1ex
      When you start \texttt{R} again, the workspace will be automatically loaded from the existing \texttt{.RData} file.\\
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
        q()  # quit R session
      @
  \end{columns}
\end{block}

\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{history()} displays recent commands.\\
      \vskip1ex
      You can also save and load the command history from a file.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
history(5)  # Display last 5 commands
savehistory(file="myfile")  # Default is ".Rhistory"
loadhistory(file="myfile")  # Default is ".Rhistory"
      @
  \end{columns}
\end{block}
% \pause

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{R} Session Info}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sessionInfo()} returns information about the current \texttt{R} session.
      \begin{itemize}
        \item \texttt{R version},
        \item \texttt{OS platform},
        \item \texttt{locale} settings,
        \item list of packages that are loaded and attached to the search path,
        \item list of packages that are loaded, but \emph{not} attached to the search path,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
sessionInfo()  # Get R version and other session info
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Environment Variables}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} uses environment variables to store information about its environment, such as paths to directories containing files used by \texttt{R} (startup, history, OS).
      \vskip1ex
      For example the environment variables:
      \begin{itemize}
        \item \texttt{R\_USER} and \texttt{HOME} store the \texttt{R} user Home directory,
        \item \texttt{R\_HOME} stores the root directory of the \texttt{R} installation,
      \end{itemize}
      The functions \texttt{Sys.getenv()} and \texttt{Sys.setenv()} display and set the values environment variables.
      \vskip1ex
      \texttt{Sys.getenv("env\_var")} displays the environment variable \texttt{"env\_var"}.
      \vskip1ex
      \texttt{Sys.setenv("env\_var=value")} sets the environment variable \texttt{"env\_var"} equal to \texttt{"value"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
Sys.getenv()[5:7]  # List some environment variables

Sys.getenv("HOME")  # Get R user HOME directory

Sys.setenv(Home="C:/Develop/data")  # Set HOME directory

Sys.getenv("HOME")  # Get user HOME directory

Sys.getenv("R_HOME")  # Get R_HOME directory

R.home()  # Get R_HOME directory

R.home("etc")  # Get "etc" sub-directory of R_HOME
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Global \protect\emph{Options} Settings}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} uses a list of global \emph{options} which affect how \texttt{R} computes and displays results.
      \vskip1ex
      The function \texttt{options()} either sets or displays the values of global \emph{options}.
      \vskip1ex
      \texttt{options("globop")} displays the current value of option \texttt{"globop"}.
      \vskip1ex
      \texttt{getOption("globop")} displays the current value of option \texttt{"globop"}.
      \vskip1ex
      \texttt{options(globop=value)} sets the option \texttt{"globop"} equal to \texttt{"value"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# ?options  # Long list of global options
# Interpret strings as characters, not factors
getOption("stringsAsFactors")  # Display option
options("stringsAsFactors")  # Display option
options(stringsAsFactors=FALSE)  # Set option
# Number of digits printed for numeric values
options(digits=3)
# Control exponential scientific notation of print method
# Positive "scipen" values bias towards fixed notation
# Negative "scipen" values bias towards scientific notation
options(scipen=100)
# Maximum number of items printed to console
options(max.print=30)
# Warning levels options
# Negative - warnings are ignored
options(warn=-1)
# zero - warnings are stored and printed after top-level function has completed
options(warn=0)
# One - warnings are printed as they occur
options(warn=1)
# 2 or larger - warnings are turned into errors
options(warn=2)
# Save all options in variable
op_tions <- options()
# Restore all options from variable
options(op_tions)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Environments in \texttt{R}}


%%%%%%%%%%%%%%%
\subsection{\secname}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Environments consist of a \emph{frame} (a set of symbol-value pairs) and an \emph{enclosure} (a pointer to an enclosing environment).
      \vskip1ex
      There are three system environments:
      \begin{itemize}
        \item \texttt{globalenv()} the user's workspace,
        \item \texttt{baseenv()} the environment of the base package,
        \item \texttt{emptyenv()} the only environment without an enclosure,
      \end{itemize}
      Environments form a tree structure of successive enclosures, with the empty environment at its root.
      \vskip1ex
      Packages have their own environments.
      \vskip1ex
      The enclosure of the base package is the empty environment.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
rm(list=ls())
# Get base environment
baseenv()
# Get global environment
globalenv()
# Get current environment
environment()
# Get environment class
class(environment())
# Define variable in current environment
glob_var <- 1
# Get objects in current environment
ls(environment())
# Create new environment
new_env <- new.env()
# Get calling environment of new environment
parent.env(new_env)
# Assign Value to Name
assign("new_var1", 3, envir=new_env)
# Create object in new environment
new_env$new_var2 <- 11
# Get objects in new environment
ls(new_env)
# Get objects in current environment
ls(environment())
# Environments are subset like lists
new_env$new_var1
# Environments are subset like lists
new_env[["new_var1"]]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{R} Search Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} evaluates variables using the search path, a series of environments:
      \begin{itemize}
        \item global environment,
        \item package environments,
        \item base environment,
      \end{itemize}
      The function \texttt{search()} returns the search path for \texttt{R} objects.
      \vskip1ex
      The function \texttt{attach()} attaches objects to the search path.
      \vskip1ex
      Using \texttt{attach()} allows referencing object components by their names alone, rather than as components of objects.
      \vskip1ex
      The function \texttt{detach()} detaches objects from the search path.
      \vskip1ex
      The function \texttt{find()} finds where objects are located on the search path.
      \vspace{-1em}
      \begin{block}{\color{red}{Rule of Thumb}}
        Be very careful with using \texttt{attach()}.
        \vskip1ex
        Make sure to \texttt{detach()} objects once they're not needed.
      \end{block}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=TRUE>>=
search()  # Get search path for R objects
my_list <- list(flowers=c("rose", "daisy", "tulip"),
                trees=c("pine", "oak", "maple"))
my_list$trees
attach(my_list)
trees
search()  # Get search path for R objects
detach(my_list)
head(trees)  # "trees" is in datasets base package
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extracting Time Series from Environments}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{mget()} accepts a vector of strings and returns a list of the corresponding objects extracted from an \emph{environment}.
      \vskip1ex
      The extractor (accessor) functions from package \emph{quantmod}: \texttt{Cl()}, \texttt{Vo()}, etc., extract columns from \emph{OHLC} data.
      \vskip1ex
      A list of \emph{xts} series can be flattened into a single \emph{xts} series using the function \texttt{do.call()}.
      \vskip1ex
      The function \texttt{do.call()} executes a function call using a function name and a list of arguments.
      \vskip1ex
      \texttt{do.call()} passes the list elements individually, instead of passing the whole list as one argument.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
      \vskip1ex
      Time series can also be extracted from an \emph{environment} by coercing it into a \texttt{list}, and then subsetting and merging it into an \emph{xts} series using the function \texttt{do.call()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)  # Load package rutils
# Define ETF symbols
sym_bols <- c("VTI", "VEU", "IEF", "VNQ")
# Extract sym_bols from rutils::etf_env
price_s <- mget(sym_bols, envir=rutils::etf_env)
# price_s is a list of xts series
class(price_s)
class(price_s[[1]])
# Extract Close prices
price_s <- lapply(price_s, quantmod::Cl)
# Collapse list into time series the hard way
xts_1 <- cbind(price_s[[1]], price_s[[2]], price_s[[3]], price_s[[4]])
class(xts_1)
dim(xts_1)
# Collapse list into time series using do.call()
price_s <- do.call(cbind, price_s)
all.equal(xts_1, price_s)
class(price_s)
dim(price_s)
# Extract and cbind in single step
price_s <- do.call(cbind, lapply(
  mget(sym_bols, envir=rutils::etf_env), quantmod::Cl))
# Or
# Extract and bind all data, subset by sym_bols
price_s <- lapply(sym_bols, function(sym_bol) {
    quantmod::Cl(get(sym_bol, envir=rutils::etf_env))
})  # end lapply
# Same, but loop over etf_env without anonymous function
price_s <- do.call(cbind,
  lapply(as.list(rutils::etf_env)[sym_bols], quantmod::Cl))
# Same, but works only for OHLC series - produces error
price_s <- do.call(cbind,
  eapply(rutils::etf_env, quantmod::Cl)[sym_bols])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Managing Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time series columns can be renamed, and then saved into \texttt{.csv} files.
      \vskip1ex
      The function \texttt{strsplit()} splits the elements of a character vector.
      \vskip1ex
      The package \emph{zoo} contains functions \texttt{write.zoo()} and \texttt{read.zoo()} for writing and reading \emph{zoo} time series from \texttt{.txt} and \texttt{.csv} files.
      \vskip1ex
      The function \texttt{eapply()} is similar to \texttt{lapply()}, and applies a function to objects in an \emph{environment}, and returns a list.
      \vskip1ex
      The function \texttt{assign()} assigns a value to an object in a specified \emph{environment}, by referencing it using a character string (name).
      \vskip1ex
      The function \texttt{save()} writes objects to compressed binary \texttt{.RData} files.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Drop ".Close" from column names
colnames(price_s[, 1:4])
do.call(rbind, strsplit(colnames(price_s[, 1:4]), split="[.]"))[, 1]
colnames(price_s) <- do.call(rbind, strsplit(colnames(price_s), split="[.]"))[, 1]
# Or
colnames(price_s) <- unname(sapply(colnames(price_s),
    function(col_name) strsplit(col_name, split="[.]")[[1]][1]))
tail(price_s, 3)
# Which objects in global environment are class xts?
unlist(eapply(globalenv(), is.xts))
# Save xts to csv file
write.zoo(price_s,
  file="C:/Develop/lecture_slides/data/etf_series.csv", sep=",")
# Copy price_s into etf_env
etf_env$etf_list <- etf_list
# Or
assign("price_s", price_s, envir=etf_env)
# Save to .RData file
save(etf_env, file="etf_data.RData")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Referencing Object Components Using \texttt{with()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{with()} evaluates an expression in an environment constructed from the data.
      \vskip1ex
      \texttt{with()} allows referencing object components by their names alone.
      \vskip1ex
      It's often better to use \texttt{with()} instead of \texttt{attach()}.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=TRUE>>=
# "trees" is in datasets base package
head(trees, 3)
colnames(trees)
mean(Girth)
mean(trees$Girth)
with(trees,
     c(mean(Girth), mean(Height), mean(Volume)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{\texttt{R} Packages}


%%%%%%%%%%%%%%%
\subsection{\secname}
\begin{frame}[fragile,t]{\subsecname}

\begin{block}{Types of \subsecname}
  \texttt{R} can run libraries of functions called packages,
  \vskip1ex
  \texttt{R} packages can can also  contain data,
  \vskip1ex
  Most packages need to be \emph{loaded} into \texttt{R} before they can be used,
  \vskip1ex
  \texttt{R} includes a number of \texttt{base} packages that are already installed and loaded,
  \vskip1ex
  There's also a special package called the \texttt{base} package, which is responsible for all the basic \texttt{R} functionality,
  \vskip1ex
  \texttt{datasets} is a \texttt{base} package containing various datasets, for example \texttt{EuStockMarkets},
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{base} Packages}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} includes a number of packages that are pre-installed (often called \emph{base} packages),\\
      Some \emph{base} packages:
      \begin{itemize}
        \item \emph{base} - basic \texttt{R} functionality,
        \item \emph{stats} - statistical functions and random number generation,
        \item \emph{graphics} - basic graphics,
        \item \emph{utils} - utility functions,
        \item \emph{datasets} - popular datasets,
        \item \emph{parallel} - support for parallel computation,
      \end{itemize}
    \column{0.5\textwidth}
      Very popular packages:
      \begin{itemize}
        \item \emph{MASS} - functions and datasets for "Modern Applied Statistics with S",
        \item \emph{ggplot2} - grammar of graphics plots,
        \item \emph{shiny} - interactive web graphics from R,
        \item \emph{slidify} - HTML5 slide shows from R,
        \item \emph{devtools} - create R packages,
        \item \emph{roxygen2} - document R packages,
        \item \emph{Rcpp} - integrate C++ code with R,
        \item \emph{RcppArmadillo} - interface to Armadillo linear algebra library,
        \item \emph{forecast} - linear models and forecasting,
        \item \emph{tseries} - time series analysis and computational finance,
        \item \emph{zoo} - time series and ordered objects,
        \item \emph{xts} - advanced time series objects,
        \item \emph{quantmod} - quantitative financial modeling framework,
        \item \emph{caTools} - moving window statistics for graphics and time series objects,
      \end{itemize}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CRAN} Package Views}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{columns}[T]
  \column{0.4\textwidth}
    \begin{block}{}
      \emph{CRAN} view for package \emph{AER}:\\
      \hskip1em\url{http://cran.r-project.org/web/packages/AER/}\\
    \end{block}
    \begin{block}{}
      Note:
      \begin{itemize}
        \item Authors,
        \item Version number,
        \item Reference manual,
        \item Vignettes,
        \item Dependencies on other packages.
      \end{itemize}
    \end{block}
    \begin{block}{}
      The package source code can be downloaded by clicking on the \href{package source}{package source} link,
    \end{block}
  \column{0.6\textwidth}
    \includegraphics[height=1.0\textwidth]{image/CRAN_Package.png}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{CRAN} Task Views}
\begin{frame}[fragile,t]{\subsecname}

\begin{columns}[T]
  \column{0.4\textwidth}
    \begin{block}{}
      \emph{CRAN} Finance Task View\\
      \hskip1em\url{http://cran.r-project.org//}\\
    \end{block}
    \begin{block}{}
      Note:
      \begin{itemize}
        \item Maintainer,
        \item Topics,
        \item List of packages.
      \end{itemize}
    \end{block}
  \column{0.6\textwidth}
    \vspace{-1em}
    \includegraphics[height=1.0\textwidth]{image/CRAN_Views.png}
\end{columns}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Installing Packages}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most packages need to be \emph{installed} before they can be loaded and used.
      \vskip1ex
      Some packages like \emph{MASS} are installed with base \texttt{R} (but not loaded).
      \vskip1ex
      \emph{Installing} a package means downloading and saving its files to a local computer directory (hard disk), so they can be \emph{loaded} by the \texttt{R} system.
      \vskip1ex
      The function \texttt{install.packages()} installs packages from the \texttt{R} command line.
      \vskip1ex
      Most widely used packages are available on the \emph{CRAN} repository:\\
      \hskip1em\url{http://cran.r-project.org/web/packages/}
      \vskip1ex
      Or on \emph{R-Forge} or \emph{GitHub}:\\
      \hskip1em\url{https://r-forge.r-project.org/}\\
      \hskip1em\url{https://github.com/}
      \vskip1ex
      Packages can also be installed in \emph{RStudio} from the menu (go to \alert{Tools} and then \alert{Install packages}),\\
      \vskip1ex
      Packages residing on GitHub can be installed using the devtools packages.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
getOption("repos")  # get default package source
.libPaths()  # get package save directory
      @
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
install.packages("AER")  # install "AER" from CRAN
# install "PerformanceAnalytics" from R-Forge
install.packages(
  pkgs="PerformanceAnalytics",  # name
  lib="C:/Users/Jerzy/Downloads",  # directory
  repos="http://R-Forge.R-project.org")  # source
      @
      \vspace{-2em}
      <<echo=TRUE,eval=FALSE>>=
# install devtools from CRAN
install.packages("devtools")
# load devtools
library(devtools)
# install package "babynames" from GitHub
install_github(repo="hadley/babynames")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Installing Packages From Source}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Sometimes packages aren't available in compiled form, so it's necessary to install them from their source code.
      \vskip1ex
      To install a package from source, the user needs to first install compilers and development tools:\\
      \hskip1em For Windows install \texttt{Rtools}:\\
\url{https://cran.r-project.org/bin/windows/Rtools/}\\
      \hskip1em For Mac OSX install \texttt{XCode} developer tools:\\
\url{https://developer.apple.com/xcode/downloads/}
      \vskip1ex
      The function \texttt{install.packages()} with argument \texttt{type="source"} installs a package from source.
      \vskip1ex
      The function \texttt{download.packages()} downloads the package's installation files (compressed tar format) to a local directory.
      \vskip1ex
      The function \texttt{install.packages()} can then be used to install the package from the downloaded files.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE, eval=FALSE>>=
# install package "PortfolioAnalytics" from source
install.packages("PortfolioAnalytics",
  type="source",
  repos="http://r-forge.r-project.org")
# download files for package "PortfolioAnalytics"
download.packages(pkgs = "PortfolioAnalytics",
  destdir = ".",  # download to cwd
  type = "source",
  repos="http://r-forge.r-project.org")
# install "PortfolioAnalytics" from local tar source
install.packages(
  "C:/Users/Jerzy/Downloads/PortfolioAnalytics_0.9.3598.tar.gz",
  repos=NULL, type="source")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Installed Packages}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{defaultPackages} contains a list of packages loaded on startup by default.
      \vskip1ex
      The function \texttt{installed.packages()} returns a matrix of all packages installed on the system.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
getOption("defaultPackages")
# matrix of installed package information
pack_info <- installed.packages()
dim(pack_info)
# get all installed package names
sort(unname(pack_info[, "Package"]))
# get a few package names and their versions
pack_info[sample(x=1:100, 5), c("Package", "Version")]
# get info for package "xts"
t(pack_info["xts", ])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package Files and Directories}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Package installation files are organized into multiple directories, including some of the following:
      \begin{itemize}
        \item \texttt{\textasciitilde{}/R} containing \texttt{R} source code files,
        \item \texttt{\textasciitilde{}/src} containing \texttt{C++} and \texttt{Fortran} source code files,
        \item \texttt{\textasciitilde{}/data} containing datasets,
        \item \texttt{\textasciitilde{}/man} containing documentation files,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE, eval=TRUE>>=
# list directories in "PortfolioAnalytics" sub-directory
gsub(
  "C:/Users/Jerzy/Documents/R/win-library/3.1",
  "~",
  list.dirs(
    file.path(
      .libPaths()[1],
      "PortfolioAnalytics")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Loading Packages}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most packages need to be \emph{loaded} before they can be used in an \texttt{R} session.
      \vskip1ex
      Loading a package means attaching the package \emph{namespace} to the \emph{search path}, which allows \texttt{R} to call the package functions and data.
      \vskip1ex
      The functions \texttt{library()} and \texttt{require()} load packages, but in slightly different ways.
      \vskip1ex
      \texttt{library()} produces an \emph{error} (halts execution) if the package can't be loaded.
      \vskip1ex
      \texttt{require()} returns \texttt{TRUE} if the package is loaded successfully, and \texttt{FALSE} otherwise.
      \vskip1ex
      Therefore \texttt{library()} is usually used in script files that might be sourced, while \texttt{require()} is used inside functions.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# load package, produce error if can't be loaded
library(MASS)
# load package, return TRUE if loaded successfully
require(MASS)
# load quietly
library(MASS, quietly=TRUE)
# load without any messages
suppressMessages(library(MASS))
# remove package from search path
detach(MASS)
# install package if it can't be loaded successfully
if (!require("xts")) install.packages("xts")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Referencing Package Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      After a package is \emph{loaded}, the package functions and data can be accessed by name.
      \vskip1ex
      Package objects can also be accessed without \emph{loading} the package, by using the double-colon \texttt{"::"} reference operator.
      \vskip1ex
      For example, \texttt{TTR::VWAP()} references the function \texttt{VWAP()} from the package \emph{TTR}.
      \vskip1ex
      This way users don't have to load the package \emph{TTR} (with \texttt{library(TTR)}) to use functions from the package \emph{TTR}.
      \vskip1ex
      Using the \texttt{"::"} operator displays the source of objects, and makes \texttt{R} code easier to analyze.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# calculate VTI volume-weighted average price
v_wap <- TTR::VWAP(
  price=quantmod::Cl(rutils::etf_env$VTI), 
  volume=quantmod::Vo(rutils::etf_env$VTI), n=10)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exploring Packages}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{Ecdat} contains data sets for econometric analysis.
      \vskip1ex
      The data frame \texttt{Garch} contains daily currency prices.
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package.
      \vskip1ex
      Some packages provide \emph{lazy loading} of their data sets, which means they automatically load their data sets when they're needed (when they are called by some operation).
      \vskip1ex
      The package's data isn't loaded into \texttt{R} memory when the package is \emph{loaded}, so it's not listed using \texttt{ls()}, but the package data is available without calling the function \texttt{data()}.
      \vskip1ex
      The function \texttt{data()} isn't required to load data sets that are set up for \emph{lazy loading}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library()  # list all packages installed on the system
search()  # list all loaded packages on search path

# get documentation for package "Ecdat"
packageDescription("Ecdat")  # get short description
help(package="Ecdat")  # load help page
library(Ecdat)  # load package "Ecdat"
data(package="Ecdat")  # list all datasets in "Ecdat"
ls("package:Ecdat")  # list all objects in "Ecdat"
browseVignettes("Ecdat")  # view package vignette
detach("package:Ecdat")  # remove Ecdat from search path
      @
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(Ecdat)  # load econometric data sets
class(Garch)  # Garch is a data frame from "Ecdat"
dim(Garch)  # daily currency prices
head(Garch[, -2])  # col 'dm' is Deutsch Mark
detach("package:Ecdat")  # remove Ecdat from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package Namespaces}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Package \emph{namespaces}:
      \begin{itemize}
        \item Provide a mechanism for calling objects from a package,
        \item Hide functions and data internal to the package,
        \item Prevent naming conflicts between user and package names,
      \end{itemize}
      When a package is loaded using \texttt{library()} or \texttt{require()}, its \emph{namespace} is attached to the search path.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-1)>>=
rm(list=ls())
search()  # get search path for R objects
library(MASS)  # load package "MASS"
head(ls("package:MASS"))  # list some objects in "MASS"
detach("package:MASS")  # remove "MASS" from search path
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package Namespaces and the Search Path}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Packages may be loaded without their \emph{namespace} being attached to the search path.
      \vskip1ex
      When packages are loaded, then packages they depend on are also loaded, but their \emph{namespaces} aren't necessarily attached to the search path.
      \vskip1ex
      The function \texttt{loadedNamespaces()} lists all loaded \emph{namespaces}, including those that aren't on the search path.
      \vskip1ex
      The function \texttt{search()} returns the current search path for \texttt{R} objects.
      \vskip1ex
      \texttt{search()} returns many package \emph{namespaces}, but not all the loaded \emph{namespaces}.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
loadedNamespaces()  # get names of loaded namespaces

search()  # get search path for R objects
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Not Attached Namespaces}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{sessionInfo()} returns information about the current \texttt{R} session, including packages that are loaded, but \emph{not attached} to the search path.
      \vskip1ex
      \texttt{sessionInfo()} lists those packages as "loaded via a \emph{namespace} (and not attached)"
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# get session info,
# including packages not attached to the search path
sessionInfo()
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Non-Visible Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Non-visible objects (variables or functions) are either:
      \begin{itemize}
        \item objects from \emph{not attached} \emph{namespaces},
        \item objects \emph{not exported} outside a package,
      \end{itemize}
      Objects from packages that aren't attached can be accessed using the double-colon \texttt{"::"} reference operator.
      \vskip1ex
      Objects that are \emph{not exported} outside a package can be accessed using the triple-colon \texttt{":::"} reference operator.
      \vskip1ex
      Colon operators automatically load the associated package.
      \vskip1ex
      Non-visible objects in namespaces often use the \texttt{".*"} name syntax.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
plot.xts  # package xts isn't loaded and attached
head(xts::plot.xts, 3)
methods("cbind")  # get all methods for function "cbind"
stats::cbind.ts  # cbind isn't exported from package stats
stats:::cbind.ts  # view the non-visible function
getAnywhere("cbind.ts")
library(MASS)  # load package 'MASS'
select  # code of primitive function from package 'MASS'
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exploring Namespaces and Non-Visible Objects}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{getAnywhere()} displays information about \texttt{R} objects, including non-visible objects.
      \vskip1ex
      Objects referenced \emph{within} packages have different search paths than other objects:\\
      Their search path starts in the package \emph{namespace}, then the global environment and then finally the regular search path.
      \vskip1ex
      This way references to objects from \texttt{within} a package are resolved to the package, and they're not masked by objects of the same name in other environments.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
getAnywhere("cbind.ts")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Writing Fast \texttt{R} Code Using Vectorized Operations}


%%%%%%%%%%%%%%%
\subsection{Benchmarking the Speed of \texttt{R} Code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{system.time()} calculates the execution time (in seconds) used to evaluate a given expression.
      \vskip1ex
      \texttt{system.time()} returns the \emph{"user time"} (execution time of user instructions), the \emph{"system time"} (execution time of operating system calls), and \emph{"elapsed time"} (total execution time, including system latency waiting).
      \vskip1ex
      The function \texttt{microbenchmark()} from package \texttt{microbenchmark} calculates and compares the execution time of \texttt{R} expressions (in milliseconds), and is more accurate than \texttt{system.time()}.
      \vskip1ex
      The time it takes to execute an expression is not always the same, since it depends on the state of the processor, caching, etc.
      \vskip1ex
      \texttt{microbenchmark()} executes the expression many times, and returns the distribution of total execution times.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
vec_tor <- runif(1e6)
# sqrt() and "^0.5" are the same
all.equal(sqrt(vec_tor), vec_tor^0.5)
# sqrt() is much faster than "^0.5"
system.time(vec_tor^0.5)
microbenchmark(
  power = vec_tor^0.5, 
  sqrt = sqrt(vec_tor), 
  times=10)
      @
      The "\texttt{times}" parameter is the number of times the expression is evaluated.
      \vskip1ex
      The choice of the "\texttt{times}" parameter is a tradeoff between the time it takes to run \texttt{microbenchmark()}, and the desired accuracy,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Using \texttt{apply()} Instead of \texttt{for()} and \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      All the different \texttt{R} loops have similar speed, with \texttt{apply()} the fastest, then \texttt{vapply()}, \texttt{lapply()} and \texttt{sapply()} slightly slower, and \texttt{for()} loops the slowest.
      \vskip1ex
      More importantly, the \texttt{apply()} syntax is more readable and concise, and fits the functional language paradigm of \texttt{R},  so it's preferred over \texttt{for()} loops.
      \vskip1ex
      Both \texttt{vapply()} and \texttt{lapply()} are \emph{compiled} (\emph{primitive}) functions, and therefore can be faster than other \texttt{apply()} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate matrix of random data with 5,000 rows
mat_rix <- matrix(rnorm(10000), ncol=2)
# Allocate memory for row sums
row_sums <- numeric(NROW(mat_rix))
summary(microbenchmark(
  row_sums = rowSums(mat_rix),  # end row_sums
  ap_ply = apply(mat_rix, 1, sum),  # end apply
  l_apply = lapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ])),  # end lapply
  v_apply = vapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ]),
    FUN.VALUE = c(sum=0)),  # end vapply
  s_apply = sapply(1:NROW(mat_rix), function(in_dex)
    sum(mat_rix[in_dex, ])),  # end sapply
  for_loop = for (i in 1:NROW(mat_rix)) {
    row_sums[i] <- sum(mat_rix[i,])
  },  # end for
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Increasing Speed of Loops by Pre-allocating Memory}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} performs automatic memory management as users assign values to objects.
      \vskip1ex
      \texttt{R} doesn't require allocating the full memory for vectors or lists, and allows appending new data to existing objects ("growing" them).
      \vskip1ex
      For example, \texttt{R} allows assigning a value to a vector element that doesn't exist yet.
      \vskip1ex
      This forces \texttt{R} to allocate additional memory for that element, which carries a small speed penalty.
      \vskip1ex
      But when data is appended to an object using the functions \texttt{c()}, \texttt{append()}, \texttt{cbind()}, or \texttt{rbind()}, then \texttt{R} allocates memory for the whole new object and copies all the existing values, which is very memory intensive and slow.
      \vskip1ex
      It is therefore preferable to pre-allocate memory for large objects before performing loops.
      \vskip1ex
      The function \texttt{numeric(k)} returns a numeric vector of zeros of length \texttt{k}, while \texttt{numeric(0)} returns an empty (zero length) numeric vector (not to be confused with a \texttt{NULL} object).
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
vec_tor <- rnorm(5000)
summary(microbenchmark(
# Allocate full memory for cumulative sum
  for_loop = {cum_sum <- numeric(NROW(vec_tor))
    cum_sum[1] <- vec_tor[1]
    for (i in 2:NROW(vec_tor)) {
      cum_sum[i] <- cum_sum[i-1] + vec_tor[i]
    }},  # end for
# Allocate zero memory for cumulative sum
  grow_vec = {cum_sum <- numeric(0)
    cum_sum[1] <- vec_tor[1]
    for (i in 2:NROW(vec_tor)) {
# Add new element to "cum_sum" ("grow" it)
      cum_sum[i] <- cum_sum[i-1] + vec_tor[i]
    }},  # end for
# Allocate zero memory for cumulative sum
  com_bine = {cum_sum <- numeric(0)
    cum_sum[1] <- vec_tor[1]
    for (i in 2:NROW(vec_tor)) {
# Add new element to "cum_sum" ("grow" it)
      cum_sum <- c(cum_sum, vec_tor[i])
    }},  # end for
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Vector Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Vectorized} functions accept \texttt{vectors} as their arguments, and return a vector of the same length as their value.
      \vskip1ex
      Many \emph{vectorized} functions are also \emph{compiled} (they pass their data to compiled \texttt{C++} code), which makes them very fast.
      \vskip1ex
      The following \emph{vectorized compiled} functions calculate cumulative values over large vectors:
      \begin{itemize}
        \item \texttt{cummax()}
        \item \texttt{cummin()}
        \item \texttt{cumsum()}
        \item \texttt{cumprod()}
      \end{itemize}
      Standard arithmetic operations (\texttt{"+", "-"}, etc.) can be applied to \texttt{vectors}, and are implemented as \emph{vectorized compiled} functions.
      \vskip1ex
      \texttt{ifelse()} and \texttt{which()} are \emph{vectorized compiled} functions for logical operations.
      \vskip1ex
      But many \emph{vectorized} functions perform their calculations in \texttt{R} code, and are therefore slow, but convenient to use.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
vector1 <- rnorm(1000000)
vector2 <- rnorm(1000000)
big_vector <- numeric(1000000)
# Sum two vectors in two different ways
summary(microbenchmark(
  # Sum vectors using "for" loop
  r_loop = (for (i in 1:NROW(vector1)) {
    big_vector[i] <- vector1[i] + vector2[i]
  }),
  # Sum vectors using vectorized "+"
  vec_torized = (vector1 + vector2),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
# Allocate memory for cumulative sum
cum_sum <- numeric(NROW(big_vector))
cum_sum[1] <- big_vector[1]
# Calculate cumulative sum in two different ways
summary(microbenchmark(
# Cumulative sum using "for" loop
  r_loop = (for (i in 2:NROW(big_vector)) {
    cum_sum[i] <- cum_sum[i-1] + big_vector[i]
  }),
# Cumulative sum using "cumsum"
  vec_torized = cumsum(big_vector),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{apply()} loops are very inefficient for calculating statistics over rows and columns of very large matrices.
      \vskip1ex
      \texttt{R} has very fast \emph{vectorized compiled} functions for calculating sums and means of rows and columns:
      \begin{itemize}
        \item \texttt{rowSums()}
        \item \texttt{colSums()}
        \item \texttt{rowMeans()}
        \item \texttt{colMeans()}
      \end{itemize}
      These \emph{vectorized} functions are also \emph{compiled} functions, so they're very fast because they pass their data to compiled \texttt{C++} code, which performs the loop calculations.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate matrix of random data with 5,000 rows
mat_rix <- matrix(rnorm(10000), ncol=2)
# Calculate row sums two different ways
all.equal(rowSums(mat_rix),
  apply(mat_rix, 1, sum))
summary(microbenchmark(
  row_sums = rowSums(mat_rix),
  ap_ply = apply(mat_rix, 1, sum),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast \texttt{R} Code for Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functions \texttt{pmax()} and \texttt{pmin()} calculate the "parallel" maxima (minima) of multiple vector arguments.
      \vskip1ex
      \texttt{pmax()} and \texttt{pmin()} return a vector, whose \emph{n}-th element is equal to the maximum (minimum) of the \emph{n}-th elements of the arguments, with shorter vectors recycled if necessary.
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are methods of generic functions \texttt{pmax()} and \texttt{pmin()}, designed for atomic vectors.
      \vskip1ex
      \texttt{pmax()} can be used to quickly calculate the maximum values of rows of a matrix, by first converting the matrix columns into a list, and then passing them to \texttt{pmax()}.
      \vskip1ex
      \texttt{pmax.int()} and \texttt{pmin.int()} are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(microbenchmark)
str(pmax)
# Calculate row maximums two different ways
summary(microbenchmark(
  p_max=do.call(pmax.int,
      lapply(seq_along(mat_rix[1, ]),
        function(in_dex) mat_rix[, in_dex])),
  l_apply=unlist(lapply(seq_along(mat_rix[, 1]),
        function(in_dex) max(mat_rix[in_dex, ]))),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \texttt{matrixStats} for Fast Matrix Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package
      \href{https://cran.r-project.org/web/packages/matrixStats}{\color{blue}{\emph{matrixStats}}}
      contains functions for calculating aggregations over matrix columns and rows, and other matrix computations, such as:
      \begin{itemize}
        \item estimating location and scale: \texttt{rowRanges()}, \texttt{colRanges()}, and \texttt{rowMaxs()}, \texttt{rowMins()}, etc.,
        \item testing and counting values: \texttt{colAnyMissings()}, \texttt{colAnys()}, etc.,
        \item cumulative functions: \texttt{colCumsums()}, \texttt{colCummins()}, etc.,
        \item binning and differencing: \texttt{binCounts()}, \texttt{colDiffs()}, etc.,
      \end{itemize}
      A summary of \texttt{matrixStats} functions can be found under:\\
      \url{https://cran.r-project.org/web/packages/matrixStats/vignettes/matrixStats-methods.html}
      \vskip1ex
      The \texttt{matrixStats} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
install.packages("matrixStats")  # Install package matrixStats
library(matrixStats)  # Load package matrixStats
# Calculate row min values three different ways
summary(microbenchmark(
  row_mins = rowMins(mat_rix),
  p_min =
    do.call(pmin.int,
            lapply(seq_along(mat_rix[1, ]),
                   function(in_dex)
                     mat_rix[, in_dex])),
  as_data_frame =
    do.call(pmin.int,
            as.data.frame.matrix(mat_rix)),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \texttt{Rfast} for Fast Matrix and Numerical Computations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package
      \href{https://cran.r-project.org/web/packages/Rfast}{\color{blue}{\emph{Rfast}}}
      contains functions for fast matrix and numerical computations, such as:
      \begin{itemize}
        \item \texttt{colMedians()} and \texttt{rowMedians()} for matrix column and row medians,
        \item \texttt{colCumSums()}, \texttt{colCumMins()} for cumulative sums and min/max,
        \item \texttt{eigen.sym()} for performing eigenvalue matrix decomposition,
      \end{itemize}
      The \texttt{Rfast} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
install.packages("Rfast")  # Install package Rfast
library(Rfast)  # Load package Rfast
# Benchmark speed of calculating ranks
vec_tor <- 1e3
all.equal(rank(vec_tor), Rfast::Rank(vec_tor))
library(microbenchmark)
summary(microbenchmark(
  Rcode = rank(vec_tor),
  Rfast = Rfast::Rank(vec_tor),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
# Benchmark speed of calculating column medians
mat_rix <- matrix(1e4, nc=10)
all.equal(matrixStats::colMedians(mat_rix), Rfast::colMedians(mat_rix))
summary(microbenchmark(
  matrixStats = matrixStats::colMedians(mat_rix),
  Rfast = Rfast::colMedians(mat_rix),
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Writing Fast \texttt{R} Code Using Vectorized Operations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R}-style code is code that relies on \emph{vectorized compiled} functions, instead of \texttt{for()} loops.
      \vskip1ex
      \texttt{for()} loops in \texttt{R} are slow because they call functions multiple times, and individual function calls are compute-intensive and slow.
      \vskip1ex
      The brackets \texttt{"[]"} operator is a \emph{vectorized compiled} function, and is therefore very fast.
      \vskip1ex
      Vectorized assignments using brackets \texttt{"[]"} and \texttt{Boolean} or \texttt{integer} vectors to subset vectors or matrices are therefore preferable to \texttt{for()} loops.
      \vskip1ex
      \texttt{R} code that uses \emph{vectorized compiled} functions can be as fast as \texttt{C++} code.
      \vskip1ex
      \texttt{R}-style code is also very \emph{expressive}, i.e. it allows performing complex operations with very few lines of code.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
summary(microbenchmark(  # Assign values to vector three different ways
# Fast vectorized assignment loop performed in C using brackets "[]"
  brack_ets = {vec_tor <- numeric(10)
    vec_tor[] <- 2},
# Slow because loop is performed in R
  for_loop = {vec_tor <- numeric(10)
    for (in_dex in seq_along(vec_tor))
      vec_tor[in_dex] <- 2},
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
summary(microbenchmark(  # Assign values to vector two different ways
# Fast vectorized assignment loop performed in C using brackets "[]"
  brack_ets = {vec_tor <- numeric(10)
    vec_tor[4:7] <- rnorm(4)},
# Slow because loop is performed in R
  for_loop = {vec_tor <- numeric(10)
    for (in_dex in 4:7)
      vec_tor[in_dex] <- rnorm(1)},
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Functions which use vectorized operations and functions are automatically \emph{vectorized} themselves.
      \vskip1ex
      Functions which only call other compiled \texttt{C++} vectorized functions, are also very fast.
      \vskip1ex
      But not all functions are vectorized, or they're not vectorized with respect to their \emph{parameters}.
      \vskip1ex
      Some \emph{vectorized} functions perform their calculations in \texttt{R} code, and are therefore slow, but convenient to use.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define function vectorized automatically
my_fun <- function(in_put, pa_ram) {
  pa_ram*in_put
}  # end my_fun
# "in_put" is vectorized
my_fun(in_put=1:3, pa_ram=2)
# "pa_ram" is vectorized
my_fun(in_put=10, pa_ram=2:4)
# Define vectors of parameters of rnorm()
std_devs <- structure(1:3, names=paste0("sd=", 1:3))
me_ans <- structure(-1:1, names=paste0("mean=", -1:1))
# "sd" argument of rnorm() isn't vectorized
rnorm(1, sd=std_devs)
# "mean" argument of rnorm() isn't vectorized
rnorm(1, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing \texttt{sapply()} Loops Over Function Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Many functions aren't vectorized with respect to their \emph{parameters}.
      \vskip1ex
      Performing \texttt{sapply()} loops over a function's parameters produces vector output.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Loop over std_devs produces vector output
set.seed(1121)
sapply(std_devs, function(std_dev) rnorm(n=2, sd=std_dev))
# Same
set.seed(1121)
sapply(std_devs, rnorm, n=2, mean=0)
# Loop over me_ans
set.seed(1121)
sapply(me_ans, function(me_an) rnorm(n=2, mean=me_an))
# Same
set.seed(1121)
sapply(me_ans, rnorm, n=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Creating Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In order to \emph{vectorize} a function with respect to one of its \emph{parameters}, it's necessary to perform a loop over it.
      \vskip1ex
      The function \texttt{Vectorize()} performs an \texttt{apply()} loop over the arguments of a function, and returns a vectorized version of the function.
      \vskip1ex
      \texttt{Vectorize()} vectorizes the arguments passed to \texttt{"vectorize.args"}.
      \vskip1ex
      \texttt{Vectorize()} is an example of a \emph{higher order} function: it accepts a function as its argument and returns a function as its value.
      \vskip1ex
      Functions that are vectorized using \texttt{Vectorize()} or \texttt{apply()} loops are just as slow as \texttt{apply()} loops, but convenient to use.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# rnorm() vectorized with respect to "std_dev"
vec_rnorm <- function(n, mean=0, sd=1) {
  if (NROW(sd)==1)
    rnorm(n=n, mean=mean, sd=sd)
  else
    sapply(sd, rnorm, n=n, mean=mean)
}  # end vec_rnorm
set.seed(1121)
vec_rnorm(n=2, sd=std_devs)
# rnorm() vectorized with respect to "mean" and "sd"
vec_rnorm <- Vectorize(FUN=rnorm,
              vectorize.args=c("mean", "sd")
)  # end Vectorize
set.seed(1121)
vec_rnorm(n=2, sd=std_devs)
set.seed(1121)
vec_rnorm(n=2, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \texttt{mapply()} Functional}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{mapply()} functional is a multivariate version of \texttt{sapply()}, that allows calling a non-vectorized function in a vectorized way.
      \vskip1ex
      \texttt{mapply()} accepts a multivariate function passed to the \texttt{"FUN"} argument and any number of vector arguments passed to the dots \texttt{"..."}.
      \vskip1ex
      \texttt{mapply()} calls \texttt{"FUN"} on the vectors passed to the dots \texttt{"..."}, one element at a time:
      \begin{multline*}
        mapply(FUN=fun, vec_1, vec_2, \ldots) = \\
        [ fun(vec_{1,1}, vec_{2,1}, \ldots), \ldots, \\
        fun(vec_{1,i}, vec_{2,i}, \ldots), \ldots ]
      \end{multline*}
      \texttt{mapply()} passes the first vector to the first argument of \texttt{"FUN"}, the second vector to the second argument, etc.
      \vskip1ex
      The first element of the output vector is equal to \texttt{"FUN"} called on the first elements of the input vectors, the second element is \texttt{"FUN"} called on the second elements, etc.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
str(sum)
# na.rm is bound by name
mapply(sum, 6:9, c(5, NA, 3), 2:6, na.rm=TRUE)
str(rnorm)
# mapply vectorizes both arguments "mean" and "sd"
mapply(rnorm, n=5, mean=me_ans, sd=std_devs)
mapply(function(in_put, e_xp) in_put^e_xp,
       1:5, seq(from=1, by=0.2, length.out=5))
      @
      The output of \texttt{mapply()} is a vector of length equal to the longest vector passed to the dots \texttt{"..."} argument, with the elements of the other vectors recycled if necessary,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorizing Functions Using \texttt{mapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{mapply()} functional is a multivariate version of \texttt{sapply()}, that allows calling a non-vectorized function in a vectorized way.
      \vskip1ex
      \texttt{mapply()} can be used to vectorize several function arguments simultaneously.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# rnorm() vectorized with respect to "mean" and "sd"
vec_rnorm <- function(n, mean=0, sd=1) {
  if (NROW(mean)==1 && NROW(sd)==1)
    rnorm(n=n, mean=mean, sd=sd)
  else
    mapply(rnorm, n=n, mean=mean, sd=sd)
}  # end vec_rnorm
# Call vec_rnorm() on vector of "sd"
vec_rnorm(n=2, sd=std_devs)
# Call vec_rnorm() on vector of "mean"
vec_rnorm(n=2, mean=me_ans)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Vectorized \texttt{if-else} Statements Using Function \texttt{ifelse()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{ifelse()} performs \emph{vectorized} \texttt{if-else} statements on vectors.
      \vskip1ex
      \texttt{ifelse()} is much faster than performing an element-wise loop in \texttt{R}.
        <<func_ifelse,eval=FALSE,echo=TRUE,fig.show='hide'>>=
# Create two numeric vectors
vector1 <- sin(0.25*pi*1:20)
vector2 <- cos(0.25*pi*1:20)
# Create third vector using 'ifelse'
vector3 <- ifelse(vector1 > vector2, vector1, vector2)
# cbind all three together
vector3 <- cbind(vector1, vector2, vector3)
colnames(vector3)[3] <- "Max"
# Set plotting parameters
x11(width=6, height=7)
par(oma=c(0, 1, 1, 1), mar=c(0, 2, 2, 1), 
    mgp=c(2, 1, 0), cex.lab=0.5, cex.axis=1.0, cex.main=1.8, cex.sub=0.5)
# Plot matrix
zoo::plot.zoo(vector3, lwd=2, ylim=c(-1, 1), 
  xlab="", col=c("green", "blue", "red"), 
  main="ifelse() Calculates The Max of Two Data Sets")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/ifelse_example.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{It's \protect\emph{Always} Important to Write Fast \texttt{R} Code}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      How to write fast \texttt{R} code:
      \begin{itemize}
        \item Avoid using \texttt{apply()} and \texttt{for()} loops for large datasets.
        \item Use \texttt{R} functions which are \emph{compiled} \texttt{C++} code, instead of using interpreted \texttt{R} code.
        \item Avoid using too many \texttt{R} function calls (every command in \texttt{R} is a function).
        \item Pre-allocate memory for new objects, instead of appending to them ("growing" them).
        \item Write \texttt{C++} functions in \emph{Rcpp} and \emph{RcppArmadillo}.
        \item Use \emph{function methods} directly instead of using \emph{generic functions}.
        \item Create specialized functions by extracting only the essential \texttt{R} code from \emph{function methods}.
        \item \emph{Byte-compile} \texttt{R} functions using the \emph{byte compiler} in package \emph{compiler}.
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{image/Jeremy_Clarkson_Linus_Torvalds.jpg}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative sum of a vector
vec_tor <- runif(1e5)
# Use compiled function
cum_sum <- cumsum(vec_tor)
# Use for loop
cum_sum2 <- vec_tor
for (i in 2:NROW(cum_sum2))
  cum_sum2[i] <- (cum_sum2[i] + cum_sum2[i-1])
# Compare the two methods
all.equal(cum_sum, cum_sum2)
# Microbenchmark the two methods
library(microbenchmark)
summary(microbenchmark(
  cumsum=cumsum(vec_tor),
  loop_alloc={
    cum_sum2 <- vec_tor
    for (i in 2:NROW(cum_sum2))
      cum_sum2[i] <- (cum_sum2[i] + cum_sum2[i-1])
  },
  loop_nalloc={
    # Doesn't allocate memory to cum_sum3
    cum_sum3 <- vec_tor[1]
    for (i in 2:NROW(vec_tor))
      # This command adds an extra element to cum_sum3
      cum_sum3[i] <- (vec_tor[i] + cum_sum3[i-1])
  },
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Parallel Computing}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing in \texttt{R}}
\begin{frame}[t]{\subsecname}
\vspace{-1em}

\begin{block}{Parallel Computing in \texttt{R}}
      Parallel computing means splitting a computing task into separate sub-tasks, and then simultaneously computing the sub-tasks on several computers or CPU cores.
      \vskip1ex
      There are many different packages that allow parallel computing in \texttt{R}, most importantly package \emph{parallel}, and packages \texttt{foreach}, \texttt{doParallel}, and related packages:\\
      \hskip1em\url{http://cran.r-project.org/web/views/HighPerformanceComputing.html}\\
      \hskip1em\url{http://blog.revolutionanalytics.com/high-performance-computing/}\\
      \hskip1em\url{http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/}\\
\end{block}

\begin{block}{\texttt{R} Base Package \emph{parallel}}
  The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs,
  \vskip1ex
  The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed.\\
  \hskip1em\url{http://adv-r.had.co.nz/Profiling.html\#parallelise}\\
  \hskip1em\url{https://github.com/tobigithub/R-parallel/wiki/R-parallel-package-overview}\\
\end{block}

\begin{block}{Packages \texttt{foreach}, \texttt{doParallel}, and Related Packages}
      \hskip1em\url{http://blog.revolutionanalytics.com/2015/10/updates-to-the-foreach-package-and-its-friends.html}\\
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{parallel} provides functions for parallel computing using multiple cores of CPUs.
      \vskip1ex
      The package \emph{parallel} is part of the standard \texttt{R} distribution, so it doesn't need to be installed.
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}).
      \vskip1ex
      Parallel computing requires additional resources and time for distributing the computing tasks and collecting the output, which produces a computing overhead.
      \vskip1ex
      Therefore parallel computing can actually be slower for small computations, or for computations that can't be naturally separated into sub-tasks.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
# Get short description
packageDescription("parallel")
# Load help page
help(package="parallel")
# list all objects in "parallel"
ls("package:parallel")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Parallel Loops Using Package \protect\emph{parallel}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some computing tasks naturally lend themselves to parallel computing, like for example performing loops.
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}).
      \vskip1ex
      The function \texttt{mclapply()} performs loops (similar to \texttt{lapply()}) using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      Under \emph{Windows}, a cluster of \texttt{R} processes (one per each CPU core) need to be started first, by calling the function \texttt{makeCluster()}.
      \vskip1ex
      \emph{Mac-OSX} and \emph{Linux} don't require calling the function \texttt{makeCluster()}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define function that pauses execution
paws <- function(x, sleep_time=0.01) {
  Sys.sleep(sleep_time)
  x
}  # end paws
library(parallel)  # Load package parallel
# Calculate number of available cores
n_cores <- detectCores() - 1
# Initialize compute cluster under Windows
clus_ter <- makeCluster(n_cores)
# Perform parallel loop under Windows
paw_s <- parLapply(clus_ter, 1:10, paws)
# Perform parallel loop under Mac-OSX or Linux
paw_s <- mclapply(1:10, paws, mc.cores=n_cores)
library(microbenchmark)  # Load package microbenchmark
# Compare speed of lapply versus parallel computing
summary(microbenchmark(
  standard = lapply(1:10, paws),
  parallel = parLapply(clus_ter, 1:10, paws),
  times=10)
)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Computing Advantage of Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Parallel computing provides an increasing advantage for larger number of loop iterations. 
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
      \vskip1ex
      The function \texttt{plot()} by default plots a scatterplot, but can also plot lines using the argument \texttt{type="l"}.
      \vskip1ex
      The function \texttt{lines()} adds lines to a plot.
      \vskip1ex
      The function \texttt{legend()} adds a legend to a plot.
      <<echo=TRUE,eval=FALSE>>=
# Compare speed of lapply with parallel computing
iter_ations <- 3:10
compute_times <- sapply(iter_ations,
  function(max_iterations) {
    summary(microbenchmark(
      standard = lapply(1:max_iterations, paws),
      parallel = parLapply(clus_ter, 1:max_iterations, paws),
      times=10))[, 4]
    })  # end sapply
compute_times <- t(compute_times)
colnames(compute_times) <- c("standard", "parallel")
rownames(compute_times) <- iter_ations
# Stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/parallel_plot.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
x11(width=6, height=5)
plot(x=rownames(compute_times),
     y=compute_times[, "standard"],
     type="l", lwd=2, col="blue",
     main="Compute times",
     xlab="number of iterations in loop", ylab="",
     ylim=c(0, max(compute_times[, "standard"])))
lines(x=rownames(compute_times),
      y=compute_times[, "parallel"], lwd=2, col="green")
legend(x="topleft", legend=colnames(compute_times),
       inset=0.1, cex=1.0, bg="white",
       lwd=2, lty=1, col=c("blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Computing Over Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Very often we need to perform time consuming calculations over columns of matrices.
      \vskip1ex
      The function \texttt{parCapply()} performs an apply loop over columns of matrices using parallel computing on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # Load package parallel
# Calculate number of available cores
n_cores <- detectCores() - 1
# Initialize compute cluster under Windows
clus_ter <- makeCluster(n_cores)
# Calculate matrix of random data
mat_rix <- matrix(rnorm(1e5), ncol=100)
# Define aggregation function over column of matrix
agg_regate <- function(col_umn) {
  out_put <- 0
  for (in_dex in 1:NROW(col_umn))
    out_put <- out_put + col_umn[in_dex]
  out_put
}  # end agg_regate
# Perform parallel aggregations over columns of matrix
agg_regations <- parCapply(clus_ter, mat_rix, agg_regate)
# Compare speed of apply with parallel computing
summary(microbenchmark(
  ap_ply=apply(mat_rix, MARGIN=2, agg_regate),
  parl_apply=parCapply(clus_ter, mat_rix, agg_regate),
  times=10)
)[, c(1, 4, 5)]
# Stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Initializing Parallel Clusters Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under \emph{Windows} the child processes in the parallel compute cluster don't inherit data and objects from their parent process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      Objects from packages must be either referenced using the double-colon operator \texttt{"::"}, or the packages must be loaded in the child processes.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:5)),eval=FALSE>>=
library(parallel)  # Load package parallel
# Calculate number of available cores
n_cores <- detectCores() - 1
# Initialize compute cluster under Windows
clus_ter <- makeCluster(n_cores)
ba_se <- 2
# Fails because child processes don't know ba_se:
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# ba_se passed to child via dots ... argument:
parLapply(clus_ter, 2:4,
          function(exponent, ba_se) ba_se^exponent,
          ba_se=ba_se)
# ba_se passed to child via clusterExport:
clusterExport(clus_ter, "ba_se")
parLapply(clus_ter, 2:4,
          function(exponent) ba_se^exponent)
# Fails because child processes don't know zoo::index():
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(index(get(sym_bol, envir=rutils::etf_env))))
# zoo function referenced using "::" in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol)
            NROW(zoo::index(get(sym_bol, envir=rutils::etf_env))))
# Package zoo loaded in child process:
parSapply(clus_ter, c("VTI", "IEF", "DBC"),
          function(sym_bol) {
            stopifnot("package:zoo" %in% search() || require("zoo", quietly=TRUE))
            NROW(index(get(sym_bol, envir=rutils::etf_env)))
          })  # end parSapply
# Stop R processes over cluster under Windows
stopCluster(clus_ter)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reproducible Parallel Simulations Under \protect\emph{Windows}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations use pseudo-random number generators, and in order to perform reproducible results, they must set the \emph{seed} value, so that the number generators produce the same sequence of pseudo-random numbers.
      \vskip1ex
      The function \texttt{set.seed()} initializes the random number generator by specifying the \emph{seed} value, so that the number generator produces the same sequence of numbers for a given \emph{seed} value.
      \vskip1ex
      But under \emph{Windows} \texttt{set.seed()} doesn't initialize the random number generators of child processes, and they don't produce the same sequence of numbers.
      \vskip1ex
      The function \texttt{clusterSetRNGStream()} initializes the random number generators of child processes under \emph{Windows}.
      \vskip1ex
      The function \texttt{set.seed()} does initialize the random number generators of child processes under \emph{Mac-OSX} and \emph{Linux}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
# Calculate number of available cores
n_cores <- detectCores() - 1
# Initialize compute cluster under Windows
clus_ter <- makeCluster(n_cores)
# Set seed for cluster under Windows
# Doesn't work: set.seed(1121)
clusterSetRNGStream(clus_ter, 1121)
# Perform parallel loop under Windows
out_put <- parLapply(clus_ter, 1:70, rnorm, n=100)
sum(unlist(out_put))
# Stop R processes over cluster under Windows
stopCluster(clus_ter)
# Perform parallel loop under Mac-OSX or Linux
out_put <- mclapply(1:10, rnorm, mc.cores=n_cores, n=100)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Simulation}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{while()} loops are often used in simulations, when the number of required loops is unknown in advance.
      \vskip1ex
      Below is an example of a simulation of the path of \emph{Brownian Motion} crossing a barrier level.
        <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
bar_rier <- 20  # Barrier level
n_rows <- 1000  # Number of simulation steps
pa_th <- numeric(n_rows)  # Allocate path vector
pa_th[1] <- 0  # Initialize path
in_dex <- 2  # Initialize simulation index
while ((in_dex <= n_rows) && (pa_th[in_dex - 1] < bar_rier)) {
# Simulate next step
  pa_th[in_dex] <- pa_th[in_dex - 1] + rnorm(1)
  in_dex <- in_dex + 1  # Advance in_dex
}  # end while
# Fill remaining pa_th after it crosses bar_rier
if (in_dex <= n_rows)
  pa_th[in_dex:n_rows] <- pa_th[in_dex - 1]
# Plot the Brownian motion
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
plot(pa_th, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=3, col="red")
title(main="Brownian Motion Crossing a Barrier Level", line=0.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop.
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{while()} loops.
        <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
bar_rier <- 20  # Barrier level
n_rows <- 1000  # Number of simulation steps
# Simulate path of Brownian motion
pa_th <- cumsum(rnorm(n_rows))
# Find index when pa_th crosses bar_rier
cro_ss <- which(pa_th > bar_rier)
# Fill remaining pa_th after it crosses bar_rier
if (NROW(cro_ss)>0) {
  pa_th[(cro_ss[1]+1):n_rows] <- pa_th[cro_ss[1]]
}  # end if
# Plot the Brownian motion
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
plot(pa_th, type="l", col="black",
     lty="solid", lwd=2, xlab="", ylab="")
abline(h=bar_rier, lwd=3, col="red")
title(main="Brownian Motion Crossing a Barrier Level", line=0.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_barrier.png}
      The tradeoff between speed and memory usage: more memory may be used than necessary, since the simulation may stop before all the pre-computed random numbers are used up.
      \vskip1ex
      But the simulation is much faster because the path is simulated using \emph{vectorized} functions,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating the Statistics of Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The statistics of Brownian motion can be estimated by simulating multiple paths.
      \vskip1ex
      An example of a statistic is the expected value of Brownian motion at a fixed time horizon,  which is the option payout for the strike price $k$: $\mathbb{E}[(p_t - k)_{+}]$.
      \vskip1ex
      Another statistic is the probability of Brownian motion crossing a boundary (barrier) $b$: $\mathbb{E}[\mathbbm{1}(p_t - b)]$.
      <<echo=TRUE,eval=FALSE>>=
# Define Brownian motion parameters
sig_ma <- 1.0  # Volatility
dri_ft <- 0.0  # Drift
n_rows <- 1000  # Number of simulation steps
n_simu <- 100  # Number of simulations
# Simulate multiple paths of Brownian motion
set.seed(1121)
path_s <- rnorm(n_simu*n_rows, mean=dri_ft, sd=sig_ma)
path_s <- matrix(path_s, nc=n_simu)
path_s <- matrixStats::colCumsums(path_s)
# Final distribution of paths
mean(path_s[n_rows, ]) ; sd(path_s[n_rows, ])
# Calculate option payout
strik_e <- 50  # Strike price
pay_outs <- (path_s[n_rows, ] - strik_e)
sum(pay_outs[pay_outs > 0])/n_simu
# Calculate probability of crossing a barrier
bar_rier <- 50
cross_ed <- colSums(path_s > bar_rier) > 0
sum(cross_ed)/n_simu
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/simu_brown_multiple.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot in window
x11(width=6, height=5)
par(mar=c(4, 3, 2, 2), oma=c(0, 0, 0, 0), mgp=c(2.5, 1, 0))
# Select and plot full range of paths
or_der <- order(path_s[n_rows, ])
in_dex <- or_der[seq(1, 100, 9)]
zoo::plot.zoo(path_s[, in_dex], main="Paths of Brownian Motion",
  xlab="time steps", ylab=NA, plot.type="single")
abline(h=strik_e, col="red", lwd=3)
text(x=(n_rows-60), y=strik_e, labels="strike price", pos=3, cex=1)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Monte Carlo Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution.
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals.
      \vskip1ex
      The \emph{quantile} of a probability distribution is the value of the \emph{random variable} \texttt{x}, such that the probability of values less than \texttt{x} is equal to the given \emph{probability} $p$.
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} $p$.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean - MC estimate
mean(da_ta)
# Sample standard deviation - MC estimate
sd(da_ta)
# Monte Carlo estimate of cumulative probability
pnorm(1)
sum(da_ta < 1)/n_rows
# Monte Carlo estimate of quantile
conf_level <- 0.98
qnorm(conf_level)  # Exact value
cut_off <- conf_level*n_rows
da_ta <- sort(da_ta)
da_ta[cut_off]  # Naive Monte Carlo value
quantile(da_ta, probs=conf_level)
# Analyze the source code of quantile()
stats:::quantile.default
# Microbenchmark quantile
library(microbenchmark)
summary(microbenchmark(
  monte_carlo = da_ta[cut_off],
  quan_tile = quantile(da_ta, probs=conf_level),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} procedure uses \emph{Monte Carlo} simulation to generate a distribution of estimator values.
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed (empirical) data set.
      \vskip1ex
      If the original data consists of simulated random numbers then we simply simulate another set of these random numbers.
      \vskip1ex
      The \emph{bootstrapped} datasets are used to recalculate the estimator many times, to provide a distribution of the estimator and its standard error.
      \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Sample from Standard Normal Distribution
n_rows <- 1000; da_ta <- rnorm(n_rows)
# Sample mean and standard deviation
mean(da_ta); sd(da_ta)
# Bootstrap of sample mean and median
n_boot <- 10000
boot_data <- sapply(1:n_boot, function(x) {
  # Sample from Standard Normal Distribution
  sampl_e <- rnorm(n_rows)
  c(mean=mean(sampl_e), median=median(sampl_e))
})  # end sapply
boot_data[, 1:3]
boot_data <- t(boot_data)
# Standard error from formula
sd(da_ta)/sqrt(n_rows)
# Standard error of mean from bootstrap
sd(boot_data[, "mean"])
# Standard error of median from bootstrap
sd(boot_data[, "median"])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Estimators Using Bootstrap Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of estimators can be calculated using a \emph{bootstrap} simulation.
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed (empirical) data set.
      \vskip1ex
      The \emph{bootstrapped} dataset is used to recalculate the estimator many times.
      \vskip1ex
      The \emph{bootstrapped} estimator values are then used to calculate the probability distribution of the estimator and its standard error.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/boot_median.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot the densities of the bootstrap data
x11(width=6, height=5)
plot(density(boot_data[, "mean"]), lwd=3, xlab="Estimator Value",
     main="Distribution of Bootstrapped Mean and Median", col="green")
lines(density(boot_data[, "median"]), lwd=3, col="blue")
abline(v=mean(boot_data[, "mean"]), lwd=2, col="red")
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("mean", "median"), bty="n",
       lwd=6, bg="white", col=c("green", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping Using Vectorized Operations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrap simulations can be accelerated by using vectorized operations instead of \texttt{R} loops.       \vskip1ex
      But using vectorized operations requires calculating a matrix of random data, instead of calculating random vectors in a loop.
      \vskip1ex
      This is another example of the tradeoff between speed and memory usage in simulations.
      \vskip1ex
      Faster code often requires more memory than slower code.
      \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
n_rows <- 1000
# Bootstrap of sample mean and median
n_boot <- 100
boot_data <- sapply(1:n_boot, function(x) median(rnorm(n_rows)))
# Perform vectorized bootstrap
set.seed(1121)  # Reset random number generator
# Calculate matrix of random data
sampl_e <- matrix(rnorm(n_boot*n_rows), ncol=n_boot)
boot_vec <- Rfast::colMedians(sampl_e)
all.equal(boot_data, boot_vec)
# Compare speed of loops with vectorized R code
library(microbenchmark)
summary(microbenchmark(
  loop = sapply(1:n_boot, function(x) median(rnorm(n_rows))),
  cpp = {
    sampl_e <- matrix(rnorm(n_boot*n_rows), ncol=n_boot)
    Rfast::colMedians(sampl_e)
    },
  times=10))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping Standard Errors Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}).
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster under Windows
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
# Bootstrap mean and median under Windows
n_boot <- 10000
boot_data <- parLapply(clus_ter, 1:n_boot,
  function(x, da_ta, n_rows) {
  sampl_e <- rnorm(n_rows)
  c(mean=mean(sampl_e), median=median(sampl_e))
  }, da_ta=da_ta, n_rows=n_rows)  # end parLapply
# Bootstrap mean and median under Mac-OSX or Linux
boot_data <- mclapply(1:n_boot,
  function(x) {
  sampl_e <- rnorm(n_rows)
  c(mean=mean(sampl_e), median=median(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
# Standard error from formula
sd(da_ta)/sqrt(n_rows)
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Parallel Bootstrapping of the \protect\emph{Median Absolute Deviation}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
n_rows <- 1000
da_ta <- rnorm(n_rows)
sd(da_ta); mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
n_boot <- 10000
boot_data <- sapply(1:n_boot, function(x) {
  sampl_e <- rnorm(n_rows)
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
boot_data <- parLapply(clus_ter, 1:n_boot,
  function(x, da_ta) {
    sampl_e <- rnorm(n_rows)
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:n_boot, function(x) {
  sampl_e <- rnorm(n_rows)
  c(sd=sd(sampl_e), mad=mad(sampl_e))
}, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x) 
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Resampling From Empirical Datasets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Resampling is randomly selecting data from an existing dataset, to create a new dataset with similar properties to the existing dataset.
      \vskip1ex
      Resampling is usually performed with replacement, so that each draw is independent from the others.
      \vskip1ex
      Resampling is performed when it's not possible or convenient to obtain another set of empirical data, so we simulate a new data set by randomly sampling from the existing data.
      \vskip1ex
      The function \texttt{sample()} selects a random sample from a vector of data elements.
      \vskip1ex
      The function \texttt{sample.int()} is a \emph{method} that selects a random sample of \emph{integers}.
      \vskip1ex
      The function \texttt{sample.int()} with argument \texttt{replace=TRUE} selects a sample with replacement (the \emph{integers} can repeat).
      \vskip1ex
      The function \texttt{sample.int()} is a little faster than \texttt{sample()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate time series of VTI returns
library(rutils)
re_turns <- rutils::etf_env$re_turns$VTI
re_turns <- na.omit(re_turns)
n_rows <- NROW(re_turns)
# Sample from VTI returns
sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
c(sd=sd(sampl_e), mad=mad(sampl_e))
# sample.int() is a little faster than sample()
library(microbenchmark)
summary(microbenchmark(
  sample.int = sample.int(1e3), 
  sample = sample(1e3), 
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping From Empirical Datasets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping is usually performed by resampling from an observed (empirical) dataset.
      \vskip1ex
      Resampling consists of randomly selecting data from an existing dataset, with replacement.
      \vskip1ex
      Resampling produces a new \emph{bootstrapped} dataset with similar properties to the existing dataset.
      \vskip1ex
      The \emph{bootstrapped} dataset is used to recalculate the estimator many times.
      \vskip1ex
      The \emph{bootstrapped} estimator values are then used to calculate the probability distribution of the estimator and its standard error.
      \vskip1ex
      Bootstrapping shows that for asset returns, the \emph{Median Absolute Deviation} (\emph{MAD}) has a smaller relative standard error than the standard deviation.
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators which are sensitive to the ordering and correlations in the data.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Sample from time series of VTI returns
library(rutils)
re_turns <- rutils::etf_env$re_turns$VTI
re_turns <- na.omit(re_turns)
n_rows <- NROW(re_turns)
# Bootstrap sd and MAD under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster under Windows
clusterSetRNGStream(clus_ter, 1121)  # Reset random number generator in all cores
n_boot <- 10000
boot_data <- parLapply(clus_ter, 1:n_boot,
  function(x, re_turns, n_rows) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, re_turns=re_turns, n_rows=n_rows)  # end parLapply
# Bootstrap sd and MAD under Mac-OSX or Linux
boot_data <- mclapply(1:n_boot, function(x) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
boot_data <- rutils::do_call(rbind, boot_data)
# Standard error assuming normal distribution of returns
sd(re_turns)/sqrt(n_boot)
# Means and standard errors from bootstrap
std_errors <- apply(boot_data, MARGIN=2,
  function(x) c(mean=mean(x), std_error=sd(x)))
std_errors
# Relative standard errors
std_errors[2, ]/std_errors[1, ]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping From Time Series of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping from a time series of prices requires first converting the prices to \emph{percentage} returns, then bootstrapping the returns, and finally converting them back to prices.
      \vskip1ex
      Bootstrapping from \emph{percentage} returns ensures that the bootstrapped prices are not negative.
      \vskip1ex
      Below is a simulation of the frequency of bootstrapped prices crossing a barrier level.
      <<echo=TRUE,eval=FALSE>>=
# Calculate percentage returns from VTI prices
library(rutils)
price_s <- quantmod::Cl(rutils::etf_env$VTI)
star_t <- as.numeric(price_s[1, ])
re_turns <- rutils::diff_it(log(price_s))
class(re_turns); head(re_turns)
sum(is.na(re_turns))
n_rows <- NROW(re_turns)
# Define barrier level with respect to price_s
bar_rier <- 1.5*max(price_s)
# Calculate single bootstrap sample
sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
# Calculate prices from percentage returns
sampl_e <- star_t*exp(cumsum(sampl_e))
# Calculate if prices crossed barrier
sum(sampl_e > bar_rier) > 0
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster under Windows
# Perform parallel bootstrap under Windows
clusterSetRNGStream(clus_ter, 1121)  # Reset random number generator in all cores
clusterExport(clus_ter, c("star_t", "bar_rier"))
n_boot <- 10000
boot_data <- parLapply(clus_ter, 1:n_boot,
  function(x, re_turns, n_rows) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    # Calculate prices from percentage returns
    sampl_e <- star_t*exp(cumsum(sampl_e))
    # Calculate if prices crossed barrier
    sum(sampl_e > bar_rier) > 0
  }, re_turns=re_turns, n_rows=n_rows)  # end parLapply
# Perform parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:n_boot, function(x) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    # Calculate prices from percentage returns
    sampl_e <- star_t*exp(cumsum(sampl_e))
    # Calculate if prices crossed barrier
    sum(sampl_e > bar_rier) > 0
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
boot_data <- rutils::do_call(rbind, boot_data)
# Calculate frequency of crossing barrier
sum(boot_data)/n_boot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping From \protect\emph{OHLC} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Bootstrapping from \emph{OHLC} prices requires updating all the price columns, not just the \emph{Close} prices.
      \vskip1ex
      The \emph{Close} prices are bootstrapped first, and then the other columns are updated using the differences of the \emph{OHLC} price columns.
      \vskip1ex
      Below is a simulation of the frequency of the \emph{High} prices crossing a barrier level.
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate percentage returns from VTI prices
library(rutils)
oh_lc <- rutils::etf_env$VTI
price_s <- as.numeric(oh_lc[, 4])
star_t <- price_s[1]
re_turns <- rutils::diff_it(log(price_s))
n_rows <- NROW(re_turns)
# Calculate difference of OHLC price columns
ohlc_diff <- oh_lc[, 1:3] - price_s
class(re_turns); head(re_turns)
# Calculate bootstrap prices from percentage returns
da_ta <- sample.int(n_rows, replace=TRUE)
boot_prices <- star_t*exp(cumsum(re_turns[da_ta]))
boot_ohlc <- ohlc_diff + boot_prices
boot_ohlc <- cbind(boot_ohlc, boot_prices)
# Define barrier level with respect to price_s
bar_rier <- 1.5*max(price_s)
# Calculate if High bootstrapped prices crossed barrier level
sum(boot_ohlc[, 2] > bar_rier) > 0
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster under Windows
# Perform parallel bootstrap under Windows
clusterSetRNGStream(clus_ter, 1121)  # Reset random number generator in all cores
clusterExport(clus_ter, c("star_t", "bar_rier", "ohlc_diff"))
n_boot <- 10000
boot_data <- parLapply(clus_ter, 1:n_boot,
  function(x, re_turns, n_rows) {
    # Calculate OHLC prices from percentage returns
    da_ta <- sample.int(n_rows, replace=TRUE)
    boot_prices <- star_t*exp(cumsum(re_turns[da_ta]))
    boot_ohlc <- ohlc_diff + boot_prices
    boot_ohlc <- cbind(boot_ohlc, boot_prices)
    # Calculate statistic
    sum(boot_ohlc[, 2] > bar_rier) > 0
  }, re_turns=re_turns, n_rows=n_rows)  # end parLapply
# Perform parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:n_boot, function(x) {
    # Calculate OHLC prices from percentage returns
    da_ta <- sample.int(n_rows, replace=TRUE)
    boot_prices <- star_t*exp(cumsum(re_turns[da_ta]))
    boot_ohlc <- ohlc_diff + boot_prices
    boot_ohlc <- cbind(boot_ohlc, boot_prices)
    # Calculate statistic
    sum(boot_ohlc[, 2] > bar_rier) > 0
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
boot_data <- rutils::do_call(rbind, boot_data)
# Calculate frequency of crossing barrier
sum(boot_data)/n_boot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Regression Coefficients Using Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of the regression coefficients can be calculated using a \emph{bootstrap} simulation.
      \vskip1ex
      The \emph{bootstrap} procedure creates new design matrices by randomly sampling with replacement from the regression design matrix.
      \vskip1ex
      Regressions are performed on the \emph{bootstrapped} design matrices, and the regression coefficients are saved into a matrix of \emph{bootstrapped} coefficients.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Initialize random number generator
set.seed(1121)
# Define explanatory and response variables
predic_tor <- rnorm(100, mean=2)
noise <- rnorm(100)
res_ponse <- (-3 + predic_tor + noise)
de_sign <- cbind(res_ponse, predic_tor)
# Calculate alpha and beta regression coefficients
be_ta <- cov(de_sign[, 1], de_sign[, 2])/var(de_sign[, 2])
al_pha <- mean(de_sign[, 1]) - be_ta*mean(de_sign[, 2])
x11(width=6, height=5)
plot(res_ponse ~ predic_tor, data=de_sign)
abline(a=al_pha, b=be_ta, lwd=3, col="blue")
# Bootstrap of beta regression coefficient
n_boot <- 100
boot_data <- sapply(1:n_boot, function(x) {
  sampl_e <- sample.int(NROW(de_sign), replace=TRUE)
  de_sign <- de_sign[sampl_e, ]
  cov(de_sign[, 1], de_sign[, 2])/var(de_sign[, 2])
})  # end sapply
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Bootstrapped Regression Coefficients}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrapped} coefficient values can be used to calculate the probability distribution of the coefficients and their standard errors,
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \vskip1ex
      \texttt{abline()} plots a straight line on the existing plot.
      \vskip1ex
      The function \texttt{text()} draws text on a plot, and can be used to draw plot labels.
        <<eval=FALSE,echo=(-(1:2))>>=
x11(width=6, height=5)
par(oma=c(1, 2, 1, 0), mgp=c(2, 1, 0), mar=c(1, 1, 1, 1), cex.lab=0.8, cex.axis=1.0, cex.main=0.8, cex.sub=0.5)
# Mean and standard error of beta regression coefficient
c(mean=mean(boot_data), std_error=sd(boot_data))
# Plot density of bootstrapped beta coefficients
plot(density(boot_data), lwd=2, xlab="Regression slopes",
     main="Bootstrapped Regression Slopes")
# Add line for expected value
abline(v=mean(boot_data), lwd=2, col="red")
text(x=mean(boot_data)-0.01, y=1.0, labels="expected value",
     lwd=2, srt=90, pos=3)
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.5\paperwidth]{figure/boot_reg.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Bootstrapping Regressions Using Parallel Computing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      Different functions from package \emph{parallel} need to be called depending on the operating system (\emph{Windows}, \emph{Mac-OSX}, or \emph{Linux}).
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be passed into \texttt{parLapply()} via the dots \texttt{"..."} argument.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster under Windows
# Bootstrap of regression under Windows
boot_data <- parLapply(clus_ter, 1:1000,
  function(x, de_sign) {
    sampl_e <- sample.int(NROW(de_sign), replace=TRUE)
    de_sign <- de_sign[sampl_e, ]
    cov(de_sign[, 1], de_sign[, 2])/var(de_sign[, 2])
  }, de_sign=de_sign)  # end parLapply
# Bootstrap of regression under Mac-OSX or Linux
boot_data <- mclapply(1:1000,
  function(x) {
    sampl_e <- sample.int(NROW(de_sign), replace=TRUE)
    de_sign <- de_sign[sampl_e, ]
    cov(de_sign[, 1], de_sign[, 2])/var(de_sign[, 2])
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster under Windows
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Analyzing the Bootstrap Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bootstrap} loop produces a \emph{list} which can be collapsed into a vector.
      \vskip1ex
      The function \texttt{unlist()} collapses a list with atomic elements into a vector (which can cause type coercion).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Collapse the bootstrap list into a vector
class(boot_data)
boot_data <- unlist(boot_data)
# Mean and standard error of beta regression coefficient
c(mean=mean(boot_data), std_error=sd(boot_data))
# Plot density of bootstrapped beta coefficients
plot(density(boot_data),
     lwd=2, xlab="Regression slopes",
     main="Bootstrapped Regression Slopes")
# Add line for expected value
abline(v=mean(boot_data), lwd=2, col="red")
text(x=mean(boot_data)-0.01, y=1.0, labels="expected value",
     lwd=2, srt=90, pos=3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance Reduction Using Antithetic Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Variance reduction} are techniques for increasing the precision of Monte Carlo simulations.
      \vskip1ex
      \emph{Naive Monte Carlo} refers to \emph{Monte Carlo} simulation without using \emph{variance reduction} techniques.
      \vskip1ex
      \emph{Antithetic Sampling} is a \emph{variance reduction} technique in which a new random sample is computed from an existing sample, without generating new random numbers.
      \vskip1ex
      In the case of a \emph{Normal} random sample $\phi$, the new \emph{antithetic} sample is equal to minus the existing sample: $\phi_{new} = -\phi$.
      \vskip1ex
      In the case of a \emph{Uniform} random sample $\phi$, the new \emph{antithetic} sample is equal to $1$ minus the existing sample: $\phi_{new} = 1-\phi$.
      \vskip1ex
      \emph{Antithetic Sampling} doubles the number of independent samples, so it reduces the standard error by $\sqrt{2}$.
      \vskip1ex
      \emph{Antithetic Sampling} doesn't change any other parameters of the simulation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Estimate the 95% quantile
n_boot <- 10000
boot_data <- sapply(1:n_boot, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  quantile(sampl_e, 0.95)
})  # end sapply
sd(boot_data)
# Estimate the 95% quantile using antithetic sampling
boot_data <- sapply(1:n_boot, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  quantile(c(sampl_e, -sampl_e), 0.95)
})  # end sapply
# Standard error of quantile from bootstrap
sd(boot_data)
sqrt(2)*sd(boot_data)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Rare Events Using Probability Tilting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Rare events can be simulated more accurately by \emph{tilting} (deforming) their probability distribution, so that rare events occur more frequently.
      \vskip1ex
      A popular probability \emph{tilting} method is exponential (Esscher) tilting:
      \begin{displaymath}
        p(x, \lambda) = \frac{\exp(\lambda x) p(x)}{\int _{-\infty}^{\infty} {\exp(\lambda x) p(x)} dx}
      \end{displaymath}
      Where $p(x)$ is the probability density, $p(x, \lambda)$ is the tilted density, and $\lambda$ is the tilt parameter.
      \vskip1ex
      For the \emph{Normal} distribution $\phi(x) = \frac{\exp(-x^2/2)}{\sqrt{2\pi}}$, exponential tilting is equivalent to shifting the distribution by $\lambda$: $x \rightarrow x + \lambda$.
      \begin{align*}
        \phi(x, \lambda) = \frac{\exp(\lambda x) \exp(-x^2/2)}{\int _{-\infty}^{\infty} {\exp(\lambda x) \exp(-x^2/2)} dx} = \\
        \frac{\exp(-(x - \lambda)^2/2)}{\sqrt{2\pi}} = 
        \exp(x \lambda - \lambda^2/2) \cdot \phi(x, \lambda = 0)
      \end{align*}
      Shifting the random variable $x \rightarrow x + \lambda$ is equivalent to multiplying the distribution by the weight factor: $\exp(x \lambda - \lambda^2/2)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/norm_dist_shifted.png}\\
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot a Normal probability distribution
curve(expr=dnorm, xlim=c(-3, 4),
      main="Shifted Normal distribution function",
      xlab="", ylab="", lwd=3, col="blue")
# Add shifted Normal probability distribution
curve(expr=dnorm(x, mean=1), add=TRUE, lwd=3, col="red")
# Add vertical dashed lines
abline(v=0, lwd=3, col="blue", lty="dashed")
abline(v=1, lwd=3, col="red", lty="dashed")
arrows(x0=0, y0=0.1, x1=1, y1=0.1, lwd=3,
       code=2, angle=20, length=grid::unit(0.2, "cm"))
text(x=0.3, 0.1, labels=bquote(lambda), pos=3, cex=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance Reduction Using Importance Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Importance sampling} is a \emph{variance reduction} technique for simulating rare events more accurately.
      \vskip1ex
      The \emph{variance} of an estimate produced by simulation decreases with the number of events which contribute to the estimate: $\sigma^2 \propto \frac{1}{n}$.
      \vskip1ex
      \emph{Importance sampling} simulates rare events more frequently by \emph{tilting} the probability distribution, so that more events contribute to the estimate.
      \vskip1ex
      In standard Monte Carlo simulation, the simulated data points have equal probabilities.
      \vskip1ex
      But in \emph{importance sampling}, the simulated data must be weighted (multiplied) to compensate for the tilting of the probability.
      \vskip1ex
      The tilt weights are equal to the ratio of the base probability distribution divided by the tilted distribution, which for the \emph{Normal} distribution are equal to:
      \begin{displaymath}
        w_x = \frac{\phi(x, \lambda = 0)}{\phi(x, \lambda)} = \exp(-x \lambda + \lambda^2/2)
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
set.seed(1121) # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Cumulative probability from formula
quan_tile <- (-2)
pnorm(quan_tile)
integrate(dnorm, lower=-Inf, upper=quan_tile)
# Cumulative probability from Naive Monte Carlo
sum(da_ta < quan_tile)/n_rows
# Generate importance sample
lamb_da <- (-1.5)  # Tilt parameter
data_tilt <- da_ta + lamb_da  # Tilt the random numbers
# Cumulative probability from importance sample
sum(data_tilt < quan_tile)/n_rows
weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
sum((data_tilt < quan_tile)*weight_s)/n_rows
# Bootstrap of standard errors of cumulative probability
n_boot <- 1000
boot_data <- sapply(1:n_boot, function(x) {
  da_ta <- rnorm(n_rows)
  na_ive <- sum(da_ta < quan_tile)/n_rows
  da_ta <- (da_ta + lamb_da)
  weight_s <- exp(-lamb_da*da_ta + lamb_da^2/2)
  im_port <- sum((da_ta < quan_tile)*weight_s)/n_rows
  c(naive_mc=na_ive, importance=im_port)
}) # end sapply
apply(boot_data, MARGIN=1,
  function(x) c(mean=mean(x), sd=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating Quantiles Using Importance Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The quantiles can be calculated from the cumulative probabilities of the importance sample data.
      \vskip1ex
      The importance sample data points must be weighted to compensate for the tilting of the probability.
      \vskip1ex
      Importance sampling can be used to estimate the \emph{VaR} (\emph{quantile}) corresponding to a given \emph{confidence level}.
      \vskip1ex
      The standard error of the \emph{VaR} estimate using importance sampling can be several times smaller than that of \emph{naive Monte Carlo}.
      \vskip1ex
      The reduction of standard error is greater for higher \emph{confidence levels}.
      \vskip1ex
      \emph{Naive Monte Carlo} refers to \emph{Monte Carlo} simulation without using \emph{variance reduction} techniques.
      \vskip1ex
      The function \texttt{findInterval()} returns the indices of the intervals specified by \texttt{"vec"} that contain the elements of \texttt{"x"}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Quantile from Naive Monte Carlo
conf_level <- 0.02
qnorm(conf_level)  # Exact value
da_ta <- sort(da_ta)
cut_off <- n_rows*conf_level
da_ta[cut_off]  # Naive Monte Carlo value
# Importance sample weights
data_tilt <- da_ta + lamb_da  # Tilt the random numbers
weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
# Cumulative probabilities using importance sample
cum_prob <- cumsum(weight_s)/n_rows
# Quantile from importance sample
data_tilt[findInterval(conf_level, cum_prob)]
# Bootstrap of standard errors of quantile
n_boot <- 1000
boot_data <- sapply(1:n_boot, function(x) {
  da_ta <- sort(rnorm(n_rows))
  na_ive <- da_ta[cut_off]
  data_tilt <- da_ta + lamb_da
  weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
  cum_prob <- cumsum(weight_s)/n_rows
  im_port <- data_tilt[findInterval(conf_level, cum_prob)]
  c(naive_mc=na_ive, importance=im_port)
}) # end sapply
apply(boot_data, MARGIN=1,
  function(x) c(mean=mean(x), sd=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating \protect\emph{CVaR} Using Importance Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Importance sampling can be used to estimate the Conditional Value at Risk (\emph{CVaR}) corresponding to a given \emph{confidence level}.
      \vskip1ex
      First the \emph{VaR} (\emph{quantile}) is estimated, and then the \emph{expected value} (\emph{CVaR}) is estimated using it.
      \vskip1ex
      The standard error of the \emph{CVaR} estimate using importance sampling can be several times smaller than that of \emph{naive Monte Carlo}.
      \vskip1ex
      The reduction of standard error is greater for higher \emph{confidence levels}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# CVaR from Naive Monte Carlo
va_r <- da_ta[cut_off]
sum((da_ta < va_r)*da_ta)/sum((da_ta < va_r))
# CVaR from importance sample
va_r <- data_tilt[findInterval(conf_level, cum_prob)]
sum((data_tilt < va_r)*data_tilt*weight_s)/sum((data_tilt < va_r)*weight_s)
# CVaR from integration
integrate(function(x) x*dnorm(x), low=-Inf, up=va_r)$value/pnorm(va_r)
# Bootstrap of standard errors of expected value
n_boot <- 1000
boot_data <- sapply(1:n_boot, function(x) {
  da_ta <- sort(rnorm(n_rows))
  va_r <- da_ta[cut_off]
  na_ive <- sum((da_ta < va_r)*da_ta)/sum((da_ta < va_r))
  data_tilt <- da_ta + lamb_da
  weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
  cum_prob <- cumsum(weight_s)/n_rows
  va_r <- data_tilt[findInterval(conf_level, cum_prob)]
  im_port <- sum((data_tilt < va_r)*data_tilt*weight_s)/sum((data_tilt < va_r)*weight_s)
  c(naive_mc=na_ive, importance=im_port)
}) # end sapply
apply(boot_data, MARGIN=1,
  function(x) c(mean=mean(x), sd=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Optimal Tilt Parameter for Importance Sampling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The tilt parameter $\lambda$ should be chosen to minimize the standard error of the estimator.
      \vskip1ex
      The optimal tilt parameter depends on the estimator and on the required confidence level.
      \vskip1ex
      More tilting is needed at higher confidence levels, to provide enough significant data points.
      \vskip1ex
      When performing a loop over the tilt parameters, the same matrix of random data can be used for different tilt parameters.
      \vskip1ex
      The function \texttt{Rfast::sort\_mat()} sorts the columns of a matrix using very fast \texttt{C++} code.
      <<echo=TRUE,eval=FALSE>>=
# Calculate matrix of random data
set.seed(1121) # Reset random number generator
n_rows <- 1000; n_boot <- 100
da_ta <- matrix(rnorm(n_boot*n_rows), ncol=n_boot)
da_ta <- Rfast::sort_mat(da_ta)  # Sort the columns
# Calculate vector of quantiles for tilt parameter
conf_level <- 0.02; cut_off <- conf_level*n_rows
calc_quant <- function(lamb_da) {
  data_tilt <- da_ta + lamb_da  # Tilt the random numbers
  weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
  # Calculate quantiles for columns
  sapply(1:n_boot, function(boo_t) {
    cum_prob <- cumsum(weight_s[, boo_t])/n_rows
    data_tilt[findInterval(conf_level, cum_prob), boo_t]
  })  # end sapply
}  # end calc_quant
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/optim_tilt.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define vector of tilt parameters
lambda_s <- seq(-3.0, -1.2, by=0.2)
# Calculate vector of quantiles for tilt parameters
quantile_s <- sapply(lambda_s, calc_quant)
# Calculate standard deviations of quantiles for tilt parameters
std_devs <- apply(quantile_s, MARGIN=2, sd)
# Calculate the optimal tilt parameter
lambda_s[which.min(std_devs)]
# Plot the standard deviations
x11(width=6, height=5)
plot(x=lambda_s, y=std_devs, 
     main="Standard Deviations of Simulated Quantiles",
     xlab="tilt parameter", ylab="standard deviation",
     type="l", col="blue", lwd=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Importance Sampling for Binomial Variables}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability $p$ of a binomial variable can be tilted to $p(\lambda)$ as follows:
      \begin{displaymath}
        p(\lambda) = \frac{\lambda p}{1 + p (\lambda - 1)}
      \end{displaymath}
      Where $\lambda$ is the tilt parameter.
      \vskip1ex
      The weight is equal to the ratio of the base probability divided by the tilted probability:
      \begin{displaymath}
        w = \frac{1 + p (\lambda - 1)}{\lambda}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Binomial sample
n_rows <- 1000
pro_b <- 0.1
da_ta <- rbinom(n=n_rows, size=1, pro_b)
head(da_ta, 33)
fre_q <- sum(da_ta)/n_rows
# Tilted binomial sample
lamb_da <- 5
p_tilted <- lamb_da*pro_b/(1 + pro_b*(lamb_da - 1))
weigh_t <- (1 + pro_b*(lamb_da - 1))/lamb_da
da_ta <- rbinom(n=n_rows, size=1, p_tilted)
head(da_ta, 33)
weigh_t*sum(da_ta)/n_rows
# Bootstrap of standard errors
n_boot <- 1000
boot_data <- sapply(1:n_boot, function(x) {
  c(naive_mc=sum(rbinom(n=n_rows, size=1, pro_b))/n_rows,
    importance=weigh_t*sum(rbinom(n=n_rows, size=1, p_tilted))/n_rows)
}) # end sapply
apply(boot_data, MARGIN=1,
  function(x) c(mean=mean(x), sd=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Importance Sampling of Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The statistics that depend on extreme paths of Brownian motion can be simulated more accurately using \emph{importance sampling}. 
      \vskip1ex
      The normally distributed variables $x_i$ are shifted by the tilt parameter $\lambda$ to obtain the importance sample variables $x^{tilt}_i$: $x^{tilt}_i = x_i + \lambda$.
      \vskip1ex
      The Brownian paths $p_t$ are equal to the cumulative sums of the tilted variables $x^{tilt}_i$: $p_t = \sum_{i=1}^t x^{tilt}_i$.
      \vskip1ex
      Each tilted Brownian path has an associated weight factor equal to the product: $\prod_{i=1}^t \exp(- x^{tilt}_i \lambda + \lambda^2/2)$.
      \vskip1ex
      To compensate for the probability tilting, the statistics derived from the tilted Brownian paths must be multiplied by their weight factors.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define Brownian motion parameters
sig_ma <- 1.0  # Volatility
dri_ft <- 0.0  # Drift
n_rows <- 100  # Number of simulation steps
n_simu <- 10000  # Number of simulations
# Calculate matrix of normal variables
set.seed(1121)
da_ta <- rnorm(n_simu*n_rows, mean=dri_ft, sd=sig_ma)
da_ta <- matrix(da_ta, nc=n_simu)
# Simulate paths of Brownian motion
path_s <- matrixStats::colCumsums(da_ta)
# Tilt the da_ta
lamb_da <- 0.04  # Tilt parameter
data_tilt <- da_ta + lamb_da  # Tilt the random numbers
paths_tilt <- matrixStats::colCumsums(data_tilt)
# Calculate path weights
weight_s <- exp(-lamb_da*data_tilt + lamb_da^2/2)
path_weights <- matrixStats::colProds(weight_s)
# Or
path_weights <- exp(-lamb_da*colSums(data_tilt) + n_rows*lamb_da^2/2)
# Calculate option payout using standard MC
strik_e <- 10  # Strike price
pay_outs <- (path_s[n_rows, ] - strik_e)
sum(pay_outs[pay_outs > 0])/n_simu
# Calculate option payout using importance sampling
pay_outs <- (paths_tilt[n_rows, ] - strik_e)
sum((path_weights*pay_outs)[pay_outs > 0])/n_simu
# Calculate crossing probability using standard MC
bar_rier <- 10
cross_ed <- colSums(path_s > bar_rier) > 0
sum(cross_ed)/n_simu
# Calculate crossing probability using importance sampling
cross_ed <- colSums(paths_tilt > bar_rier) > 0
sum(path_weights*cross_ed)/n_simu
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Market Databases}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Index Constituent Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The file \texttt{sp500.RData} contains the \emph{environment} \texttt{sp500\_env} with \emph{OHLC} prices and trading volumes of \emph{S\&P500} stock index constituents.
      \vskip1ex
      The \emph{S\&P500} stock index constituent data is of poor quality before \texttt{2000}, so we'll mostly use the data after the year \texttt{2000}.
      <<echo=TRUE,eval=FALSE>>=
# Load S&P500 constituent stock prices
load("/Users/jerzy/Develop/lecture_slides/data/sp500.RData")
price_s <- eapply(sp500_env, quantmod::Cl)
price_s <- rutils::do_call(cbind, price_s)
# Carry forward non-NA prices
price_s <- zoo::na.locf(price_s, na.rm=FALSE)
# Drop ".Close" from column names
colnames(price_s[, 1:4])
colnames(price_s) <- rutils::get_name(colnames(price_s))
# Or
# colnames(price_s) <- do.call(rbind,
#   strsplit(colnames(price_s), split="[.]"))[, 1]
# Calculate percentage returns of the S&P500 constituent stocks
# re_turns <- xts::diff.xts(log(price_s))
re_turns <- xts::diff.xts(price_s)/
  rutils::lag_it(price_s, pad_zeros=FALSE)
set.seed(1121)
sam_ple <- sample(NCOL(re_turns), s=100, replace=FALSE)
prices_100 <- price_s[, sam_ple]
returns_100 <- re_turns[, sam_ple]
save(price_s, prices_100,
  file="/Users/jerzy/Develop/lecture_slides/data/sp500_prices.RData")
save(re_turns, returns_100,
  file="/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_without_prices.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate number of constituents without prices
da_ta <- rowSums(is.na(price_s))
da_ta <- xts::xts(da_ta, order.by=index(price_s))
dygraphs::dygraph(da_ta, main="Number of S&P 500 Constituents Without Prices") %>%
  dyOptions(colors="blue", strokeWidth=2) %>%
  dyAxis("y", valueRange=c(0, 300))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{S\&P500} Stock Portfolio Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The price-weighted index of \emph{S\&P500} constituents closely follows the VTI \emph{ETF}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate price weighted index of constituent
n_cols <- NCOL(price_s)
in_dex <- xts(rowSums(price_s)/n_cols, index(price_s))
colnames(in_dex) <- "index"
# Combine index with VTI
da_ta <- cbind(in_dex[index(etf_env$VTI)], etf_env$VTI[, 4])
col_names <- c("index", "VTI")
colnames(da_ta) <- col_names
# Plot index with VTI
dygraphs::dygraph(da_ta,
  main="S&P 500 Price-weighted Index and VTI") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="red") %>%
  dySeries(name=col_names[2], axis="y2", col="blue")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.5\paperwidth]{figure/sp500_portfolio_index.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{ETF} Database}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Exchange-traded Funds (\emph{ETFs}) are funds which invest in portfolios of assets, such as stocks, commodities, or bonds.
      \vskip1ex
      \emph{ETFs} are shares in portfolios of assets, and they are traded just like stocks.
      \vskip1ex
      \emph{ETFs} provide investors with convenient, low cost, and liquid instruments to invest in various portfolios of assets.
      \vskip1ex
      The file \texttt{etf\_list.csv} contains a database of exchange-traded funds (\emph{ETFs}) and exchange traded notes (\emph{ETNs}).
      \vskip1ex
      We will select a portfolio of \emph{ETFs} for illustrating various investment strategies.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=TRUE,size="tiny">>=
# Select ETF symbols for asset allocation
sym_bols <- c("VTI", "VEU", "EEM", "XLY", "XLP", "XLE", "XLF",
  "XLV", "XLI", "XLB", "XLK", "XLU", "VYM", "IVW", "IWB", "IWD",
  "IWF", "IEF", "TLT", "VNQ", "DBC", "GLD", "USO", "VXX", "SVXY",
  "MTUM", "IVE", "VLUE", "QUAL", "VTV", "USMV")
# Read etf database into data frame
etf_list <- read.csv(file="/Users/jerzy/Develop/lecture_slides/data/etf_list.csv")
rownames(etf_list) <- etf_list$Symbol
# Select from etf_list only those ETF's in sym_bols
etf_list <- etf_list[sym_bols, ]
# Shorten names
etf_names <- sapply(etf_list$Name, function(name) {
  name_split <- strsplit(name, split=" ")[[1]]
  name_split <- name_split[c(-1, -NROW(name_split))]
  name_match <- match("Select", name_split)
  if (!is.na(name_match))
    name_split <- name_split[-name_match]
  paste(name_split, collapse=" ")
})  # end sapply
etf_list$Name <- etf_names
etf_list["IEF", "Name"] <- "10 year Treasury Bond Fund"
etf_list["TLT", "Name"] <- "20 plus year Treasury Bond Fund"
etf_list["XLY", "Name"] <- "Consumer Discr. Sector Fund"
etf_list["EEM", "Name"] <- "Emerging Market Stock Fund"
etf_list["MTUM", "Name"] <- "Momentum Factor Fund"
etf_list["SVXY", "Name"] <- "Short VIX Futures"
etf_list["VXX", "Name"] <- "Long VIX Futures"
etf_list["DBC", "Name"] <- "Commodity Futures Fund"
etf_list["USO", "Name"] <- "WTI Oil Futures Fund"
etf_list["GLD", "Name"] <- "Physical Gold Fund"
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{ETF} Portfolio for Investment Strategies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio contains \emph{ETFs} representing different \emph{industry sectors} and \emph{investment styles}.
      \vskip1ex
      The \emph{ETFs} with names \emph{X*} represent industry \emph{sector funds} (energy, financial, etc.)
      \vskip1ex
      The \emph{ETFs} with names \emph{I*} represent \emph{style funds} (value, growth, size).
      \vskip1ex
      \emph{IWB} is the Russell 1000 small-cap fund.
      \vskip1ex
      \emph{MTUM} is an \emph{ETF} which owns a stock portfolio representing the \emph{momentum factor}.
      \vskip1ex
      \emph{DBC} is an \emph{ETF} providing the total return on a portfolio of commodity futures.
      \vskip1ex
      \emph{VXX} is an \emph{ETN} providing the total return of \emph{long VIX} futures contracts (specifically the \emph{S\&P} VIX Short-Term Futures Index).
      \vskip1ex
      \emph{VXX} is \emph{bearish} because it's \emph{long} VIX futures, and the VIX \emph{rises} when stock prices \emph{drop}.
      \vskip1ex
      \emph{SVXY} is an \emph{ETF} providing the total return of \emph{short VIX} futures contracts.
      \vskip1ex
      \emph{SVXY} is \emph{bullish} because it's \emph{short} VIX futures, and the VIX \emph{drops} when stock prices \emph{rise}.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=FALSE,eval=TRUE,results='asis'>>=
print(xtable::xtable(etf_list), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Exchange Traded Notes (\protect\emph{ETNs})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{ETNs} are similar to \emph{ETFs}, with the difference that \emph{ETFs} are shares in a fund which owns the underlying assets, while \emph{ETNs} are notes from issuers which promise payouts according to a formula tied to the underlying asset.
      \vskip1ex
      \emph{ETFs} are similar to mutual funds, while \emph{ETNs} are similar to corporate bonds.
      \vskip1ex
      \emph{ETNs} are technically unsecured corporate debt, but instead of fixed coupons, they promise to provide returns on a market index or futures contract.
      \vskip1ex
      The \emph{ETN} issuer promises the payout and is responsible for tracking the index.
      \vskip1ex
      The \emph{ETN} investor has counterparty credit risk to the \emph{ETN} issuer.
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Modeling and Fitting Asset Returns}


%%%%%%%%%%%%%%%
\subsection{Kernel Density of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The kernel density is proportional to the number of data points close to a given point.
      \vskip1ex
      The kernel density is analogous to a histogram, but it provides more detailed information about the distribution of the data.
      \vskip1ex
      The smoothing kernel $K(x)$ is a symmetric function which decreases with the distance $x$.
      \vskip1ex
      The kernel density $d_i$ of a data sample $r_i$ is equal to the sum over the kernel function $K(x)$:
      \begin{displaymath}
        d_i = \sum_{j=1}^n {K(r_i - r_j)}
      \end{displaymath}
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \vskip1ex
      The parameter \emph{smoothing bandwidth} is the standard deviation of the smoothing kernel $K(x)$. 
      \vskip1ex
      The function \texttt{density()} returns a vector of densities at equally spaced points, not for the original data points.
      \vskip1ex
      The function \texttt{approx()} interpolates a vector of data into another vector.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)  # Load package rutils
# Calculate VTI percentage returns
re_turns <- rutils::etf_env$re_turns$VTI
re_turns <- drop(coredata(na.omit(re_turns)))
n_rows <- NROW(re_turns)
# Mean and standard deviation of returns
c(mean(re_turns), sd(re_turns))
# Calculate the smoothing bandwidth as the MAD of returns 10 points apart
re_turns <- sort(re_turns)
b_w <- 10*mad(rutils::diff_it(re_turns, lagg=10))
# Calculate the kernel density
den_sity <- sapply(1:n_rows, function(i_d) {
  sum(dnorm(re_turns-re_turns[i_d], sd=b_w))
})  # end sapply
ma_d <- mad(re_turns)
plot(re_turns, den_sity, xlim=c(-5*ma_d, 5*ma_d),
     t="l", col="blue", lwd=3,
     xlab="returns", ylab="density",
     main="Density of VTI Returns")
# Calculate the kernel density using density()
den_sity <- density(re_turns, bw=b_w)
NROW(den_sity$y)
x11(width=6, height=5)
plot(den_sity, xlim=c(-5*ma_d, 5*ma_d),
     xlab="returns", ylab="density",
     col="blue", lwd=3, main="Density of VTI Returns")
# Interpolate the den_sity vector into re_turns
den_sity <- approx(den_sity$x, den_sity$y, xout=re_turns)
all.equal(den_sity$x, re_turns)
plot(den_sity, xlim=c(-5*ma_d, 5*ma_d),
     xlab="returns", ylab="density",
     t="l", col="blue", lwd=3,
     main="Density of VTI Returns")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are usually not normally distributed and they exhibit \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
      \vskip1ex
      The function \texttt{lines()} draws a line through specified points.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hist_vti_dens.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram
histo_gram <- hist(re_turns, breaks=100, freq=FALSE,
  xlim=c(-5*ma_d, 5*ma_d), xlab="", ylab="",
  main="VTI Return Distribution")
# Draw kernel density of histogram
lines(den_sity, col="red", lwd=2)
# Add density of normal distribution
curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)),
      add=TRUE, lwd=2, col="blue")
# Add legend
legend("topright", inset=0.05, cex=0.8, title=NULL,
       leg=c("VTI", "Normal"), bty="n",
       lwd=6, bg="white", col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Quantile-Quantile Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{Quantile-Quantile} (\emph{Q-Q}) plot is a plot of points with the same \emph{quantiles}, from two probability distributions.
      \vskip1ex
      If the two distributions are similar then all the points in the \emph{Q-Q} plot lie along the diagonal.
      \vskip1ex
      The \emph{VTI} \emph{Q-Q} plot shows that the \emph{VTI} return distribution has fat tails.
      \vskip1ex
      The \emph{p}-value of the \emph{Shapiro-Wilk} test is very close to zero, which shows that the \emph{VTI} returns are very unlikely to be normal.
      \vskip1ex
      The function \texttt{qqnorm()} produces a normal \emph{Q-Q} plot.
      \vskip1ex
      The function \texttt{qqline()} fits a line to the normal quantiles.
      <<echo=TRUE,eval=FALSE>>=
# Create normal Q-Q plot
qqnorm(re_turns, ylim=c(-0.1, 0.1), main="VTI Q-Q Plot",
       xlab="Normal Quantiles")
# Fit a line to the normal quantiles
qqline(re_turns, col="red", lwd=2)
# Perform Shapiro-Wilk test
shapiro.test(as.numeric(re_turns))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/qq_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Distributions of Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Box-and-whisker plots (\emph{boxplots}) are graphical representations of a distribution of values.
      \vskip1ex
      The bottom and top box edges (\emph{hinges}) are equal to the first and third quartiles, and the \emph{box} width is equal to the interquartile range (\emph{IQR}).
      \vskip1ex
      The nominal range is equal to 1.5 times the \emph{IQR} above and below the box \emph{hinges}.
      \vskip1ex
      The \emph{whiskers} are dashed vertical lines representing values beyond the first and third quartiles, but within the nominal range.
      \vskip1ex
      The \emph{whiskers} end at the last values within the nominal range, while the open circles represent outlier values beyond the nominal range.
      \vskip1ex
      The function \texttt{boxplot()} has two \texttt{methods}: one for \texttt{formula} objects (for categorical variables), and another for \texttt{data frames}.
      <<box_plots,eval=FALSE>>=
# Boxplot method for formula
boxplot(formula=mpg ~ cyl, data=mtcars,
        main="Mileage by number of cylinders",
        xlab="Cylinders", ylab="Miles per gallon")
# Boxplot method for data frame of EuStockMarkets percentage returns
boxplot(x=diff(log(EuStockMarkets)))
      @
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.45\paperwidth]{figure/box_plots-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Moments of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution are given by:
      \vskip1ex
      Sample mean: $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}] = \mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma] = \sigma$
      \vskip1ex
      The sample skewness (third moment):
      \begin{displaymath}
        \varsigma = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      \end{displaymath}
      The sample kurtosis (fourth moment):
      \begin{displaymath}
        \kappa = \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{displaymath}
      The normal distribution has skewness equal to $0$ and kurtosis equal to $3$.
      \vskip1ex
      Stock returns typically have negative skewness and kurtosis much greater than $3$.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Number of observations
n_rows <- NROW(re_turns)
# Mean of VTI returns
mean_rets <- mean(re_turns)
# Standard deviation of VTI returns
sd_rets <- sd(re_turns)
# Skewness of VTI returns
n_rows/((n_rows-1)*(n_rows-2))*
  sum(((re_turns - mean_rets)/sd_rets)^3)
# Kurtosis of VTI returns
n_rows*(n_rows+1)/((n_rows-1)^3)*
  sum(((re_turns - mean_rets)/sd_rets)^4)
# Random normal returns
re_turns <- rnorm(n_rows, sd=sd_rets)
# Mean and standard deviation of random normal returns
mean_rets <- mean(re_turns)
sd_rets <- sd(re_turns)
# Skewness of random normal returns
n_rows/((n_rows-1)*(n_rows-2))*
  sum(((re_turns - mean_rets)/sd_rets)^3)
# Kurtosis of random normal returns
n_rows*(n_rows+1)/((n_rows-1)^3)*
  sum(((re_turns - mean_rets)/sd_rets)^4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functions for Calculating Skew and Kurtosis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} provides an easy way for users to write functions.
      \vskip1ex
      The function \texttt{calc\_skew()} calculates the skew of returns, and \texttt{calc\_kurt()} calculates the kurtosis.
      \vskip1ex
      Functions return the value of the last expression that is evaluated.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# calc_skew() calculates skew of returns
calc_skew <- function(re_turns) {
  re_turns <- na.omit(re_turns)
  sum(((re_turns - mean(re_turns))/sd(re_turns))^3)/NROW(re_turns)
}  # end calc_skew
# calc_kurt() calculates kurtosis of returns
calc_kurt <- function(re_turns) {
  re_turns <- na.omit(re_turns)
  sum(((re_turns - mean(re_turns))/sd(re_turns))^4)/NROW(re_turns)
}  # end calc_kurt
# Calculate skew and kurtosis of VTI returns
calc_skew(re_turns)
calc_kurt(re_turns)
# calc_mom() calculates the moments of returns
calc_mom <- function(re_turns, mo_ment=3) {
  re_turns <- na.omit(re_turns)
  sum(((re_turns - mean(re_turns))/sd(re_turns))^mo_ment)/NROW(re_turns)
}  # end calc_mom
# Calculate skew and kurtosis of VTI returns
calc_mom(re_turns, mo_ment=3)
calc_mom(re_turns, mo_ment=4)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
set.seed(1121)  # Reset random number generator
# Sample from Standard Normal Distribution
n_rows <- 1000
da_ta <- rnorm(n_rows)
# Sample mean
mean(da_ta)
# Sample standard deviation
sd(da_ta)
# Standard error of sample mean
sd(da_ta)/sqrt(n_rows)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Normal (Gaussian)} probability density function is given by:
      \begin{displaymath}
        \phi(x, \mu, \sigma) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      The \emph{Standard Normal} distribution $\phi(0, 1)$ is a special case of the \emph{Normal} $\phi(\mu, \sigma)$ with $\mu=0$ and $\sigma=1$.
      \vskip1ex
      The function \texttt{dnorm()} calculates the \emph{Normal} probability density.
      <<echo=TRUE,eval=FALSE>>=
x_var <- seq(-5, 7, length=100)
y_var <- dnorm(x_var, mean=1.0, sd=2.0)
plot(x_var, y_var, type="l", lty="solid", xlab="", ylab="")
title(main="Normal Density Function", line=0.5)
star_t <- 3; fin_ish <- 5  # Set lower and upper bounds
# Plot polygon area
are_a <- ((x_var >= star_t) & (x_var <= fin_ish))
polygon(c(star_t, x_var[are_a], fin_ish),
        c(-1, y_var[are_a], -1), col="red")
      @
    \column{0.5\textwidth}
    \includegraphics[width=0.45\paperwidth]{figure/norm_dist}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Plots of several \emph{Normal} distributions with different values of $\sigma$, using the function \texttt{curve()} for plotting functions given by their name.
      <<norm_dist_mult_curves,eval=FALSE,echo=(-(1:1)),fig.show="hide">>=
par(mar=c(7, 2, 1, 2), mgp=c(2, 1, 0), cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
sig_mas <- c(0.5, 1, 1.5, 2)  # Sigma values
# Create plot colors
col_ors <- c("red", "black", "blue", "green")
# Create legend labels
lab_els <- paste("sigma", sig_mas, sep="=")
for (in_dex in 1:4) {  # Plot four curves
  curve(expr=dnorm(x, sd=sig_mas[in_dex]),
        xlim=c(-4, 4), xlab="", ylab="", lwd=2,
        col=col_ors[in_dex], add=as.logical(in_dex-1))
}  # end for
# Add title
title(main="Normal Distributions", line=0.5)
# Add legend
legend("topright", inset=0.05, title="Sigmas",
       lab_els, cex=0.8, lwd=2, lty=1, bty="n", col=col_ors)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/norm_dist_mult_curves-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_{1},\ldots , z_{\nu}$ be independent standard normal random variables, with sample mean: $\bar{z}=\frac{1}{\nu} \sum_{i=1}^{\nu} z_i$ ($\mathbb{E}[\bar{z}]=\mu$) and sample variance: $\hat\sigma^2=\frac{1}{\nu-1} \sum_{i=1}^{\nu} (z_i-\bar{z})^2$
      \vskip1ex
      Then the random variable (\emph{t-ratio}):
      \begin{displaymath}
        t = \frac{\bar{z} - \mu}{\hat\sigma / \sqrt{\nu}}
      \end{displaymath}
      Follows the \emph{t-distribution} with $\nu$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu}\,\Gamma(\nu/2)}\, (1 + t^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
deg_free <- c(3, 6, 9)  # Df values
col_ors <- c("black", "red", "blue", "green")
lab_els <- c("normal", paste("df", deg_free, sep="="))
# Plot a Normal probability distribution
curve(expr=dnorm, xlim=c(-4, 4), xlab="", ylab="", lwd=2)
for (in_dex in 1:3) {  # Plot three t-distributions
  curve(expr=dt(x, df=deg_free[in_dex]), xlab="", ylab="",
        lwd=2, col=col_ors[in_dex+1], add=TRUE)
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_mult.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="t-distributions", line=0.5)
# Add legend
legend("topright", inset=0.05, bty="n",
       title="Degrees\n of freedom", lab_els,
       cex=0.8, lwd=6, lty=1, col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mixture Models of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Mixture models} are produced by randomly sampling data from different distributions.
      \vskip1ex
      The mixture of two normal distributions with different variances produces a distribution with \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      Student's \emph{t-distribution} has fat tails because the sample variance in the denominator of the \emph{t-ratio} is variable.
      \vskip1ex
      The time-dependent volatility of asset returns is referred to as \emph{heteroskedasticity}.
      \vskip1ex
      Random processes with \emph{heteroskedasticity} can be considered a type of mixture model.
      \vskip1ex
      The \emph{heteroskedasticity} produces \emph{leptokurtosis} (large kurtosis, or fat tails).
      <<echo=TRUE,eval=FALSE>>=
# Mixture of two normal distributions with sd=1 and sd=2
n_rows <- 1e5
re_turns <- c(rnorm(n_rows/2), 2*rnorm(n_rows/2))
re_turns <- (re_turns-mean(re_turns))/sd(re_turns)
# Kurtosis of normal
calc_kurt(rnorm(n_rows))
# Kurtosis of mixture
calc_kurt(re_turns)
# Or
n_rows*sum(re_turns^4)/(n_rows-1)^2
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/mix_normal.png}
      \vspace{-2em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot the distributions
plot(density(re_turns), xlab="", ylab="",
  main="Mixture of Normal Returns",
  xlim=c(-3, 3), type="l", lwd=3, col="red")
curve(expr=dnorm, lwd=2, col="blue", add=TRUE)
curve(expr=dt(x, df=3), lwd=2, col="green", add=TRUE)
# Add legend
legend("topright", inset=0.05, lty=1, lwd=6, bty="n",
  legend=c("Mixture", "Normal", "t-distribution"),
  col=c("red", "blue", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Likelihood Function of Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The non-standard Student's \emph{t-distribution} is:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \sigma \, \Gamma(\nu/2)} \, (1 + (\frac{t - \mu}{\sigma})^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
       It has non-zero mean equal to the location parameter $\mu$, and a standard deviation proportional to the scale parameter $\sigma$.
      \vskip1ex
      The negative logarithm of the probability density is equal to:
      \begin{multline*}
        -\log(f(t)) = -\log(\frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \Gamma(\nu/2)}) + \log(\sigma) + \\
        \frac{\nu+1}{2} \, \log(1 + (\frac{t - \mu}{\sigma})^2/\nu)
      \end{multline*}
      The \emph{likelihood} function $\mathcal{L}(\theta|\bar{x})$ is a function of the model parameters $\theta$, given the observed values $\bar{x}$, under the model's probability distribution $f(x|\theta)$:
      \begin{displaymath}
        \mathcal{L}(\theta|x) = \prod_{i=1}^{n} f(x_i|\theta)
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Objective function is log-likelihood
likeli_hood <- function(pa_r, free_dom, da_ta) {
  sum(
    -log(gamma((free_dom+1)/2) /
      (sqrt(pi*free_dom) * gamma(free_dom/2))) +
    log(pa_r[2]) +
    (free_dom+1)/2 * log(1 + ((da_ta - pa_r[1])/
                            pa_r[2])^2/free_dom))
}  # end likeli_hood
# Demonstrate equivalence with log(dt())
likeli_hood(c(1, 0.5), 2, 2:5)
-sum(log(dt(x=(2:5-1)/0.5, df=2)/0.5))
# Simpler objective function
likeli_hood <- function(pa_r, free_dom, da_ta) {
  -sum(log(dt(x=(da_ta-pa_r[1])/pa_r[2],
              df=free_dom)/pa_r[2]))
}  # end likeli_hood
      @
      \vspace{-1em}
      The \emph{likelihood} function measures how \emph{likely} are the parameters, given the observed values $\bar{x}$.
      \vskip1ex
      The \emph{maximum-likelihood} estimate (\emph{MLE}) of the parameters are those that maximize the \emph{likelihood} function:
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
      \end{displaymath}
      In practice the logarithm of the \emph{likelihood} $\log(\mathcal{L})$ is maximized, instead of the \emph{likelihood} itself.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Asset Returns into Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution to a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{fitdistr()} performs a \emph{maximum likelihood} optimization to find the non-standardized Student's \emph{t-distribution} location and scale parameters.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Initial parameters
par_init <- c(mean=0, scale=0.01)
# Fit distribution using optim()
optim_fit <- optim(par=par_init,
  fn=likeli_hood, # Log-likelihood function
  da_ta=re_turns,
  free_dom=2, # Degrees of freedom
  method="L-BFGS-B", # quasi-Newton method
  upper=c(1, 0.1), # upper constraint
  lower=c(-1, 1e-7)) # Lower constraint
# optimal parameters
lo_cation <- optim_fit$par["mean"]
scal_e <- optim_fit$par["scale"]
# Fit VTI returns using MASS::fitdistr()
optim_fit <- MASS::fitdistr(re_turns,
  densfun="t", df=2)
optim_fit$estimate
optim_fit$sd
lo_cation <- optim_fit$estimate[1]
scal_e <- optim_fit$estimate[2]
summary(optim_fit)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Student's \protect\emph{t-distribution} Fitted to Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns typically exhibit \emph{negative skewness} and \emph{large kurtosis} (leptokurtosis), or fat tails.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot histogram of VTI returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", breaks=100, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE, main="VTI Returns Histogram")
lines(density(re_turns, adjust=1.5), lwd=3, col="blue")
# Plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(re_turns),
  sd=sd(re_turns)), add=TRUE, lwd=3, col="green")
# Plot t-distribution function
curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
      type="l", lwd=3, col="red", add=TRUE)
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("density", "t-distr", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_rets.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Leptokurtosis Fat Tails of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability under the \emph{normal} distribution decreases exponentially for large values of $x$:
      \begin{displaymath}
        \phi(x) \propto e^{-{x^2/2\sigma^2}} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a normal variable can be thought of as the sum of a large number of independent binomial variables of equal size.
      \vskip1ex
      So large values are produced only when all the contributing binomial variables are of the same sign, which is very improbable, so it produces extremely low tail probabilities (thin tails),
      \vskip1ex
      But in reality, the probability of large negative asset returns decreases much slower, as the negative power of the returns (fat tails).
      \vskip1ex
      The probability under Student's \emph{t-distribution} decreases as a power for large values of $x$:
      \begin{displaymath}
        f(x) \propto {\left| x \right|}^{-(\nu+1)} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a \emph{t-variable} can be thought of as the sum of normal variables with different volatilities (different sizes).
    \column{0.5\textwidth}
      \includegraphics[width=0.4\paperwidth]{figure/t_dist_tail_rets.png}
      \vspace{-2em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Plot histogram of VTI returns
histo_gram <- hist(re_turns, breaks=100, plot=FALSE)
plot(histo_gram, xlab="returns", ylab="frequency",
     col="lightgrey", freq=FALSE, main="VTI Left Tail Returns Histogram",
     xlim=c(min(re_turns), -0.02),
     ylim=c(0.0, histo_gram$density[findInterval(-0.02, histo_gram$breaks)]))
lines(density(re_turns, adjust=1.5), lwd=4, col="blue")
# Plot t-distribution function
curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e, type="l", lwd=4, col="red", add=TRUE)
# Plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)), add=TRUE, lwd=4, col="green")
# Add legend
legend("topleft", inset=0.05, bty="n",
  leg=c("density", "t-distr", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Volumes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling average trading volumes have increased significantly since the 2008 crisis, mostly because of high frequency trading (HFT).
      \vskip1ex
      Higher levels of volatility coincide with higher \emph{trading volumes}.
      \vskip1ex
      The time-dependent volatility of asset returns (\emph{heteroskedasticity}) produces their fat tails (\emph{leptokurtosis}).
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns and trading volumes
oh_lc <- rutils::etf_env$VTI
clos_e <- drop(coredata(quantmod::Cl(oh_lc)))
re_turns <- rutils::diff_it(log(clos_e))
vol_ume <- coredata(quantmod::Vo(oh_lc))
# Calculate rolling variance
look_back <- 121
vari_ance <- HighFreq::roll_var_ohlc(log(oh_lc), method="close", look_back=look_back, scale=FALSE)
vari_ance[1:look_back, ] <- vari_ance[look_back+1, ]
# Calculate rolling average volume
volume_roll <- HighFreq::roll_vec(vol_ume, look_back=look_back)/look_back
# dygraph plot of VTI variance and trading volumes
da_ta <- xts::xts(cbind(vari_ance, volume_roll), index(oh_lc))
col_names <- c("variance", "volume")
colnames(da_ta) <- col_names
dygraphs::dygraph(da_ta, main="VTI Variance and Trading Volumes") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], strokeWidth=2, axis="y", col="blue") %>%
  dySeries(name=col_names[2], strokeWidth=2, axis="y2", col="red")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/volume_volat_dyg.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Asset Returns in Trading Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time-dependent volatility of asset returns (\emph{heteroskedasticity}) produces their fat tails (\emph{leptokurtosis}).
      \vskip1ex
      If asset returns were measured at fixed intervals of \emph{trading volumes} (\emph{trading time} instead of clock time), then the volatility would be lower and less time-dependent.
      \vskip1ex
      The asset returns can be adjusted to \emph{trading time} by dividing them by the \emph{square root of the trading volumes}, to obtain scaled returns over equal trading volumes.
      \vskip1ex
      The scaled returns have a more positive \emph{skewness} and a smaller \emph{kurtosis} than unscaled returns.
      <<echo=TRUE,eval=FALSE>>=
# Scale returns using volume (volume clock)
rets_scaled <- ifelse(vol_ume > 0, sqrt(volume_roll)*re_turns/sqrt(vol_ume), 0)
rets_scaled <- sd(re_turns)*rets_scaled/sd(rets_scaled)
# rets_scaled <- ifelse(vol_ume > 1e4, re_turns/vol_ume, 0)
# Calculate moments of scaled returns
n_rows <- NROW(re_turns)
sapply(list(re_turns=re_turns, rets_scaled=rets_scaled),
  function(rets) {sapply(c(skew=3, kurt=4),
           function(x) sum((rets/sd(rets))^x)/n_rows)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/vti_scaled.png}
      <<echo=TRUE,eval=FALSE>>=
# x11(width=6, height=5)
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
# Plot densities of SPY returns
ma_d <- mad(re_turns)
# b_w <- mad(rutils::diff_it(re_turns))
plot(density(re_turns, bw=ma_d/10), xlim=c(-5*ma_d, 5*ma_d),
     lwd=3, mgp=c(2, 1, 0), col="blue",
     xlab="returns (standardized)", ylab="frequency",
     main="Density of Volume-scaled VTI Returns")
lines(density(rets_scaled, bw=ma_d/10), lwd=3, col="red")
curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)),
      add=TRUE, lwd=3, col="green")
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("unscaled", "scaled", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
quartz.save("figure/vti_scaled.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Shapiro-Wilk} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Shapiro-Wilk} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        W = \frac {(\sum_{i=1}^n a_i x_{(i)})^2} {\sum_{i=1}^n (x_i-\bar{x})^2}
      \end{displaymath}
      Where the: $\{a_1, \ldots, a_n\}$ are proportional to the \emph{order statistics} of random variables from the normal distribution.
      \vskip1ex
      $x_{(k)}$ is the \emph{k}-th \emph{order statistic}, and is equal to the \emph{k}-th smallest value in the sample: $\{x_1, \ldots, x_n\}$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic follows its own distribution, and is less than or equal to $1$.
      \vskip1ex
      The \emph{Shapiro-Wilk} statistic is close to $1$ for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
# Calculate VTI percentage returns
library(rutils)
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
# Reset output digits
dig_its <- options(digits=5)
# Shapiro-Wilk test for normal distribution
shapiro.test(rnorm(NROW(re_turns)))
# Shapiro-Wilk test for VTI returns
shapiro.test(as.numeric(re_turns))
# Shapiro-Wilk test for uniform distribution
shapiro.test(runif(NROW(re_turns)))
# Restore output digits
options(digits=dig_its$digits)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Jarque-Bera} Test of Normality}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Jarque-Bera} test is designed to test the \emph{null hypothesis} that a sample: $\{x_1, \ldots, x_n\}$ is from a normally distributed population.
      \vskip1ex
      The test statistic is equal to:
      \begin{displaymath}
        JB = \frac{n}{6} (\varsigma^2 + \frac{1}{4} (\kappa - 3)^2)
      \end{displaymath}
      Where the \emph{skewness} and \emph{kurtosis} are defined as:
      \begin{align*}
        \varsigma = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      &&
        \kappa = \frac{1}{n} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{align*}
      The \emph{Jarque-Bera} statistic asymptotically follows the \emph{chi-squared} distribution with two degrees of freedom.
      \vskip1ex
      The \emph{Jarque-Bera} statistic is small for samples from normal distributions.
      \vskip1ex
      The \emph{p}-value for \emph{VTI} returns is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and the \emph{VTI} returns are not from a normally distributed population.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=TRUE>>=
library(tseries)  # Load package tseries
# Jarque-Bera test for normal distribution
jarque.bera.test(rnorm(NROW(re_turns)))
# Jarque-Bera test for VTI returns
jarque.bera.test(re_turns)
# Jarque-Bera test for uniform distribution
jarque.bera.test(runif(NROW(re_turns)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} performs the \emph{Kolmogorov-Smirnov} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The second argument is either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a probability distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# KS test for normal distribution
ks.test(rnorm(100), pnorm)
# KS test for uniform distribution
ks.test(runif(100), pnorm)
# KS test for two similar normal distributions
ks.test(rnorm(100), rnorm(100, mean=0.1))
# KS test for two different normal distributions
ks.test(rnorm(100), rnorm(100, mean=1.0))
# Fit t-dist into VTI returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
optim_fit <- MASS::fitdistr(re_turns, densfun="t", df=2)
lo_cation <- optim_fit$estimate[1]
scal_e <- optim_fit$estimate[2]
# Perform Kolmogorov-Smirnov test on VTI returns
da_ta <- lo_cation + scal_e*rt(NROW(re_turns), df=2)
ks.test(as.numeric(re_turns), da_ta)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Chi-squared} Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_1, \ldots , z_k$ be independent standard \emph{Normal} random variables.
      \vskip1ex
      Then the random variable $X = \sum_{i=1}^k z^2_i$ is distributed according to the \emph{Chi-squared} distribution with $k$ degrees of freedom: $X \sim \chi_k^2$, and its probability density function is given by:
      \begin{displaymath}
        f(x) = \frac{x^{k/2-1}\,e^{-x/2}}{2^{k/2}\, \Gamma(k/2)}
      \end{displaymath}
      \vskip1ex
      The \emph{Chi-squared} distribution with $k$ degrees of freedom has mean equal to $k$ and variance equal to $2k$.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
# Degrees of freedom
deg_free <- c(2, 5, 8, 11)
# Plot four curves in loop
col_ors <- c("red", "black", "blue", "green")
for (in_dex in 1:4) {
  curve(expr=dchisq(x, df=deg_free[in_dex]),
        xlim=c(0, 20), ylim=c(0, 0.3),
        xlab="", ylab="", col=col_ors[in_dex],
        lwd=2, add=as.logical(in_dex-1))
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chisq_dist_mult.png}\\
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Add title
title(main="Chi-squared Distributions", line=0.5)
# Add legend
lab_els <- paste("df", deg_free, sep="=")
legend("topright", inset=0.05, bty="n",
       title="Degrees of freedom", lab_els,
       cex=0.8, lwd=6, lty=1, col=col_ors)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Chi-squared} Test for the Goodness of Fit}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Goodness of Fit} tests are designed to test if a set of observations fits an assumed theoretical probability distribution.
      \vskip1ex
      The \emph{Chi-squared} test tests if a frequency of counts fits the specified distribution.
      \vskip1ex
      The \emph{Chi-squared} statistic is the sum of squared differences between the observed frequencies $o_i$ and the theoretical frequencies $p_i$:
      \begin{displaymath}
        \chi^2 = N \sum_{i=1}^{n} {\frac{(o_i - p_i )^2}{p_i}}
      \end{displaymath}
      Where $N$ is the total number of observations.
      \vskip1ex
      The \emph{null hypothesis} is that the observed frequencies are consistent with the theoretical distribution.
      \vskip1ex
      The function \texttt{chisq.test()} performs the \emph{Chi-squared} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} in the function \texttt{hist()} should be chosen large enough to capture the shape of the frequency distribution.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Observed frequencies from random normal data
histo_gram <- hist(rnorm(1e3, mean=0), breaks=100, plot=FALSE)
freq_o <- histo_gram$counts
# Theoretical frequencies
freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
# Perform Chi-squared test for normal data
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
# Return p-value
chisq_test <- chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
chisq_test$p.value
# Observed frequencies from shifted normal data
histo_gram <- hist(rnorm(1e3, mean=2), breaks=100, plot=FALSE)
freq_o <- histo_gram$counts/sum(histo_gram$counts)
# Theoretical frequencies
freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
# Perform Chi-squared test for shifted normal data
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
# Calculate histogram of VTI returns
histo_gram <- hist(re_turns, breaks=100, plot=FALSE)
freq_o <- histo_gram$counts
# Calculate cumulative probabilities and then difference them
freq_t <- pt((histo_gram$breaks-lo_cation)/scal_e, df=2)
freq_t <- rutils::diff_it(freq_t)
# Perform Chi-squared test for VTI returns
chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Risk and Performance Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PerformanceAnalytics} for Risk and Performance Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{PerformanceAnalytics} contains functions for risk and performance analysis.
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package.
      \vskip1ex
      \texttt{managers} is an \emph{xts} time series containing monthly percentage returns of six asset managers (HAM1 through HAM6), the EDHEC Long-Short Equity hedge fund index, the \texttt{S\&P 500}, and US Treasury 10-year bond and 3-month bill total returns.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package PerformanceAnalytics
library(PerformanceAnalytics)
# Get documentation for package PerformanceAnalytics
# Get short description
packageDescription("PerformanceAnalytics")
# Load help page
help(package="PerformanceAnalytics")
# List all objects in PerformanceAnalytics
ls("package:PerformanceAnalytics")
# List all datasets in PerformanceAnalytics
data(package="PerformanceAnalytics")
# Remove PerformanceAnalytics from search path
detach("package:PerformanceAnalytics")
      @
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
perf_data <- unclass(data(
    package="PerformanceAnalytics"))$results[, -(1:2)]
apply(perf_data, 1, paste, collapse=" - ")
# Load "managers" data set
data(managers)
class(managers)
dim(managers)
head(managers, 3)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plots of Cumulative Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.CumReturns()} from package \emph{PerformanceAnalytics} plots the cumulative returns of a time series of returns.
      <<echo=TRUE,eval=FALSE>>=
# Load package "PerformanceAnalytics"
library(PerformanceAnalytics)
# Calculate ETF returns
re_turns <- rutils::etf_env$re_turns[, c("VTI", "DBC", "IEF")]
re_turns <- na.omit(re_turns)
# Plot cumulative ETF returns
x11(width=6, height=5)
chart.CumReturns(re_turns, lwd=2, ylab="",
  legend.loc="topleft", main="ETF Cumulative Returns")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_cum_returns.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Histogram()} from package \emph{PerformanceAnalytics} plots the histogram (frequency distribution) and the density of returns.
      <<echo=TRUE,eval=FALSE>>=
re_turns <- rutils::etf_env$re_turns$VTI
re_turns <- na.omit(re_turns)
x11(width=6, height=5)
chart.Histogram(re_turns, xlim=c(-0.04, 0.04),
  colorset = c("lightgray", "red", "blue"), lwd=3,
  main=paste("Distribution of", colnames(re_turns), "Returns"),
  methods = c("add.density", "add.normal"))
legend("topright", inset=0.05, bty="n",
       leg=c("VTI Density", "Normal"),
       lwd=6, lty=1, col=c("red", "blue"))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/returns_histogram.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Boxplot()} from package \emph{PerformanceAnalytics} plots a box-and-whisker plot for a distribution of returns.
      \vskip1ex
      The function \texttt{chart.Boxplot()} is a wrapper and calls the function \texttt{graphics::boxplot()} to plot the box plots.
      \vskip1ex
      A \emph{box plot} (box-and-whisker plot) is a graphical display of a distribution of data: \\
      The \emph{box} represents the upper and lower quartiles, \\
      The vertical lines (whiskers) represent values beyond the quartiles, \\
      Open circles represent values beyond the nominal range (outliers).
      <<echo=TRUE,eval=FALSE>>=
re_turns <- rutils::etf_env$re_turns[,
  c("VTI", "IEF", "IVW", "VYM", "IWB", "DBC", "VXX")]
x11(width=6, height=5)
chart.Boxplot(names=FALSE, re_turns)
par(cex.lab=0.8, cex.axis=0.8)
axis(side=2, at=(1:NCOL(re_turns))/7.5-0.05,labels=colnames(re_turns))
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_box_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation Estimator of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate normally distributed data
n_rows <- 1000
da_ta <- rnorm(n_rows)
sd(da_ta)
mad(da_ta)
median(abs(da_ta - median(da_ta)))
median(abs(da_ta - median(da_ta)))/qnorm(0.75)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Analyze bootstrapped variance
head(boot_data)
sum(is.na(boot_data))
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x)
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
boot_data <- parLapply(clus_ter, 1:10000,
  function(x, da_ta) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, da_ta=da_ta)  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000, function(x) {
    sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x)
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI returns
re_turns <- rutils::etf_env$re_turns$VTI
re_turns <- na.omit(re_turns)
n_rows <- NROW(re_turns)
sd(re_turns)
mad(re_turns)
# Bootstrap of sd and mad estimators
boot_data <- sapply(1:10000, function(x) {
  sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
  c(sd=sd(sampl_e), mad=mad(sampl_e))
})  # end sapply
boot_data <- t(boot_data)
# Means and standard errors from bootstrap
100*apply(boot_data, MARGIN=2, function(x)
  c(mean=mean(x), std_error=sd(x)))
# Parallel bootstrap under Windows
library(parallel)  # Load package parallel
n_cores <- detectCores() - 1  # Number of cores
clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
clusterExport(clus_ter, c("n_rows", "re_turns"))
boot_data <- parLapply(clus_ter, 1:10000,
  function(x) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  })  # end parLapply
# Parallel bootstrap under Mac-OSX or Linux
boot_data <- mclapply(1:10000, function(x) {
    sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
    c(sd=sd(sampl_e), mad=mad(sampl_e))
  }, mc.cores=n_cores)  # end mclapply
stopCluster(clus_ter)  # Stop R processes over cluster
boot_data <- rutils::do_call(rbind, boot_data)
# Means and standard errors from bootstrap
apply(boot_data, MARGIN=2, function(x)
  c(mean=mean(x), std_error=sd(x)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Downside Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some investors argue that positive returns don't represent risk, only those returns less than the target rate of return $r_t$.
      \vskip1ex
      The \emph{Downside Deviation} (semi-deviation) $\sigma_{d}$ is equal to the standard deviation of returns less than the target rate of return $r_t$:
      \begin{displaymath}
        \sigma_{d} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ([r_i-r_t]_{-})^2}
      \end{displaymath}
      The function \texttt{DownsideDeviation()} from package \emph{PerformanceAnalytics} calculates the downside deviation, for either the full time series (\texttt{method="full"}) or only for the subseries less than the target rate of return $r_t$ (\texttt{method="subset"}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(PerformanceAnalytics)
# Define target rate of return of 50 bps
tar_get <- 0.005
# Calculate the full downside returns
returns_sub <- (re_turns - tar_get)
returns_sub <- ifelse(returns_sub < 0, returns_sub, 0)
n_rows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/n_rows),
  drop(DownsideDeviation(re_turns, MAR=tar_get, method="full")))
# Calculate the subset downside returns
returns_sub <- (re_turns - tar_get)
returns_sub <- returns_sub[returns_sub < 0]
n_rows <- NROW(returns_sub)
# Calculate the downside deviation
all.equal(sqrt(sum(returns_sub^2)/n_rows),
  DownsideDeviation(re_turns, MAR=tar_get, method="subset"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{drawdown} is the drop in prices from their historical peak, and is equal to the difference between the prices minus the cumulative maximum of the prices.
      \vskip1ex
      \emph{Drawdown risk} determines the risk of liquidation due to stop loss limits.
      \vskip1ex
      The function \texttt{table.Drawdowns()} from package \emph{PerformanceAnalytics} calculates a data frame of drawdowns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate time series of VTI drawdowns
clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
draw_downs <- (clos_e - cummax(clos_e))
# PerformanceAnalytics plot of VTI drawdowns
re_turns <- rutils::diff_it(log(clos_e))
PerformanceAnalytics::chart.Drawdown(re_turns,
  ylab="", main="VTI Drawdowns")
# PerformanceAnalytics table of VTI drawdowns
PerformanceAnalytics::table.Drawdowns(re_turns)
# dygraph plot of VTI drawdowns
da_ta <- cbind(clos_e, draw_downs)
col_names <- c("VTI", "Drawdowns")
colnames(da_ta) <- col_names
dygraphs::dygraph(da_ta, main="VTI Drawdowns") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], valueRange=c(min(da_ta[, "Drawdowns"]), 5), independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", col="blue") %>%
  dySeries(name=col_names[2], axis="y2", col="red")
# Plot VTI drawdowns
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
x11(width=6, height=5)
quantmod::chart_Series(x=draw_downs, name="VTI Drawdowns", theme=plot_theme)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/drawdown_plot.png}
      <<echo=FALSE,eval=TRUE,size="tiny",results='asis'>>=
library(xtable)
library(PerformanceAnalytics)
clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
re_turns <- rutils::diff_it(log(clos_e))
# Calculate table of VTI drawdowns
tabl_e <- PerformanceAnalytics::table.Drawdowns(re_turns)
# Convert dates to strings
tabl_e <- cbind(sapply(tabl_e[, 1:3], as.character), tabl_e[, 4:7])
# Print table of VTI drawdowns
print(xtable(tabl_e), comment=FALSE, size="tiny", include.rownames=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{PerformanceSummary} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{charts.PerformanceSummary()} from package \emph{PerformanceAnalytics} plots three charts: cumulative returns, return bars, and drawdowns, for time series of returns.
      <<echo=(-(1:1)),eval=FALSE>>=
# Load "managers" data set
data(managers)
charts.PerformanceSummary(ham_1,
  main="", lwd=2, ylog=TRUE)
      @
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.45\paperwidth]{figure/performance_summary-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Loss Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns has a long left tail of negative returns representing the risk of loss.
      \vskip1ex
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$.
      \vskip1ex
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
conf_level <- 0.1
va_r <- quantile(re_turns, conf_level)
c_var <- mean(re_turns[re_turns < va_r])
# Plot histogram of VTI returns
histo_gram <- hist(re_turns, col="lightgrey",
  xlab="returns", ylab="frequency", breaks=100,
  xlim=c(-0.05, 0.01), freq=FALSE, main="VTI Returns Histogram")
# Calculate density
densi_ty <- density(re_turns, adjust=1.5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_var.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot density
lines(densi_ty, lwd=3, col="blue")
# Plot line for VaR
abline(v=va_r, col="red", lwd=3)
text(x=va_r, y=20, labels="VaR", lwd=2, srt=90, pos=2)
# Plot polygon shading for CVaR
var_max <- -0.06
rang_e <- (densi_ty$x < va_r) &  (densi_ty$x > var_max)
polygon(c(var_max, densi_ty$x[rang_e], va_r),
  c(0, densi_ty$y[rang_e], 0), col=rgb(1, 0, 0,0.5), border=NA)
text(x=va_r, y=3, labels="CVaR", lwd=2, pos=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk (\protect\emph{VaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$:
      \begin{displaymath}
        \alpha = \int_{-\infty}^{\mathrm{VaR}(\alpha)} \operatorname{f}(r) \, \mathrm{d}r
      \end{displaymath}
      Where $\operatorname{f}(r)$ is the probability density (distribution) of returns.
      \vskip1ex
      At a high confidence level, the value of $\mathrm{VaR}$ is subject to estimation error, and various numerical methods are used to approximate it.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      The function \texttt{VaR()} from package \emph{PerformanceAnalytics} calculates the \emph{Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# VTI percentage returns
re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
conf_level <- 0.02
# Calculate VaR as quantile
va_r <- quantile(re_turns, conf_level)
# Or by sorting
sort_ed <- sort(as.numeric(re_turns))
in_dex <- round(conf_level*NROW(re_turns))
va_r <- sort_ed[in_dex]
# PerformanceAnalytics VaR
PerformanceAnalytics::VaR(re_turns,
  p=(1-conf_level), method="historical")
all.equal(unname(va_r),
  as.numeric(PerformanceAnalytics::VaR(re_turns,
  p=(1-conf_level), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk (\protect\emph{CVaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the \emph{Expected Shortfall} (\emph{ES}), or the Expected Tail Loss (\emph{ETL}).
      \vskip1ex
      The function \texttt{ETL()} from package \emph{PerformanceAnalytics} calculates the \emph{Conditional Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 2, 1, 0), oma=c(0, 0, 0, 0))
# Calculate VaR as quantile
va_r <- quantile(re_turns, conf_level)
# Calculate CVaR as expected loss
c_var <- mean(re_turns[re_turns < va_r])
# Or by sorting
sort_ed <- sort(as.numeric(re_turns))
in_dex <- round(conf_level*NROW(re_turns))
va_r <- sort_ed[in_dex]
c_var <- mean(sort_ed[1:in_dex])
# PerformanceAnalytics VaR
PerformanceAnalytics::ETL(re_turns,
  p=(1-conf_level), method="historical")
all.equal(c_var,
  as.numeric(PerformanceAnalytics::ETL(re_turns,
  p=(1-conf_level), method="historical")))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Return Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Stats()} from package \emph{PerformanceAnalytics} calculates a data frame of risk and return statistics of the return distributions.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the risk-return statistics
risk_ret <-
  PerformanceAnalytics::table.Stats(rutils::etf_env$re_turns)
class(risk_ret)
# Transpose the data frame
risk_ret <- as.data.frame(t(risk_ret))
# Add Name column
risk_ret$Name <- rownames(risk_ret)
# Add Sharpe ratio column
risk_ret$Sharpe <- risk_ret$"Arithmetic Mean"/risk_ret$Stdev
# Sort on Sharpe ratio
risk_ret <- risk_ret[order(risk_ret$Sharpe, decreasing=TRUE), ]
      @
    \column{0.5\textwidth}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Copy from rutils to save time
risk_ret <- rutils::etf_env$risk_return
# Add Sharpe ratio column
risk_ret$Sharpe <- risk_ret$"Arithmetic Mean"/risk_ret$Stdev
# Sort on Sharpe ratio
risk_ret <- risk_ret[order(risk_ret$Sharpe, decreasing=TRUE), ]
# Print data frame
knitr::kable(risk_ret[, c("Sharpe", "Skewness", "Kurtosis")])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Risk and Return Preferences}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investors typically prefer larger \emph{odd moments} of the return distribution (mean, skewness), and smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      But positive skewness is often associated with lower returns, which can be observed in the \emph{VIX} volatility ETFs, \emph{VXX} and \emph{SVXY}.
      \vskip1ex
      The \emph{VXX} ETF is long the \emph{VIX} index (effectively long an option), so it has positive skewness and small kurtosis, but negative returns (it's short market risk).
      \vskip1ex
      Since the \emph{VXX} is effectively long an option, it pays option premiums so it has negative returns most of the time, with isolated periods of positive returns when markets drop.
      \vskip1ex
      The \emph{SVXY} ETF is short the \emph{VIX} index, so it has negative skewness and large kurtosis, but positive returns (it's long market risk).
      \vskip1ex
      Since the \emph{SVXY} is effectively short an option, it earns option premiums so it has positive returns most of the time, but it suffers sharp losses when markets drop.
    \column{0.5\textwidth}
    \vspace{1em}
      <<echo=FALSE,eval=TRUE,size="tiny">>=
# Print data frame
knitr::kable(risk_ret[c("VXX", "SVXY"), c("Sharpe", "Skewness", "Kurtosis")])
      @
      \includegraphics[width=0.45\paperwidth]{figure/vix_vxx_svxy.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of VTI drawdowns
price_s <- na.omit(rutils::etf_env$price_s[, c("VXX", "SVXY")])
price_s <- price_s["2017/"]
col_names <- c("VXX", "SVXY")
colnames(price_s) <- col_names
dygraphs::dygraph(price_s, main="Prices of VXX and SVXY") %>%
  dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
  dySeries(name=col_names[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="green") %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness and Return Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Similarly to the \emph{VXX} and \emph{SVXY}, for most other ETFs positive skewness is often associated with lower returns.
      \vskip1ex
      Some of the exceptions are bond ETFs (like \emph{IEF}), which have both non-negative skewness and positive returns.
      \vskip1ex
      Another exception are commodity ETFs (like \emph{USO} oil), which have both negative skewness and negative returns.
      <<echo=TRUE,eval=FALSE>>=
# Remove VIX volatility ETF data
risk_ret <- risk_ret[-match(c("VXX", "SVXY"), risk_ret$Name), ]
# Plot scatterplot of Sharpe vs Skewness
plot(Sharpe ~ Skewness, data=risk_ret,
     ylim=1.1*range(risk_ret$Sharpe),
     main="Sharpe vs Skewness")
# Add labels
text(x=risk_ret$Skewness, y=risk_ret$Sharpe,
          labels=risk_ret$Name, pos=3, cex=0.8)
# Plot scatterplot of Kurtosis vs Skewness
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
plot(Kurtosis ~ Skewness, data=risk_ret,
     ylim=c(1, max(risk_ret$Kurtosis)),
     main="Kurtosis vs Skewness")
# Add labels
text(x=risk_ret$Skewness, y=risk_ret$Kurtosis,
          labels=risk_ret$Name, pos=1, cex=0.8)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/etf_skew_sharp.png}
      % \includegraphics[width=0.45\paperwidth]{figure/etf_skew_kurtosis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Return Measures}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratio measures the excess returns per unit of risk, and is equal to the excess returns (over a risk-free return $r_f$) divided by the standard deviation of the returns:
      \begin{displaymath}
        S_{r}=\frac{E[r-r_f]}{\sigma}
      \end{displaymath}
      The \emph{Sortino} ratio is equal to the excess returns divided by the \emph{downside deviation} $\sigma_{d}$ (standard deviation of returns that are less than a target rate of return $r_t$):
      \begin{displaymath}
        S_{r}=\frac{E[r-r_t]}{\sigma_{d}}
      \end{displaymath}
      The \emph{Calmar} ratio is equal to the excess returns divided by the maximum drawdown of the returns:
      \begin{displaymath}
        C_{r}=\frac{E[r-r_f]}{DD}
      \end{displaymath}
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-1)>>=
library(PerformanceAnalytics)
re_turns <- rutils::etf_env$re_turns[, c("VTI", "IEF")]
re_turns <- na.omit(re_turns)
# Calculate the Sharpe ratio
PerformanceAnalytics::SharpeRatio(re_turns)
# Calculate the Sortino ratio
PerformanceAnalytics::SortinoRatio(re_turns)
# Calculate the Calmar ratio
PerformanceAnalytics::CalmarRatio(re_turns)
# Calculate the returns statistics
tail(PerformanceAnalytics::table.Stats(re_turns), 4)
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}

%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Study all the lecture slides in \texttt{FRE7241\_Lecture\_1.pdf}, and run all the code in \texttt{FRE7241\_Lecture\_1.R},
  \end{itemize}
\end{block}
\begin{block}{Recommended}
  \begin{itemize}[]
    \item Read the documentation for packages \texttt{rutils.pdf} and \texttt{HighFreq.pdf},
  \end{itemize}
\end{block}

\end{frame}



\end{document}
