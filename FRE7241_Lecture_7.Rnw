% FRE7241_Lecture_7
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{animate}
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#7]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#7, Fall 2022}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{May 17, 2022}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Portfolio Optimization Strategies}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy invests in the best performing portfolio in the past \emph{in-sample} interval, expecting that it will continue performing well \emph{out-of-sample}.
      \vskip1ex
      The \emph{portfolio optimization} strategy consists of:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculating the maximum Sharpe ratio portfolio weights in the \emph{in-sample} interval,
        \item Applying the weights and calculating the portfolio returns in the \emph{out-of-sample} interval.
      \end{enumerate}
      The optimal portfolio weights $\mathbf{w}$ are equal to the past in-sample excess returns $\mu = \mathbf{r} - r_f$ (in excess of the risk-free rate $r_f$) multiplied by the inverse of the covariance matrix $\mathbb{C}$:
      \begin{displaymath}
        \mathbf{w} = \mathbb{C}^{-1} \mu
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Select all the ETF symbols except "VXX", "SVXY" "MTUM", "QUAL", "VLUE", and "USMV"
symbolv <- colnames(rutils::etfenv$returns)
symbolv <- symbolv[!(symbolv %in% c("VXX", "SVXY", "MTUM", "QUAL", "VLUE", "USMV"))]
# Extract columns of rutils::etfenv$returns and overwrite NA values
retsp <- rutils::etfenv$returns[, symbolv]
nstocks <- NCOL(retsp)
# retsp <- na.omit(retsp)
retsp[1, is.na(retsp[1, ])] <- 0
retsp <- zoo::na.locf(retsp, na.rm=FALSE)
dates <- zoo::index(retsp)
# Returns in excess of risk-free rate
riskf <- 0.03/252
retsx <- (retsp - riskf)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_weights_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
retsis <- retsp["/2014"]
invmat <- MASS::ginv(cov(retsis))
weightv <- invmat %*% colMeans(retsx["/2014"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retsp)
# Plot portfolio weights
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
barplot(sort(weightv), main="Maximum Sharpe Weights", cex.names=0.7)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy In-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The in-sample performance of the optimal portfolio is much better than the equal weight portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
indeks <- xts::xts(rowMeans(retsis), zoo::index(retsis))
insample <- insample*sd(indeks)/sd(insample)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_in_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- cbind(indeks, insample)
colnames(pnls) <- c("Equal Weight", "Optimal")
endp <- rutils::calc_endpoints(pnls, interval="months")
dygraphs::dygraph(cumsum(pnls)[endp], main="In-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy Out-of-Sample}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The out-of-sample performance of the optimal portfolio is not nearly as good as in-sample.
      \vskip1ex
      Combining the optimal portfolio with the equal weight portfolio produces and even better performing portfolio.
      <<echo=TRUE,eval=FALSE>>=
# Calculate out-of-sample portfolio returns
retsos <- retsp["2015/"]
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retsos), zoo::index(retsos))
outsample <- outsample*sd(indeks)/sd(outsample)
pnls <- cbind(indeks, outsample, (outsample + indeks)/2)
colnames(pnls) <- c("Equal Weight", "Optimal", "Combined")
sqrt(252)*sapply(pnls, function(x) mean(x)/sd(x))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etf_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
endp <- rutils::calc_endpoints(pnls, interval="months")
dygraphs::dygraph(cumsum(pnls)[endp], main="Out-of-sample Optimal Portfolio Returns") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy for ETFs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for ETFs is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy underperforms in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
# Maximum Sharpe weights in-sample interval
invmat <- MASS::ginv(cov(retsis))
weightv <- invmat %*% colMeans(retsx["/2014"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retsp)
# Calculate in-sample portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
# Calculate out-of-sample portfolio returns
retsos <- retsp["2015/"]
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
indeks <- xts::xts(rowMeans(retsp), dates)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
endp <- rutils::calc_endpoints(pnls, interval="months")
dygraphs::dygraph(cumsum(pnls)[endp], main="Out-of-sample Optimal Portfolio Returns for ETFs") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regularized Inverse of Singular Covariance Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In an \emph{S\&P500} stock portfolio, the number of data columns is very large, which may make the covariance matrix of returns simgular.
      \vskip1ex
      \emph{Regularization} is the removal of zero singular values, to make calculating the inverse of a simgular matrix possible.
      \vskip1ex
      If the number of time periods of returns is less than the number of assets (columns), then the covariance matrix of returns is \emph{singular}, and some of its \emph{eigenvalues} are zero, so it doesn't have an inverse.
      \vskip1ex
      The \emph{regularized inverse} $\mathbb{C}_n^{-1}$ is calculated by removing the higher order eigenvalues that are almost zero, and keeping only the first $n$ \emph{eigenvalues}:
      \begin{displaymath}
        \mathbb{C}_n^{-1} = \mathbb{O}_n \, \mathbb{D}_n^{-1} \, \mathbb{O}_n^T
      \end{displaymath}
      Where $\mathbb{D}_n$ and $\mathbb{O}_n$ are matrices with the higher order eigenvalues and eigenvectors removed.
      \vskip1ex
      The function \texttt{MASS::ginv()} calculates the \emph{regularized} inverse of a matrix.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Create rectangular matrix with collinear columns
matrixv <- matrix(rnorm(10*8), nc=10)
# Calculate covariance matrix
covmat <- cov(matrixv)
# Calculate inverse of covmat - error
invmat <- solve(covmat)
# Perform eigen decomposition
eigend <- eigen(covmat)
eigenvec <- eigend$vectors
eigenval <- eigend$values
# Set tolerance for determining zero singular values
precision <- sqrt(.Machine$double.eps)
# Calculate regularized inverse matrix
not_zero <- (eigenval > (precision * eigenval[1]))
invreg <- eigenvec[, not_zero] %*%
  (t(eigenvec[, not_zero]) / eigenval[not_zero])
# Verify inverse property of invreg
all.equal(covmat, covmat %*% invreg %*% covmat)
# Calculate regularized inverse of covmat
invmat <- MASS::ginv(covmat)
# Verify inverse property of matrixv
all.equal(invmat, invreg)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shrinkage Inverse of Covariance Matrices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the higher order singular values are very small then the inverse matrix amplifies the statistical noise in the response matrix.
      \vskip1ex
      The technique of \emph{eigen shrinkage} calculates the inverse of a covariance matrix by removing the very small, higher order eigenvalues, to reduce the propagation of statistical noise and improve the signal-to-noise ratio:
      \begin{displaymath}
        \mathbb{C}_{shrinkage}^{-1} = \mathbb{O}_{max\_eigen} \, \mathbb{D}_{max\_eigen}^{-1} \, \mathbb{O}_{max\_eigen}^T
      \end{displaymath}
      The parameter \texttt{eigen\_max} specifies the number of eigenvalues used for calculating the \emph{shrinkage inverse} of the covariance matrix of returns.
      \vskip1ex
      Even though the \emph{shrinkage inverse} $\mathbb{C}_{shrinkage}^{-1}$ does not satisfy the matrix inverse property (so it's biased), its out-of-sample forecasts are usually more accurate than those using the actual inverse matrix.
      \vskip1ex
      But removing a larger number of eigenvalues increases the bias of the covariance matrix, which is an example of the \emph{bias-variance tradeoff}.
      \vskip1ex
      The optimal value of the parameter \texttt{eigen\_max} can be determined using \emph{backtesting} (\emph{cross-validation}).
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate in-sample covariance matrix
covmat <- cov(retsis)
eigend <- eigen(covmat)
eigenvec <- eigend$vectors
eigenval <- eigend$values
# Calculate shrinkage inverse of covariance matrix
eigen_max <- 3
invmat <- eigenvec[, 1:eigen_max] %*%
  (t(eigenvec[, 1:eigen_max]) / eigenval[1:eigen_max])
# Verify inverse property of inverse
all.equal(covmat, covmat %*% invmat %*% covmat)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization for ETFs with Eigen Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retsis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retsp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
dygraphs::dygraph(cumsum(pnls)[endp], main="Optimal Portfolio Returns With Eigen Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
retsxm <- rowMeans(retsx["/2014"])
retsxis <- (1-alpha)*retsx["/2014"] + alpha*retsxm
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_etfs_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retsxis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
# Plot cumulative portfolio returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
dygraphs::dygraph(cumsum(pnls)[endp], main="Optimal Portfolio Returns With Eigen and Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In a \emph{rolling portfolio optimization strategy}, the portfolio is optimized periodically and held out-of-sample.
      \setlength{\leftmargini}{1.0em}
      \begin{itemize}
        \item Calculate the \emph{end points} for portfolio rebalancing,
        \item Define an objective function for optimizing the portfolio weights,
        \item Calculate the optimal portfolio weights from the past (in-sample) performance,
        \item Calculate the out-of-sample returns by applying the portfolio weights to the future returns.
      \end{itemize}
      <<echo=TRUE,eval=FALSE>>=
# Define monthly end points
endp <- rutils::calc_endpoints(retsp, interval="months")
endp <- endp[endp > (nstocks+1)]
npts <- NROW(endp)
look_back <- 3
startp <- c(rep_len(0, look_back), endp[1:(npts-look_back)])
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate the portfolio weights
    insample <- retsx[startp[ep-1]:endp[ep-1], ]
    invmat <- MASS::ginv(cov(insample))
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retsp[(endp[ep-1]+1):endp[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls*sd(indeks)/sd(pnls))
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Monthly ETF Rolling Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
@
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Strategy With Eigen Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling portfolio optimization strategy with eigen shrinkage performs better than the standard strategy because eigen shrinkage suppresses the data noise.
      \vskip1ex
      The strategy performs especially well during sharp market selloffs, like in the years \texttt{2008} and \texttt{2020}.
      <<echo=TRUE,eval=FALSE>>=
# Define monthly end points
look_back <- 3; eigen_max <- 9
startp <- c(rep_len(0, look_back), endp[1:(npts-look_back)])
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- retsx[startp[ep-1]:endp[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:eigen_max] %*%
      (t(eigenvec[, 1:eigen_max]) / eigenval[1:eigen_max])
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retsp[(endp[ep-1]+1):endp[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_shrinkeigen.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls*sd(indeks)/sd(pnls))
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Rolling Portfolio Strategy With Eigen Shrinkage") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Strategy With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling portfolio optimization strategy with return shrinkage performs better than the standard strategy because return shrinkage suppresses the data noise.
      \vskip1ex
      The strategy performs especially well during sharp market selloffs, like in the years \texttt{2008} and \texttt{2020}.
      <<echo=TRUE,eval=FALSE>>=
# Define the return shrinkage intensity
alpha <- 0.7
# Perform loop over end points
pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- retsx[startp[ep-1]:endp[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:eigen_max] %*%
      (t(eigenvec[, 1:eigen_max]) / eigenval[1:eigen_max])
    # Shrink the in-sample returns to their mean
    insample <- (1-alpha)*insample + alpha*rowMeans(insample)
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retsp[(endp[ep-1]+1):endp[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- do.call(rbind, pnls)
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_shrinkrets.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of rolling ETF portfolio strategy
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls*sd(indeks)/sd(pnls))
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Rolling Portfolio Strategy With Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functional for Rolling Portfolio Optimization Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define backtest functional for rolling portfolio strategy
roll_portf <- function(excess, # Excess returns
                       returns, # Stock returns
                       endp, # End points
                       look_back=12, # Look-back interval
                       eigen_max=3, # Eigen shrinkage intensity
                       alpha=0.0, # Return shrinkage intensity
                       bid_offer=0.0, # Bid-offer spread
                       ...) {
  npts <- NROW(endp)
  startp <- c(rep_len(0, look_back), endp[1:(npts-look_back)])
  pnls <- lapply(2:npts, function(ep) {
    # Calculate regularized inverse of covariance matrix
    insample <- excess[startp[ep-1]:endp[ep-1], ]
    eigend <- eigen(cov(insample))
    eigenvec <- eigend$vectors
    eigenval <- eigend$values
    invmat <- eigenvec[, 1:eigen_max] %*%
      (t(eigenvec[, 1:eigen_max]) / eigenval[1:eigen_max])
    # Shrink the in-sample returns to their mean
    insample <- (1-alpha)*insample + alpha*rowMeans(insample)
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- returns[(endp[ep-1]+1):endp[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
  })  # end lapply
  pnls <- do.call(rbind, pnls)
  # Add warmup period to pnls
  rbind(indeks[paste0("/", start(pnls)-1)], pnls)
}  # end roll_portf
@
    \column{0.5\textwidth}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Look-backs}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of \emph{look-back} parameters.
      <<echo=TRUE,eval=FALSE>>=
# Simulate a monthly ETF momentum strategy
pnls <- roll_portf(excess=retsx, returns=retsp, endp=endp, 
  look_back=look_back, eigen_max=eigen_max)
# Perform sapply loop over look_backs
look_backs <- seq(2, 15, by=1)
pnls <- lapply(look_backs, roll_portf,
  returns=retsp, excess=retsx, endp=endp, eigen_max=eigen_max)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("look_back=", look_backs)
pnlsums <- sapply(pnls, sum)
look_back <- look_backs[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_multlb.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endp], main="Rolling Portfolio Strategies") %>%
  dyOptions(colors=colors, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retsp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Eigen Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of the eigen shrinkage parameter.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest for different eigen_max values
eigenvals <- 2:11
pnls <- lapply(eigenvals, roll_portf, excess=retsx, 
  returns=retsp, endp=endp, look_back=look_back)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("eigenval=", eigenvals)
pnlsums <- sapply(pnls, sum)
eigen_max <- eigenvals[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_multeigen.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endp], main="Rolling Portfolio Strategies With Eigen Shrinkage") %>%
  dyOptions(colors=colors, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retsp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization With Different Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Multiple \emph{rolling portfolio optimization} strategies can be backtested by calling the function \texttt{roll\_portf()} in a loop over a vector of return shrinkage parameters.
      \vskip1ex
      The best return shrinkage parameter for ETFs is equal to \texttt{0}, which means no return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over vector of return shrinkage intensities
alphav <- seq(from=0.0, to=0.9, by=0.1)
pnls <- lapply(alphav, roll_portf, excess=retsx, 
  returns=retsp, endp=endp, look_back=look_back, eigen_max=eigen_max)
pnls <- do.call(cbind, pnls)
colnames(pnls) <- paste0("alpha=", alphav)
pnlsums <- sapply(pnls, sum)
alpha <- alphav[which.max(pnlsums)]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_etf_monthly_multalpha.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of daily ETF momentum strategies
colors <- colorRampPalette(c("blue", "red"))(NCOL(pnls))
dygraphs::dygraph(cumsum(pnls)[endp], main="Rolling Portfolio Strategies With Return Shrinkage") %>%
  dyOptions(colors=colors, strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot EWMA strategies with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <-
  colorRampPalette(c("blue", "red"))(NCOL(pnls))
quantmod::chart_Series(cumsum(pnls),
  theme=plot_theme, name="Rolling Portfolio Strategies")
legend("bottomleft", legend=colnames(pnls),
  inset=0.02, bg="white", cex=0.7, lwd=rep(6, NCOL(retsp)),
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Strategy for Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{portfolio optimization} strategy for stocks is \emph{overfitted} in the \emph{in-sample} interval.
      \vskip1ex
      Therefore the strategy completely fails in the \emph{out-of-sample} interval.
      <<echo=TRUE,eval=FALSE>>=
load("/Users/jerzy/Develop/lecture_slides/data/sp500_returns.RData")
# Overwrite NA values in returns
retsp <- returns["2000/"]
nstocks <- NCOL(retsp)
retsp[1, is.na(retsp[1, ])] <- 0
retsp <- zoo::na.locf(retsp, na.rm=FALSE)
dates <- zoo::index(retsp)
riskf <- 0.03/252
retsx <- (retsp - riskf)
retsis <- retsp["/2010"]
retsos <- retsp["2011/"]
# Maximum Sharpe weights in-sample interval
covmat <- cov(retsis)
invmat <- MASS::ginv(covmat)
weightv <- invmat %*% colMeans(retsx["/2010"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retsp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retsp), dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_optim_out_sample.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
endp <- rutils::calc_endpoints(pnls, interval="months")
dygraphs::dygraph(cumsum(pnls)[endp], main="Out-of-sample Optimal Portfolio Returns for Stocks") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization for Stocks with Eigen Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{out-of-sample} performance of the \emph{portfolio optimization} strategy is greatly improved by shrinking the inverse of the covariance matrix.
      \vskip1ex
      The \emph{in-sample} performance is worse because shrinkage reduces \emph{overfitting}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate regularized inverse of covariance matrix
look_back <- 8; eigen_max <- 21
eigend <- eigen(cov(retsis))
eigenvec <- eigend$vectors
eigenval <- eigend$values
invmat <- eigenvec[, 1:eigen_max] %*%
  (t(eigenvec[, 1:eigen_max]) / eigenval[1:eigen_max])
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retsx["/2010"])
weightv <- drop(weightv/sqrt(sum(weightv^2)))
names(weightv) <- colnames(retsp)
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
indeks <- xts::xts(rowMeans(retsp), dates)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_optim_out_sample_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
endp <- rutils::calc_endpoints(pnls, interval="months")
dygraphs::dygraph(cumsum(pnls)[endp], main="Out-of-sample Returns for Stocks with Eigen Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stock Portfolio Weights With Return Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To further reduce the statistical noise, the individual returns $r_i$ can be \emph{shrunk} to the average portfolio returns $\bar{r}$:
      \begin{displaymath}
        r_i = (1 - \alpha) \, r_i + \alpha \, \bar{r}
      \end{displaymath}
      The parameter $\alpha$ is the \emph{shrinkage} intensity, and it determines the strength of the \emph{shrinkage} of individual returns to their mean.
      \vskip1ex
      If $\alpha = 0$ then there is no \emph{shrinkage}, while if $\alpha = 1$ then all the returns are \emph{shrunk} to their common mean: $r_i = \bar{r}$.
      \vskip1ex
      The optimal value of the \emph{shrinkage} intensity $\alpha$ can be determined using \emph{backtesting} (\emph{cross-validation}).
      <<echo=TRUE,eval=FALSE>>=
# Shrink the in-sample returns to their mean
alpha <- 0.7
retsxm <- rowMeans(retsx["/2010"])
retsxis <- (1-alpha)*retsx["/2010"] + alpha*retsxm
# Calculate portfolio weights
weightv <- invmat %*% colMeans(retsxis)
weightv <- drop(weightv/sqrt(sum(weightv^2)))
# Calculate portfolio returns
insample <- xts::xts(retsis %*% weightv, zoo::index(retsis))
outsample <- xts::xts(retsos %*% weightv, zoo::index(retsos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_optim_out_sample_rets_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine in-sample and out-of-sample returns
pnls <- rbind(insample, outsample)
pnls <- pnls*sd(indeks)/sd(pnls)
pnls <- cbind(indeks, pnls)
colnames(pnls) <- c("Equal Weight", "Optimal")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(pnls[index(outsample)],
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot the cumulative portfolio returns
dygraphs::dygraph(cumsum(pnls)[endp], main="Out-of-sample Returns for Stocks with Return Shrinkage") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyEvent(zoo::index(last(retsis[, 1])), label="in-sample", strokePattern="solid", color="red") %>%
  dyLegend(width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Covariance Matrix Inverse Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
  \column{0.5\textwidth}
      \emph{RcppArmadillo} can be used to quickly calculate the regularized inverse of a covariance matrix.
      <<echo=TRUE,eval=FALSE>>=
library(RcppArmadillo)
# Source Rcpp functions from file
Rcpp::sourceCpp("/Users/jerzy/Develop/lecture_slides/scripts/back_test.cpp")
# Create random matrix of returns
matrixv <- matrix(rnorm(300), nc=5)
# Regularized inverse of covariance matrix
eigen_max <- 4
eigend <- eigen(cov(matrixv))
covinv <- eigend$vectors[, 1:eigen_max] %*%
  (t(eigend$vectors[, 1:eigen_max]) / eigend$values[1:eigen_max])
# Regularized inverse using RcppArmadillo
covinv_arma <- calc_inv(matrixv, eigen_max)
all.equal(covinv, covinv_arma)
# Microbenchmark RcppArmadillo code
library(microbenchmark)
summary(microbenchmark(
  rcode={eigend <- eigen(cov(matrixv))
    eigend$vectors[, 1:eigen_max] %*%
      (t(eigend$vectors[, 1:eigen_max]) / eigend$values[1:eigen_max])
  },
  cppcode=calc_inv(matrixv, eigen_max),
  times=100))[, c(1, 4, 5)]  # end microbenchmark summary
      @
  \column{0.5\textwidth}
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat calc_inv(const arma::mat& tseries,
                   double eigen_thresh = 0.001, 
                   arma::uword eigen_max = 0) {
  
  if (eigen_max == 0) {
    // Calculate the inverse using arma::pinv()
    return arma::pinv(tseries, eigen_thresh);
  } else {
    // Calculate the regularized inverse using SVD decomposition
    
    // Allocate SVD
    arma::vec svdval;
    arma::mat svdu, svdv;
    
    // Calculate the SVD
    arma::svd(svdu, svdval, svdv, tseries);
    
    // Subset the SVD
    eigen_max = eigen_max - 1;
    // For no regularization: eigen_max = tseries.n_cols
    svdu = svdu.cols(0, eigen_max);
    svdv = svdv.cols(0, eigen_max);
    svdval = svdval.subvec(0, eigen_max);
    
    // Calculate the inverse from the SVD
    return svdv*arma::diagmat(1/svdval)*svdu.t();
    
  }  // end if
  
}  // end calc_inv
    \end{lstlisting}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast portfolio optimization using matrix algebra can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::vec calc_weights(const arma::mat& returns, // Portfolio returns
                       std::string method = "ranksharpe",
                       double eigen_thresh = 0.001,
                       arma::uword eigen_max = 0,
                       double confi = 0.1,
                       double alpha = 0.0,
                       bool scale = true,
                       double vol_target = 0.01) {
  // Initialize
  arma::vec weightv(returns[ncols, fill::zeros);
  if (eigen_max == 0)  eigen_max = returns[ncols;
  
  // Switch for the different methods for weights
  switch(calc_method(method)) {
  case method::ranksharpe: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end ranksharpe
  case method::max_sharpe: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Shrink meancols to the mean of returns
    meancols = ((1-alpha)*meancols + alpha*arma::mean(meancols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), eigen_max);
    // weightv = calc_inv(cov(returns), eigen_max)*meancols;
    weightv = calc_inv(cov(returns), eigen_thresh, eigen_max)*meancols;
    break;
  }  // end max_sharpe
  case method::max_sharpe_median: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::median(returns, 0));
    // Shrink meancols to the mean of returns
    meancols = ((1-alpha)*meancols + alpha*arma::median(meancols));
    // Apply regularized inverse
    // arma::mat inverse = calc_inv(cov(returns), eigen_max);
    weightv = calc_inv(cov(returns), eigen_thresh, eigen_max)*meancols;
    break;
  }  // end max_sharpe_median
  case method::min_var: {
    // Apply regularized inverse to unit vector
    weightv = calc_inv(cov(returns), eigen_thresh, eigen_max)*arma::ones(returns[ncols);
    break;
  }  // end min_var
  case method::min_varpca: {
    // Calculate highest order principal component
    arma::vec eigenval;
    arma::mat eigenvec;
    arma::eig_sym(eigenval, eigenvec, arma::cov(returns));
    weightv = eigenvec.col(0);
    break;
  }  // end min_varpca
  case method::rank: {
    // Mean returns by columns
    arma::vec meancols = arma::trans(arma::mean(returns, 0));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end rank
  case method::rankrob: {
    // Median returns by columns
    arma::vec meancols = arma::trans(arma::median(returns, 0));
    // meancols = ((1-alpha)*meancols + alpha*arma::mean(meancols));
    // Standard deviation by columns
    arma::vec sd_cols = arma::trans(arma::stddev(returns, 0));
    sd_cols.replace(0, 1);
    meancols = meancols/sd_cols;
    // Apply regularized inverse
    // arma::mat invmat = calc_inv(cov(returns), eigen_max);
    // weightv = calc_inv(cov(returns), eigen_max)*meancols;
    // weightv = calc_inv(cov(returns), eigen_max)*meancols;
    // // Standard deviation by columns
    // arma::vec sd_cols = meancols;
    // for (arma::uword it=0; it < returns[ncols; it++) {
    //   sd_cols(it) = arma::median(arma::abs((returns.col(it) - sd_cols)));
    // }  // end for
    // sd_cols.replace(0, 1);
    // meancols = meancols/sd_cols;
    // Weights equal to ranks of Sharpe
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(meancols)));
    // level;
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end rankrob
  case method::quantile: {
    // Sum of quantiles for columns
    arma::vec levels = {confi, 1-confi};
    weightv = conv_to<vec>::from(arma::sum(arma::quantile(returns, levels, 0), 0));
    // Weights equal to ranks
    weightv = conv_to<vec>::from(arma::sort_index(arma::sort_index(weightv)));
    weightv = (weightv - arma::mean(weightv));
    break;
  }  // end quantile
  default : {
    cout << "Warning: Invalid method parameter: " << method << endl;
    return arma::ones(returns[ncols);
  }  // end default
  }  // end switch
  
  if (scale == TRUE) {
    // return weightv/std::sqrt(sum(square(weightv)));
    // return weightv/sum(weightv);
    // Returns of equally weighted portfolio
    // arma::vec meanrows = arma::mean(returns, 1);
    // Returns of weighted portfolio
    // arma::vec returns_portf = returns*weightv;
    // Scale weights to equally weighted portfolio and return them
    // return weightv*arma::stddev(arma::mean(returns, 1))/arma::stddev(returns*weightv);
    // Scale weights so the resulting portfolio has a volatility equal to vol_target
    return weightv*vol_target/arma::stddev(returns*weightv);
  }  // end if
  
  return weightv;
  
}  // end calc_weights
    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Strategy Backtesting Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  % \begin{columns}[T]
    % \column{0.5\textwidth}
      Fast backtesting of strategies can be implemented using \protect\emph{RcppArmadillo}.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
arma::mat back_test(const arma::mat& retsx, // Portfolio excess returns
                    const arma::mat& returns, // Portfolio returns
                    arma::uvec startp, 
                    arma::uvec endp, 
                    std::string method = "ranksharpe",
                    double eigen_thresh = 0.001,
                    arma::uword eigen_max = 0,
                    double confi = 0.1,
                    double alpha = 0.0,
                    bool scale = true,
                    double vol_target = 0.01,
                    double coeff = 1.0,
                    double bid_offer = 0.0) {
  
  arma::vec weightv(returns[ncols, fill::zeros);
  arma::vec weights_past = zeros(returns[ncols);
  arma::mat pnls = zeros(returns*nrows, 1);
  
  // Perform loop over the end points
  for (arma::uword it = 1; it < endp.size(); it++) {
    // cout << "it: " << it << endl;
    // Calculate portfolio weights
    weightv = coeff*calc_weights(retsx.rows(startp(it-1), endp(it-1)), method, eigen_thresh, eigen_max, confi, alpha, scale, vol_target);
    // Calculate out-of-sample returns
    pnls.rows(endp(it-1)+1, endp(it)) = returns.rows(endp(it-1)+1, endp(it))*weightv;
    // Add transaction costs
    pnls.row(endp(it-1)+1) -= bid_offer*sum(abs(weightv - weights_past))/2;
    weights_past = weightv;
  }  // end for
  
  // Return the strategy pnls
  return pnls;
  
}  // end back_test

    \end{lstlisting}
    % \column{0.5\textwidth}
      % \vspace{-1em}
  % \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy for S\&P500 Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling portfolio optimization} strategy consists of rebalancing a portfolio over the end points:
      \setlength{\leftmargini}{1.0em}
      \begin{enumerate}
        \item Calculate the maximum Sharpe ratio portfolio weights at each end point,
        \item Apply the weights in the next interval and calculate the out-of-sample portfolio returns.
      \end{enumerate}
      The strategy parameters are: the rebalancing frequency (annual, monthly, etc.), and the length of look-back interval.
      <<echo=TRUE,eval=FALSE>>=
# Overwrite NA values in returns100
retsp <- returns100
retsp[1, is.na(retsp[1, ])] <- 0
retsp <- zoo::na.locf(retsp, na.rm=FALSE)
retsx <- (retsp - riskf)
nstocks <- NCOL(retsp) ; dates <- zoo::index(retsp)
# Define monthly end points
endp <- rutils::calc_endpoints(retsp, interval="months")
endp <- endp[endp > (nstocks+1)]
npts <- NROW(endp) ; look_back <- 12
startp <- c(rep_len(0, look_back), endp[1:(npts-look_back)])
# Perform loop over end points - takes very long !!!
pnls <- lapply(2:npts, function(ep) {
    # Subset the excess returns
    insample <- retsx[startp[ep-1]:endp[ep-1], ]
    invmat <- MASS::ginv(cov(insample))
    # Calculate the maximum Sharpe ratio portfolio weights
    weightv <- invmat %*% colMeans(insample)
    weightv <- drop(weightv/sqrt(sum(weightv^2)))
    # Calculate the out-of-sample portfolio returns
    outsample <- retsp[(endp[ep-1]+1):endp[ep], ]
    xts::xts(outsample %*% weightv, zoo::index(outsample))
})  # end lapply
pnls <- rutils::do_call(rbind, pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate returns of equal weight portfolio
indeks <- xts::xts(rowMeans(retsp), dates)
pnls <- rbind(indeks[paste0("/", start(pnls)-1)], pnls*sd(indeks)/sd(pnls))
# Calculate the Sharpe and Sortino ratios
wealth <- cbind(indeks, pnls)
colnames(wealth) <- c("Equal Weight", "Strategy")
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot cumulative strategy returns
dygraphs::dygraph(cumsum(wealth)[endp], main="Rolling Portfolio Optimization Strategy for S&P500 Stocks") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling Portfolio Optimization Strategy With Shrinkage}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{rolling portfolio optimization} strategy can be improved by applying both eigen shrinkage and return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Shift end points to C++ convention
endp <- (endp - 1)
endp[endp < 0] <- 0
startp <- (startp - 1)
startp[startp < 0] <- 0
# Specify eigen shrinkage and return shrinkage
alpha <- 0.7
eigen_max <- 21
# Perform backtest in Rcpp
pnls <- HighFreq::back_test(excess=retsx, returns=retsp,
  startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
pnls <- pnls*sd(indeks)/sd(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Rolling S&P500 Portfolio Optimization Strategy With Shrinkage") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Shrinkage Parameters Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal values of the eigen shrinkage parameter \texttt{eigen\_max} and the return shrinkage intensity parameter $\alpha$ can be determined using \emph{backtesting}.
      \vskip1ex
      The best eigen shrinkage parameter for this portfolio of stocks is equal to \texttt{eigen\_max=33}, which means relatively weak eigen shrinkage.
      \vskip1ex
      The best return shrinkage parameter for this portfolio of stocks is equal to $\alpha=0.81$, which means strong return shrinkage.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over vector of return shrinkage intensities
alphav <- seq(from=0.01, to=0.91, by=0.1)
pnls <- lapply(alphav, function(alpha) {
  HighFreq::back_test(excess=retsx, returns=retsp,
  startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=alphav, y=profilev, t="l", main="Rolling Strategy as Function of Return Shrinkage",
  xlab="Shrinkage Intensity Alpha", ylab="pnl")
whichmax <- which.max(profilev)
alpha <- alphav[whichmax]
pnls <- pnls[[whichmax]]
# Perform backtest over vector of eigen shrinkage eigenvals
eigenvals <- seq(from=3, to=40, by=2)
pnls <- lapply(eigenvals, function(eigen_max) {
  HighFreq::back_test(excess=retsx, returns=retsp,
    startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=eigenvals, y=profilev, t="l", main="Strategy PnL as Function of eigen_max",
  xlab="eigen_max", ylab="pnl")
whichmax <- which.max(profilev)
eigen_max <- eigenvals[whichmax]
pnls <- pnls[[whichmax]]
pnls <- pnls*sd(indeks)/sd(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink_optim.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Look-back Interval Using Backtesting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The optimal value of the look-back interval can be determined using \emph{backtesting}.
      \vskip1ex
      The optimal value of the look-back interval for this portfolio of stocks is equal to \texttt{look\_back=9} months, which roughly agrees with the research literature on momentum strategies.
      <<echo=TRUE,eval=FALSE>>=
# Perform backtest over look-backs
look_backs <- seq(from=3, to=12, by=1)
pnls <- lapply(look_backs, function(look_back) {
  startp <- c(rep_len(0, look_back), endp[1:(npts-look_back)])
  startp <- (startp - 1)
  startp[startp < 0] <- 0
  HighFreq::back_test(excess=retsx, returns=retsp,
    startp=startp, endp=endp, alpha=alpha, eigen_max=eigen_max, method="max_sharpe")
})  # end lapply
profilev <- sapply(pnls, sum)
plot(x=look_backs, y=profilev, t="l", main="Strategy PnL as Function of Look-back Interval",
  xlab="Look-back Interval", ylab="pnl")
whichmax <- which.max(profilev)
look_back <- look_backs[whichmax]
pnls <- pnls[[whichmax]]
pnls <- pnls*sd(indeks)/sd(pnls)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_rolling_sp500_shrink_optim_lookback.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative strategy returns
wealth <- cbind(indeks, pnls, (pnls+indeks)/2)
colnames(wealth) <- c("Index", "Strategy", "Combined")
# Calculate the out-of-sample Sharpe and Sortino ratios
sqrt(252)*sapply(wealth,
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
dygraphs::dygraph(cumsum(wealth)[endp], main="Optimal Rolling S&P500 Portfolio Strategy") %>%
  dyOptions(colors=c("blue", "red", "green"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}




%%%%%%%%%%%%%%%
\section{Portfolio Efficient Frontier}


%%%%%%%%%%%%%%%
\subsection{Vector and Matrix Calculus}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
    \begin{columns}[T]
    \column{0.5\textwidth}
      Let $\mathbf{v}$ and $\mathbf{w}$ be vectors, with $\mathbf{v} = \left\{ v_i \right\}_{i=1}^{i=n}$, and let $\mathbbm{1}$ be the unit vector, with $\mathbbm{1} = \left\{ 1 \right\}_{i=1}^{i=n}$.
      \vskip1ex
      Then the inner product of $\mathbf{v}$ and $\mathbf{w}$ can be written as $\mathbf{v}^T \mathbf{w} = \mathbf{w}^T \mathbf{v} = {\sum_{i=1}^n {v_i w_i}}$.
      \vskip1ex
      We can then express the sum of the elements of $\mathbf{v}$ as the inner product: $\mathbf{v}^T \mathbbm{1} = \mathbbm{1}^T \mathbf{v} = {\sum_{i=1}^n v_i}$.
      \vskip1ex
      And the sum of squares of $\mathbf{v}$ as the inner product: $\mathbf{v}^T \mathbf{v} = {\sum_{i=1}^n v^2_i}$.
      \vskip1ex
      Let $\mathbb{A}$ be a matrix, with $\mathbb{A} = \left\{ A_{ij} \right\}_{{i,j}=1}^{{i,j}=n}$.
      \vskip1ex
      Then the inner product of matrix $\mathbb{A}$ with vectors $\mathbf{v}$ and $\mathbf{w}$ can be written as:
      \begin{displaymath}
        \mathbf{v}^T \mathbb{A} \, \mathbf{w} = \mathbf{w}^T \mathbb{A}^T \mathbf{v} = {\sum_{{i,j}=1}^n {A_{ij} v_i w_j}}
      \end{displaymath}
    \column{0.5\textwidth}
      The derivative of a scalar variable with respect to a vector variable is a vector, for example:
      \begin{align*}
        \frac{d (\mathbf{v}^T \mathbbm{1})}{d \mathbf{v}} = d_v[\mathbf{v}^T \mathbbm{1}] = d_v[\mathbbm{1}^T \mathbf{v}] = \mathbbm{1}^T\\
        d_v[\mathbf{v}^T \mathbf{w}] = d_v[\mathbf{w}^T \mathbf{v}] = \mathbf{w}^T\\
        d_v[\mathbf{v}^T \mathbb{A} \, \mathbf{w}] = \mathbf{w}^T \mathbb{A}^T\\
        d_v[\mathbf{v}^T \mathbb{A} \, \mathbf{v}] = \mathbf{v}^T \mathbb{A} + \mathbf{v}^T \mathbb{A}^T
      \end{align*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Weight Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolio optimization requires constraints on the portfolio weights to prevent excessive leverage (size of positions relative to capital).
      \vskip1ex
      Portfolio-confl constraints limit the combined size of the weights.
      \vskip1ex
      For example, under \emph{linear} constraints the sum of the weights is equal to \texttt{1}: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, so that the weights are constrained to a \emph{hyperplane}.
      \vskip1ex
      The disadvantage of \emph{linear} constraints is that they allow highly leveraged portfolios, with very large positive and negative weights.
      \vskip1ex
      Under \emph{quadratic} constraints the sum of the \emph{squared} weights is equal to \texttt{1}: $\mathbf{w}^T \mathbf{w} = {\sum_{i=1}^n w^2_i} = 1$, so that the weights are constrained to a \emph{hypersphere}.
      \vskip1ex
      Box constraints limit the individual weights, for example: $0 \leq w_i \leq 1$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Linear constraint
weightv <- weightv/sum(weightv)
# Quadratic constraint
weightv <- weightv/sqrt(sum(weightv^2))
# Box constraints
weightv[weightv > 1] <- 1
weightv[weightv < 0] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum Return Portfolio Using Linear Programming}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the maximum return portfolio are obtained by maximizing the portfolio returns:
      \begin{displaymath}
        w_{max} = \operatorname*{arg\,max}_{w} [ \, \mathbf{r}^T \mathbf{w} \, ] = \operatorname*{arg\,max}_{w} [ \, \sum_{i=1}^n w_i r_i \, ]
      \end{displaymath}
      Where $\mathbf{r}$ is the vector of returns, and $\mathbf{w}$ is the vector of portfolio weights, constrained by:
      \begin{align*}
        \mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1\\
        0 \leq w_i \leq 1
      \end{align*}
      The weights of the maximum return portfolio can be calculated using linear programming (\emph{LP}), which is the optimization of linear objective functions subject to linear constraints.
      \vskip1ex
      The function \texttt{Rglpk\_solve\_LP()} from package \emph{Rglpk} solves linear programming problems by calling the \emph{GNU Linear Programming Kit} library.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
library(quantmod)
library(Rglpk)
# Vector of symbol names
symbolv <- c("VTI", "IEF", "DBC")
nweights <- NROW(symbolv)
# Calculate mean returns
returns <- rutils::etfenv$returns[, symbolv]
returns <- zoo::na.locf(returns, na.rm=FALSE)
returns <- na.omit(returns)
retsm <- colMeans(returns)
# Specify weight constraints
constr <- matrix(c(rep(1, nweights), 1, 1, 0),
                       nc=nweights, byrow=TRUE)
directs <- c("==", "<=")
rhs <- c(1, 0)
# Specify weight bounds (-1, 1) (default is c(0, Inf))
bounds <- list(lower=list(ind=1:nweights, val=rep(-1, nweights)),
               upper=list(ind=1:nweights, val=rep(1, nweights)))
# Perform optimization
optiml <- Rglpk::Rglpk_solve_LP(
  obj=retsm,
  mat=constr,
  dir=directs,
  rhs=rhs,
  bounds=bounds,
  max=TRUE)
unlist(optiml[1:2])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Minimum Variance} Portfolio Under \protect\emph{Linear} Constraints}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The portfolio variance is equal to: $\mathbf{w}^T \mathbb{C} \, \mathbf{w}$, where $\mathbb{C}$ is the covariance matrix of returns.
      \vskip1ex
      If the portfolio weights $\mathbf{w}$ are subject to \emph{linear} constraints: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, then the weights that minimize the portfolio variance can be found by minimizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - \, \lambda \, (\mathbf{w}^T \mathbbm{1} - 1)
      \end{displaymath}
      Where $\lambda$ is a \emph{Lagrange multiplier}.
      \vskip1ex
      The derivative of a scalar variable with respect to a vector variable is a vector, for example:
      \begin{align*}
        d_w[\mathbf{w}^T \mathbbm{1}] = d_w[\mathbbm{1}^T \mathbf{w}] = \mathbbm{1}^T\\
        d_w[\mathbf{w}^T \mathbf{r}] = d_w[\mathbf{r}^T \mathbf{w}] = \mathbf{r}^T\\
        d_w[\mathbf{w}^T \mathbb{C} \, \mathbf{w}] = \mathbf{w}^T \mathbb{C} + \mathbf{w}^T \mathbb{C}^T
      \end{align*}
      Where $\mathbbm{1}$ is the unit vector, and $\mathbf{w}^T \mathbbm{1} = \mathbbm{1}^T \mathbf{w} = \sum_{i=1}^n {x_i}$
    \column{0.5\textwidth}
      The derivative of the \emph{Lagrangian} $\mathcal{L}$ with respect to $\mathbf{w}$ is given by:
      \begin{displaymath}
        d_w \mathcal{L} = 2 \mathbf{w}^T \mathbb{C} - \lambda \mathbbm{1}^T
      \end{displaymath}
      By setting the derivative to zero we find $\mathbf{w}$ equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{1}{2} \lambda \, \mathbb{C}^{-1} \mathbbm{1}
      \end{displaymath}
      By multiplying the above from the left by $\mathbbm{1}^T$, and using $\mathbf{w}^T \mathbbm{1} = 1$, we find $\lambda$ to be equal to:
      \begin{displaymath}
        \lambda = \frac{2}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      And finally the portfolio weights are then equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mathbbm{1}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      If the portfolio weights are subject to \emph{quadratic} constraints: $\mathbf{w}^T \mathbf{w} = 1$ then the minimum variance weights are equal to the highest order \emph{principal component} (with the smallest eigenvalue) of the covariance matrix $\mathbb{C}$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance of the \protect\emph{Minimum Variance} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the \emph{minimum variance} portfolio under the constraint $\mathbf{w}^T \mathbbm{1} = 1$ can be calculated using the inverse of the covariance matrix:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mathbbm{1}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      The variance of the \emph{minimum variance} portfolio is equal to:
      \begin{displaymath}
        \sigma^2 = \frac{\mathbbm{1}^T \mathbb{C}^{-1} \mathbb{C} \, \mathbb{C}^{-1} \mathbbm{1}}{(\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1})^2} = \frac{1}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{displaymath}
      The function \texttt{solve()} solves systems of linear equations, and also inverts square matrices.
      \vskip1ex
      The \texttt{\%*\%} operator performs \emph{inner} (\emph{scalar}) multiplication of vectors and matrices.
      \vskip1ex
      \emph{Inner} multiplication multiplies the rows of one matrix with the columns of another matrix, so that each pair produces a single number:
      \vskip1ex
      The function \texttt{drop()} removes any dimensions of length \emph{one}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate covariance matrix of returns and its inverse
covmat <- cov(returns)
covinv <- solve(a=covmat)
unitv <- rep(1, NCOL(covmat))
# Minimum variance weights with constraint
# weightv <- solve(a=covmat, b=unitv)
weightv <- covinv %*% unitv
weightv <- weightv/drop(t(unitv) %*% weightv)
# Minimum variance
t(weightv) %*% covmat %*% weightv
1/(t(unitv) %*% covinv %*% unitv)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Portfolios}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A portfolio which has the smallest variance, given a target return, is an \emph{efficient portfolio}.
      \vskip1ex
      The \emph{efficient portfolio} weights have two constraints: the sum of portfolio weights $\mathbf{w}$ is equal to \texttt{1}: $\mathbf{w}^T \mathbbm{1} = {\sum_{i=1}^n w_i} = 1$, and the mean portfolio return is equal to the target return $r_t$: $\mathbf{w}^T \mathbf{r} = {\sum_{i=1}^n w_i r_i} = r_t$.
      \vskip1ex
      The weights that minimize the portfolio variance under these constraints can be found by minimizing the \emph{Lagrangian}:
      \begin{displaymath}
        \mathcal{L} = \mathbf{w}^T \mathbb{C} \, \mathbf{w} - \, \lambda_1 \, (\mathbf{w}^T \mathbbm{1} - 1) - \, \lambda_2 \, (\mathbf{w}^T \mathbf{r} - r_t)
      \end{displaymath}
      Where $\lambda_1$ and $\lambda_2$ are the \emph{Lagrange multipliers}.
      \vskip1ex
      The derivative of the \emph{Lagrangian} $\mathcal{L}$ with respect to $\mathbf{w}$ is given by:
      \begin{displaymath}
        d_w \mathcal{L} = 2 \mathbf{w}^T \mathbb{C} - \lambda_1 \mathbbm{1}^T - \lambda_2 \mathbf{r}^T
      \end{displaymath}
      By setting the derivative to zero we obtain the \emph{efficient portfolio} weights $\mathbf{w}$:
      \begin{displaymath}
        \mathbf{w} = \frac{1}{2} (\lambda_1 \, \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbb{C}^{-1} \mathbf{r})
      \end{displaymath}
    \column{0.5\textwidth}
      By multiplying the above from the left first by $\mathbbm{1}^T$, and then by $\mathbf{r}^T$, we obtain a system of two equations for $\lambda_1$ and $\lambda_2$:
      \begin{align*}
        2 \mathbbm{1}^T \mathbf{w} = \lambda_1 \, \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} = 2\\
        2 \mathbf{r}^T \mathbf{w} = \lambda_1 \, \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r} = 2 r_t
      \end{align*}
      The above can be written in matrix notation as:
      \begin{displaymath}
        \begin{bmatrix}
          \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} & \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} \\
          \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} & \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}
        \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix} =
        \begin{bmatrix}
          2 \\
          2 r_t
        \end{bmatrix}
      \end{displaymath}
      Or:
      \begin{displaymath}
        \begin{bmatrix}
          a & b \\
          b & c
        \end{bmatrix}
        \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix} =
        \mathbb{F} \lambda =
        2 \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} =
        2 u
      \end{displaymath}
      With $a = \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$, $b = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r}$, $c = \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}$,
      $\lambda = \begin{bmatrix}
          \lambda_1 \\
          \lambda_2
        \end{bmatrix}$,
      $u = \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix}$,
        and
      $\mathbb{F} = u^T \mathbb{C}^{-1} u = \begin{bmatrix}
          a & b \\
          b & c
        \end{bmatrix}$.
      \vskip1ex
      The \emph{Lagrange multipliers} can be solved as:
      \begin{displaymath}
        \lambda = 2 \mathbb{F}^{-1} u
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Portfolio} Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolio} weights $\mathbf{w}$ can now be solved as:
      \begin{align*}
        \mathbf{w} = \frac{1}{2} (\lambda_1 \, \mathbb{C}^{-1} \mathbbm{1} + \lambda_2 \, \mathbb{C}^{-1} \mathbf{r}) = \\
        \frac{1}{2}
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \lambda =
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \mathbb{F}^{-1} \, u = \\
        \frac{1}{a c-b^2}
        {\begin{bmatrix}
          \mathbb{C}^{-1} \mathbbm{1} \\
          \mathbb{C}^{-1} \mathbf{r}
        \end{bmatrix}}^T
        \begin{bmatrix}
          c & -b \\
          -b & a
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} = \\
        \frac{(c - b r_t)  \, \mathbb{C}^{-1} \mathbbm{1} + (a r_t - b)  \, \mathbb{C}^{-1} \mathbf{r}}{a c-b^2}
      \end{align*}
      With $a = \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$, $b = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r}$, $c = \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}$.
      \vskip1ex
      The above formula shows that a convex sum of two \emph{efficient portfolio} weights: $w = \alpha w_1 + (1-\alpha) w_2$ \\
      Are also the weights of an \emph{efficient portfolio}, with target return equal to: $r_t = \alpha r_1 + (1-\alpha) r_2$
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate vector of mean returns
retsm <- colMeans(returns)
# Specify the target return
rett <- 1.5*mean(returns)
# Products of inverse with mean returns and unit vector
fmat <- matrix(c(
  t(unitv) %*% covinv %*% unitv,
  t(unitv) %*% covinv %*% retsm,
  t(retsm) %*% covinv %*% unitv,
  t(retsm) %*% covinv %*% retsm), nc=2)
# Solve for the Lagrange multipliers
lagm <- solve(a=fmat, b=c(2, 2*rett))
# Calculate weights
weightv <- drop(0.5*covinv %*% cbind(unitv, retsm) %*% lagm)
# Calculate constraints
all.equal(1, sum(weightv))
all.equal(rett, sum(retsm*weightv))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Variance of the \protect\emph{Efficient Portfolios}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolio} variance is equal to:
      \begin{align*}
        \sigma^2 = \mathbf{w}^T \mathbb{C} \, \mathbf{w} = \frac{1}{4} \lambda^T \mathbb{F} \, \lambda = u^T \mathbb{F}^{-1} \, u =\\
        \frac{1}{a c-b^2}
        {\begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix}}^T
        \begin{bmatrix}
          c & -b \\
          -b & a
        \end{bmatrix}
        \begin{bmatrix}
          1 \\
          r_t
        \end{bmatrix} =\\
        \frac{a r^2_t - 2b r_t + c}{a c-b^2}
      \end{align*}
      The above formula shows that the variance of the \emph{efficient portfolios} is a \emph{parabola} with respect to the target return $r_t$.
      \vskip1ex
      The vertex of the \emph{parabola} is at $r_t = \mathbbm{1}^T \mathbb{C}^{-1} \mathbf{r} / \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$ and $\sigma^2 = 1 / \mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1}$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio return and standard deviation
retsp <- drop(returns %*% weightv)
c(return=mean(retsp), sd=sd(retsp))
all.equal(mean(retsp), rett)
# Calculate portfolio variance
uu <- c(1, rett)
finv <- solve(fmat)
all.equal(var(retsp), drop(t(uu) %*% finv %*% uu))
# Calculate vertex of variance parabola
weightv <- drop(covinv %*% unitv /
  drop(t(unitv) %*% covinv %*% unitv))
retsp <- drop(returns %*% weightv)
retsv <- drop(t(unitv) %*% covinv %*% retsm /
  t(unitv) %*% covinv %*% unitv)
all.equal(mean(retsp), retsv)
varmin <- drop(1/t(unitv) %*% covinv %*% unitv)
all.equal(var(retsp), varmin)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Frontier}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient frontier} is the plot of the \emph{efficient portfolio} standard deviations with respect to the target return $r_t$, which is a \emph{hyperbola}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate efficient frontier from target returns
retst <- retsv*(1+seq(from=(-1), to=1, by=0.1))
effront <- sapply(retst, function(rett) {
  uu <- c(1, rett)
  sqrt(drop(t(uu) %*% finv %*% uu))
})  # end sapply
# Plot efficient frontier
x11(width=6, height=5)
plot(x=effront, y=retst, t="l", col="blue", lwd=2,
     main="Efficient Frontier and Minimum Variance Portfolio",
     xlab="standard deviation", ylab="return")
points(x=sqrt(varmin), y=retsv, col="green", lwd=6)
text(x=sqrt(varmin), y=retsv, labels="minimum \nvariance",
     pos=4, cex=0.8)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Tangent Line} and the \protect\emph{Risk-free} Rate}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{tangent} line can be drawn at every point on the \protect\emph{efficient frontier}.
      \vskip1ex
      The slope $\beta$ of the \emph{tangent} line can be calculated by differentiating the variance $\sigma^2$ by the target return $r_t$:
      \begin{align*}
        \frac{d \sigma^2}{d r_t} = 2 \sigma \frac{d \sigma}{d r_t} = \frac{2 a r_t - 2 b}{a c-b^2} \\
        \frac{d \sigma}{d r_t} = \frac{a r_t - b}{\sigma \, (a c-b^2)} \\
        \beta = \frac{\sigma \, (a c-b^2)}{a r_t - b}
      \end{align*}
      The \emph{tangent} line connects the \emph{tangent} point on the \protect\emph{efficient frontier} with a \emph{risk-free} rate $r_f$.
    \column{0.5\textwidth}
      The \emph{risk-free} rate $r_f$ can be calculated as the intercept of the tangent line:
      \begin{align*}
        r_f = r_t - \sigma \, \beta = r_t - \frac{\sigma^2 \, (a c-b^2)}{a r_t - b} = \\
        r_t - \frac{a r^2_t - 2b r_t + c}{a c-b^2} \frac{a c-b^2}{a r_t - b} = \\
        r_t - \frac{a r^2_t - 2b r_t + c}{a r_t - b} = \frac{b r_t - c}{a r_t - b}
      \end{align*}
      <<echo=TRUE,eval=FALSE>>=
# Calculate portfolio standard deviation
stdev <- sqrt(drop(t(uu) %*% finv %*% uu))
# Calculate the slope of the tangent line
slopev <- (stdev*det(fmat))/(fmat[1, 1]*rett-fmat[1, 2])
# Calculate the risk-free rate as intercept of the tangent line
riskf <- rett - slopev*stdev
# Calculate the risk-free rate from target return
riskf <- (rett*fmat[1, 2]-fmat[2, 2]) /
  (rett*fmat[1, 1]-fmat[1, 2])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Tangent Line} on the \protect\emph{Efficient Frontier}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{efficient portfolios} are also called \emph{tangency portfolios}, since they are the tangent points on the \emph{efficient frontier}.
      \vskip1ex
      The \emph{tangency portfolio} is the \emph{market portfolio} corresponding to the given \emph{risk-free} rate.
      \vskip1ex
      The \emph{tangent line} at the \emph{market portfolio} is known as the \emph{Capital Market Line} (CML).
      <<echo=TRUE,eval=FALSE>>=
# Plot efficient frontier
plot(x=effront, y=retst, t="l", col="blue", lwd=2,
     xlim=c(0.0, max(effront)),
     main="Efficient Frontier and Tangency Portfolio",
     xlab="standard deviation", ylab="return")
# Plot minimum variance
points(x=sqrt(varmin), y=retsv, col="green", lwd=6)
text(x=sqrt(varmin), y=retsv, labels="minimum \nvariance",
     pos=4, cex=0.8)
# Plot tangent point
points(x=stdev, y=rett, col="red", lwd=6)
text(x=stdev, y=rett, labels="tangency\nportfolio", pos=2, cex=0.8)
# Plot risk-free point
points(x=0, y=riskf, col="red", lwd=6)
text(x=0, y=riskf, labels="risk-free", pos=4, cex=0.8)
# Plot tangent line
abline(a=riskf, b=slopev, lwd=2, col="green")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_tangent2.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum \protect\emph{Sharpe} Portfolio Weights}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratio is defined as the ratio of excess returns divided by the portfolio standard deviation:
      \begin{displaymath}
        SR = \frac{\mathbf{w}^T \mu}{\sigma}
      \end{displaymath}
      Where $\mu = \mathbf{r} - r_f$ is the vector of excess returns (in excess of the risk-free rate $r_f$), $\mathbf{w}$ is the vector of portfolio weights, and $\sigma = \sqrt{\mathbf{w}^T \mathbb{C} \, \mathbf{w}}$, where $\mathbb{C}$ is the covariance matrix of returns.
      \vskip1ex
      We can calculate the maximum \emph{Sharpe} portfolio weights by setting the derivative of the \emph{Sharpe} ratio with respect to the weights, to zero:
      \begin{displaymath}
        d_w {SR} = \frac{1}{\sigma} (\mu^T - \frac{(\mathbf{w}^T \mu) (\mathbf{w}^T \mathbb{C})}{\sigma^2}) = 0
      \end{displaymath}
      We then get:
      \begin{displaymath}
        (\mathbf{w}^T \mathbb{C} \, \mathbf{w}) \, \mu = (\mathbf{w}^T \mu) \, \mathbb{C} \mathbf{w}
      \end{displaymath}
      We can multiply the above equation by $\mathbb{C}^{-1}$ to get:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbf{w}^T \mathbb{C} \, \mathbf{w}}{\mathbf{w}^T \mu} \, \mathbb{C}^{-1} \mu
      \end{displaymath}
    \column{0.5\textwidth}
      We can finally rescale the weights so that they satisfy the constraint $\mathbf{w}^T \mathbbm{1} = 1$:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      These are the weights of the maximum \emph{Sharpe} portfolio, with the vector of excess returns equal to $\mu$, and the covariance matrix equal to $\mathbb{C}$.
      \vskip1ex
      The maximum \emph{Sharpe} portfolio is an \emph{efficient portfolio}, and so its mean return is equal to some target return $r_t$: $\mathbf{w}^T \mathbf{r} = {\sum_{i=1}^n w_i r_i} = r_t$.
      \vskip1ex
      The mean portfolio return can be written as:
      \begin{align*}
        \mathbf{r}^T \mathbf{w} = \frac{\mathbf{r}^T \mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu} =
        \frac{\mathbf{r}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)}{\mathbbm{1}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)} = \\
        r_t = \frac{\mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1} \, r_f - \mathbf{r}^T \mathbb{C}^{-1} \mathbf{r}}{\mathbbm{1}^T \mathbb{C}^{-1} \mathbbm{1} \, r_f - \mathbf{r}^T \mathbb{C}^{-1} \mathbbm{1}}
      \end{align*}
      The above formula calculates the target return $r_t$ from the risk-free rate $r_f$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Returns and Variance of Maximum \protect\emph{Sharpe} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The weights of the maximum \emph{Sharpe} portfolio are equal to:
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      Where $\mu$ is the vector of excess returns, and $\mathbb{C}$ is the covariance matrix.
      \vskip1ex
      The excess returns of the maximum \emph{Sharpe} portfolio are equal to:
      \begin{displaymath}
        R = \mathbf{w}^T \mu = \frac{\mu^T \mathbb{C}^{-1} \mu}{\mathbbm{1}^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
      The variance of the maximum \emph{Sharpe} portfolio is equal to:
      \begin{displaymath}
        \sigma^2 = \frac{\mu^T \mathbb{C}^{-1} \mathbb{C} \, \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2} = \frac{\mu^T \mathbb{C}^{-1} \mu}{(\mathbbm{1}^T \mathbb{C}^{-1} \mu)^2}
      \end{displaymath}
      The \emph{Sharpe} ratio is equal to:
      \begin{displaymath}
        SR = \sqrt{\mu^T \mathbb{C}^{-1} \mu}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate excess returns
riskf <- 0.03/252
retexc <- returns - riskf
# Calculate covariance and inverse matrix
covmat <- cov(returns)
unitv <- rep(1, NCOL(covmat))
covinv <- solve(a=covmat)
# Calculate mean excess returns
retexc <- sapply(retexc, mean)
# Weights of maximum Sharpe portfolio
# weightv <- solve(a=covmat, b=returns)
weightv <- covinv %*% retexc
weightv <- weightv/drop(t(unitv) %*% weightv)
# Sharpe ratios
sqrt(252)*sum(weightv*retexc) /
  sqrt(drop(weightv %*% covmat %*% weightv))
sapply(returns - riskf,
  function(x) sqrt(252)*mean(x)/sd(x))
weights_maxsharpe <- weightv
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Portfolios Under Zero Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the correlations of returns are equal to zero, then the covariance matrix is diagonal:
      \begin{displaymath}
        \mathbb{C} = \begin{pmatrix}
          \sigma^2_1 & 0 & \cdots & 0 \\
          0 & \sigma^2_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^2_n
        \end{pmatrix}
      \end{displaymath}
      Where $\sigma^2_i$ is the variance of returns of asset \texttt{i}.
      \vskip1ex
      The inverse of $\mathbb{C}$ is then simply:
      \begin{displaymath}
        \mathbb{C}^{-1} = \begin{pmatrix}
          \sigma^{-2}_1 & 0 & \cdots & 0 \\
          0 & \sigma^{-2}_2 & \cdots & 0 \\
          \vdots  & \vdots  & \ddots & \vdots  \\
          0 & 0 & \cdots & \sigma^{-2}_n
        \end{pmatrix}
      \end{displaymath}
    \column{0.5\textwidth}
      The \emph{minimum variance} portfolio weights are proportional to the inverse of the individual variances:
      \begin{displaymath}
        w_i = \frac{1}{\sigma^2_i \sum_{i=1}^n \sigma^{-2}_i}
      \end{displaymath}
      The maximum \emph{Sharpe} portfolio weights are proportional to the ratio of excess returns divided by the individual variances:
      \begin{displaymath}
        w_i = \frac{\mu_i}{\sigma^2_i \sum_{i=1}^n \mu_i \sigma^{-2}_i}
      \end{displaymath}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Maximum \protect\emph{Sharpe} and \protect\emph{Minimum Variance} Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The maximum \emph{Sharpe} and \emph{Minimum Variance} portfolios are both \emph{efficient portfolios}, with the lowest risk (standard deviation) for the given level of return.
      <<echo=TRUE,eval=FALSE>>=
library(quantmod)
# Calculate minimum variance weights
weightv <- covinv %*% unitv
weights_minvar <- weightv/drop(t(unitv) %*% weightv)
# Calculate optimal portfolio returns
optim_rets <- xts(
  x=cbind(exp(cumsum(returns %*% weights_maxsharpe)),
          exp(cumsum(returns %*% weights_minvar))),
  order.by=zoo::index(returns))
colnames(optim_rets) <- c("maxsharpe", "minvar")
# Plot optimal portfolio returns, with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "green")
x11(width=6, height=5)
chart_Series(optim_rets, theme=plot_theme,
  name="Maximum Sharpe and
  Minimum Variance portfolios")
legend("top", legend=colnames(optim_rets), cex=0.8,
       inset=0.1, bg="white", lty=1, lwd=6,
       col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/maxsharpe_minvar.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Efficient Frontier} and \protect\emph{Capital Market Line}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The maximum \emph{Sharpe} portfolio weights depend on the value of the risk-free rate $r_f$,
      \begin{displaymath}
        \mathbf{w} = \frac{\mathbb{C}^{-1} (\mathbf{r} - r_f)}{\mathbbm{1}^T \mathbb{C}^{-1} (\mathbf{r} - r_f)}
      \end{displaymath}
      The \emph{Efficient Frontier} is the set of \emph{efficient portfolios}, that have the lowest risk (standard deviation) for the given level of return.
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are \emph{efficient portfolios}, and they lie on the \emph{Efficient Frontier}, forming a tangent line from the risk-free rate to the \emph{Efficient Frontier}, known as the \emph{Capital Market Line} (CML).
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are considered to be the \emph{market portfolios}, corresponding to different values of the risk-free rate $r_f$.
      \vskip1ex
      The maximum \emph{Sharpe} portfolios are also called \emph{tangency} portfolios, since they are the tangent point on the \emph{Efficient Frontier}.
      \vskip1ex
      The \emph{Capital Market Line} is the line drawn from the \emph{risk-free} rate to the \emph{market portfolio} on the \emph{Efficient Frontier}.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{Efficient Frontier} and Maximum \protect\emph{Sharpe} Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),eval=FALSE>>=
x11(widthp <- 6, heightp <- 6)
# Calculate minimum variance weights
weightv <- covinv %*% unitv
weightv <- weightv/drop(t(unitv) %*% weightv)
# Minimum standard deviation and return
stdev <- sqrt(252*drop(weightv %*% covmat %*% weightv))
retsp <- 252*sum(weightv*retsm)
# Calculate maximum Sharpe portfolios
riskf <- (retsp * seq(-10, 10, by=0.1)^3)/252
effront <- sapply(riskf, function(riskf) {
  weightv <- covinv %*% (retsm - riskf)
  weightv <- weightv/drop(t(unitv) %*% weightv)
  # Portfolio return and standard deviation
  c(return=252*sum(weightv*retsm),
    stddev=sqrt(252*drop(weightv %*% covmat %*% weightv)))
})  # end sapply
effront <- cbind(252*riskf, t(effront))
colnames(effront)[1] <- "risk-free"
effront <- effront[is.finite(effront[, "stddev"]), ]
effront <- effront[order(effront[, "return"]), ]
# Plot maximum Sharpe portfolios
plot(x=effront[, "stddev"],
     y=effront[, "return"], t="l",
     xlim=c(0.0*stdev, 3.0*stdev),
     ylim=c(0.0*retsp, 2.0*retsp),
     main="Efficient Frontier and Capital Market Line",
     xlab="standard deviation", ylab="return")
points(x=effront[, "stddev"], y=effront[, "return"],
       col="red", lwd=3)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting the \protect\emph{Capital Market Line}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot minimum variance portfolio
points(x=stdev, y=retsp, col="green", lwd=6)
text(stdev, retsp, labels="minimum \nvariance",
     pos=4, cex=0.8)
# Draw Capital Market Line
sortv <- sort(effront[, 1])
riskf <- sortv[findInterval(x=0.5*retsp, vec=sortv)]
points(x=0, y=riskf, col="blue", lwd=6)
text(x=0, y=riskf, labels="risk-free",
     pos=4, cex=0.8)
marketp <- match(riskf, effront[, 1])
points(x=effront[marketp, "stddev"],
       y=effront[marketp, "return"],
       col="blue", lwd=6)
text(x=effront[marketp, "stddev"],
     y=effront[marketp, "return"],
     labels="market portfolio",
     pos=2, cex=0.8)
sharper <- (effront[marketp, "return"]-riskf)/
  effront[marketp, "stddev"]
abline(a=riskf, b=sharper, col="blue", lwd=2)
text(x=0.7*effront[marketp, "stddev"],
     y=0.7*effront[marketp, "return"]+0.01,
     labels="Capital Market Line", pos=2, cex=0.8,
     srt=45*atan(sharper*heightp/widthp)/(0.25*pi))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_market.png}
      The \emph{Capital Market Line} represents delevered and levered portfolios, consisting of the \emph{market portfolio} combined with the \emph{risk-free} rate.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Random Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate random portfolios
nportf <- 1000
randportf <- sapply(1:nportf, function(it) {
  weightv <- runif(nweights-1, min=-0.25, max=1.0)
  weightv <- c(weightv, 1-sum(weightv))
  # Portfolio return and standard deviation
  c(return=252*sum(weightv*retsm),
    stddev=sqrt(252*drop(weightv %*% covmat %*% weightv)))
})  # end sapply
# Plot scatterplot of random portfolios
x11(widthp <- 6, heightp <- 6)
plot(x=randportf["stddev", ], y=randportf["return", ],
     main="Efficient Frontier and Random Portfolios",
     xlim=c(0.5*stdev, 0.8*max(randportf["stddev", ])),
     xlab="standard deviation", ylab="return")
# Plot maximum Sharpe portfolios
lines(x=effront[, "stddev"],
     y=effront[, "return"], lwd=2)
points(x=effront[, "stddev"], y=effront[, "return"],
       col="red", lwd=3)
# Plot minimum variance portfolio
points(x=stdev, y=retsp, col="green", lwd=6)
text(stdev, retsp, labels="minimum\nvariance",
     pos=2, cex=0.8)
# Plot market portfolio
points(x=effront[marketp, "stddev"],
       y=effront[marketp, "return"], col="green", lwd=6)
text(x=effront[marketp, "stddev"],
     y=effront[marketp, "return"],
     labels="market\nportfolio",
     pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_random.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
points(x=sqrt(252*diag(covmat)),
       y=252*retsm, col="blue", lwd=6)
text(x=sqrt(252*diag(covmat)), y=252*retsm,
     labels=names(retsm),
     col="blue", pos=1, cex=0.8)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Efficient Frontier for Two-asset Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<corr_two_assets,echo=TRUE,eval=FALSE>>=
riskf <- 0.03
returns <- c(asset1=0.05, asset2=0.06)
stdevs <- c(asset1=0.4, asset2=0.5)
corrp <- 0.6
covmat <- matrix(c(1, corrp, corrp, 1), nc=2)
covmat <- t(t(stdevs*covmat)*stdevs)
weightv <- seq(from=(-1), to=2, length.out=31)
weightv <- cbind(weightv, 1-weightv)
retsp <- weightv %*% returns
portfsd <- sqrt(rowSums(weightv*(weightv %*% covmat)))
sharper <- (retsp-riskf)/portfsd
whichmax <- which.max(sharper)
sharpem <- max(sharper)
# Plot efficient frontier
x11(widthp <- 6, heightp <- 5)
par(mar=c(3,3,2,1)+0.1, oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
plot(portfsd, retsp, t="l",
 main=paste0("Efficient frontier and CML for two assets\ncorrelation = ", 100*corrp, "%"),
 xlab="standard deviation", ylab="return",
 lwd=2, col="orange",
 xlim=c(0, max(portfsd)),
 ylim=c(0.02, max(retsp)))
# Add Market Portfolio (maximum Sharpe ratio portfolio)
points(portfsd[whichmax], retsp[whichmax],
       col="blue", lwd=3)
text(x=portfsd[whichmax], y=retsp[whichmax],
     labels=paste(c("market portfolio\n",
       structure(c(weightv[whichmax], 1-weightv[whichmax]),
               names=names(returns))), collapse=" "),
     pos=2, cex=0.8)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/cml_two_assets.png}\\
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
points(stdevs, returns, col="green", lwd=3)
text(stdevs, returns, labels=names(returns), pos=4, cex=0.8)
# Add point at risk-free rate and draw Capital Market Line
points(x=0, y=riskf, col="blue", lwd=3)
text(0, riskf, labels="risk-free\nrate", pos=4, cex=0.8)
abline(a=riskf, b=sharpem, lwd=2, col="blue")
range_s <- par("usr")
text(portfsd[whichmax]/2, (retsp[whichmax]+riskf)/2,
     labels="Capital Market Line", cex=0.8, , pos=3,
     srt=45*atan(sharpem*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   heightp/widthp)/(0.25*pi))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Efficient Frontier of Stock and Bond Portfolios}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:3)),eval=FALSE>>=
# Plot portfolios in x11() window
x11(widthp <- 6, heightp <- 5)
par(oma=c(0, 0, 0, 0), mar=c(3,3,2,1)+0.1, mgp=c(2, 1, 0), cex.lab=1.0, cex.axis=1.0, cex.main=1.0, cex.sub=1.0)
# Vector of symbol names
symbolv <- c("VTI", "IEF")
# Matrix of portfolio weights
weightv <- seq(from=(-1), to=2, length.out=31)
weightv <- cbind(weightv, 1-weightv)
# Calculate portfolio returns and volatilities
returns <- rutils::etfenv$returns[, symbolv]
retsp <- returns %*% t(weightv)
portfv <- cbind(252*colMeans(retsp),
  sqrt(252)*matrixStats::colSds(retsp))
colnames(portfv) <- c("returns", "stddev")
riskf <- 0.06
portfv <- cbind(portfv,
  (portfv[, "returns"]-riskf)/portfv[, "stddev"])
colnames(portfv)[3] <- "Sharpe"
whichmax <- which.max(portfv[, "Sharpe"])
sharpem <- portfv[whichmax, "Sharpe"]
plot(x=portfv[, "stddev"], y=portfv[, "returns"],
     main="Stock and Bond portfolios", t="l",
     xlim=c(0, 0.7*max(portfv[, "stddev"])), ylim=c(0, max(portfv[, "returns"])),
     xlab="standard deviation", ylab="return")
# Add blue point for market portfolio
points(x=portfv[whichmax, "stddev"], y=portfv[whichmax, "returns"], col="blue", lwd=6)
text(x=portfv[whichmax, "stddev"], y=portfv[whichmax, "returns"],
     labels=paste(c("market portfolio\n", 
        structure(c(weightv[whichmax, 1], weightv[whichmax, 2]), names=symbolv)), collapse=" "),
     pos=3, cex=0.8)
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/eff_front_stocks_bonds.png}\\
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot individual assets
retsm <- 252*sapply(returns, mean)
stdevs <- sqrt(252)*sapply(returns, sd)
points(stdevs, retsm, col="green", lwd=6)
text(stdevs, retsm, labels=names(returns), pos=2, cex=0.8)
# Add point at risk-free rate and draw Capital Market Line
points(x=0, y=riskf, col="blue", lwd=6)
text(0, riskf, labels="risk-free", pos=4, cex=0.8)
abline(a=riskf, b=sharpem, col="blue", lwd=2)
range_s <- par("usr")
text(max(portfv[, "stddev"])/3, 0.75*max(portfv[, "returns"]),
     labels="Capital Market Line", cex=0.8, , pos=3,
     srt=45*atan(sharpem*(range_s[2]-range_s[1])/
                   (range_s[4]-range_s[3])*
                   heightp/widthp)/(0.25*pi))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performance of Market Portfolio for Stocks and Bonds}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:2)),eval=FALSE>>=
# Plot portfolios in x11() window
x11(widthp <- 6, heightp <- 5)
# Calculate cumulative returns of VTI and IEF
optim_rets <- lapply(returns,
  function(returns) exp(cumsum(returns)))
optim_rets <- rutils::do_call(cbind, optim_rets)
# Calculate market portfolio returns
optim_rets <- cbind(exp(cumsum(returns %*%
    c(weightv[whichmax], 1-weightv[whichmax]))),
  optim_rets)
colnames(optim_rets)[1] <- "market"
# Plot market portfolio with custom line colors
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green")
chart_Series(optim_rets, theme=plot_theme,
             name="Market portfolio for stocks and bonds")
legend("top", legend=colnames(optim_rets),
       cex=0.8, inset=0.1, bg="white", lty=1,
       lwd=6, col=plot_theme$col$line.col, bty="n")
      @
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/market_stocks_bonds.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Modeling Volatility and Skewness}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Variance of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The variance of asset returns exhibits \emph{heteroskedasticity}, i.e. it changes over time.
      \vskip1ex
      The rolling variance of returns is given by:
      \begin{flalign*}
        \sigma^2_i &= \frac{1}{k-1} \sum_{j=0}^{k-1} (r_{i-j}-\bar{r_i})^2 \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} r_{i-j}}
      \end{flalign*}
      Where \texttt{k} is the \emph{look-back interval} equal to the number of data points
      for performing aggregations over the past.
      \vskip1ex
      It's also possible to calculate the rolling variance in \texttt{R} using vectorized functions, without using an \texttt{apply()} loop.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
nrows <- NROW(retsp)
# Define end points
endp <- 1:NROW(retsp)
# Start points are multi-period lag of endp
look_back <- 11
startp <- c(rep_len(0, look_back), endp[1:(nrows-look_back)])
# Calculate rolling variance in sapply() loop - takes long
variance <- sapply(1:nrows, function(indeks) {
  retsp <- retsp[startp[indeks]:endp[indeks]]
  sum((retsp - mean(retsp))^2)
}) / (look_back)  # end sapply
# Use only vectorized functions
retc <- cumsum(retsp)
retc <- (retc -
  c(rep_len(0, look_back), retc[1:(nrows-look_back)]))
retc2 <- cumsum(retsp^2)
retc2 <- (retc2 -
  c(rep_len(0, look_back), retc2[1:(nrows-look_back)]))
variance2 <- (retc2 - retc^2/look_back)/(look_back)
all.equal(variance[-(1:look_back)], as.numeric(variance2)[-(1:look_back)])
# Same, using package rutils
retc <- rutils::roll_sum(retsp, look_back=look_back, min_obs=1)
retc2 <- rutils::roll_sum(retsp^2, look_back=look_back, min_obs=1)
variance2 <- (retc2 - retc^2/look_back)/(look_back)
# Coerce variance into xts
tail(variance)
class(variance)
variance <- xts(variance, order.by=zoo::index(retsp))
colnames(variance) <- "VTI.variance"
head(variance)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for the \emph{weighted} rolling sum,
        \item \texttt{roll\_var()} for the \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for the rolling scaling and centering of time series,
        \item \texttt{roll\_pcr()} for the rolling principal component regressions of time series.
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling VTI variance using package roll
library(roll)  # Load roll
variance <- roll::roll_var(retsp, width=look_back)
colnames(variance) <- "VTI.variance"
head(variance)
sum(is.na(variance))
variance[1:(look_back-1)] <- 0
# Benchmark calculation of rolling variance
library(microbenchmark)
summary(microbenchmark(
  roll_sapply=sapply(2:nrows, function(indeks) {
    retsp <- retsp[startp[indeks]:endp[indeks]]
    sum((retsp - mean(retsp))^2)
  }),
  ro_ll=roll::roll_var(retsp, width=look_back),
  times=10))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{EWMA} Realized Volatility Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time-varying volatility can be more accurately estimated using an \emph{Exponentially Weighted Moving Average} (\emph{EWMA}) variance estimator.
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      \begin{displaymath}
        \sigma^2_i = (1-\lambda) r^2_i + \lambda \sigma^2_{i-1} = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j r^2_{i-j}
      \end{displaymath}
      $\sigma^2_i$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period \texttt{i} and the past \emph{realized} variance.
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa.
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} calculates the convolution of a vector or a time series with a filter of coefficients (weights).
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} is very fast because it's compiled \texttt{C++} code.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_vti_ewma.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate EWMA VTI variance using compiled C++ function
look_back <- 51
weights <- exp(-0.1*1:look_back)
weights <- weights/sum(weights)
variance <- .Call(stats:::C_cfilter, retsp^2, 
  filter=weights, sides=1, circular=FALSE)
variance[1:(look_back-1)] <- variance[look_back]
# Plot EWMA volatility
variance <- xts:::xts(sqrt(variance), order.by=zoo::index(retsp))
dygraphs::dygraph(variance, main="VTI EWMA Volatility") %>%
  dyOptions(colors="blue")
quantmod::chart_Series(xtes, name="VTI EWMA Volatility")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating \protect\emph{EWMA} Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the \emph{time series} has non-zero \emph{expected} mean, then the rolling \emph{EWMA} variance is a vector given by the estimator:
      \begin{flalign*}
        \sigma^2_i &= \frac{1}{k-1} \sum_{j=0}^{k-1} {w_j (r_{i-j}-\bar{r_i})^2} \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} {w_j r_{i-j}}}
      \end{flalign*}
      Where $w_j$ is the vector of weights:
      \begin{displaymath}
        w_j = \frac{\lambda^j}{\sum_{j=0}^{k-1} \lambda^j}
      \end{displaymath}
      The function \texttt{roll\_var()} from package \emph{roll} calculates the rolling \emph{EWMA} variance.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate rolling VTI variance using package roll
library(roll)  # Load roll
variance <- roll::roll_var(retsp,
  weights=rev(weights), width=look_back)
colnames(variance) <- "VTI.variance"
class(variance)
head(variance)
sum(is.na(variance))
variance[1:(look_back-1)] <- 0
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Daily Volatility From Intraday Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{close-to-close} volatility $\sigma$ depends on the \emph{Close} prices $C_i$ from \emph{OHLC} data:
      \begin{flalign*}
        \sigma^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (r_i - \bar{r})^2 \\
        \bar{r} &= \frac{1}{n}{\sum_{i=0}^{n} r_i} \quad r_i = \log(\frac{C_i}{C_{i-1}})
      \end{flalign*}
      But intraday time series of prices (for example \texttt{HighFreq::SPY} prices), can have large overnight jumps which inflate the volatility estimates.
      \vskip1ex
      So the overnight returns must be divided by the overnight time interval (in seconds), which produces per second returns.
      \vskip1ex
      The per second returns can be multiplied by \texttt{60} to scale them back up to per minute returns.
      \vskip1ex
      The function \texttt{zoo::index()} extracts the time index of a time series.
      \vskip1ex
      The function \texttt{xts::.index()} extracts the time index expressed in the number of seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely SPY returns (unit per minute) single day
# Minutely SPY volatility (unit per minute)
retsp <- rutils::diffit(log(SPY["2012-02-13", 4]))
sd(retsp)
# SPY returns multiple days (includes overnight jumps)
retsp <- rutils::diffit(log(SPY[, 4]))
sd(retsp)
# Table of time intervals - 60 second is most frequent
indeks <- rutils::diffit(.zoo::index(SPY))
table(indeks)
# SPY returns divided by the overnight time intervals (unit per second)
retsp <- retsp / indeks
retsp[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retsp)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Range Volatility Estimators of \protect\emph{OHLC} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Range estimators of return volatility utilize the \texttt{high} and \texttt{low} prices, and therefore have lower standard errors than the standard \emph{close-to-close} estimator.
      \vskip1ex
      The \emph{Garman-Klass} estimator uses the \emph{low-to-high} price range, but it underestimates volatility because it doesn't account for \emph{close-to-open} price jumps:
      \begin{displaymath}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)\log(\frac{C_i}{O_i})^2)
      \end{displaymath}
      The \emph{Yang-Zhang} estimator accounts for \emph{close-to-open} price jumps and has the lowest standard error among unbiased estimators:
      \begin{multline*}
        \hspace{-1em}\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{O_i}{C_{i-1}})-\bar{r}_{co})^2 + \\
        0.134(\log(\frac{C_i}{O_i})-\bar{r}_{oc})^2 + \\
        \frac{0.866}{n} \sum_{i=1}^{n} (\log(\frac{H_i}{O_i})\log(\frac{H_i}{C_i}) + \log(\frac{L_i}{O_i})\log(\frac{L_i}{C_i}))
      \end{multline*}
    \column{0.5\textwidth}
      The \emph{Yang-Zhang} (\emph{YZ}) and \emph{Garman-Klass-Yang-Zhang} (\emph{GKYZ}) estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator.
      \vskip1ex
      But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the \emph{YZ} and \emph{GKYZ} range estimators.
      \vskip1ex
      Therefore in practice the \emph{YZ} and \emph{GKYZ} range estimators underestimate the volatility, and their standard errors are reduced less than by the theoretical amount, for the same reason.
      \vskip1ex
      The \emph{Garman-Klass-Yang-Zhang} estimator is another very efficient and unbiased estimator, and also accounts for \emph{close-to-open} price jumps:
      \begin{multline*}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} ((\log(\frac{O_i}{C_{i-1}})-\bar{r})^2 + \\
        0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)(\log(\frac{C_i}{O_i})^2))
      \end{multline*}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Range Variance Using \protect\emph{HighFreq}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{HighFreq::calc\_var\_ohlc()} calculates the \emph{variance} of returns using several different range volatility estimators.
      \vskip1ex
      If the logarithms of the \emph{OHLC} prices are passed into \texttt{HighFreq::calc\_var\_ohlc()} then it calculates the variance of percentage returns, and if simple \emph{OHLC} prices are passed then it calculates the variance of dollar returns. 
      \vskip1ex
      The function \texttt{HighFreq::roll\_var\_ohlc()} calculates the \emph{rolling} variance of returns using several different range volatility estimators.
      \vskip1ex
      The functions \texttt{HighFreq::calc\_var\_ohlc()} and \texttt{HighFreq::roll\_var\_ohlc()} are very fast because they are written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{TTR::volatility()} calculates the range volatility, but it's significantly slower than \texttt{HighFreq::calc\_var\_ohlc()}.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
spy <- HighFreq::SPY["2009"]
# Calculate daily SPY volatility using package HighFreq
sqrt(6.5*60*HighFreq::calcvar_ohlc(log(spy), 
  method="yang_zhang"))
# Calculate daily SPY volatility from minutely prices using package TTR
sqrt((6.5*60)*mean(na.omit(
  TTR::volatility(spy, N=1, calc="yang.zhang"))^2))
# Calculate rolling SPY variance using package HighFreq
variance <- HighFreq::roll_var_ohlc(log(spy), method="yang_zhang", 
  look_back=look_back)
# Plot range volatility
variance <- xts:::xts(sqrt(variance), order.by=zoo::index(spy))
dygraphs::dygraph(variance["2009-02"], 
  main="SPY Rolling Range Volatility") %>%
  dyOptions(colors="blue")
# Benchmark the speed of HighFreq vs TTR
library(microbenchmark)
summary(microbenchmark(
  ttr=TTR::volatility(rutils::etfenv$VTI, N=1, calc="yang.zhang"),
  highfreq=HighFreq::calcvar_ohlc(log(rutils::etfenv$VTI), method="yang_zhang"),
  times=2))[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{VXX Prices and the Rolling Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VXX} ETF invests in \emph{VIX} futures, so its price is tied to the level of the \emph{VIX} index, with higher \emph{VXX} prices corresponding to higher levels of the \emph{VIX} index. 
      \vskip1ex
      The rolling volatility of past returns moves in sympathy with the implied volatility and \emph{VXX} prices, but with a lag.
      \vskip1ex
      But \emph{VXX} prices exhibit a very strong downward trend which makes them hard to compare with the rolling volatility.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VXX log prices
vxx <- na.omit(rutils::etfenv$prices$VXX)
dates <- zoo::index(vxx)
look_back <- 41
vxx <- log(vxx)
# Calculate rolling VTI volatility
closep <- get("VTI", rutils::etfenv)[dates]
closep <- log(closep)
volat <- sqrt(HighFreq::roll_var_ohlc(ohlc=closep, look_back=look_back, scalev=FALSE))
volat[1:look_back] <- volat[look_back+1]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vxx_volat.png}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of VXX and VTI volatility
datav <- cbind(vxx, volat)
colnames(datav)[2] <- "VTI Volatility"
colnamev <- colnames(datav)
cap_tion <- "VXX and VTI Volatility"
dygraphs::dygraph(datav[, 1:2], main=cap_tion) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variance calculated over non-overlapping intervals has very statistically significant autocorrelations.
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 3, 1, 1), oma=c(0, 0, 0, 0))
# Calculate VTI percentage returns
retsp <- na.omit(rutils::etfenv$returns$VTI)
# Calculate rolling VTI variance using package roll
look_back <- 22
variance <- roll::roll_var(retsp, width=look_back)
variance[1:(look_back-1)] <- 0
colnames(variance) <- "VTI.variance"
# Number of look_backs that fit over returns
nrows <- NROW(retsp)
nagg <- nrows %/% look_back
# Define endp with beginning stub
endp <- c(0, nrows-look_back*nagg + (0:nagg)*look_back)
nrows <- NROW(endp)
# Subset variance to endp
variance <- variance[endp]
# Plot autocorrelation function
rutils::plot_acf(variance, lag=10, main="ACF of Variance")
# Plot partial autocorrelation
pacf(variance, lag=10, main="PACF of Variance", ylab=NA)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/acf_var.png}\\
      \includegraphics[width=0.4\paperwidth]{figure/pacf_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{GARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} is a volatility model defined by two coupled equations:
      \begin{flalign*}
        r_i &= \mu + \sigma_{i-1} \xi_i \\
        \sigma^2_i &= \omega + \alpha (r_i - \mu)^2 + \beta \sigma^2_{i-1}
      \end{flalign*}
      Where $\sigma^2_i$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance $(r_i - \mu)^2$ and the past variance $\sigma^2_{i-1}$, and $\xi_i$ are standard normal \emph{innovations}.
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance.
      \vskip1ex
      The return process $r_i$ follows a normal distribution, \emph{conditional} on the variance in the previous period $\sigma^2_{i-1}$.
      \vskip1ex
      But the \emph{unconditional} distribution of returns is \emph{not} normal, since their standard deviation is time-dependent, so they are \emph{leptokurtic} (fat tailed).
      \vskip1ex
      The long-term expected value of the variance is proportional to the parameter $\omega$:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      So the sum of $\alpha$ plus $\beta$ should be less than $1$, otherwise the volatility is explosive.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define GARCH parameters
alpha <- 0.3; betav <- 0.5;
om_ega <- 1e-4*(1-alpha-betav)
nrows <- 1000
# Calculate matrix of standard normal innovations
set.seed(1121)  # Reset random numbers
innov <- rnorm(nrows)
retsp <- numeric(nrows)
variance <- numeric(nrows)
variance[1] <- om_ega/(1-alpha-betav)
retsp[1] <- sqrt(variance[1])*innov[1]
# Simulate GARCH model
for (i in 2:nrows) {
  retsp[i] <- sqrt(variance[i-1])*innov[i]
  variance[i] <- om_ega + alpha*retsp[i]^2 +
    betav*variance[i-1]
}  # end for
# Simulate the GARCH process using Rcpp
garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha,  
  beta=betav, innov=matrix(innov))
all.equal(garch_data, cbind(retsp, variance), 
  check.attributes=FALSE)
      @
      The \emph{GARCH} process must be simulated using an explicit loop, so it's better to perform it in \texttt{C++} instead of \texttt{R}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} volatility model produces volatility clustering - periods of high volatility followed by a quick decay.
      \vskip1ex
      But the decay of the volatility in the \emph{GARCH} model is faster than what is observed in practice.
      \vskip1ex
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Larger values of $\alpha$ produce a stronger feedback between the realized returns and variance, which produce larger variance spikes, which produce larger kurtosis.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window on Mac
dev.new(width=6, height=5, noRStudioGD=TRUE)
# Set plot parameters to reduce whitespace around plot
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
# Plot GARCH cumulative returns
plot(cumsum(retsp), t="l", col="blue", xlab="", ylab="",
  main="GARCH Cumulative Returns")
quartz.save("figure/garch_returns.png", type="png", 
  width=6, height=5)
# Plot GARCH volatility
plot(sqrt(variance), t="l", col="blue", xlab="", ylab="",
  main="GARCH Volatility")
quartz.save("figure/garch_volat.png", type="png", 
  width=6, height=5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_returns.png}\\
      \includegraphics[width=0.4\paperwidth]{figure/garch_volat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Returns Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} volatility model produces \emph{leptokurtic} returns with fat tails in their the distribution.
      \vskip1ex
      Student's \emph{t-distribution} has fat tails, so it fits asset returns much better than the normal distribution.
      \vskip1ex
      Student's \emph{t-distribution} with \texttt{3} degrees of freedom is often used to represent asset returns.
      \vskip1ex
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution into a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      <<echo=TRUE,eval=FALSE>>=
# Calculate kurtosis of GARCH returns
mean(((retsp-mean(retsp))/sd(retsp))^4)
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(retsp)
# Fit t-distribution into GARCH returns
optiml <- MASS::fitdistr(retsp, densfun="t", df=2)
loc <- optiml$estimate[1]
scalev <- optiml$estimate[2]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_hist.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot histogram of GARCH returns
histp <- hist(retsp, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.03, 0.03),
  ylab="frequency", freq=FALSE, main="GARCH Returns Histogram")
lines(density(retsp, adjust=1.5), lwd=2, col="blue")
curve(expr=dt((x-loc)/scalev, df=2)/scalev,
  type="l", xlab="", ylab="", lwd=2,
  col="red", add=TRUE)
legend("topright", inset=-0, bty="n",
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
quartz.save("figure/garch_hist.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package 
      \href{https://cran.r-project.org/web/packages/fGarch/index.html}{\emph{fGarch}}
      contains functions for applying \emph{GARCH} models.
      \vskip1ex
      The function \texttt{fGarch::garchSpec()} specifies a \emph{GARCH} model.
      \vskip1ex
      The function \texttt{fGarch::garchSim()} simulates a \emph{GARCH} model, but it uses its own random innovations, so its output is not reproducible.
      <<echo=TRUE,eval=FALSE>>=
# Specify GARCH model
garch_spec <- fGarch::garchSpec(model=list(ar=c(0, 0), omega=om_ega, 
  alpha=alpha, beta=betav))
# Simulate GARCH model
garch_sim <- fGarch::garchSim(spec=garch_spec, n=nrows)
retsp <- as.numeric(garch_sim)
# Calculate kurtosis of GARCH returns
moments::moment(retsp, order=4) /
  moments::moment(retsp, order=2)^2
# Perform Jarque-Bera test of normality
tseries::jarque.bera.test(retsp)
# Plot histogram of GARCH returns
histp <- hist(retsp, col="lightgrey",
  xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
  ylab="frequency", freq=FALSE,
  main="GARCH Returns Histogram")
lines(density(retsp, adjust=1.5), lwd=3, col="blue")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_fGarch_hist.png}
      <<echo=TRUE,eval=FALSE>>=
# Fit t-distribution into GARCH returns
optiml <- MASS::fitdistr(retsp, densfun="t", df=2, lower=c(-1, 1e-7))
loc <- optiml$estimate[1]
scalev <- optiml$estimate[2]
curve(expr=dt((x-loc)/scalev, df=2)/scalev,
  type="l", xlab="", ylab="", lwd=3,
  col="red", add=TRUE)
legend("topright", inset=0.05, bty="n",
       leg=c("density", "t-distr w/ 2 dof"),
       lwd=6, lty=1, col=c("blue", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Returns Kurtosis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The expected value of the variance $\sigma^2$ of \emph{GARCH} returns is proportional to the parameter $\omega$:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      The expected value of the kurtosis $\kappa$ of \emph{GARCH} returns is equal to:
      \begin{flalign*}
        \kappa = 3 + \frac{6 \alpha^2}{1 - 2 \alpha^2 - (\alpha + \beta)^2}
      \end{flalign*}
      The excess kurtosis $\kappa - 3$ is proportional to $\alpha^2$ because larger values of the parameter $\alpha$ produce larger variance spikes which produce larger kurtosis.
      \vskip1ex
      The distribution of kurtosis is highly positively skewed, especially for short returns samples, so most kurtosis values will be significantly below their expected value. 
      <<echo=TRUE,eval=FALSE>>=
# Calculate variance of GARCH returns
var(retsp)
# Calculate expected value of variance
om_ega/(1-alpha-betav)
# Calculate kurtosis of GARCH returns
mean(((retsp-mean(retsp))/sd(retsp))^4)
# Calculate expected value of kurtosis
3 + 6*alpha^2/(1-2*alpha^2-(alpha+betav)^2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_kurtosis.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the distribution of GARCH kurtosis
kurt <- sapply(1:1e4, function(x) {
  garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha, 
    beta=betav, innov=matrix(rnorm(nrows)))
  retsp <- garch_data[, 1]
  c(var(retsp), mean(((retsp-mean(retsp))/sd(retsp))^4))
})  # end sapply
kurt <- t(kurt)
apply(kurt, 2, mean)
# Plot the distribution of GARCH kurtosis
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
histp <- hist(kurt[, 2], breaks=500, col="lightgrey",
  xlim=c(2, 8), xlab="returns", ylab="frequency", freq=FALSE,
  main="Distribution of GARCH Kurtosis")
lines(density(kurt[, 2], adjust=1.5), lwd=3, col="blue")
abline(v=(3 + 6*alpha^2/(1-2*alpha^2-(alpha+betav)^2)), lwd=3, col="red")
text(x=7.0, y=0.4, "Expected Kurtosis")
quartz.save("figure/garch_kurtosis.png", type="png", width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance Estimation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} model can be used to estimate the rolling variance of empirical (historical) returns.
      \vskip1ex
      If the time series of returns $r_i$ is given, then it can be used in the \emph{GARCH(1,1)} formula to estimate the rolling variance $\sigma^2_i$: 
      \begin{displaymath}
        \sigma^2_i = \omega + \alpha (r_i - \mu)^2 + \beta \sigma^2_{i-1}
      \end{displaymath}
      If the returns from the \emph{GARCH(1,1)} simulation are used in the above formula, then it produces the simulated \emph{GARCH(1,1)} variance.
      \vskip1ex
      But to estimate the rolling variance of historical returns, the parameters $\omega$, $\alpha$, and $\beta$ must be estimated through model calibration.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate the GARCH process using Rcpp
garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha,  
  beta=betav, innov=matrix(innov))
# Extract the returns
retsp <- garch_data[, 1]
# Estimate the rolling variance from the returns
variance <- numeric(nrows)
variance[1] <- om_ega/(1-alpha-betav)
for (i in 2:nrows) {
  variance[i] <- om_ega + alpha*returns[i]^2 +
    betav*variance[i-1]
}  # end for
all.equal(garch_data[, 2], variance, check.attributes=FALSE)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Calibration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GARCH} models can be calibrated from the returns using the \emph{maximum-likelihood} method.
      \vskip1ex
      But it's a complex optimization procedure which requires a large amount of data for accurate results.
      \vskip1ex
      The function \texttt{fGarch::garchFit()} calibrates a \emph{GARCH} model on a time series of returns.
      \vskip1ex
      The function \texttt{garchFit()} returns an \texttt{S4} object of class \emph{fGARCH}, with multiple slots containing the \emph{GARCH} model outputs and diagnostic information.
      <<echo=TRUE,eval=FALSE>>=
library(fGarch)
# Fit returns into GARCH
garch_fit <- fGarch::garchFit(data=retsp)
# Fitted GARCH parameters
garch_fit@fit$coef
# Actual GARCH parameters
c(mu=mean(retsp), omega=om_ega,alpha=alpha, beta=betav)
# Plot GARCH fitted volatility
plot(sqrt(garch_fit@fit$series$h), t="l",
  col="blue", xlab="", ylab="",
  main="GARCH Fitted Volatility")
quartz.save("figure/garch_fGarch_fitted.png", 
  type="png", width=6, height=5)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_fGarch_fitted.png}\\
      \includegraphics[width=0.4\paperwidth]{figure/garch_volat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{GARCH(1,1)} volatility model, the returns follow the process: $r_i = \mu + \sigma_{i-1} \xi_i$.  (We can assume that the returns have been de-meaned, so that $\mu = 0$.)
      \vskip1ex
      So the \emph{conditional} distribution of returns is normal with standard deviation equal to $\sigma_{i-1}$:
      \begin{displaymath}
        \phi(r_i, \sigma_{i-1}) = \frac{e^{-r^2_i/2\sigma^2_{i-1}}}{\sqrt{2 \pi} \sigma_{i-1}}
      \end{displaymath}
      The \emph{log-likelihood} function $\mathcal{L}(\omega, \alpha, \beta | r_i)$ for the normally distributed returns is therefore equal to:
      \begin{displaymath}
        \mathcal{L}(\omega, \alpha, \beta | r_i) = - \sum_{i=1}^n (\frac{r^2_i}{\sigma^2_{i-1}} + \log(\sigma^2_{i-1}))
      \end{displaymath}
      The \emph{log-likelihood} depends on the \emph{GARCH(1,1)} parameters $\omega$, $\alpha$, and $\beta$ because the rolling variance $\sigma^2_i$ depends on the \emph{GARCH(1,1)} parameters:
      \begin{displaymath}
        \sigma^2_i = \omega + \alpha r^2_i + \beta \sigma^2_{i-1}
      \end{displaymath}
      The \emph{GARCH} process must be simulated using an explicit loop, so it's better to perform it in \texttt{C++} instead of \texttt{R}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define likelihood function
likefun <- function(om_ega, alpha, betav) {
  # Estimate the rolling variance from the returns
  variance <- numeric(nrows)
  variance[1] <- om_ega/(1-alpha-betav)
  for (i in 2:nrows) {
    variance[i] <- om_ega + alpha*returns[i]^2 + betav*variance[i-1]
  }  # end for
  variance <- ifelse(variance > 0, variance, 0.000001)
  # Lag the variance
  variance <- rutils::lagit(variance, pad_zeros=FALSE)
  # Calculate the likelihood
  -sum(retsp^2/variance + log(variance))
}  # end likefun
# Calculate the likelihood in R
likefun(om_ega, alpha, betav)
# Calculate the likelihood in Rcpp
HighFreq::lik_garch(omega=om_ega, alpha=alpha,
  beta=betav, returns=matrix(retsp))
# Benchmark speed of likelihood calculations
library(microbenchmark)
summary(microbenchmark(
  Rcode=likefun(om_ega, alpha, betav),
  Rcpp=HighFreq::lik_garch(omega=om_ega, alpha=alpha, beta=betav, returns=matrix(retsp))
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function Matrix}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} \emph{log-likelihood} function depends on three parameters $\mathcal{L}(\omega, \alpha, \beta | r_i)$.
      \vskip1ex
      The more parameters the harder it is to find their optimal values using optimization.
      \vskip1ex
      We can simplify the optimization task by assuming that the expected variance is equal to the realized variance:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta} = \frac{1}{n-1} \sum_{i=1}^n (r_i-\bar{r})^2
      \end{displaymath}
      This way the \emph{log-likelihood} becomes a function of only two parameters, say $\alpha$ and $\beta$. 
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
# Calculate the variance of returns
retsp <- garch_data[, 1, drop=FALSE]
variance <- var(retsp)
retsp <- (retsp - mean(retsp))
# Calculate likelihood as function of alpha and betav parameters
likefun <- function(alpha, betav) {
  om_ega <- variance*(1 - alpha - betav)
  -HighFreq::lik_garch(omega=om_ega, alpha=alpha, beta=betav, returns=retsp)
}  # end likefun
# Calculate matrix of likelihood values
alphas <- seq(from=0.15, to=0.35, len=50)
betas <- seq(from=0.35, to=0.5, len=50)
lik_mat <- sapply(alphas, function(alpha) sapply(betas,
  function(betav) likefun(alpha, betav)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Perspective Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The perspective plot shows that the \emph{log-likelihood} is much more sensitive to the $\beta$ parameter than to $\alpha$. 
      \vskip1ex
      The function \texttt{rgl::persp3d()} plots an \emph{interactive} 3d surface plot of a \emph{vectorized} function or a matrix.
      \vskip1ex
      The optimal values of $\alpha$ and $\beta$ can be found approximately using a grid search on the \emph{log-likelihood} matrix. 
      <<eval=FALSE,echo=TRUE>>=
# Set rgl options and load package rgl
options(rgl.useNULL=TRUE); library(rgl)
# Draw and render 3d surface plot of likelihood function
n_col <- 100
color <- rainbow(n_col, start=2/6, end=4/6)
z_col <- cut(lik_mat, n_col)
rgl::persp3d(alphas, betas, lik_mat, col=color[z_col],
        xlab="alpha", ylab="beta", zlab="likelihood")
rgl::rglwidget(elementId="plot3drgl", width=700, height=700)
# Perform grid search
coord <- which(lik_mat == min(lik_mat), arr.ind=TRUE)
c(alphas[coord[2]], betas[coord[1]])
lik_mat[coord]
likefun(alphas[coord[2]], betas[coord[1]])
# Optimal and actual parameters
options(scipen=2)  # Use fixed not scientific notation
cbind(actual=c(alpha=alpha, beta=betav, omega=om_ega),
  optimal=c(alphas[coord[2]], betas[coord[1]], variance*(1 - sum(alphas[coord[2]], betas[coord[1]]))))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garchlik.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Likelihood Function Optimization}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The flat shape of the \emph{GARCH} likelihood function makes it difficult for steepest descent optimizers to find the best parameters.
      \vskip1ex
      The function \texttt{DEoptim()} from package \emph{DEoptim} performs \emph{global} optimization using the \emph{Differential Evolution} algorithm.
      \vskip1ex
      \emph{Differential Evolution} is a genetic algorithm which evolves a population of solutions over several generations:\\
      \hskip1em\url{http://www1.icsi.berkeley.edu/~storn/code.html}
      \vskip1ex
      The first generation of solutions is selected randomly.
      \vskip1ex
      Each new generation is obtained by combining the best solutions from the previous generation.
      \vskip1ex
      The \emph{Differential Evolution} algorithm is well suited for very large multi-dimensional optimization problems, such as portfolio optimization.
      \vskip1ex
      \emph{Gradient} optimization methods are more efficient than \emph{Differential Evolution} for smooth objective functions with no local minima.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<eval=FALSE,echo=TRUE>>=
# Define vectorized likelihood function
likefun <- function(x, retsp) {
  alpha <- x[1]; betav <- x[2]; om_ega <- x[3]
  -HighFreq::lik_garch(omega=om_ega, alpha=alpha, beta=betav, returns=retsp)
}  # end likefun
# Initial parameters
initp <- c(alpha=0.2, beta=0.4, omega=variance/0.2)
# Find max likelihood parameters using steepest descent optimizer
optiml <- optim(par=initp,
  fn=likefun, # Log-likelihood function
  method="L-BFGS-B", # Quasi-Newton method
  returns=retsp, 
  upper=c(0.35, 0.55, variance), # Upper constraint
  lower=c(0.15, 0.35, variance/100)) # Lower constraint
# Optimal and actual parameters
cbind(actual=c(alpha=alpha, beta=betav, omega=om_ega),
      optimal=c(optiml$par["alpha"], optiml$par["beta"], optiml$par["omega"]))
# Find max likelihood parameters using DEoptim
optiml <- DEoptim::DEoptim(fn=likefun,
  upper=c(0.35, 0.55, variance), # Upper constraint
  lower=c(0.15, 0.35, variance/100), # Lower constraint
  returns=retsp, 
  control=list(trace=FALSE, itermax=1000, parallelType=1))
# Optimal and actual parameters
cbind(actual=c(alpha=alpha, beta=betav, omega=om_ega),
      optimal=c(optiml$optim$bestmem[1], optiml$optim$bestmem[2], optiml$optim$bestmem[3]))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance of Stock Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH} model can be used to estimate the rolling variance of empirical (historical) returns.
      \vskip1ex
      If the time series of returns $r_i$ is given, then it can be used in the \emph{GARCH(1,1)} formula to estimate the rolling variance $\sigma^2_i$: 
      \begin{displaymath}
        \sigma^2_i = \omega + \alpha (r_i - \mu)^2 + \beta \sigma^2_{i-1}
      \end{displaymath}
      The \emph{GARCH} formula can be viewed as a generalization of the \emph{EWMA} rolling variance.
      <<eval=FALSE,echo=TRUE>>=
# Calculate VTI returns
retsp <- rutils::diffit(log(quantmod::Cl(rutils::etfenv$VTI)))
# Find max likelihood parameters using DEoptim
optiml <- DEoptim::DEoptim(fn=likefun,
  upper=c(0.4, 0.9, variance), # Upper constraint
  lower=c(0.1, 0.5, variance/100), # Lower constraint
  returns=retsp, 
  control=list(trace=FALSE, itermax=1000, parallelType=1))
# Optimal parameters
par_am <- unname(optiml$optim$bestmem)
alpha <- par_am[1]; betav <- par_am[2]; om_ega <- par_am[3]
c(alpha, betav, om_ega)
# Equilibrium GARCH variance
om_ega/(1-alpha-betav)
drop(var(retsp))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_volat_vti.png}
      <<eval=FALSE,echo=TRUE>>=
# Estimate the GARCH volatility of VTI returns
nrows <- NROW(retsp)
variance <- numeric(nrows)
variance[1] <- om_ega/(1-alpha-betav)
for (i in 2:nrows) {
  variance[i] <- om_ega + alpha*retsp[i]^2 + betav*variance[i-1]
}  # end for
# Estimate the GARCH volatility using Rcpp
garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha, 
  beta=betav, innov=retsp, is_random=FALSE)
all.equal(garch_data[, 2], variance, check.attributes=FALSE)
# Plot dygraph of the estimated GARCH volatility
dygraphs::dygraph(xts::xts(sqrt(variance), zoo::index(retsp)), 
  main="Estimated GARCH Volatility of VTI") %>%
  dyOptions(colors="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Variance Forecasts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one-step-ahead forecast of the squared returns is equal to their expected value: $r^2_{i+1} = \mathbb{E}[(\sigma_i \xi_i)^2] = \sigma^2_i$, since $\mathbb{E}[\xi^2_i] = 1$.
      \vskip1ex
      So the variance forecasts depend on the variance in the previous period: 
      $\sigma^2_{i+1} = \mathbb{E}[\omega + \alpha r^2_{i+1} + \beta \sigma^2_i] = \omega + (\alpha + \beta) \sigma^2_i$ 
      \vskip1ex
      The variance forecasts gradually settles to the equilibrium value $\sigma^2$, such that the forecast is equal to itself: $\sigma^2 = \omega + (\alpha + \beta) \sigma^2$.
      \vskip1ex
      This gives: $\sigma^2 = \frac{\omega}{1 - \alpha - \beta}$, which is the long-term expected value of the variance.
      \vskip1ex
      So the variance forecasts decay exponentially to their equilibrium value $\sigma^2$ at the decay rate equal to $(\alpha + \beta)$: 
      \begin{displaymath}
        \sigma^2_{i+1} - \sigma^2 = (\alpha + \beta) (\sigma^2_i - \sigma^2)
      \end{displaymath}
      <<echo=TRUE,eval=FALSE>>=
# Simulate GARCH model
garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha, 
  beta=betav, innov=matrix(innov))
variance <- garch_data[, 2]
# Calculate the equilibrium variance
var_eq <- om_ega/(1-alpha-betav)
# Calculate the variance forecasts
varcasts <- numeric(10)
varcasts[1] <- var_eq + 
  (alpha + betav)*(xts::last(variance) - var_eq)
for (i in 2:10) {
  varcasts[i] <- var_eq + (alpha + betav)*(varcasts[i-1] - var_eq)
}  # end for
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/garch_forecast.png}
      <<echo=TRUE,eval=FALSE>>=
# Open plot window on Mac
dev.new(width=6, height=5, noRStudioGD=TRUE)
par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
# Plot GARCH variance forecasts
plot(tail(variance, 30), t="l", col="blue", xlab="", ylab="",
  xlim=c(1, 40), ylim=c(0, max(tail(variance, 30))), 
  main="GARCH Variance Forecasts")
text(x=15, y=0.5*var_eq, "realized variance")
lines(x=30:40, y=c(xts::last(variance), varcasts), col="red", lwd=3)
text(x=35, y=0.6*var_eq, "variance forecasts")
abline(h=var_eq, lwd=3, col="red")
text(x=10, y=1.1*var_eq, "Equilibrium variance")
quartz.save("figure/garch_forecast.png", type="png", 
  width=6, height=5)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: old stuff about Estimating Volatility of Intraday Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{close-to-close} estimator depends on \emph{Close} prices specified over the aggregation intervals:
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{C_i}{C_{i-1}})-\bar{r})^2
      \end{displaymath}
      \vspace{-1em}
      \begin{displaymath}
        \bar{r} = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{C_i}{C_{i-1}})
      \end{displaymath}
      Volatility estimates for intraday time series depend both on the units of returns (per second, minute, day, etc.), and on the aggregation interval (secondly, minutely, daily, etc.)
      \vskip1ex
      A minutely time interval is equal to \texttt{60} seconds, a daily time interval is equal to \texttt{24*60*60 = 86,400} seconds.
      \vskip1ex
      For example, it's possible to measure returns in minutely intervals in units per second.
      \vskip1ex
      The estimated volatility is directly proportional to the measurement units.
      \vskip1ex
      For example, the volatility estimated from per minute returns is \texttt{60} times the volatility estimated from per second returns.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely SPY returns (unit per minute) single day
retsp <- rutils::diffit(log(SPY["2012-02-13", 4]))
# Minutely SPY volatility (unit per minute)
sd(retsp)
# Divide minutely SPY returns by time intervals (unit per second)
retsp <- retsp / rutils::diffit(.zoo::index(SPY["2012-02-13"]))
retsp[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retsp)
# SPY returns multiple days
retsp <- rutils::diffit(log(SPY[, 4]))
# Minutely SPY volatility (includes overnight jumps)
sd(retsp)
# Table of intervals - 60 second is most frequent
indeks <- rutils::diffit(.zoo::index(SPY))
table(indeks)
# hist(indeks)
# SPY returns with overnight scaling (unit per second)
retsp <- retsp / indeks
retsp[1] <- 0
# Minutely SPY volatility scaled to unit per minute
60*sd(retsp)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility as Function of Aggregation Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimated volatility $\sigma$ scales as the \emph{power} of the length of the aggregation time interval $\Delta t$:
      \begin{displaymath}
        \frac{\sigma_t}{\sigma} = {\Delta t} ^ H
      \end{displaymath}
      Where \texttt{H} is the \emph{Hurst} exponent, $\sigma$ is the return volatility, and $\sigma_t$ is the volatility of the aggregated returns.
      \vskip1ex
      If returns follow \emph{Brownian motion} then the volatility scales as the \emph{square root} of the length of the aggregation interval (\texttt{H = 0.5}).
      \vskip1ex
      If returns are \emph{mean reverting} then the volatility scales slower than the \emph{square root} (\texttt{H < 0.5}).
      \vskip1ex
      If returns are \emph{trending} then the volatility scales faster than the \emph{square root} (\texttt{H > 0.5}).
      \vskip1ex
      The length of the daily time interval is often approximated to be equal to \texttt{390 = 6.5*60} minutes, since the exchange trading session is equal to \texttt{6.5} hours, and daily volatility is dominated by the trading session.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=(-(1:1)),eval=FALSE>>=
library(HighFreq)  # Load HighFreq
# Minutely OHLC SPY prices aggregated to daily prices
SPY_daily <- rutils::to_period(ohlc=HighFreq::SPY, period="days")
# Daily SPY volatility from daily returns
sd(rutils::diffit(log(SPY_daily[, 4])))
# Minutely SPY returns (unit per minute)
retsp <- rutils::diffit(log(SPY[, 4]))
# Minutely SPY volatility scaled to daily interval
sqrt(6.5*60)*sd(retsp)
# Minutely SPY returns with overnight scaling (unit per second)
retsp <- rutils::diffit(log(SPY[, 4]))
indeks <- rutils::diffit(.zoo::index(SPY))
retsp <- retsp / indeks
retsp[1] <- 0
# Daily SPY volatility from minutely returns
sqrt(6.5*60)*60*sd(retsp)
# Daily SPY volatility
# Scale by extra time over weekends and holidays
24*60*60*sd(rutils::diffit(log(SPY_daily[, 4]))[-1] /
            rutils::diffit(.zoo::index(SPY_daily))[-1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For a single aggregation interval, the \emph{Hurst exponent} \texttt{H} is equal to:
      \begin{displaymath}
        H = \frac{\log{\sigma_t} - \log{\sigma}}{\log{\Delta t}}
      \end{displaymath}
      For a vector of aggregation intervals, the \emph{Hurst exponent} \texttt{H} can be calculated by regressing the volatility against the aggregation intervals.
        <<echo=TRUE,eval=FALSE>>=
# Calculate SPY returns adjusted for overnight jumps
closep <- log(as.numeric(Cl(HighFreq::SPY[, 4])))
retsp <- rutils::diffit(closep) / 
  rutils::diffit(.zoo::index(HighFreq::SPY))
retsp[1] <- 0
closep <- cumsum(retsp)
nrows <- NROW(closep)
# Calculate volatilities for vector of aggregation intervals
interval_s <- seq.int(from=3, to=35, length.out=9)^2
vol_s <- sapply(interval_s, function(interval) {
  num_agg <- nrows %/% interval
  endp <- c(0, nrows - num_agg*interval + (0:num_agg)*interval)
  # endp <- rutils::calc_endpoints(closep, interval=interval)
  sd(rutils::diffit(closep[endp]))
})  # end sapply
# Calculate Hurst as regression slope using formula
vol_log <- log(vol_s)
inter_log <- log(interval_s)
hurs_t <- cov(vol_log, inter_log)/var(inter_log)
# Or using function lm()
model <- lm(vol_log ~ inter_log)
coef(model)[2]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hurst_vol.png}
        <<echo=TRUE,eval=FALSE>>=
# Calculate Hurst from single data point
(last(vol_log) - log(sd(retsp)))/last(inter_log)
# Plot the volatilities
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(1, 1, 1, 1))
plot(vol_log ~ inter_log, lwd=6, col="red",
     xlab="aggregation intervals (log)", ylab="volatility (log)",
     main="Hurst Exponent for SPY From Volatilities")
abline(model, lwd=3, col="blue")
text(inter_log[2], vol_log[NROW(vol_log)-1], 
     paste0("Hurst = ", round(hurs_t, 4)))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rescaled Range Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The range $R_{\Delta t}$ of prices $p_t$ over an interval $\Delta t$, is the difference between the highest attained price minus the lowest:
      \begin{displaymath}
        R_t = \max_{\Delta t}{[p_{\tau}]} - \min_{\Delta t}{[p_{\tau}]}
      \end{displaymath}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is equal to the range $R_{\Delta t}$ divided by the standard deviation of the price differences $\sigma_t$: $RS_{\Delta t} = R_t / \sigma_t$.
      \vskip1ex
      The \emph{Rescaled Range} $RS_{\Delta t}$ for a time series of prices is calculated by:
      \begin{itemize}
        \item Dividing the time series into non-overlapping intervals of length $\Delta t$,
        \item Calculating the \emph{rescaled range} $RS_{\Delta t}$ for each interval,
        \item Calculating the average of the \emph{rescaled ranges} $RS_{\Delta t}$ for all the intervals.
      \end{itemize}
      \emph{Rescaled Range Analysis} (R/S) consists of calculating the average \emph{rescaled range} $RS_{\Delta t}$ as a function of the length of the aggregation interval $\Delta t$.
    \column{0.5\textwidth}
      \vspace{-1em}
        <<echo=TRUE,eval=FALSE>>=
# Calculate the rescaled range
interval <- 500
nrows <- NROW(closep); num_agg <- nrows %/% interval
endp <- c(0, nrows - num_agg*interval + (0:num_agg)*interval)
# Or
# endp <- rutils::calc_endpoints(closep, interval=interval)
r_s <- sapply(2:NROW(endp), function(ep) {
  indeks <- endp[ep-1]:endp[ep]
  diff(range(closep[indeks]))/sd(retsp[indeks])
})  # end sapply
mean(r_s)
# Calculate Hurst from single data point
log(mean(r_s))/log(interval)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Rescaled Range}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is proportional to the length of the aggregation interval $\Delta t$ raised to the power of the \emph{Hurst exponent} \texttt{H}:
      \begin{displaymath}
        RS_{\Delta t} \propto {\Delta t}^H
      \end{displaymath}
      The \emph{Hurst exponents} calculated from the \emph{rescaled range} and the \emph{volatility} are similar because they both measure the dependence of returns over time, but they're not exactly equal because they use different methods to estimate price dispersion.
        <<echo=TRUE,eval=FALSE>>=
# Calculate rescaled range for vector of aggregation intervals
nrows <- NROW(closep)
r_s <- sapply(interval_s, function(interval) {
# Calculate end points
  num_agg <- nrows %/% interval
  endp <- c(0, nrows - num_agg*interval + (0:num_agg)*interval)
# Calculate rescaled ranges
  r_s <- sapply(2:NROW(endp), function(ep) {
    indeks <- endp[ep-1]:endp[ep]
    diff(range(closep[indeks]))/sd(retsp[indeks])
  })  # end sapply
  mean(na.omit(r_s))
})  # end sapply
# Calculate Hurst as regression slope using formula
rs_log <- log(r_s)
inter_log <- log(interval_s)
hurs_t <- cov(rs_log, inter_log)/var(inter_log)
# Or using function lm()
model <- lm(rs_log ~ inter_log)
coef(model)[2]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hurst_reg.png}
      \vspace{-1em}
        <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(4, 4, 2, 1), oma=c(1, 1, 1, 1))
plot(rs_log ~ inter_log, lwd=6, col="red",
     xlab="aggregation intervals (log)",
     ylab="rescaled range (log)",
     main="Rescaled Range Analysis for SPY")
abline(model, lwd=3, col="blue")
text(inter_log[2], rs_log[NROW(rs_log)-1], 
     paste0("Hurst = ", round(hurs_t, 4)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{High Frequency and Intraday Time Series Data}


%%%%%%%%%%%%%%%
\subsection{Trade and Quote (\protect\emph{TAQ}) Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{High frequency data} is typically formatted as either Trade and Quote (\emph{TAQ}) data, or \emph{Open-High-Low-Close} (\emph{OHLC}) data.
      \vskip1ex
      Trade and Quote (\emph{TAQ}) data contains intraday \emph{trades} and \emph{quotes} on exchange-traded stocks and futures.
      \vskip1ex
      \emph{TAQ} data is often called \emph{tick data}, with a \emph{tick} being a row of data containing new \emph{trades} or \emph{quotes}.
      \vskip1ex
      The \emph{TAQ} data is spaced irregularly in time, with data recorded each time a new trade or quote arrives.
      \vskip1ex
      Each row of \emph{TAQ} data may contain the quote and trade prices, and the corresponding quote size or trade volume:
      \emph{Bid.Price, Bid.Size, Ask.Price, Ask.Size, Trade.Price, Volume}.
      \vskip1ex
      \emph{TAQ} data is often split into \emph{trade} data and \emph{quote} data.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=(-(1:1)),size="tiny",tidy=FALSE,eval=TRUE>>=
options(width=200)
# Load package HighFreq
library(HighFreq)
# Or load the high frequency data file directly:
# symbolv <- load("/Users/jerzy/Develop/R/HighFreq/data/hf_data.RData")
head(HighFreq::SPY_TAQ)
head(HighFreq::SPY)
tail(HighFreq::SPY)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Downloading \protect\emph{TAQ} Data From \protect\emph{WRDS}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{TAQ} data can be downloaded from the
      \href{https://wrds-web.wharton.upenn.edu/wrds/ds/taq/ctm/index.cfm}{\emph{WRDS TAQ}} web page.
      \vskip1ex
      The \emph{TAQ} data are at millisecond frequency, and are \emph{consolidated} (combined) from the New York Stock Exchange \emph{NYSE} and other exchanges.
      \vskip1ex
      The
      \href{https://wrds-web.wharton.upenn.edu/wrds/ds/taq/ctm/index.cfm}{\emph{WRDS TAQ}}
      web page provides separately \emph{trades} data and separately \emph{quotes} data.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/wrds_taq_data.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Reading \protect\emph{TAQ} Data From \texttt{.csv} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{TAQ} data stored in \texttt{.csv} files can be very large, so it's better to read it using the function \texttt{data.table::fread()} which is much faster than the function \texttt{read.csv()}.
      \vskip1ex
      Each \emph{trade} or \emph{quote} contributes a \emph{tick} (row) of data, and the number of ticks can be very large (hundred of thousands per day, or more).
      \vskip1ex
      The function \texttt{strptime()} coerces \texttt{character} strings representing the date and time into \texttt{POSIXlt} \emph{date-time} objects.
      \vskip1ex
      The argument \texttt{format="\%H:\%M:\%OS"} allows the parsing of fractional seconds, for example \texttt{"15:59:59.989847074"}.
      \vskip1ex
      The function \texttt{as.POSIXct()} coerces objects into \texttt{POSIXct} \emph{date-time} objects, with a \texttt{numeric} value representing the \emph{moment of time} in seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(rutils)
# Read TAQ trade data from csv file
taq <- data.table::fread(file="/Users/jerzy/Develop/data/xlk_tick_trades2020_0316.csv")
# Inspect the TAQ data
taq
class(taq)
colnames(taq)
sapply(taq, class)
symbol <- taq$SYM_ROOT[1]
# Create date-time index
dates <- paste(taq$DATE, taq$TIME_M)
# Coerce date-time index to POSIXlt
dates <- strptime(dates, "%Y%m%d %H:%M:%OS")
class(dates)
# Display more significant digits
# options("digits")
options(digits=20, digits.secs=10)
last(dates)
unclass(last(dates))
as.numeric(last(dates))
# Coerce date-time index to POSIXct
dates <- as.POSIXct(dates)
class(dates)
last(dates)
unclass(last(dates))
as.numeric(last(dates))
# Calculate the number of ticks per second
n_secs <- as.numeric(last(dates)) - as.numeric(first(dates))
NROW(taq)/(6.5*3600)
# Select TAQ data columns
taq <- taq[, .(price=PRICE, volume=SIZE)]
# Add date-time index
taq <- cbind(index=dates, taq)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Microstructure Noise in High Frequency Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      High frequency data contains \emph{microstructure noise} in the form of \emph{price jumps} and the \emph{bid-ask bounce}.
      \vskip1ex
      \emph{Price jumps} are single ticks with prices far away from the average.
      \vskip1ex
      \emph{Price jumps} are often caused by data collection errors, but sometimes they represent actual very large lot trades.
      \vskip1ex
      The \emph{bid-ask bounce} is the bouncing of traded prices between the bid and ask prices.
      \vskip1ex
      The \emph{bid-ask bounce} creates an illusion of rapidly changing prices, while in fact the mid price is constant.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_dygraph.png}
      % \vspace{-1em}
      % \includegraphics[width=0.45\paperwidth]{figure/hf_stocks_bounce.png}
      <<echo=TRUE,eval=FALSE>>=
# Coerce trade ticks to xts series
xtes <- xts::xts(taq[, .(price, volume)], taq$index)
colnames(xtes) <- paste(symbol, c("Close", "Volume"), sep=".")
save(xtes, file="/Users/jerzy/Develop/data/xlk_tick_trades2020_0316.RData")
# save(xtes, file="/Users/jerzy/Develop/data/xlk_tick_trades2020_0316.RData")
# Plot dygraph
dygraphs::dygraph(xtes$XLK.Close,
  main="XLK Trade Ticks for 2020-03-16")
# Plot in x11 window
x11(width=6, height=5)
quantmod::chart_Series(x=xtes$XLK.Close,
  name="XLK Trade Ticks for 2020-03-16")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Removing Odd Lot Trades From \protect\emph{TAQ} Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most of the trade ticks are \emph{odd lots} with a small volume of less than $100$ shares.
      \vskip1ex
      The \emph{odd lot} ticks are often removed to reduce the size of the \emph{TAQ} data.
      \vskip1ex
      Selecting only the large lot trades reduces microstructure noise (price jumps, bid-ask bounce) in high frequency data.
      <<echo=TRUE,eval=FALSE>>=
# Select the large lots greater than 100
dim(taq)
big_ticks <- taq[taq$volume > 100]
dim(big_ticks)
# Number of large lot ticks per second
NROW(big_ticks)/(6.5*3600)
# Save trade ticks with large lots
data.table::fwrite(big_ticks, file="/Users/jerzy/Develop/data/xlk_tick_trades2020_0316_biglots.csv")
# Coerce trade prices to xts
xtes <- xts::xts(big_ticks[, .(price, volume)], big_ticks$index)
colnames(xtes) <- c("XLK.Close", "XLK.Volume")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_biglots_dygraph.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of the large lots
dygraphs::dygraph(xtes$XLK.Close,
  main="XLK Trade Ticks for 2020-03-16 (large lots only)")
# Plot the large lots
x11(width=6, height=5)
quantmod::chart_Series(x=xtes$XLK.Close,
  name="XLK Trade Ticks for 2020-03-16 (large lots only)")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Aggregating \protect\emph{TAQ} Data to \protect\emph{OHLC}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{data table} columns can be \emph{aggregated} over categories (factors) defined by one or more columns passed to the \texttt{"by"} argument.
      \vskip1ex
      Multiple \emph{data table} columns can be referenced by passing a list of names specified by the dot \texttt{.()} operator.
      \vskip1ex
      The function \texttt{round.POSIXt()} rounds date-time objects to seconds, minutes, hours, days, months or years.
      \vskip1ex
      The function \texttt{as.POSIXct()} coerces objects to class \texttt{POSIXct}.
      <<echo=TRUE,eval=FALSE>>=
# Round time index to seconds
good_ticks[, zoo::index := as.POSIXct(round.POSIXt(index, "secs"))]
# Aggregate to OHLC by seconds
ohlc <- good_ticks[, .(open=first(price), high=max(price), low=min(price), close=last(price), volume=sum(volume)), by=index]
# Round time index to minutes
good_ticks[, zoo::index := as.POSIXct(round.POSIXt(index, "mins"))]
# Aggregate to OHLC by minutes
ohlc <- good_ticks[, .(open=first(price), high=max(price), low=min(price), close=last(price), volume=sum(volume)), by=index]
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_ohlc.png}
      <<echo=TRUE,eval=FALSE>>=
# Coerce OHLC prices to xts
xtes <- xts::xts(ohlc[, -"index"], ohlc$index)
# Plot dygraph of the OHLC prices
dygraphs::dygraph(xtes[, -5], main="XLK Trade Ticks for 2020-03-16 (OHLC)") %>%
  dyCandlestick()
# Plot the OHLC prices
x11(width=6, height=5)
quantmod::chart_Series(x=xtes, TA="add_Vo()",
  name="XLK Trade Ticks for 2020-03-16 (OHLC)")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Open-High-Low-Close (\protect\emph{OHLC}) Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Open-High-Low-Close} (\emph{OHLC}) data contains intraday trade prices and trade volumes.
      \vskip1ex
      \emph{OHLC} data is evenly spaced in time, with each row containing the \emph{Open, High, Low, Close} prices, and the trade \emph{Volume}, recorded over the past time interval (called a \emph{bar} of data).
      \vskip1ex
      The \emph{Open} and \emph{Close} prices are the first and last trade prices recorded in the time bar.
      \vskip1ex
      The \emph{High} and \emph{Low} prices are the highest and lowest trade prices recorded in the time bar.
      \vskip1ex
      The \emph{Volume} is the total trading volume recorded in the time bar.
      \vskip1ex
      The \emph{OHLC} data format provides a way of efficiently compressing \emph{TAQ} data, while preserving information about price levels, volatility (range), and trading volumes.
      \vskip1ex
      In addition, evenly spaced \emph{OHLC} data allows for easier analysis of multiple time series, since the prices for different assets are given at the same moments in time.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,size="tiny",tidy=FALSE,eval=TRUE>>=
# Load package HighFreq
library(HighFreq)
head(HighFreq::SPY)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting High Frequency \protect\emph{OHLC} Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Aggregating high frequency \emph{TAQ} data into \emph{OHLC} format with lower periodicity allows for data compression while maintaining some information about volatility.
      <<earl_ohlc_chart,echo=TRUE,eval=FALSE,fig.width=7,fig.height=6,fig.show='hide'>>=
# Load package HighFreq
library(HighFreq)
# Define symbol
symbol <- "SPY"
# Load OHLC data
output_dir <- "/Users/jerzy/Develop/data/hfreq/scrub/"
symbol <- load(file.path(output_dir, paste0(symbol, ".RData")))
interval <-"2013-11-11 09:30:00/2013-11-11 10:30:00"
chart_Series(SPY[interval], name=symbol)
      @
      The package \emph{HighFreq} contains both \emph{TAQ} data and \emph{Open-High-Low-Close} (\emph{OHLC}) data.
      \vskip1ex
      If you are not able to install package \emph{HighFreq} then download the file \texttt{hf\_data.RData} from Brightspace and load it.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/earl_ohlc_chart-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{HighFreq} for Managing High Frequency Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{HighFreq} contains functions for managing high frequency time series data, such as:
      \begin{itemize}
        \item converting \emph{TAQ} data to \emph{OHLC} format,
        \item chaining and joining time series,
        \item scrubbing bad data,
        \item managing time zones and alligning time indices,
        \item aggregating data to lower frequency (periodicity),
        \item calculating rolling aggregations (VWAP, Hurst exponent, etc.),
        \item calculating seasonality aggregations,
        \item estimating volatility, skewness, and higher moments,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Install package HighFreq from github
devtools::install_github(repo="algoquant/HighFreq")
# Load package HighFreq
library(HighFreq)
# Get documentation for package HighFreq
# Get short description
packageDescription(HighFreq)
# Load help page
help(package=HighFreq)
# List all datasets in HighFreq
data(package=HighFreq)
# List all objects in HighFreq
ls("package:HighFreq")
# Remove HighFreq from search path
detach("package:HighFreq")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Datasets in Package \protect\emph{HighFreq}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{HighFreq} contains several high frequency time series, in \emph{xts} format, stored in a file called \texttt{hf\_data.RData}:
      \begin{itemize}
        \item a time series called \texttt{SPY\_TAQ}, containing a single day of \emph{TAQ} data for the \emph{SPY} ETF.
        \item three time series called \texttt{SPY}, \texttt{TLT}, and \texttt{VXX}, containing intraday 1-minute \emph{OHLC} data for the \emph{SPY}, \emph{TLT}, and \emph{VXX} ETFs.
      \end{itemize}
      Even after the \emph{HighFreq} package is loaded, its datasets aren't loaded into the workspace, so they aren't listed in the workspace.
      \vskip1ex
      That's because the datasets in package \emph{HighFreq} are set up for \emph{lazy loading}, which means they can be called as if they were loaded, even though they're not loaded into the workspace.
      \vskip1ex
      The datasets in package \emph{HighFreq} can be loaded into the workspace using the function \texttt{data()}.
      \vskip1ex
      The data is set up for \emph{lazy loading}, so it doesn't require calling \texttt{data(hf\_data)} to load it into the workspace before calling it.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load package HighFreq
library(HighFreq)
# You can see SPY when listing objects in HighFreq
ls("package:HighFreq")
# You can see SPY when listing datasets in HighFreq
data(package=HighFreq)
# But the SPY dataset isn't listed in the workspace
ls()
# HighFreq datasets are lazy loaded and available when needed
head(HighFreq::SPY)
# Load all the datasets in package HighFreq
data(hf_data)
# HighFreq datasets are now loaded and in the workspace
head(HighFreq::SPY)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      High frequency returns exhibit \emph{large negative skewness} and \emph{very large kurtosis} (leptokurtosis), or fat tails.
      \vskip1ex
      Student's \emph{t-distribution} has fat tails, so it fits high frequency returns much better than the normal distribution.
      \vskip1ex
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution into a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      <<echo=(-(1:1)),eval=FALSE>>=
library(rutils)  # Load package rutils
# SPY percentage returns
ohlc <- HighFreq::SPY
nrows <- NROW(ohlc)
closep <- log(quantmod::Cl(ohlc))
returns <- rutils::diffit(closep)
colnames(returns) <- "SPY"
# Standardize raw returns to make later comparisons
returns <- (returns - mean(returns))/sd(returns)
# Calculate moments and perform normality test
sapply(c(var=2, skew=3, kurt=4), function(x) sum(returns^x)/nrows)
tseries::jarque.bera.test(returns)
# Fit SPY returns using MASS::fitdistr()
optiml <- MASS::fitdistr(returns, densfun="t", df=2)
loc <- optiml$estimate[1]
scalev <- optiml$estimate[2]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_dist.png}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
# Plot histogram of SPY returns
histp <- hist(returns, col="lightgrey", mgp=c(2, 1, 0),
  xlab="returns (standardized)", ylab="frequency", xlim=c(-3, 3),
  breaks=1e3, freq=FALSE, main="Distribution of High Frequency SPY Returns")
# lines(density(returns, bw=0.2), lwd=3, col="blue")
# Plot t-distribution function
curve(expr=dt((x-loc)/scalev, df=2)/scalev,
      type="l", lwd=3, col="red", add=TRUE)
# Plot the Normal probability distribution
curve(expr=dnorm(x, mean=mean(returns),
  sd=sd(returns)), add=TRUE, lwd=3, col="blue")
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("t-distr", "normal"),
  lwd=6, lty=1, col=c("red", "blue"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Aggregated High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns depends on the sampling frequency.
      \vskip1ex
      High frequency returns aggregated to a lower periodicity become less negatively skewed and less fat tailed, and closer to the normal distribution.
      \vskip1ex
      The function \texttt{xts::to.period()} converts a time series to a lower periodicity (for example from hourly to daily periodicity).
      <<echo=TRUE,eval=FALSE>>=
# Hourly SPY percentage returns
closep <- log(Cl(xts::to.period(x=ohlc, period="hours")))
retsh <- rutils::diffit(closep)
retsh <- (retsh - mean(retsh))/sd(retsh)
# Daily SPY percentage returns
closep <- log(Cl(xts::to.period(x=ohlc, period="days")))
retsd <- rutils::diffit(closep)
retsd <- (retsd - mean(retsd))/sd(retsd)
# Calculate moments
sapply(list(minutely=returns, hourly=retsh, daily=retsd),
       function(rets) {sapply(c(var=2, skew=3, kurt=4),
                function(x) mean(rets^x))
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_dist_agg.png}
      <<echo=(-(1:2)),eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
# Plot densities of SPY returns
plot(density(returns, bw=0.4), xlim=c(-3, 3),
     lwd=3, mgp=c(2, 1, 0), col="blue",
     xlab="returns (standardized)", ylab="frequency",
     main="Density of High Frequency SPY Returns")
lines(density(retsh, bw=0.4), lwd=3, col="green")
lines(density(retsd, bw=0.4), lwd=3, col="red")
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("minutely", "hourly", "daily"),
  lwd=6, lty=1, col=c("blue", "green", "red"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Rolling Volatility of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility of high frequency returns can be inflated by large overnight returns.
      \vskip1ex
      The large overnight returns can be scaled down by dividing them by the overnight time interval.
        <<echo=TRUE,eval=FALSE>>=
# Calculate rolling volatility of SPY returns
ret2013 <- returns["2013-11-11/2013-11-15"]
# Calculate rolling volatility
look_back <- 11
endp <- seq_along(ret2013)
startp <- c(rep_len(1, look_back),
  endp[1:(NROW(endp)-look_back)])
endp[endp < look_back] <- look_back
vol_rolling <- sapply(seq_along(endp),
  function(it) sd(ret2013[startp[it]:endp[it]]))
vol_rolling <- xts::xts(vol_rolling, zoo::index(ret2013))
# Extract time intervals of SPY returns
indeks <- c(60, diff(xts::.index(ret2013)))
head(indeks)
table(indeks)
# Scale SPY returns by time intervals
ret2013 <- 60*ret2013/indeks
# Calculate scaled rolling volatility
vol_scaled <- sapply(seq_along(endp),
  function(it) sd(ret2013[startp[it]:endp[it]]))
vol_rolling <- cbind(vol_rolling, vol_scaled)
vol_rolling <- na.omit(vol_rolling)
sum(is.na(vol_rolling))
sapply(vol_rolling, range)
      @
    \column{0.5\textwidth}
      % \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_vol.png}
        <<echo=TRUE,eval=FALSE>>=
# Plot rolling volatility
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue", "red")
chart_Series(vol_rolling, theme=plot_theme,
             name="Rolling Volatility with Overnight Spikes")
legend("topright", legend=colnames(vol_rolling),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Bid-ask Bounce of High Frequency Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{bid-ask bounce} is the bouncing of traded prices between the bid and ask prices.
      \vskip1ex
      The \emph{bid-ask bounce} is prominent at very high frequency time scales or in periods of low volatility.
      \vskip1ex
      The \emph{bid-ask bounce} creates an illusion of rapidly changing prices, while in fact the mid price is constant.
      \vskip1ex
      The \emph{bid-ask bounce} produces very high realized volatility and the appearance of mean reversion (negative autocorrelation), that isn't tradeable for most traders.
      <<echo=TRUE,eval=FALSE>>=
prices <- read.zoo(file="/Users/jerzy/Develop/lecture_slides/data/bid_ask_bounce.csv",
  header=TRUE, sep=",")
prices <- as.xts(prices)
x11(width=6, height=4)
par(mar=c(2, 2, 0, 0), oma=c(1, 1, 0, 0))
chart_Series(x=prices, name="S&P500 Futures Bid-Ask Bounce")
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_futures_bounce.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Volume and Volatility of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Trading volumes typically rise together with market price volatility.
      \vskip1ex
      The function \texttt{apply.daily()} from package \texttt{xts} applies functions to time series over daily periods.
      \vskip1ex
      The function \texttt{calc\_var\_ohlc()} from package \texttt{HighFreq} calculates the variance of an \emph{OHLC} time series using range estimators.
      <<echo=TRUE,eval=FALSE>>=
# Volatility of SPY
sqrt(HighFreq::calcvar_ohlc(ohlc))
# Daily SPY volatility and volume
vol_daily <- sqrt(xts::apply.daily(ohlc, FUN=calcvar_ohlc))
colnames(vol_daily) <- ("SPY_volatility")
volumes <- quantmod::Vo(ohlc)
volume_daily <- xts::apply.daily(volumes, FUN=sum)
colnames(volume_daily) <- ("SPY_volume")
# Plot SPY volatility and volume
datav <- cbind(vol_daily, volume_daily)["2008/2009"]
colnamev <- colnames(datav)
dygraphs::dygraph(datav,
  main="SPY Daily Volatility and Trading Volume") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=3) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=3)
      @
    \column{0.5\textwidth}
      % \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_daily_volat_volume.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Beta of Volume vs Volatility of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      As a general empirical rule, the \emph{trading volume} $\upsilon$ in a given time period is roughly proportional to the \emph{volatility} of the returns $\sigma$: $\upsilon \propto \sigma$.
      \vskip1ex
      The regression of the \emph{log trading volume} versus the \emph{log volatility} fails the \emph{Durbin-Watson test} for the autocorrelation of residuals.
      \vskip1ex
      But the regression of the \emph{differences} passes the \emph{Durbin-Watson test}.
      <<echo=TRUE,eval=FALSE>>=
# Regress log of daily volume vs volatility
datav <- log(cbind(volume_daily, vol_daily))
colnamev <- colnames(datav)
dframe <- as.data.frame(datav)
formulav <- as.formula(paste(colnamev, collapse="~"))
model <- lm(formulav, data=dframe)
# Durbin-Watson test for autocorrelation of residuals
lmtest::dwtest(model)
# Regress diff log of daily volume vs volatility
dframe <- as.data.frame(rutils::diffit(datav))
model <- lm(formulav, data=dframe)
lmtest::dwtest(model)
summary(model)
plot(formulav, data=dframe, main="SPY Daily Trading Volume vs Volatility (log scale)")
abline(model, lwd=3, col="red")
mtext(paste("beta =", round(coef(model)[2], 3)), cex=1.2, lwd=3, side=2, las=2, adj=(-0.5), padj=(-7))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_daily_volume_volat_reg.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Beta of Hourly Trading Volume vs Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Hourly aggregations of high frequency data also support the rule that the \emph{trading volume} is roughly proportional to the \emph{volatility} of the returns: $\upsilon \propto \sigma$.
      <<echo=TRUE,eval=FALSE>>=
# 60 minutes of data in look_back interval
look_back <- 60
vol2013 <- volumes["2013"]
ret2013 <- returns["2013"]
# Define end points with beginning stub
nrows <- NROW(ret2013)
nagg <- nrows %/% look_back
endp <- nrows-look_back*nagg + (0:nagg)*look_back
startp <- c(1, endp[1:(NROW(endp)-1)])
# Calculate SPY volatility and volume
datav <- sapply(seq_along(endp), function(it) {
  point_s <- startp[it]:endp[it]
  c(volume=sum(vol2013[point_s]),
    volatility=sd(ret2013[point_s]))
})  # end sapply
datav <- t(datav)
datav <- rutils::diffit(log(datav))
dframe <- as.data.frame(datav)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_hourly_volume_volat_reg.png}
      <<echo=TRUE,eval=FALSE>>=
formulav <- as.formula(paste(colnames(datav), collapse="~"))
model <- lm(formulav, data=dframe)
lmtest::dwtest(model)
summary(model)
plot(formulav, data=dframe,
     main="SPY Hourly Trading Volume vs Volatility (log scale)")
abline(model, lwd=3, col="red")
mtext(paste("beta =", round(coef(model)[2], 3)), cex=1.2, lwd=3, side=2, las=2, adj=(-0.5), padj=(-7))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{High Frequency Returns in Trading Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{trading time} (volume clock) is the time measured by the level of \emph{trading volume}, with the \emph{volume clock} running faster in periods of higher \emph{trading volume}.
      \vskip1ex
      The time-dependent volatility of high frequency returns (\emph{heteroskedasticity}) produces their \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      The returns can be divided by the \emph{trading volume} to obtain scaled returns over equal trading volumes.
      \vskip1ex
      But the returns should not be divided by very small volumes below a certain threshold.
      \vskip1ex
      The scaled returns have a smaller \emph{skewness} and \emph{kurtosis}, and they also have even higher autocorrelations than unscaled returns.
      <<echo=TRUE,eval=FALSE>>=
# Scale returns using volume (volume clock)
rets_scaled <- ifelse(volumes > 1e4, returns/volumes, 0)
rets_scaled <- rets_scaled/sd(rets_scaled)
# Calculate moments of scaled returns
nrows <- NROW(returns)
sapply(list(returns=returns, rets_scaled=rets_scaled),
  function(rets) {sapply(c(skew=3, kurt=4),
           function(x) sum((rets/sd(rets))^x)/nrows)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_scaled.png}
      <<echo=TRUE,eval=FALSE>>=
x11(width=6, height=5)
par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
# Plot densities of SPY returns
plot(density(returns), xlim=c(-3, 3),
     lwd=3, mgp=c(2, 1, 0), col="blue",
     xlab="returns (standardized)", ylab="frequency",
     main="Density of Volume-scaled High Frequency SPY Returns")
lines(density(rets_scaled, bw=0.4), lwd=3, col="red")
curve(expr=dnorm, add=TRUE, lwd=3, col="green")
# Add legend
legend("topright", inset=0.05, bty="n",
  leg=c("minutely", "scaled", "normal"),
  lwd=6, lty=1, col=c("blue", "red", "green"))
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test, tests if the autocorrelations of a time series are \emph{statistically significant}.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Ljung-Box} test is that the autocorrelations are equal to zero.
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that have \emph{statistically insignificant} autocorrelations.
      \vskip1ex
      The function \texttt{Box.test()} calculates the \emph{Ljung-Box} test and returns the test statistic and its p-value.
      \vskip1ex
      For \emph{minutely} \emph{SPY} returns, the \emph{Ljung-Box} statistic is large and its \emph{p}-value is very small, so we can conclude that it has statistically significant autocorrelations.
      \vskip1ex
      For \emph{scaled minutely} \emph{SPY} returns, the \emph{Ljung-Box} statistic is even larger, so its autocorrelations are even more statistically significant.
      \vskip1ex
      \emph{SPY} returns aggregated to longer time intervals are less autocorrelated.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Ljung-Box test for minutely SPY returns
Box.test(returns, lag=10, type="Ljung")
# Ljung-Box test for daily SPY returns
Box.test(retsd, lag=10, type="Ljung")
# Ljung-Box test statistics for scaled SPY returns
sapply(list(returns=returns, rets_scaled=rets_scaled),
  function(rets) {
    Box.test(rets, lag=10, type="Ljung")$statistic
})  # end sapply
# Ljung-Box test statistics for aggregated SPY returns
sapply(list(minutely=returns, hourly=retsh, daily=retsd),
  function(rets) {
    Box.test(rets, lag=10, type="Ljung")$statistic
})  # end sapply
      @
      The level of the autocorrelations depends on the sampling frequency, with higher frequency returns having more significant negative autocorrelations.
      \vskip1ex
      As the returns are aggregated to a lower periodicity, they become less autocorrelated, with daily returns having almost insignificant autocorrelations.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations of High Frequency Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      High frequency minutely \emph{SPY} returns have statistically significant negative autocorrelations.
      \vskip1ex
      \emph{SPY} returns \emph{scaled} by the trading volumes have even more significant negative autocorrelations.
      <<echo=TRUE,eval=FALSE>>=
# Set plot parameters
x11(width=6, height=8)
par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
layout(matrix(c(1, 2), ncol=1), widths=c(6, 6), heights=c(4, 4))
# Plot the partial autocorrelations of minutely SPY returns
pa_cf <- pacf(as.numeric(returns), lag=10,
     xlab="lag", ylab="partial autocorrelation", main="")
title("Partial Autocorrelations of Minutely SPY Returns", line=1)
# Plot the partial autocorrelations of scaled SPY returns
pacf_scaled <- pacf(as.numeric(rets_scaled), lag=10,
     xlab="lag", ylab="partial autocorrelation", main="")
title("Partial Autocorrelations of Scaled SPY Returns", line=1)
# Calculate the sums of partial autocorrelations
sum(pa_cf$acf)
sum(pacf_scaled$acf)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_pacf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Market Liquidity, Trading Volume and Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Market illiquidity is defined as the market price impact resulting from supply-demand imbalance.
      \vskip1ex
      Market liquidity $\mathcal{L}$ is proportional to the square root of the \emph{trading volume} $\upsilon$ divided by the price volatility $\sigma$:
      \begin{displaymath}
        \mathcal{L} \sim \frac{\sqrt{\upsilon}}{\sigma}
      \end{displaymath}
      Market illiquidity spiked during the May 6, 2010 \emph{flash crash}.
      \vskip1ex
      Research suggests that market crashes are caused by declining market liquidity:\\
      \href{http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2583743}{\emph{Donier et al., Why Do Markets Crash?}}
      <<echo=TRUE,eval=FALSE>>=
# Calculate market illiquidity
liquidi_ty <- sqrt(volume_daily)/vol_daily
# Plot market illiquidity
x11(width=6, height=7) ; par(mfrow=c(2, 1))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
chart_Series(liquidi_ty["2010"], theme=plot_theme,
  name="SPY Liquidity in 2010", plot=FALSE)
plot_theme$col$line.col <- c("red")
chart_Series(vol_daily["2010"],
  theme=plot_theme, name="SPY Volatility in 2010")
@
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_liquidity.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Seasonality of Volume and Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility and trading volumes are typically higher at the beginning and end of the trading sessions.
      <<echo=TRUE,eval=FALSE>>=
# Calculate intraday time index with hours and minutes
dates <- format(zoo::index(returns), "%H:%M")
# Aggregate the mean volume
volume_agg <- tapply(X=volumes, INDEX=dates, FUN=mean)
volume_agg <- drop(volume_agg)
# Aggregate the mean volatility
vol_agg <- tapply(X=returns^2, INDEX=dates, FUN=mean)
vol_agg <- sqrt(drop(vol_agg))
# Coerce to xts
intra_day <- as.POSIXct(paste(Sys.Date(), names(volume_agg)))
volume_agg <- xts::xts(volume_agg, intra_day)
vol_agg <- xts::xts(vol_agg, intra_day)
# Plot seasonality of volume and volatility
x11(width=6, height=7) ; par(mfrow=c(2, 1))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
chart_Series(volume_agg[c(-1, -NROW(volume_agg))], theme=plot_theme,
  name="Daily Seasonality of SPY Volume", plot=FALSE)
plot_theme$col$line.col <- c("red")
chart_Series(vol_agg[c(-1, -NROW(vol_agg))], theme=plot_theme,
  name="Daily Seasonality of SPY Volatility")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_intraday_volume_volat.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Daily Seasonality of Liquidity and Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Market liquidity is typically the highest at the end of the trading session, and the lowest at the beginning.
      \vskip1ex
      The end of day spike in trading volumes and liquidity is driven by computer-driven investors liquidating their positions.
      <<echo=TRUE,eval=FALSE>>=
# Calculate market liquidity
liquidi_ty <- sqrt(volume_agg)/vol_agg
# Plot daily seasonality of market liquidity
x11(width=6, height=7) ; par(mfrow=c(2, 1))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue")
chart_Series(liquidi_ty[c(-1, -NROW(liquidi_ty))], theme=plot_theme,
  name="Daily Seasonality of SPY Liquidity", plot=FALSE)
plot_theme$col$line.col <- c("red")
chart_Series(vol_agg[c(-1, -NROW(vol_agg))], theme=plot_theme,
  name="Daily Seasonality of SPY Volatility")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_intraday_liquid_volat.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Financial and Commodity Futures}


%%%%%%%%%%%%%%%
\subsection{Financial and Commodity Futures Contracts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The underlying assets delivered in \emph{commodity futures} contracts are commodities, such as grains (corn, wheat), or raw materials and metals (oil, aluminum).
      \vskip1ex
      The underlying assets delivered in \emph{financial futures} contracts are financial assets, such as stocks, bonds, and currencies.
      \vskip1ex
      Many futures contracts use cash settlement instead of physical delivery of the asset.
      \vskip1ex
      Futures contracts on different underlying assets can have quarterly, monthly, or even weekly expiration dates.
      \vskip1ex
      The front month futures contract is the contract with the closest expiration date to the current date.
      \vskip1ex
      Symbols of futures contracts are obtained by combining the contract code with the month code and the year.
      \vskip1ex
      For example, \emph{ESM9} is the symbol for the \emph{S\&P500} index E-mini futures expiring in June 2019.
    \column{0.5\textwidth}
    \vspace{-1em}
    \begin{table}[htb]
    \begin{minipage}{0.48\textwidth}
    % \centering
    <<echo=FALSE,eval=TRUE,results='asis'>>=
# Futures contracts codes
futures <- rbind(c("S&P500 index", "ES"),
                  c("10yr Treasury", "ZN"),
                  c("VIX index", "VX"),
                  c("Gold", "GC"),
                  c("Oil", "CL"),
                  c("Euro FX", "EC"),
                  c("Swiss franc", "SF"),
                  c("Japanese Yen", "JY"))
colnames(futures) <- c("Futures contract", "Code")
print(xtable::xtable(futures), comment=FALSE, size="scriptsize", include.rownames=FALSE, latex.environments="flushleft")
@
% \captionof{table}{The first table}
\end{minipage}
\begin{minipage}{0.48\textwidth}
% \centering
<<echo=FALSE,eval=TRUE,results='asis'>>=
# Monthly futures contract codes
codes <- cbind(c("January", "February", "September", "April", "May", "June", "July", "August", "September", "October", "November", "December"),
                     c("F", "G", "H", "J", "K", "M", "N", "Q", "U", "V", "X", "Z"))
colnames(codes) <- c("Month", "Code")
print(xtable::xtable(codes), comment=FALSE, size="scriptsize", include.rownames=FALSE, latex.environments="flushright")
      @
      % \captionof{table}{The second table}
      \end{minipage}
      \end{table}
      \vspace{-1em}
      Interactive Brokers provides more information about futures contracts:\\
      \hskip1em\href{https://www.interactivebrokers.com/en/index.php?f=463}{IB Contract and Symbol Database}\\
      \hskip1em\href{https://www.interactivebrokers.com/en/index.php?f=1563&p=fut}{IB Traded Products}
      \vskip1ex
      List of
      \href{https://www.purefinancialacademy.com/futures-markets}{Popular Futures Contracts}.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{E-mini} Futures Contracts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{E-mini} futures are contracts with smaller notionals and tick values, which are more suitable for retail investors.
      \vskip1ex
      For example, the
      \href{https://www.cmegroup.com/trading/energy/crude-oil/emini-crude-oil.html}{\emph{QM} E-mini oil future}
      notional is \texttt{500} barrels, while the standard
      \href{https://www.cmegroup.com/trading/energy/crude-oil/light-sweet-crude_quotes_globex.html}{\emph{CL} oil future} notional is \texttt{1,000} barrels.
      \vskip1ex
      The tick value is the change in the dollar value of the futures contract due to a one tick change in the underlying price.
      \vskip1ex
      For example, the tick value of the \emph{ES} E-mini \emph{S\&P500} future is \texttt{\$12.50}, and one tick is \texttt{0.25}.
      \vskip1ex
      So if the \emph{S\&P500} index changes by one tick (\texttt{0.25}), then the value of a single \emph{ES} E-mini contract changes by \texttt{\$12.50}, while the standard \emph{SP} contract value changes by \texttt{\$62.5}.
      \vskip1ex
      The
      \href{https://www.cmegroup.com/trading/equity-index/us-index/e-mini-sandp500.html}{\emph{ES} E-mini \emph{S\&P500} futures} trade almost continuously 24 hours per day, from 6:00 PM Eastern Time (ET) on Sunday night to 5:00 PM Friday night (with a trading halt between 4:15 and 4:30 PM ET each day).
    \column{0.5\textwidth}
    \vspace{-1em}
    \begin{table}[htb]
    <<echo=FALSE,eval=TRUE,results='asis'>>=
# Futures contracts codes
futures <- rbind(c("S&P500 index", "SP", "ES"),
                  c("10yr Treasury", "ZN", "ZN"),
                  c("VIX index", "VX", "delisted"),
                  c("Gold", "GC", "YG"),
                  c("Oil", "CL", "QM"),
                  c("Euro FX", "EC", "E7"),
                  c("Swiss franc", "SF", "MSF"),
                  c("Japanese Yen", "JY", "J7"))
colnames(futures) <- c("Futures contract", "Standard", "E-mini")
print(xtable::xtable(futures), comment=FALSE, size="scriptsize", include.rownames=FALSE, latex.environments="flushleft")
@
      \end{table}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{S\&P500} Futures Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{data.table::fread()} reads \texttt{.csv} files over five times faster than the function \texttt{read.csv()}!
      \vskip1ex
      The function \texttt{as.POSIXct.numeric()} coerces a \texttt{numeric} value representing the \emph{moment of time} into a \texttt{POSIXct} \emph{date-time}, equal to the \emph{clock time} in the local \emph{time zone}.
      <<echo=TRUE,eval=FALSE>>=
# Load data for S&P Emini futures June 2019 contract
dir_name <- "/Users/jerzy/Develop/data/ib_data"
file_name <- file.path(dir_name, "ESohlc.csv")
# Read a data table from CSV file
prices <- data.table::fread(file_name)
class(prices)
# Coerce first column from string to date-time
unlist(sapply(prices, class))
tail(prices)
prices$Index <- as.POSIXct(prices$Index,
  tz="America/New_York", origin="1970-01-01")
# Coerce prices into xts series
prices <- data.table::as.xts.data.table(prices)
class(prices)
tail(prices)
colnames(prices)[1:5] <- c("Open", "High", "Low", "Close", "Volume")
tail(prices)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/sp500_futures.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot OHLC data in x11 window
x11(width=5, height=4)  # Open x11 for plotting
par(mar=c(5, 5, 2, 1), oma=c(0, 0, 0, 0))
chart_Series(x=prices, TA="add_Vo()",
  name="S&P500 futures")
# Plot dygraph
dygraphs::dygraph(prices[, 1:4], main="OHLC prices") %>%
  dyCandlestick()
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Consecutive Contract Futures Volumes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trading volumes of a futures contract drop significantly shortly before its expiration, and the successive contract volumes increase.
      \vskip1ex
      The contract with the highest trading volume is usually considered the most liquid contract.
      <<echo=TRUE,eval=FALSE>>=
# Load ESU8 data
dir_name <- "/Users/jerzy/Develop/data/ib_data"
file_name <- file.path(dir_name, "ESU8.csv")
ESU8 <- data.table::fread(file_name)
# Coerce ESU8 into xts series
ESU8$V1 <- as.Date(as.POSIXct.numeric(ESU8$V1,
    tz="America/New_York", origin="1970-01-01"))
ESU8 <- data.table::as.xts.data.table(ESU8)
colnames(ESU8)[1:5] <- c("Open", "High", "Low", "Close", "Volume")
# Load ESM8 data
file_name <- file.path(dir_name, "ESM8.csv")
ESM8 <- data.table::fread(file_name)
# Coerce ESM8 into xts series
ESM8$V1 <- as.Date(as.POSIXct.numeric(ESM8$V1,
    tz="America/New_York", origin="1970-01-01"))
ESM8 <- data.table::as.xts.data.table(ESM8)
colnames(ESM8)[1:5] <- c("Open", "High", "Low", "Close", "Volume")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/sp500_futures_volumes.png}
      <<echo=(-(1:1)),eval=FALSE>>=
x11(width=6, height=5)  # Open x11 for plotting
# Plot last month of ESU8 and ESM8 volume data
endd <- end(ESM8)
startd <- (endd - 30)
volumes <- cbind(Vo(ESU8),
  Vo(ESM8))[paste0(startd, "/", endd)]
colnames(volumes) <- c("ESU8", "ESM8")
colors <- c("blue", "green")
plot(volumes, col=colors, lwd=3, major.ticks="days",
     format.labels="%b-%d", observation.based=TRUE,
     main="Volumes of ESU8 and ESM8 futures")
legend("topleft", legend=colnames(volumes), col=colors,
       title=NULL, bty="n", lty=1, lwd=6, inset=0.1, cex=0.7)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Chaining Together Futures Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Chaining futures means splicing together prices from several consecutive futures contracts.
      \vskip1ex
      A continuous futures contract is a time series of prices obtained by chaining together prices from consecutive futures contracts.
      \vskip1ex
      The price of the continuous contract is equal to the most liquid contract times a scaling factor.
      \vskip1ex
      When the next contract becomes more liquid, then the continuous contract price is rolled over to that contract.
      \vskip1ex
      Futures contracts with different maturities (expiration dates) trade at different prices because of the futures curve, which causes price jumps between consecutive futures contracts.
      \vskip1ex
      The old contract price is multiplied by a scaling factor after that contract is rolled, to remove price jumps.
      \vskip1ex
      So the continuous contract prices are not equal to the past futures prices.
      \vskip1ex
      Interactive Brokers provides information about Continuous Contract Futures market data:\\
      \hskip1em\href{https://www.interactivebrokers.com/en/software/tws/usersguidebook/technicalanalytics/continuous.htm}{Continuous Contract Futures Data}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/sp500_futures_chained.png}
      <<echo=TRUE,eval=FALSE>>=
# Find date when ESU8 volume exceeds ESM8
exceeds <- (volumes[, "ESU8"] > volumes[, "ESM8"])
indeks <- match(TRUE, exceeds)
# indeks <- min(which(exceeds))
# Scale the ESM8 prices
indeks <- zoo::index(exceeds[indeks])
ratio <- as.numeric(Cl(ESU8[indeks])/Cl(ESM8[indeks]))
ESM8[, 1:4] <- ratio*ESM8[, 1:4]
# Calculate continuous contract prices
chain_ed <- rbind(ESM8[zoo::index(ESM8) < indeks],
                  ESU8[zoo::index(ESU8) >= indeks])
# Or
# Chain_ed <- rbind(ESM8[paste0("/", indeks-1)],
#                   ESU8[paste0(indeks, "/")])
# Plot continuous contract prices
chart_Series(x=chain_ed["2018"], TA="add_Vo()",
  name="S&P500 chained futures")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VIX} Volatility Index}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VIX} Volatility Index is an average of the implied volatilities of options on the \emph{S\&P500} Index (SPX).
      \vskip1ex
      The \emph{VIX} index is an estimate of the \emph{future} stock market volatility that is expected (anticipated) by investors.
      \vskip1ex
      The \emph{VIX} index is not a directly tradable asset, but it can be traded using \emph{VIX} futures.
      \vskip1ex
      The \emph{CBOE} provides daily historical data for the \emph{VIX} index.
      <<echo=TRUE,eval=FALSE>>=
# Download VIX index data from CBOE
vix_index <- data.table::fread("http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/vixcurrent.csv", skip=1)
class(vix_index)
dim(vix_index)
tail(vix_index)
sapply(vix_index, class)
vix_index <- xts(vix_index[, -1],
  order.by=as.Date(vix_index$Date, format="%m/%d/%Y"))
colnames(vix_index) <- c("Open", "High", "Low", "Close")
# Save the VIX data to binary file
load(file="/Users/jerzy/Develop/data/ib_data/vix_cboe.RData")
ls(vix_env)
vix_env$vix_index <- vix_index
save(vix_env, file="/Users/jerzy/Develop/data/ib_data/vix_cboe.RData")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vix_historical.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot VIX OHLC data in x11 window
chart_Series(x=vix_index["2018"], name="VIX Index")
# Plot dygraph
dygraphs::dygraph(vix_index, main="VIX Index") %>%
  dyCandlestick()
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VIX} Futures Contracts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{VIX} futures are cash-settled futures contracts on the \emph{VIX} Index.
      \vskip1ex
      The most liquid \emph{VIX} futures are with monthly expiration dates (\href{http://www.cboe.com/framed/pdfframed?content=/aboutcboe/xcal2018.pdf&section=SEC_RESOURCES&title=2018+Cboe+Expiration+Calendar}{CBOE Expiration Calendar}), but weekly \emph{VIX} futures are also traded.
      \vskip1ex
      These are the \href{http://www.macroption.com/vix-expiration-calendar/}{VIX Futures Monthly Expiration Dates} from 2004 to 2019.
      \vskip1ex
      \emph{VIX} futures are traded on the \emph{CFE} (CBOE Futures Exchange):\\
      \hskip1em\url{http://cfe.cboe.com/}\\
      \hskip1em\url{http://www.cboe.com/vix}
      \vskip1ex
      \emph{VIX} Contract Specifications:\\
      \hskip1em\href{http://cfe.cboe.com/cfe-products/vx-cboe-volatility-index-vix-futures/contract-specifications}{VIX Contract Specifications}\\
      \hskip1em\href{http://www.macroption.com/vix-expiration-calendar/}{VIX Expiration Calendar}
      \vskip1ex
      Standard and Poor's explains the methodology of the
      \href{https://us.spindices.com/documents/methodologies/methodology-sp-vix-futures-indices.pdf
}{\emph{VIX} Futures Indices}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Read CBOE monthly futures expiration dates
dates <- read.csv(
  file="/Users/jerzy/Develop/data/vix_data/vix_dates.csv")
dates <- as.Date(dates[, 1])
years <- format(dates, format="%Y")
years <- substring(years, 4)
# Monthly futures contract codes
codes <-
  c("F", "G", "H", "J", "K", "M",
    "N", "Q", "U", "V", "X", "Z")
symbolv <- paste0("VX", codes, years)
dates <- as.data.frame(dates)
colnames(dates) <- "exp_dates"
rownames(dates) <- symbolv
# Write dates to CSV file, with row names
write.csv(dates, row.names=TRUE,
  file="/Users/jerzy/Develop/data/vix_data/vix_futures.csv")
# Read back CBOE futures expiration dates
dates <- read.csv(file="/Users/jerzy/Develop/data/vix_data/vix_futures.csv",
  row.names=1)
dates[, 1] <- as.Date(dates[, 1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VIX} Futures Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Futures contracts with different expiration dates trade at different prices, known as the \emph{futures curve} (or \emph{term structure}).
      \vskip1ex
      The \emph{VIX} futures curve is similar to the interest rate \emph{yield curve}, which displays yields at different bond maturities.
      \vskip1ex
      The \emph{VIX} futures curve is not the same as the \emph{VIX} index term structure.
      \vskip1ex
      More information about the \emph{VIX} Index and the \emph{VIX} futures curve:\\
      \hskip1em\href{http://www.macroption.com/vix-futures/}{VIX Futures}\\
      \hskip1em\href{http://www.macroption.com/vix-futures-historical-data/}{VIX Futures Data}\\
      \hskip1em\href{http://www.macroption.com/vix-futures-curve/}{VIX Futures Curve}\\
      \hskip1em\href{http://www.macroption.com/vix-term-structure/}{VIX Index Term Structure}
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load VIX futures data from binary file
load(file="/Users/jerzy/Develop/data/vix_data/vix_cboe.RData")
# Get all VIX futures for 2018 except January
symbolv <- ls(vix_env)
symbolv <- symbolv[grep("*8", symbolv)]
symbolv <- symbolv[2:9]
# Specify dates for curves
low_vol <- as.Date("2018-01-11")
hi_vol <- as.Date("2018-02-05")
# Extract all VIX futures prices on the dates
curve_s <- lapply(symbolv, function(symbol) {
  xtes <- get(x=symbol, envir=vix_env)
  Cl(xtes[c(low_vol, hi_vol)])
})  # end lapply
curve_s <- rutils::do_call(cbind, curve_s)
colnames(curve_s) <- symbolv
curve_s <- t(coredata(curve_s))
colnames(curve_s) <- c("Contango 01/11/2018",
                       "Backwardation 02/05/2018")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Contango} and \protect\emph{Backwardation} of \protect\emph{VIX} Futures Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      When prices are \emph{low} then the futures curve is usually \emph{upward sloping}, known as \emph{contango}.
      \vskip1ex
      Futures prices are in \emph{contango} most of the time.
      \vskip1ex
      When prices are \emph{high} then the curve is usually \emph{downward sloping}, known as \emph{backwardation}.
      <<echo=TRUE,eval=FALSE>>=
x11(width=7, height=5)
par(mar=c(3, 2, 1, 1), oma=c(0, 0, 0, 0))
plot(curve_s[, 1], type="l", lty=1, col="blue", lwd=3,
     xaxt="n", xlab="", ylab="", ylim=range(curve_s),
     main="VIX Futures Curves")
axis(1, at=(1:NROW(curve_s)), labels=rownames(curve_s))
lines(curve_s[, 2], lty=1, lwd=3, col="red")
legend(x="topright", legend=colnames(curve_s),
       inset=0.05, cex=1.0, bty="n",
       col=c("blue", "red"), lwd=6, lty=1)
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/vix_curves.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Futures Prices at Constant Maturity}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A constant maturity futures price is the price of a hypothetical futures contract with an expiration date at a fixed number of days in the future.
      \vskip1ex
      Futures prices at a constant maturity can be calculated by interpolating the prices of contracts with neighboring expiration dates.
    \column{0.5\textwidth}
    \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Load VIX futures data from binary file
load(file="/Users/jerzy/Develop/data/vix_data/vix_cboe.RData")
# Read CBOE futures expiration dates
dates <- read.csv(file="/Users/jerzy/Develop/data/vix_data/vix_futures.csv",
  row.names=1)
symbolv <- rownames(dates)
dates <- as.Date(dates[, 1])
todayd <- as.Date("2018-05-07")
maturi_ty <- (todayd + 30)
# Find neighboring futures contracts
indeks <- match(TRUE, dates > maturi_ty)
front_date <- dates[indeks-1]
back_date <- dates[indeks]
front_symbol <- symbolv[indeks-1]
back_symbol <- symbolv[indeks]
front_price <- get(x=front_symbol, envir=vix_env)
front_price <- as.numeric(Cl(front_price[todayd]))
back_price <- get(x=back_symbol, envir=vix_env)
back_price <- as.numeric(Cl(back_price[todayd]))
# Calculate the constant maturity 30-day futures price
ra_tio <- as.numeric(maturi_ty - front_date) /
  as.numeric(back_date - front_date)
pric_e <- (ra_tio*back_price + (1-ra_tio)*front_price)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VIX} Futures Investing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility index moves in the opposite direction to the underlying asset price.
      \vskip1ex
      An increase in the \emph{VIX} index coincides with a drop in stock prices, and vice versa.
      \vskip1ex
      Taking a \emph{long} position in \emph{VIX} futures is similar to a \emph{short} position in stocks, and vice versa.
      \vskip1ex
      There are several exchange-traded funds (\emph{ETFs}) and exchange traded notes (\emph{ETNs}) which are linked to \emph{VIX} futures.
      \vskip1ex
      \emph{VXX} is an \emph{ETN} providing the total return of a \emph{long VIX} futures contract (short market risk).
      \vskip1ex
      \emph{SVXY} is an \emph{ETF} providing the total return of a \emph{short VIX} futures contract (long market risk).
      \vskip1ex
      Standard and Poor's explains the calculation of the
      \href{http://us.spindices.com/documents/methodologies/methodology-sp-vix-future-index.pdf?force_download=true}{Total Return on VIX Futures Indices}.
      <<echo=(-(1:3)),eval=FALSE>>=
x11(width=5, height=3)  # Open x11 for plotting
# Load VIX futures data from binary file
load(file="/Users/jerzy/Develop/data/vix_data/vix_cboe.RData")
# Plot VIX and SVXY data in x11 window
plot_theme <- chart_theme()
plot_theme$col$line.col <- "blue"
chart_Series(x=Cl(vix_env$vix_index["2007/"]),
             theme=plot_theme, name="VIX Index")
chart_Series(x=Cl(rutils::etfenv$VTI["2007/"]),
             theme=plot_theme, name="VTI ETF")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vix_historical2.png}
      \includegraphics[width=0.45\paperwidth]{figure/vix_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{VIX} Crash on February 5th 2018}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{SVXY} and \emph{XIV} ETFs rallied strongly after the financial crisis of 2008, so they became very popular with individual investors, and became very "crowded trades".
      \vskip1ex
      The \emph{SVXY} and \emph{XIV} ETFs had \$3.6 billion of assets at the beginning of 2018.
      \vskip1ex
      On February 5th 2018 the U.S. stock markets experienced a mini-crash, which was exacerbated by \emph{VIX} futures short sellers.
      \vskip1ex
      As a result, the \emph{XIV} ETF hit its termination event and its value dropped to zero:\\
      \hskip1em\href{https://www.bloomberg.com/news/articles/2018-02-07/how-two-tiny-volatility-products-helped-fuel-sudden-stock-slump}{Volatility Caused Stock Market Crash}\\
      \hskip1em\href{https://riskreversal.com/2018/02/06/volatility-etn-terminated-xiv/
}{XIV ETF Termination Event}
      <<echo=TRUE,eval=FALSE>>=
chart_Series(x=Cl(vix_env$vix_index["2017/2018"]),
             theme=plot_theme, name="VIX Index")
chart_Series(x=Cl(rutils::etfenv$SVXY["2017/2018"]),
             theme=plot_theme, name="SVXY ETF")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vix_historical3.png}
      \includegraphics[width=0.45\paperwidth]{figure/vix_svxy2.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{No homework!}
  \hskip10.0em\includegraphics[scale=0.1]{image/smile.png}
\end{block}

\end{frame}


\end{document}
