% FRE7241_Lecture3
% Define knitr options
% !Rnw weave=knitr
% Set global chunk options
<<knitr_setup,include=FALSE,cache=FALSE>>=
library(knitr)
opts_chunk$set(prompt=TRUE, eval=FALSE, tidy=FALSE, strip.white=FALSE, comment=NA, highlight=FALSE, message=FALSE, warning=FALSE, size="tiny", fig.width=4, fig.height=4)
options(width=80, dev="pdf")
options(digits=3)
thm <- knit_theme$get("acid")
knit_theme$set(thm)
@


% Define document options
\documentclass[9pt]{beamer}
\DeclareMathSizes{8pt}{6pt}{6pt}{5pt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage[latin1]{inputenc}
\usepackage{bbold}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[FRE7241 Lecture\#3]{FRE7241 Algorithmic Portfolio Management}
\subtitle{Lecture\#3, Spring 2022}

\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{March 21, 2022}
% \date{\today}
% \pgfdeclareimage[height=0.5cm]{university-logo}{engineering_long_white}
% \logo{\pgfuseimage{engineering_long_white}}


%%%%%%%%%%%%%%%
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Rebalancing Strategies}


%%%%%%%%%%%%%%%
\subsection{Combining the Returns of Multiple Assets}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      There are several ways of combining the returns of multiple assets.
      \vskip1ex
      Adding the weighted simple (dollar) returns is equivalent to buying a \emph{fixed number of shares} (aka \emph{Fixed Share Amount} or FSA) proportional to the weights.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} (aka \emph{Fixed Dollar Amount} or FDA) proportional to the weights.
      \vskip1ex
      The portfolio allocations must be periodically rebalanced to keep the dollar amounts of the stocks proportional to the weights.
      \vskip1ex
      This \emph{rebalancing} acts as a mean reverting strategy - selling shares when their price goes up, and vice versa.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI and IEF dollar returns
prices <- rutils::etfenv$prices[, c("VTI", "IEF")]
prices <- na.omit(prices)
dates <- index(prices)
rets_dollar <- rutils::diffit(prices)
# Calculate VTI and IEF percentage returns
rets_percent <- rets_dollar/
  rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_weighted_log_wealth.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
weights <- c(0.5, 0.5)
rets_dollar[1, ] <- prices[1, ]
wealth_fsa <- cumsum(rets_dollar %*% weights)
# Wealth of fixed dollars (with rebalancing)
wealth_fda <- cumsum(rets_percent %*% weights)
# Plot log wealth
wealth <- cbind(wealth_fda, log(wealth_fsa))
wealth <- xts::xts(wealth, zoo::index(prices))
colnames(wealth) <- c("Fixed dollars", "Fixed shares (log)")
colnamev <- colnames(wealth)
dygraphs::dygraph(wealth, main="Wealth of Weighted Portfolios") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="red", strokeWidth=2) %>%
  dySeries(name=colnamev[2], axis="y2", col="blue", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs of Weighted Portfolio Rebalancing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Maintaining a \emph{fixed dollar amount} of stock requires periodic \emph{rebalancing}, selling shares when their price goes up, and vice versa.
      \vskip1ex
      Adding the weighted percentage returns is equivalent to investing in \emph{fixed dollar amounts of stock} proportional to the weights.
      \vskip1ex
      The dollar amount of stock that must be traded in a given period is equal to the weighted sum of the absolute percentage returns: $w1 \left| r^1_t \right| + w2 \left| r^2_t \right|$.
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ due to rebalancing are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} (w1 \left| r^1_t \right| + w2 \left| r^2_t \right|)$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{margin account} $m_t$: $m_t - \sum_{i=1}^t c^r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_transaction_costs.png}
      <<echo=TRUE,eval=FALSE>>=
# Margin account for fixed dollars (with rebalancing)
mar_gin <- cumsum(rets_percent %*% weights)
# Cumulative transaction costs
costs <- bid_offer*cumsum(abs(rets_percent) %*% weights)/2
# Subtract transaction costs from margin account
mar_gin <- (mar_gin - costs)
# dygraph plot of margin and transaction costs
datav <- cbind(mar_gin, costs)
datav <- xts::xts(datav, zoo::index(prices))
colnamev <- c("Margin", "Cumulative Transaction Costs")
colnames(datav) <- colnamev
dygraphs::dygraph(datav, main="Fixed Dollar Portfolio Transaction Costs") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio With Fixed Ratios of Dollar Amounts}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Consider a portfolio with \emph{fixed ratios of dollar amounts} (aka \emph{Constant Dollar Allocations} or CDA), not fixed dollar amounts.
      \vskip1ex
      The total wealth is not fixed and is equal to the portfolio market value, so there's no margin account.
      \vskip1ex
      Let $r_i$ be the percentage returns, $\omega_i$ be the portfolio weights, and $\bar{r}_t = \sum_{i=1}^n \omega_i r_i$ be the weighted percentage returns at time $t$.
      \vskip1ex
      The total portfolio wealth at time $t$ is equal to the wealth at time $t-1$ multiplied by the weighted returns: $w_t = w_{t-1} (1 + \bar{r}_t)$.
      \vskip1ex
      The dollar amount of stock $i$ at time $t$ increases by $\omega_i r_i$ so it's equal to $\omega_i w_{t-1} (1 + r_i)$, while the target amount is $\omega_i w_t = \omega_i w_{t-1} (1 + \bar{r}_t)$
      \vskip1ex
      The dollar amount of stock $i$ needed to trade to rebalance back to the target weight is equal to:
      \begin{flalign*}
        \varepsilon_i &= \left| \omega_i w_{t-1} (1 + \bar{r}_t) - \omega_i w_{t-1} (1 + r_i)  \right|\\
        &= \omega_i w_{t-1} \left| \bar{r}_t - r_i \right|
      \end{flalign*}
      If $\bar{r}_t > r_i$ then an amount $\varepsilon_i$ of the stock $i$ needs to be bought, and if $\bar{r}_t < r_i$ then it needs to be sold.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_weighted_fixed_ratio.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed shares (without rebalancing)
wealth_fsa <- cumsum(rets_dollar %*% weights)
# Calculate weighted percentage returns
rets_weighted <- rets_percent %*% weights
# Wealth of fixed ratio of dollar amounts (with rebalancing)
wealth_cda <- cumprod(1 + rets_weighted)
wealth_cda <- wealth_fsa[1]*wealth_cda
# Plot log wealth
wealth <- log(cbind(wealth_fsa, wealth_cda))
wealth <- xts::xts(wealth, zoo::index(prices))
colnames(wealth) <- c("Fixed Shares", "Fixed Ratio")
dygraphs::dygraph(wealth, main="Log Wealth of Fixed Dollar Ratios") %>%
  dyOptions(colors=c("blue","red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Transaction Costs With Constant Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In each period the stocks must be rebalanced to maintain the constant dollar allocations.
      \vskip1ex
      The total dollar amount of stocks that need to be traded to rebalance back to the target weight is equal to: $\sum_{i=1}^n \varepsilon_i = w_{t-1} \sum_{i=1}^n \omega_i \left| \bar{r}_t - r_i \right|$
      \vskip1ex
      The \emph{transaction costs} $c^r_t$ are equal to half the \emph{bid-offer spread} $\delta$ times the dollar amount of the traded stock: $c^r_t = \frac{\delta}{2} \sum_{i=1}^n \varepsilon_i$.
      \vskip1ex
      The \emph{cumulative transaction costs} $\sum_{i=1}^t c^r_i$ must be subtracted from the \emph{wealth} $w_t$: $w_t - \sum_{i=1}^t c^r_i$.
      <<echo=TRUE,eval=FALSE>>=
# Returns in excess of weighted returns
excess <- lapply(rets_percent, function(x) (rets_weighted - x))
excess <- do.call(cbind, excess)
sum(excess %*% weights)
# Calculate weighted sum of absolute excess returns
excess <- abs(excess) %*% weights
# Total dollar amount of stocks that need to be traded
excess <- excess*rutils::lagit(wealth_cda)
# Cumulative transaction costs
costs <- bid_offer*cumsum(excess)/2
# Subtract transaction costs from wealth
wealth_cda <- (wealth_cda - costs)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_transaction_costs_fixed_ratio.png}
      <<echo=TRUE,eval=FALSE>>=
# dygraph plot of wealth and transaction costs
wealth <- cbind(wealth_cda, costs)
wealth <- xts::xts(wealth, zoo::index(prices))
colnamev <- c("Wealth", "Cumulative Transaction Costs")
colnames(wealth) <- colnamev
dygraphs::dygraph(wealth, main="Transaction Costs With Fixed Dollar Ratios") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", col="red", strokeWidth=3) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stock and Bond Portfolio With Constant Dollar Allocations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Portfolios combining stocks and bonds can provide a much better risk versus return tradeoff than either of the assets separately, because the returns of stocks and bonds are usually negatively correlated, so they are natural hedges of each other.
      \vskip1ex
      The fixed portfolio weights represent the percentage dollar allocations to stocks and bonds, while the portfolio wealth grows over time.
      \vskip1ex
      The weights depend on the investment horizon, with a greater allocation to bonds for a shorter investment horizon.
      \vskip1ex
      Active investment strategies are expected to outperform static stock and bond portfolios.
      <<echo=TRUE,eval=FALSE>>=
# Calculate stock and bond returns
returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
weights <- c(0.4, 0.6)
returns <- cbind(returns, returns %*% weights)
colnames(returns)[3] <- "Combined"
# Calculate correlations
cor(returns)
# Calculate Sharpe ratios
sqrt(252)*sapply(returns, function(x) mean(x)/sd(x))
# Calculate standard deviation, skewness, and kurtosis
sapply(returns, function(x) {
  # Calculate standard deviation
  stddev <- sd(x)
  # Standardize the returns
  x <- (x - mean(x))/stddev
  c(stddev=stddev, skew=mean(x^3), kurt=mean(x^4))
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_stocks_bonds.png}
      <<echo=TRUE,eval=FALSE>>=
# Wealth of fixed ratio of dollar amounts
wealth <- cumprod(1 + returns)
# Plot cumulative wealth
dygraphs::dygraph(log(wealth), main="Stock and Bond Portfolio") %>%
  dyOptions(colors=c("blue","green","blue","red")) %>%
  dySeries("Combined", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{All-Weather} Portfolio}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{All-Weather} portfolio is a static portfolio of stocks (30\%), bonds (55\%), and commodities and precious metals (15\%) (approximately), and was designed by Bridgewater Associates, the largest hedge fund in the world:\\
      {\tiny
      \url{https://www.bridgewater.com/research-library/the-all-weather-strategy/} \\
      \url{http://www.nasdaq.com/article/remember-the-allweather-portfolio-its-having-a-killer-year-cm685511} \\
      }
      The three different asset classes (stocks, bonds, commodities) provide positive returns under different economic conditions (recession, expansion, inflation).
      \vskip1ex
      The combination of bonds, stocks, and commodities in the \emph{All-Weather} portfolio is designed to provide positive returns under most economic conditions, without the costs of trading.
      <<echo=TRUE,eval=FALSE>>=
# Extract ETF returns
symbolv <- c("VTI", "IEF", "DBC")
returns <- na.omit(rutils::etfenv$returns[, symbolv])
# Calculate all-weather portfolio wealth
weightsaw <- c(0.30, 0.55, 0.15)
returns <- cbind(returns, returns %*% weightsaw)
colnames(returns)[4] <- "All Weather"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_all_weather.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate cumulative wealth from returns
wealth <- cumsum(returns)
# dygraph all-weather wealth
dygraphs::dygraph(wealth, main="All-Weather Portfolio") %>%
  dyOptions(colors=c("blue", "green", "orange", "red")) %>%
  dySeries("All Weather", color="red", strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# Plot all-weather wealth
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("orange", "blue", "green", "red")
quantmod::chart_Series(wealth, theme=plot_theme, lwd=c(2, 2, 2, 4),
             name="All-Weather Portfolio")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Constant Proportion Portfolio Insurance Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Constant Proportion Portfolio Insurance} (CPPI) strategy the portfolio is rebalanced between stocks and zero-coupon bonds, to protect against the loss of principal.
      \vskip1ex
      A zero-coupon bond pays no coupon, but it's bought at a discount to par ($100\%$), and pays par at maturity.  The investor receives capital appreciation instead of coupons.
      \vskip1ex
      Let $P$ be the investor principal amount (total initial invested dollar amount), and let $F$ be the zero-coupon \emph{bond floor}.  
      The zero-coupon bond floor $F$ is set so that its value at maturity is equal to the principal $P$.  
      This guarantees that the investor is paid back at least the full principal $P$.
      \vskip1ex
      The stock investment is levered by the \emph{multiplier} $C$.  
      The initial dollar amount invested in stocks is equal to the \emph{cushion} $(P - F)$ times the \emph{multiplier} $C$: $C * (P - F)$.
      The remaining amount of the principal is invested in zero-coupon bonds and is equal to: $P - C * (P - F)$.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
returns <- na.omit(rutils::etfenv$returns$VTI["2008/2009"])
dates <- index(returns)
nrows <- NROW(returns)
returns <- drop(zoo::coredata(returns))
bfloor <- 60  # bond floor
coeff <- 2  # multiplier
portf_value <- numeric(nrows)
portf_value[1] <- 100  # principal
stock_value <- numeric(nrows)
stock_value[1] <- coeff*(portf_value[1] - bfloor)
bond_value <- numeric(nrows)
bond_value[1] <- (portf_value[1] - stock_value[1])
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{CPPI Strategy Dynamics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the stock price changes and the portfolio value becomes $P_t$, then the dollar amount invested in stocks must be adjusted to: $C * (P_t - F)$.  
      The amount invested in stocks changes both because the stock price changes and because of rebalancing with the zero-coupon bonds. 
      \vskip1ex
      The amount invested in zero-coupon bonds is then equal to: $P_t - C * (P_t - F)$.
      If the portfolio value drops to the \emph{bond floor} $P_t = F$, then all the stocks must be sold, with only the zero-coupon bonds remaining.
      But if the stock price rises, more stocks must be purchased, and vice versa.
      \vskip1ex
      Therefore the \emph{CPPI} strategy is a \emph{trend-following} strategy, buying stocks when their prices are rising, and selling when their prices are dropping.
      \vskip1ex
      The \emph{CPPI} strategy can be considered a dynamic replication of a portfolio with a zero-coupon bond and a stock call option.
      \vskip1ex
      The \emph{CPPI} strategy is exposed to \emph{gap risk}, if stock prices drop suddenly by a large amount.  
      The \emph{gap risk} is exacerbated by high leverage, when the \emph{multiplier} $C$ is large, say greater than $5$.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/portf_cppi.png}
      <<echo=TRUE,eval=FALSE>>=
# Simulate CPPI strategy
for (t in 2:nrows) {
  portf_value[t] <- portf_value[t-1] + stock_value[t-1]*returns[t]
  stock_value[t] <- coeff*(portf_value[t] - bfloor)
  bond_value[t] <- (portf_value[t] - stock_value[t])
}  # end for
# dygraph plot of CPPI strategy
vti <- 100*cumprod(1+returns)
datav <- xts::xts(cbind(stock_value, bond_value, portf_value, vti), dates)
colnames(datav) <- c("stocks", "bonds", "CPPI", "VTI")
dygraphs::dygraph(datav, main="CPPI strategy") %>%
  dyOptions(colors=c("red", "green","blue","orange"), strokeWidth=2) %>%
  dyLegend(show="always", width=300)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Risk Parity} strategy the dollar portfolio allocations are rebalanced daily so that their dollar volatilities remain the same.
      \vskip1ex
      This means that the allocations $a_i$ are proportional to the \emph{standardized prices} ($\frac{p_i}{\sigma^d_i}$ - the dollar amounts of stocks with unit dollar volatilities):
      $a_i \propto \frac{p_i}{\sigma^d_i}$, 
      where $\sigma^d_i$ is the dollar volatility.
      \vskip1ex
      But the \emph{standardized prices} are equal to the inverse of the percentage volatilities $\sigma_i$:
      $\frac{p_i}{\sigma^d_i} = \frac{1}{\sigma_i}$,
      so the allocations $a_i$ are proportional to the inverse of the percentage volatilities $a_i \propto \frac{1}{\sigma_i}$.
      \vskip1ex
      In general, the dollar allocations $a_i$ may be set proportional to some target weights $\omega_i$:
      \begin{displaymath}
        a_i \propto \frac{\omega_i}{\sigma_i}
      \end{displaymath}
      The risk parity strategy is also called the equal risk contributions (ERC) strategy.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate dollar and percentage returns for VTI and IEF.
prices <- rutils::etfenv$prices[, c("VTI", "IEF")]
prices <- na.omit(prices)
rets_dollar <- rutils::diffit(prices)
rets_percent <- rets_dollar/rutils::lagit(prices, lagg=1, pad_zeros=FALSE)
# Calculate wealth of fixed ratio of dollar amounts.
weights <- c(0.5, 0.5)
rets_weighted <- rets_percent %*% weights
wealth_cda <- cumprod(1 + rets_weighted)
# Calculate rolling percentage volatility.
look_back <- 21
volat <- roll::roll_sd(rets_percent, width=look_back)
volat <- zoo::na.locf(volat, na.rm=FALSE)
volat <- zoo::na.locf(volat, fromLast=TRUE)
# Calculate the risk parity portfolio allocations.
alloc <- lapply(1:NCOL(prices), 
  function(x) weights[x]/volat[, x])
alloc <- do.call(cbind, alloc)
# Scale allocations to 1 dollar total.
alloc <- alloc/rowSums(alloc)
# Lag the allocations
alloc <- rutils::lagit(alloc)
# Calculate wealth of risk parity.
rets_weighted <- rowSums(rets_percent*alloc)
wealth_risk_parity <- cumprod(1 + rets_weighted)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Performance}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy for \emph{VTI} and \emph{IEF} has a higher \emph{Sharpe ratio} than the fixed ratio strategy because it's more overweight bonds, which is also why it has lower absolute returns. 
      \vskip1ex
      Risk parity works better for assets with low correlations and very different volatilities, like stocks and bonds.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_risk_parity.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log wealths.
wealth <- log(cbind(wealth_cda, wealth_risk_parity))
wealth <- xts::xts(wealth, zoo::index(prices))
colnames(wealth) <- c("Fixed Ratio", "Risk Parity")
# Calculate the Sharpe ratios.
sqrt(252)*sapply(rutils::diffit(wealth), function (x) mean(x)/sd(x))
# Plot a dygraph of the log wealths.
dygraphs::dygraph(wealth, main="Log Wealth of Risk Parity vs Fixed Dollar Ratios") %>%
  dyOptions(colors=c("blue","red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk Parity Strategy Market Timing Skill}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The risk parity strategy reduces allocations to assets with rising volatilities, which is often accompanied by negative returns.
      \vskip1ex
      This allows the risk parity strategy to better time the markets - selling when prices are about to drop and buying when prices are rising.
      \vskip1ex
      The t-value of the \emph{Treynor-Mazuy} test is slightly significant, indicating some market timing skill of the risk parity strategy for \emph{VTI} and \emph{IEF}. 
      <<echo=TRUE,eval=FALSE>>=
# Test risk parity market timing of VTI using Treynor-Mazuy test
returns <- rutils::diffit(wealth)
vti <- rets_percent$VTI
design <- cbind(returns, vti, vti^2)
design <- na.omit(design)
colnames(design)[1:2] <- c("fixed","risk_parity")
colnames(design)[4] <- "treynor"
model <- lm(risk_parity ~ VTI + treynor, data=design)
summary(model)
# Plot residual scatterplot
residuals <- (design$risk_parity - model$coeff[2]*design$VTI)
residuals <- model$residuals
x11(width=6, height=5)
plot.default(x=design$VTI, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Risk Parity vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*vti^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.025, paste("Risk Parity t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_risk_parity_timing_skill.png}
      <<echo=TRUE,eval=FALSE>>=
# Test for fixed ratio market timing of VTI using Treynor-Mazuy test
model <- lm(fixed ~ VTI + treynor, data=design)
summary(model)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + model$coeff["treynor"]*vti^2)
points.default(x=design$VTI, y=fittedv, pch=16, col="blue")
text(x=0.05, y=0.02, paste("Fixed Ratio t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calendar Strategies}


%%%%%%%%%%%%%%%
\subsection{Sell in May Calendar Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \href{https://en.wikipedia.org/wiki/Sell_in_May}{\emph{Sell in May}} is a \emph{market timing} \emph{calendar strategy}, in which stocks are sold at the beginning of May, and then bought back at the beginning of November.
      <<echo=TRUE,eval=FALSE>>=
# Calculate positions
vti <- na.omit(rutils::etfenv$returns$VTI)
position_s <- rep(NA_integer_, NROW(vti))
dates <- index(vti)
dates <- format(dates, "%m-%d")
position_s[dates == "05-01"] <- 0
position_s[dates == "05-03"] <- 0
position_s[dates == "11-01"] <- 1
position_s[dates == "11-03"] <- 1
# Carry forward and backward non-NA position_s
position_s <- zoo::na.locf(position_s, na.rm=FALSE)
position_s <- zoo::na.locf(position_s, fromLast=TRUE)
# Calculate strategy returns
sell_inmay <- position_s*vti
wealth <- cbind(vti, sell_inmay)
colnames(wealth) <- c("VTI", "sell_in_may")
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_sell_inmay.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot wealth of Sell in May strategy
dygraphs::dygraph(cumsum(wealth), main="Sell in May Strategy") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2) %>%
  dyLegend(show="always", width=500)
# OR: Open x11 for plotting
x11(width=6, height=5)
par(mar=c(4, 4, 3, 1), oma=c(0, 0, 0, 0))
plot_theme <- chart_theme()
plot_theme$col$line.col <- c("blue", "red")
quantmod::chart_Series(wealth, theme=plot_theme, name="Sell in May Strategy")
legend("topleft", legend=colnames(wealth),
  inset=0.1, bg="white", lty=1, lwd=6,
  col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Sell in May Strategy Market Timing}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sell in May} strategy doesn't demonstrate any ability of \emph{timing} the \emph{VTI} ETF.
      <<echo=TRUE,eval=FALSE>>=
# Test if Sell in May strategy can time VTI
design <- cbind(vti, 0.5*(vti+abs(vti)), vti^2)
colnames(design) <- c("VTI", "merton", "treynor")
# Perform Merton-Henriksson test
model <- lm(sell_inmay ~ VTI + merton, data=design)
summary(model)
# Perform Treynor-Mazuy test
model <- lm(sell_inmay ~ VTI + treynor, data=design)
summary(model)
# Plot Treynor-Mazuy residual scatterplot
residuals <- (sell_inmay - model$coeff[2]*vti)
plot.default(x=vti, y=residuals, xlab="VTI", ylab="residuals")
title(main="Treynor-Mazuy Market Timing Test\n for Sell in May vs VTI", line=0.5)
# Plot fitted (predicted) response values
fittedv <- (model$coeff["(Intercept)"] + 
              model$coeff["treynor"]*vti^2)
points.default(x=vti, y=fittedv, pch=16, col="red")
text(x=0.05, y=0.05, paste("Treynor test t-value =", round(summary(model)$coeff["treynor", "t value"], 2)))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/timing_skill_sell_inmay.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Seasonal Overnight Market Anomaly}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/market-sentiment-and-an-overnight-anomaly/}{\emph{Overnight Market Anomaly}}
      is the consistent outperformance of overnight returns relative to the daytime returns.
      \vskip1ex
      The Overnight Strategy consists of holding a long position only overnight (buying at the close and selling at the open the next day).
      \vskip1ex
      The Daytime Strategy consists of holding a long position only during the daytime (buying at the open and selling at the close the same day).
      \vskip1ex
      The \emph{Overnight Market Anomaly} has been observed for many decades for most stock market indices, but not always for all stock sectors.
      \vskip1ex
      The \emph{Overnight Market Anomaly} has mostly disappeared after the \texttt{2008-2009} financial crisis.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the log of OHLC VTI prices
ohlc <- log(rutils::etfenv$VTI)
openp <- quantmod::Op(ohlc)
highp <- quantmod::Hi(ohlc)
lowp <- quantmod::Lo(ohlc)
closep <- quantmod::Cl(ohlc)
# Calculate the close-to-close log returns, the intraday 
# open-to-close returns and the overnight close-to-open returns.
close_close <- rutils::diffit(closep)
colnames(close_close) <- "close_close"
open_close <- (closep - openp)
colnames(open_close) <- "open_close"
close_open <- (openp - rutils::lagit(closep, lagg=1, pad_zeros=FALSE))
colnames(close_open) <- "close_open"
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_overnight.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate Sharpe and Sortino ratios
wealth <- cbind(close_close, close_open, open_close)
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# Plot log wealth
dygraphs::dygraph(cumsum(wealth), 
  main="Wealth of Close-to-Close, Close-to-Open, and Open-to-Close Strategies") %>%
  dySeries(name="close_close", label="Close-to-Close (static)", strokeWidth=2, col="blue") %>%
  dySeries(name="close_open", label="Close-to-Open (overnight)", strokeWidth=2, col="red") %>%
  dySeries(name="open_close", label="Open-to-Close (daytime)", strokeWidth=2, col="green") %>%
  dyLegend(width=600)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Turn of the Month Effect}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The 
      \href{https://quantpedia.com/strategies/turn-of-the-month-in-equity-indexes/}{\emph{Turn of the Month} (TOM) effect}
      is the outperformance of stocks on the last trading day of the month and on the first three days of the following month.
      \vskip1ex
      The \emph{TOM} effect was observed for the period from 1928 to 1975, but it has been less pronounced since the year \texttt{2000}.
      \vskip1ex
      The \emph{TOM} effect has been attributed to the investment of funds deposited at the end of the month.
      \vskip1ex
      This would explain why the \emph{TOM} effect has been more pronounced for less liquid small-cap stocks.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
vti <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(vti)
# Calculate first business day of every month
dayv <- as.numeric(format(dates, "%d"))
indeks <- which(rutils::diffit(dayv) < 0)
dates[head(indeks)]
# Calculate Turn of the Month dates
indeks <- lapply((-1):2, function(x) indeks + x)
indeks <- do.call(c, indeks)
sum(indeks > NROW(dates))
indeks <- sort(indeks)
dates[head(indeks, 11)]
# Calculate Turn of the Month pnls
pnls <- numeric(NROW(vti))
pnls[indeks] <- vti[indeks, ]
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_tom.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealth <- cbind(vti, pnls)
colnamev <- c("VTI", "Strategy")
colnames(wealth) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI Turn of the Month strategy
dygraphs::dygraph(cumsum(wealth), main="Turn of the Month Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Stop-loss Rules}


%%%%%%%%%%%%%%%
\subsection{Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules are used to reduce losses in case of a significant drawdown in returns.
      \vskip1ex
      For example, a simple stop-loss rule is to sell the stock if its price drops by $5\%$ below the recent maximum price, and buy it back when the price recovers.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the VTI returns
vti <- na.omit(rutils::etfenv$returns$VTI)
dates <- zoo::index(vti)
vti <- drop(coredata(vti))
nrows <- NROW(vti)
# Simulate stop-loss strategy
stopl <- 0.05
maxp <- 0.0
cumrets <- 0.0
pnls <- vti
for (i in 1:nrows) {
# Calculate drawdown
  cumrets <- cumrets + vti[i]
  maxp <- max(maxp, cumrets)
  dd <- (cumrets - maxp)
# Check for stop-loss
  if (dd < -stopl*maxp)
    pnls[i+1] <- 0
}  # end for
# Same but without using explicit loops
cumsumv <- cumsum(vti)
cummaxv <- cummax(cumsum(vti))
dd <- (cumsumv - cummaxv)
pnls2 <- vti
isdd <- rutils::lagit(dd < -stopl*cummaxv)
pnls2 <- ifelse(isdd, 0, pnls2)
all.equal(pnls, pnls2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/stop_loss.png}
      <<echo=TRUE,eval=FALSE>>=
# Combine data
wealth <- xts::xts(cbind(vti, pnls), dates)
colnamev <- c("VTI", "Strategy")
colnames(wealth) <- colnamev
# Calculate Sharpe and Sortino ratios
sqrt(252)*sapply(wealth, 
  function(x) c(Sharpe=mean(x)/sd(x), Sortino=mean(x)/sd(x[x<0])))
# dygraph plot VTI stop-loss strategy
dygraphs::dygraph(cumsum(wealth), main="VTI Stop-loss Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Optimal Stop-loss Rules}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stop-loss rules can reduce the largest losses but they also tend to reduce cumulative returns.
      <<echo=TRUE,eval=FALSE>>=
# Simulate multiple stop-loss strategies
cumsumv <- cumsum(vti)
cummaxv <- cummax(cumsum(vti))
dd <- (cumsumv - cummaxv)
cum_pnls <- sapply(0.01*(1:20), function(stopl) {
  pnls <- vti
  isdd <- rutils::lagit(dd < -stopl*cummaxv)
  pnls <- ifelse(isdd, 0, pnls)
  sum(pnls)
})  # end sapply
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/stop_loss_cum.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot cumulative pnls for stop-loss strategies
plot(x=0.01*(1:20), y=cum_pnls, 
     main="Cumulative PnLs for Stop-loss Strategies",
     xlab="stop-loss level", ylab="cumulative pnl", 
     t="l", lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Feature Engineering}



%%%%%%%%%%%%%%%
\subsection{Convolution Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a trailing linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi1 r_{i-1} + \varphi2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
      \vskip1ex
      \texttt{filter()} with \texttt{method="convolution"} calls the function \texttt{stats:::C\_cfilter()} to calculate the \emph{convolution}.
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled function \texttt{stats:::C\_cfilter()}.
      \vskip1ex
      The function \texttt{roll::roll\_sum()} calculates the \emph{weighted} rolling sum (convolution) even faster than \texttt{stats:::C\_cfilter()}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
# Inspect the R code of the function filter()
filter
# Calculate EWMA weights
look_back <- 21
weights <- exp(-0.1*1:look_back)
weights <- weights/sum(weights)
# Calculate convolution using filter()
filtered <- filter(closep, filter=weights,
                    method="convolution", sides=1)
# filter() returns time series of class "ts"
class(filtered)
# Get information about C_cfilter()
getAnywhere(C_cfilter)
# Filter using C_cfilter() over past values (sides=1).
filter_fast <- .Call(stats:::C_cfilter, closep, filter=weights,
                     sides=1, circular=FALSE)
all.equal(as.numeric(filtered), filter_fast, check.attributes=FALSE)
# Calculate EWMA prices using roll::roll_sum()
weights_rev <- rev(weights)
roll_ed <- roll::roll_sum(closep, width=look_back, weights=weights_rev, min_obs=1)
all.equal(filter_fast[-(1:look_back)], as.numeric(roll_ed)[-(1:look_back)])
# Benchmark speed of rolling calculations
library(microbenchmark)
summary(microbenchmark(
  filter=filter(closep, filter=weights, method="convolution", sides=1),
  filter_fast=.Call(stats:::C_cfilter, closep, filter=weights, sides=1, circular=FALSE),
  roll=roll::roll_sum(closep, width=look_back, weights=weights_rev)
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} with \texttt{method="recursive"} calls the function \texttt{stats:::C\_rfilter()} to calculate the \emph{recursive filter} as follows:
      \begin{displaymath}
        r_i = \varphi1 r_{i-1} + \varphi2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, $\varphi_i$ are the filter coefficients, and $\xi_i$ are standard normal \emph{innovations}.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      The function \texttt{HighFreq::sim\_arima()} is very fast because it's written using the \texttt{C++} \emph{Armadillo} numerical library.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Simulate AR process using filter()
nrows <- NROW(closep)
# Calculate ARIMA coefficients and innovations
coeff <- weights/4
n_coeff <- NROW(coeff)
innov <- rnorm(nrows)
arimav <- filter(x=innov, filter=coeff, method="recursive")
# Get information about C_rfilter()
getAnywhere(C_rfilter)
# Filter using C_rfilter() compiled C++ function directly
arima_fast <- .Call(stats:::C_rfilter, innov, coeff,
                    double(n_coeff + nrows))
all.equal(as.numeric(arimav), arima_fast[-(1:n_coeff)], 
          check.attributes=FALSE)
# Filter using C++ code
arima_fastest <- HighFreq::sim_arima(innov, rev(coeff))
all.equal(arima_fast[-(1:n_coeff)], drop(arima_fastest))
# Benchmark speed of the three methods
summary(microbenchmark(
  filter=filter(x=innov, filter=coeff, method="recursive"),
  filter_fast=.Call(stats:::C_rfilter, innov, coeff, double(n_coeff + nrows)),
  Rcpp=HighFreq::sim_arima(innov, rev(coeff))
  ), times=10)[, c(1, 4, 5)]
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Data Smoothing and The Bias-Variance Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering through an averaging filter produces data \emph{smoothing}.
      \vskip1ex
      Smoothing real-time data with a trailing filter reduces its \emph{variance} but it increases its \emph{bias} because it introduces a time lag.
      \vskip1ex
      Smoothing historical data with a centered filter reduces its \emph{variance} but it introduces \emph{data snooping}.
      \vskip1ex
      In engineering, smoothing is called a \emph{low-pass filter}, since it eliminates high frequency signals, and it passes through low frequency signals.
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing EWMA prices using roll::roll_sum()
look_back <- 21
weights <- exp(-0.1*1:look_back)
weights <- weights/sum(weights)
weights_rev <- rev(weights)
filtered <- roll::roll_sum(closep, width=NROW(weights), weights=weights_rev)
# Copy warmup period
filtered[1:look_back] <- closep[1:look_back]
# Combine prices with smoothed prices
prices <- cbind(closep, filtered)
colnames(prices)[2] <- "VTI Smooth"
# Calculate standard deviations of returns
sapply(rutils::diffit(prices), sd)
# Plot dygraph
dygraphs::dygraph(prices["2009"], main="VTI Prices and Trailing Smoothed Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2)
      @
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_smooth.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate centered EWMA prices using roll::roll_sum()
weights <- c(weights_rev, weights[-1])
weights <- weights/sum(weights)
filtered <- roll::roll_sum(closep, width=NROW(weights), weights=weights, online=FALSE)
# Copy warmup period
filtered[1:(2*look_back)] <- closep[1:(2*look_back)]
# Center the data
filtered <- rutils::lagit(filtered, -(look_back-1), pad_zeros=FALSE)
# Combine prices with smoothed prices
prices <- cbind(closep, filtered)
colnames(prices)[2] <- "VTI Smooth"
# Calculate standard deviations of returns
sapply(rutils::diffit(prices), sd)
# Plot dygraph
dygraphs::dygraph(prices["2009"], main="VTI Prices and Centered Smoothed Prices") %>%
  dyOptions(colors=c("blue", "red"), strokeWidth=2)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Smoothed Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Smoothing a time series of prices produces autocorrelations of their returns.
      <<echo=TRUE,eval=FALSE>>=
# Open plot window
x11(width=6, height=7)
# Set plot parameters
par(oma=c(1, 1, 0, 1), mar=c(1, 1, 1, 1), mgp=c(0, 0.5, 0),
    cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
# Set two plot panels
par(mfrow=c(2,1))
# Plot ACF of VTI returns
rutils::plot_acf(returns[, 1], lag=10, xlab="")
title(main="ACF of VTI Returns", line=-1)
# Plot ACF of smoothed VTI returns
rutils::plot_acf(returns[, 2], lag=10, xlab="")
title(main="ACF of Smoothed VTI Returns", line=-1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_acf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{EWMA} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        p^{EWMA}_i = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) p_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
      <<echo=TRUE,eval=FALSE>>=
# Extract log VTI prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
nrows <- NROW(closep)
# Calculate EWMA weights
look_back <- 21
lambda <- 0.1
weights <- exp(lambda*1:look_back)
weights <- weights/sum(weights)
# Calculate EWMA prices
ewmap <- roll::roll_sum(closep, width=look_back, weights=weights, min_obs=1)
# Copy over NA values
ewmap <- zoo::na.locf(ewmap, fromLast=TRUE)
prices <- cbind(closep, ewmap)
colnames(prices) <- c("VTI", "VTI EWMA")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_ewma.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colors <- c("blue", "red")
dygraphs::dygraph(prices["2009"], main="VTI EWMA Prices") %>%
  dyOptions(colors=colors, strokeWidth=2)
# Plot EWMA prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(prices["2009"], theme=plot_theme,
             lwd=2, name="VTI EWMA Prices")
legend("bottomright", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volume-Weighted Average Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes:
      \begin{displaymath}
        p^{VWAP}_i = \frac{\sum_{j=0}^{n} v_j p_{i-j}}{\sum_{j=0}^{n} v_j}
      \end{displaymath}
      The \emph{VWAP} is often used as a technical indicator in trend following strategies.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate log OHLC prices and volumes
symbol <- "VTI"
ohlc <- rutils::etfenv$VTI
nrows <- NROW(ohlc)
closep <- log(quantmod::Cl(ohlc))
volumes <- quantmod::Vo(ohlc)
# Calculate the VWAP prices
look_back <- 21
vwapv <- roll::roll_sum(closep*volumes, width=look_back, min_obs=1)
volume_roll <- roll::roll_sum(volumes, width=look_back, min_obs=1)
vwapv <- vwapv/volume_roll
vwapv <- zoo::na.locf(vwapv, fromLast=TRUE)
prices <- cbind(closep, vwapv)
colnames(prices) <- c(symbol, paste(symbol, "VWAP"))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_vwap.png}
      <<echo=TRUE,eval=FALSE>>=
# Dygraphs plot with custom line colors
colors <- c("blue", "red")
dygraphs::dygraph(prices["2009"], main="VTI VWAP Prices") %>%
  dyOptions(colors=colors, strokeWidth=2)
# Plot VWAP prices with custom line colors
x11(width=6, height=5)
plot_theme <- chart_theme()
plot_theme$col$line.col <- colors
quantmod::chart_Series(prices["2009"], theme=plot_theme,
             lwd=2, name="VTI VWAP Prices")
legend("bottomright", legend=colnames(prices),
       inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
       col=plot_theme$col$line.col, bty="n")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Smooth Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are calculated by filtering prices through a \emph{differencing} filter.
      \vskip1ex
      The simplest \emph{differencing} filter is the filter with coefficients $(1, -1)$: $r_i = p_i - p_{i-1}$.
      \vskip1ex
      Differencing is a \emph{high-pass filter}, since it eliminates low frequency signals, and it passes through high frequency signals.
      \vskip1ex
      An alternative measure of returns is the difference between two moving averages of prices:
      $r_i = p^{fast}_i - p^{slow}_i$
      \vskip1ex
      The difference between moving averages is a \emph{mid-pass filter}, since it eliminates both high and low frequency signals, and it passes through medium frequency signals.
      <<echo=TRUE,eval=FALSE>>=
# Calculate two EWMA prices
look_back <- 21
lambda <- 0.1
weights <- exp(lambda*1:look_back)
weights <- weights/sum(weights)
ewma_fast <- roll::roll_sum(closep, width=look_back, weights=weights, min_obs=1)
lambda <- 0.05
weights <- exp(lambda*1:look_back)
weights <- weights/sum(weights)
ewma_slow <- roll::roll_sum(closep, width=look_back, weights=weights, min_obs=1)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_returns.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI returns
returns <- (ewma_fast - ewma_slow)
prices <- cbind(closep, returns)
colnames(prices) <- c(symbol, paste(symbol, "Returns"))
# Plot dygraph of VTI Returns
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main=paste(symbol, "EWMA Returns")) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fractional Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The lag operator $L$ applies a lag (time shift) to a time series: $L(p_i) = p_{i-1}$.
      \vskip1ex
      The simple returns can then be expressed as equal to the returns operator $(1 - L)$ applied to the prices: $r_i = (1 - L) p_i$.
      \vskip1ex
      The simple returns can be generalized to the fractional returns by raising the returns operator to some power $\delta < 1$:
      \begin{multline*}
        r_i = (1 - L)^\delta p_i = \\
        p_i - \delta L p_i + \frac{\delta(\delta-1)}{2!} L^2 p_i - \frac{\delta(\delta-1)(\delta-2)}{3!} L^3 p_i + \cdots = \\
        p_i - \delta p_{i-1} + \frac{\delta(\delta-1)}{2!} p_{i-2} - \frac{\delta(\delta-1)(\delta-2)}{3!} p_{i-3} + \cdots
      \end{multline*}
      The fractional returns provide a tradeoff between simple returns (which are range-bound but with no memory) and prices (which have memory but are not range-bound). 
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracret.png}
      <<echo=TRUE,eval=FALSE>>=
# Calculate fractional weights
del_ta <- 0.1
weights <- (del_ta - 0:(look_back-2)) / 1:(look_back-1)
weights <- (-1)^(1:(look_back-1))*cumprod(weights)
weights <- c(1, weights)
weights <- (weights - mean(weights))
weights <- rev(weights)
# Calculate fractional VTI returns
returns <- roll::roll_sum(closep, width=look_back, weights=weights, min_obs=1, online=FALSE)
prices <- cbind(closep, returns)
colnames(prices) <- c(symbol, paste(symbol, "Returns"))
# Plot dygraph of VTI Returns
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main=paste(symbol, "Fractional Returns")) %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_n = {\sum_{i=1}^n r_i}$.
      \vskip1ex
      Integrated processes typically have a \emph{unit root} (they have unlimited range), even if their underlying difference process does not have a \emph{unit root} (has limited range).
      \vskip1ex
      Asset returns don't have a \emph{unit root} (they have limited range) while prices have a \emph{unit root} (they have unlimited range).
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} \emph{ADF} test is designed to test the \emph{null hypothesis} that a time series has a \emph{unit root}.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI log returns
closep <- log(quantmod::Cl(rutils::etfenv$VTI))
returns <- rutils::diffit(closep)
# Perform ADF test for prices
tseries::adf.test(closep)
# Perform ADF test for returns
tseries::adf.test(returns)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Fractional Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The fractional returns for exponent values close to zero $\delta \approx 0$ resemble the asset price, while for values close to one $\delta \approx 1$ they resemble the standard returns.
      <<echo=TRUE,eval=FALSE>>=
# Calculate fractional VTI returns
delta_s <- 0.1*c(1, 3, 5, 7, 9)
returns <- lapply(delta_s, function(del_ta) {
  weights <- (del_ta - 0:(look_back-2)) / 1:(look_back-1)
  weights <- c(1, (-1)^(1:(look_back-1))*cumprod(weights))
  weights <- rev(weights - mean(weights))
  roll::roll_sum(closep, width=look_back, weights=weights, min_obs=1, online=FALSE)
})  # end lapply
returns <- do.call(cbind, returns)
returns <- cbind(closep, returns)
colnames(returns) <- c("VTI", paste0("frac_", delta_s))
# Calculate ADF test statistics
adfstats <- sapply(returns, function(x)
  suppressWarnings(tseries::adf.test(x)$statistic)
)  # end sapply
names(adfstats) <- colnames(returns)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracrets.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of fractional VTI returns
colorv <- colorRampPalette(c("blue", "red"))(NCOL(returns))
colnamev <- colnames(returns)
dyplot <- dygraphs::dygraph(returns["2019"], main="Fractional Returns") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col=colorv[1])
for (i in 2:NROW(colnamev))
  dyplot <- dyplot %>%
  dyAxis("y2", label=colnamev[i], independentTicks=TRUE) %>%
  dySeries(name=colnamev[i], axis="y2", label=colnamev[i], strokeWidth=2, col=colorv[i])
dyplot <- dyplot %>% dyLegend(width=500)
dyplot
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Volume Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing \emph{volume z-score} is equal to the volume $v_i$ minus the trailing average volumes $\bar{v_i}$ divided by the volatility of the volumes $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      Trading volumes are typically higher when prices drop and they are also positively correlated with the return volatility.
      \vskip1ex
      The \emph{volume z-scores} are positively skewed because returns are negatively skewed.
      \vskip1ex
      <<echo=TRUE,eval=FALSE>>=
# Calculate volume z-scores
volumes <- quantmod::Vo(rutils::etfenv$VTI)
look_back <- 21
volume_mean <- roll::roll_mean(volumes, width=look_back, min_obs=1)
volume_sd <- roll::roll_sd(rutils::diffit(volumes), width=look_back, min_obs=1)
volume_scores <- (volumes - volume_mean)/volume_sd
# Plot histogram of volume z-scores
x11(width=6, height=5)
hist(volume_scores, breaks=1e2)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volume_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of volume z-scores of VTI prices
prices <- cbind(closep, volume_scores)
colnames(prices) <- c("VTI", "Z-scores")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI Volume Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The difference between high and low prices is a proxy for the spot volatility in a bar of data.
      \vskip1ex
      The \emph{volatility z-score} is equal to the spot volatility $v_i$ minus the trailing average volatility $\bar{v_i}$ divided by the standard deviation of the volatility $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      Volatility is typically higher when prices drop and it's also positively correlated with the trading volumes.
      \vskip1ex
      The \emph{volatility z-scores} are positively skewed because returns are negatively skewed.
      <<echo=TRUE,eval=FALSE>>=
# Extract VTI log OHLC prices
ohlc <- log(rutils::etfenv$VTI)
# Calculate volatility z-scores
volat <- quantmod::Hi(ohlc)-quantmod::Lo(ohlc)
look_back <- 21
volat_mean <- roll::roll_mean(volat, width=look_back, min_obs=1)
volat_sd <- roll::roll_sd(rutils::diffit(volat), width=look_back, min_obs=1)
volat_scores <- ifelse(is.na(volat_sd), 0, (volat - volat_mean)/volat_sd)
# Plot histogram of volatility z-scores
x11(width=6, height=5)
hist(volat_scores, breaks=1e2)
# Plot scatterplot of volume and volatility z-scores
plot(as.numeric(volat_scores), as.numeric(volume_scores),
     xlab="volatility z-score", ylab="volume z-score")
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volat_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of VTI volatility z-scores
closep <- quantmod::Cl(ohlc)
prices <- cbind(closep, volat_scores)
colnames(prices) <- c("VTI", "Z-scores")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI Volatility Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Centered Price Z-scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An extreme local price is a price which differs significantly from neighboring prices.
      \vskip1ex
      Extreme prices can be identified in-sample using the centered \emph{price z-score} equal to the price difference with neighboring prices divided by the volatility of returns $\sigma_i$:
      \begin{displaymath}
        z_i = \frac{2 p_i - p_{i-k} - p_{i+k}}{\sigma_i}
      \end{displaymath}
      Where $p_{i-k}$ and $p_{i+k}$ are the lagged and advanced prices.
      \vskip1ex
      The lag parameter $k$ determines the scale of the extreme local prices, with smaller $k$ producing larger z-scores for more local price extremes.
      <<echo=TRUE,eval=FALSE>>=
# Calculate the centered volatility
look_back <- 21
half_back <- look_back %/% 2
volat <- roll::roll_sd(returns, width=look_back, min_obs=1)
volat <- rutils::lagit(volat, lagg=(-half_back))
# Calculate the z-scores of prices
pricescores <- (2*closep - 
  rutils::lagit(closep, half_back, pad_zeros=FALSE) - 
  rutils::lagit(closep, -half_back, pad_zeros=FALSE))
pricescores <- ifelse(volat > 0, pricescores/volat, 0)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_price_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of z-scores of VTI prices
prices <- cbind(closep, pricescores)
colnames(prices) <- c("VTI", "Z-scores")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI Price Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Labeling the Tops and Bottoms of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The local tops and bottoms of prices can be labeled approximately in-sample using the z-scores of prices and threshold values.
      \vskip1ex
      The labeled data can be used as a response or target variable in machine learning classifier models.
      \vskip1ex
      But it's not feasible to classify the prices out-of-sample exactly according to their in-sample labels.
      <<echo=TRUE,eval=FALSE>>=
# Calculate thresholds for labeling tops and bottoms
threshv <- quantile(pricescores, c(0.1, 0.9))
# Calculate the vectors of tops and bottoms
tops <- (pricescores > threshv[2])
colnames(tops) <- "tops"
bottoms <- (pricescores < threshv[1])
colnames(bottoms) <- "bottoms"
# Backtest in-sample VTI strategy
position_s <- rep(NA_integer_, NROW(returns))
position_s[1] <- 0
position_s[tops] <- (-1)
position_s[bottoms] <- 1
position_s <- zoo::na.locf(position_s)
position_s <- rutils::lagit(position_s)
pnls <- cumsum(returns*position_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_labels.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
prices <- cbind(closep, pnls)
colnames(prices) <- c("VTI", "Strategy")
colnamev <- colnames(prices)
dygraphs::dygraph(prices, main="VTI Strategy Using In-sample Labels") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regression Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing \emph{z-score} $z_i$ of a price $p_i$ can be defined as the \emph{standardized residual} of the linear regression with respect to time $t_i$ or some other variable:
      \begin{displaymath}
        z_i = \frac{p_i - (\alpha + \beta t_i)}{\sigma_i}
      \end{displaymath}
      Where $\alpha$ and $\beta$ are the \emph{regression coefficients}, and $\sigma_i$ is the standard deviation of the residuals.
      \vskip1ex
      The regression \emph{z-scores} can be used as rich or cheap indicators, either relative to past prices, or relative to prices in a stock pair.
      \vskip1ex
      The regression residuals must be calculated in a loop, so it's much faster to calculate them using functions written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{HighFreq::roll\_zscores()} calculates the residuals of a rolling regression.
      <<echo=TRUE,eval=FALSE>>=
# Calculate trailing price z-scores
dates <- matrix(as.numeric(zoo::index(closep)))
look_back <- 21
pricescores <- drop(HighFreq::roll_zscores(response=closep, design=dates, look_back=look_back))
pricescores[1:look_back] <- 0
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_regr_zscores.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of z-scores of VTI prices
prices <- cbind(closep, pricescores)
colnames(prices) <- c("VTI", "Z-scores")
colnamev <- colnames(prices)
dygraphs::dygraph(prices["2009"], main="VTI Price Z-Scores") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hampel Filter for Outlier Detection}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability):
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(p_i - \operatorname{median}(\mathbf{p})))
      \end{displaymath}
      The \emph{Hampel filter} uses the \emph{MAD} dispersion measure to detect outliers in data.
      \vskip1ex
      The \emph{Hampel z-score} is equal to the deviation from the median divided by the \emph{MAD}:
      \begin{displaymath}
        z_i = \frac{p_i - \operatorname{median}(\mathbf{p})}{\operatorname{MAD}}
      \end{displaymath}
      A time series of \emph{z-scores} over past data can be calculated using a rolling look-back window.
      <<echo=TRUE,eval=FALSE>>=
# Extract time series of VTI log prices
closep <- log(na.omit(rutils::etfenv$prices$VTI))
# Define look-back window and a half window
look_back <- 11
# Calculate time series of medians
medi_an <- roll::roll_median(closep, width=look_back)
# medi_an <- TTR::runMedian(closep, n=look_back)
# Calculate time series of MAD
madv <- HighFreq::roll_var(closep, look_back=look_back, method="quantile")
# madv <- TTR::runMAD(closep, n=look_back)
# Calculate time series of z-scores
zscores <- (closep - medi_an)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hampel_zscores.png}\\
      \vspace{-1em}
      <<echo=(-1),eval=FALSE>>=
x11(width=6, height=5)
# Plot prices and medians
dygraphs::dygraph(cbind(closep, medi_an), main="VTI median") %>%
  dyOptions(colors=c("black", "red"))
# Plot histogram of z-scores
histp <- hist(zscores, col="lightgrey",
  xlab="z-scores", breaks=50, xlim=c(-4, 4),
  ylab="frequency", freq=FALSE, main="Hampel Z-scores histogram")
lines(density(zscores, adjust=1.5), lwd=3, col="blue")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{One-sided and Two-sided Data Filters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filters calculated over past data are referred to as \emph{one-sided} filters, and they are appropriate for filtering real-time data.
      \vskip1ex
      Filters calculated over both past and future data are called \emph{two-sided} (centered) filters, and they are appropriate for filtering historical data.
      \vskip1ex
      The function \texttt{HighFreq::roll\_var()} with parameter \texttt{method="quantile"} calculates the rolling \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      The functions \texttt{TTR::runMedian()} and \texttt{TTR::runMAD()} calculate the rolling medians and \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      If the rolling medians and \emph{MAD} are advanced (shifted backward) in time, then they are calculated over both past and future data (centered).
      \vskip1ex
      The function \texttt{rutils::lag\_it()} with a negative \texttt{lagg} parameter value advances (shifts back) future data points to the present.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Calculate one-sided Hampel z-scores
medi_an <- roll::roll_median(closep, width=look_back)
# medi_an <- TTR::runMedian(closep, n=look_back)
madv <- HighFreq::roll_var(closep, look_back=look_back, method="quantile")
# madv <- TTR::runMAD(closep, n=look_back)
zscores <- (closep - medi_an)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
# Calculate two-sided Hampel z-scores
half_back <- look_back %/% 2
medi_an <- rutils::lagit(medi_an, lagg=-half_back)
madv <- rutils::lagit(madv, lagg=-half_back)
zscores <- (closep - medi_an)/madv
zscores[1:look_back, ] <- 0
tail(zscores, look_back)
range(zscores)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hampel Filter Strategy}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Hampel filter strategy is a contrarian strategy that uses Hampel z-scores to establish long and short positions.
      \vskip1ex
      The Hampel strategy has two meta-parameters: the look-back interval and the threshold level.
      \vskip1ex
      The best choice of the meta-parameters can be determined through backtesting.
      <<echo=TRUE,eval=FALSE>>=
# Calculate VTI percentage returns
returns <- rutils::diffit(closep)
# Define threshold value
threshold <- sum(abs(range(zscores)))/8
# Backtest VTI strategy
position_s <- rep(NA_integer_, NROW(closep))
position_s[1] <- 0
position_s[zscores < -threshold] <- 1
position_s[zscores > threshold] <- (-1)
position_s <- zoo::na.locf(position_s)
position_s <- rutils::lagit(position_s)
pnls <- cumsum(returns*position_s)
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/strat_hampel.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot dygraph of in-sample VTI strategy
prices <- cbind(closep, pnls)
colnames(prices) <- c("VTI", "Strategy")
colnamev <- colnames(prices)
dygraphs::dygraph(prices, main="VTI Hampel Strategy") %>%
  dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
  dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
  dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=2, col="blue") %>%
  dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=2, col="red")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Classification of Data Outliers Using the Hampel Filter}


%%%%%%%%%%%%%%%
\subsection{Reading \protect\emph{TAQ} Data From \texttt{.csv} Files}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Trade and Quote (\emph{TAQ}) data stored in \texttt{.csv} files can be very large, so it's better to read it using the function \texttt{data.table::fread()} which is much faster than the function \texttt{read.csv()}.
      \vskip1ex
      Each \emph{trade} or \emph{quote} contributes a \emph{tick} (row) of data, and the number of ticks can be very large (hundred of thousands per day, or more).
      \vskip1ex
      The function \texttt{strptime()} coerces \texttt{character} strings representing the date and time into \texttt{POSIXlt} \emph{date-time} objects.
      \vskip1ex
      The argument \texttt{format="\%H:\%M:\%OS"} allows the parsing of fractional seconds, for example \texttt{"15:59:59.989847074"}.
      \vskip1ex
      The function \texttt{as.POSIXct()} coerces objects into \texttt{POSIXct} \emph{date-time} objects, with a \texttt{numeric} value representing the \emph{moment of time} in seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
library(HighFreq)
# Read TAQ trade data from csv file
taq <- data.table::fread(file="/Volumes/external/Develop/data/xlk_tick_trades2020_0316.csv")
# Inspect the TAQ data
taq
class(taq)
colnames(taq)
sapply(taq, class)
symbol <- taq$SYM_ROOT[1]
# Create date-time index
dates <- paste(taq$DATE, taq$TIME_M)
# Coerce date-time index to POSIXlt
dates <- strptime(dates, "%Y%m%d %H:%M:%OS")
class(dates)
# Display more significant digits
# options("digits")
options(digits=20, digits.secs=10)
last(dates)
unclass(last(dates))
as.numeric(last(dates))
# Coerce date-time index to POSIXct
dates <- as.POSIXct(dates)
class(dates)
last(dates)
unclass(last(dates))
as.numeric(last(dates))
# Calculate the number of ticks per second
n_secs <- as.numeric(last(dates)) - as.numeric(first(dates))
NROW(taq)/(6.5*3600)
# Select TAQ data columns
taq <- taq[, .(price=PRICE, volume=SIZE)]
# Add date-time index
taq <- cbind(index=dates, taq)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Microstructure Noise in High Frequency Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      High frequency data contains \emph{microstructure noise} in the form of \emph{price jumps} and the \emph{bid-ask bounce}.
      \vskip1ex
      \emph{Price jumps} are single ticks with prices far away from the average.
      \vskip1ex
      \emph{Price jumps} are often caused by data collection errors, but sometimes they represent actual very large lot trades.
      \vskip1ex
      The \emph{bid-ask bounce} is the bouncing of traded prices between the bid and ask prices.
      \vskip1ex
      The \emph{bid-ask bounce} creates an illusion of rapidly changing prices, while in fact the mid price is constant.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_dygraph.png}
      % \vspace{-1em}
      % \includegraphics[width=0.45\paperwidth]{figure/hf_stocks_bounce.png}
      <<echo=TRUE,eval=FALSE>>=
# Coerce trade ticks to xts series
xtes <- xts::xts(taq[, .(price, volume)], taq$index)
colnames(xtes) <- paste(symbol, c("Close", "Volume"), sep=".")
save(xtes, file="/Users/jerzy/Develop/data/xlk_tick_trades2020_0316.RData")
# Plot dygraph
dygraphs::dygraph(xtes$XLK.Close,
  main="XLK Trade Ticks for 2020-03-16")
# Plot in x11 window
x11(width=6, height=5)
quantmod::chart_Series(x=xtes$XLK.Close,
  name="XLK Trade Ticks for 2020-03-16")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Removing Microstructure Noise From High Frequency Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Microstructure noise can be removed from high frequency data by using a \emph{Hampel filter}.
      \vskip1ex
      The \emph{z-scores} are equal to the prices minus the median prices, divided by the median absolute deviation (\emph{MAD}) of prices:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{p_i - \operatorname{median}(\mathbf{p})}{\operatorname{MAD}}
      \end{displaymath}
      }
      If the \emph{z-score} exceeds the \emph{threshold value} then it's classified as an \emph{outlier} (jump in prices).
      <<echo=TRUE,eval=FALSE>>=
# Calculate centered Hampel filter to remove price jumps
look_back <- 111
half_back <- look_back %/% 2
medi_an <- roll::roll_median(taq$price, width=look_back)
# medi_an <- TTR::runMedian(taq$price, n=look_back)
medi_an <- rutils::lagit(medi_an, lagg=-half_back, pad_zeros=FALSE)
madv <- HighFreq::roll_var(matrix(taq$price), look_back=look_back, method="quantile")
# madv <- TTR::runMAD(taq$price, n=look_back)
madv <- rutils::lagit(madv, lagg=-half_back, pad_zeros=FALSE)
# Calculate Z-scores
zscores <- (taq$price - medi_an)/madv
zscores[is.na(zscores)] <- 0
zscores[!is.finite(zscores)] <- 0
sum(is.na(zscores))
sum(!is.finite(zscores))
range(zscores); mad(zscores)
hist(zscores, breaks=2000, xlim=c(-5*mad(zscores), 5*mad(zscores)))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_good_ticks_dygraph.png}
      <<echo=TRUE,eval=FALSE>>=
# Define discrimination threshold value
threshold <- 6*mad(zscores)
# Remove price jumps with large z-scores
bad_ticks <- (abs(zscores) > threshold)
good_ticks <- taq[!bad_ticks]
# Calculate number of price jumps
sum(bad_ticks)/NROW(zscores)
# Coerce trade prices to xts
xtes <- xts::xts(good_ticks[, .(price, volume)], good_ticks$index)
colnames(xtes) <- c("XLK.Close", "XLK.Volume")
# Plot dygraph of the clean lots
dygraphs::dygraph(xtes$XLK.Close,
  main="XLK Trade Ticks for 2020-03-16 (Hampel filtered)")
# Plot the large lots
x11(width=6, height=5)
quantmod::chart_Series(x=xtes$XLK.Close,
  name="XLK Trade Ticks for 2020-03-16 (Hampel filtered)")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Classifying Data Outliers Using the Hampel Filter}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The data points whose absolute \emph{z-scores} exceed a \emph{threshold value} are classified as outliers.
      \vskip1ex
      This procedure is a \emph{classifier}, which classifies the prices as either good or bad data points.
      \vskip1ex
      If the bad data points are not labeled, then we can add jumps to the data to test the performance of the classifier.
      \vskip1ex
      Let the \emph{null hypothesis} be that the given price is a good data point.
      \vskip1ex
      A positive result corresponds to rejecting the \emph{null hypothesis}, while a negative result corresponds to accepting the \emph{null hypothesis}.
      \vskip1ex
      The classifications are subject to two different types of errors: \emph{type I} and \emph{type II} errors.
      \vskip1ex
      A \emph{type I} error is the incorrect rejection of a \texttt{TRUE} \emph{null hypothesis} (i.e. a "false positive"), when good data is classified as bad.
      \vskip1ex
      A \emph{type II} error is the incorrect acceptance of a \texttt{FALSE} \emph{null hypothesis} (i.e. a "false negative"), when bad data is classified as good.
    \column{0.5\textwidth}
      \vspace{-1em}
      <<echo=TRUE,eval=FALSE>>=
# Define discrimination threshold value
threshold <- 6*mad(zscores)
# Calculate number of prices classified as bad data
is_bad <- (abs(zscores) > threshold)
sum(is_bad)
# Add 200 random price jumps into prices
set.seed(1121)
n_bad <- 200
is_jump <- logical(NROW(closep))
is_jump[sample(x=NROW(is_jump), size=n_bad)] <- TRUE
closep[is_jump] <- closep[is_jump]*
  sample(c(0.95, 1.05), size=n_bad, replace=TRUE)
# Plot prices and medians
dygraphs::dygraph(cbind(closep, medi_an), main="VTI median") %>%
  dyOptions(colors=c("black", "red"))
# Calculate time series of z-scores
medi_an <- roll::roll_median(closep, width=look_back)
# medi_an <- TTR::runMedian(closep, n=look_back)
madv <- HighFreq::roll_var(closep, look_back=look_back, method="quantile")
# madv <- TTR::runMAD(closep, n=look_back)
zscores <- (closep - medi_an)/madv
zscores[1:look_back, ] <- 0
# Calculate number of prices classified as bad data
is_bad <- (abs(zscores) > threshold)
sum(is_bad)
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Confusion Matrix of a Binary Classification Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{binary classification model} categorizes cases based on its forecasts whether the \emph{null hypothesis} is \texttt{TRUE} or \texttt{FALSE}.
      \vskip1ex
      The confusion matrix summarizes the performance of a classification model on a set of test data for which the actual values of the \emph{null hypothesis} are known.
      \vskip1ex
      \newcommand\MyBox[2]{
        \fbox{\lower0.75cm
          \vbox to 1.2cm{\vfil
            \hbox to 1.7cm{\parbox{\textwidth}{#1\\#2}}
            \vfil}
        }
      }
      \renewcommand\arraystretch{0.3}
      \setlength\tabcolsep{0pt}
      {\tiny
      \begin{tabular}{c >{\bfseries}r @{\hspace{0.5em}}c @{\hspace{0.4em}}c @{\hspace{0.5em}}l}
      \multirow{10}{*}{\parbox{0.5cm}{\bfseries Actual}} &
      & \multicolumn{2}{c}{\bfseries Forecast} & \\
      & & \bfseries Null is FALSE & \bfseries Null is TRUE \\
      & Null is FALSE & \MyBox{True Positive}{(sensitivity)} & \MyBox{False Negative}{(type II error)} \\[2.4em]
      & Null is TRUE & \MyBox{False Positive}{(type I error)} & \MyBox{True Negative}{(specificity)}
      \end{tabular}}
      <<echo=TRUE,eval=FALSE>>=
# Calculate confusion matrix
table(actual=!is_jump, forecast=!is_bad)
sum(is_bad)
# FALSE positive (type I error)
sum(!is_jump & is_bad)
# FALSE negative (type II error)
sum(is_jump & !is_bad)
      @
    \column{0.5\textwidth}
      Let the \emph{null hypothesis} be that the given price is a good data point.
      \vskip1ex
      The \emph{true positive} rate (known as the \emph{sensitivity}) is the fraction of \texttt{FALSE} \emph{null hypothesis} cases that are correctly classified as \texttt{FALSE}.
      \vskip1ex
      The \emph{false negative} rate is the fraction of \texttt{FALSE} \emph{null hypothesis} cases that are incorrectly classified as \texttt{TRUE} (\emph{type II} error).
      \vskip1ex
      The sum of the \emph{true positive} plus the \emph{false negative} rate is equal to $1$.
      \vskip1ex
      The \emph{true negative} rate (known as the \emph{specificity}) is the fraction of \texttt{TRUE} \emph{null hypothesis} cases that are correctly classified as \texttt{TRUE}.
      \vskip1ex
      The \emph{false positive} rate is the fraction of \texttt{TRUE} \emph{null hypothesis} cases that are incorrectly classified as \texttt{FALSE} (\emph{type I} error).
      \vskip1ex
      The sum of the \emph{true negative} plus the \emph{false positive} rate is equal to $1$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Receiver Operating Characteristic (ROC) Curve}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ROC curve} is the plot of the \emph{true positive} rate, as a function of the \emph{false positive} rate, and illustrates the performance of a binary classifier.
      \vskip1ex
      The \emph{area under the ROC curve} (AUC) is a measure of the performance of a binary classification model.
      <<echo=TRUE,eval=FALSE>>=
# Confusion matrix as function of threshold
confun <- function(actu_al, zscores, threshold) {
    confmat <- table(!actu_al, !(abs(zscores) > threshold))
    confmat <- confmat / rowSums(confmat)
    c(typeI=confmat[2, 1], typeII=confmat[1, 2])
  }  # end confun
confun(is_jump, zscores, threshold=threshold)
# Define vector of discrimination thresholds
threshv <- seq(from=0.2, to=5.0, by=0.2)
# Calculate error rates
error_rates <- sapply(threshv, confun,
  actu_al=is_jump, zscores=zscores)  # end sapply
error_rates <- t(error_rates)
rownames(error_rates) <- threshv
error_rates <- rbind(c(1, 0), error_rates)
error_rates <- rbind(error_rates, c(0, 1))
# Calculate area under ROC curve (AUC)
true_pos <- (1 - error_rates[, "typeII"])
true_pos <- (true_pos + rutils::lagit(true_pos))/2
false_pos <- rutils::diffit(error_rates[, "typeI"])
abs(sum(true_pos*false_pos))
      @
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hampel_roc.png}
      <<echo=TRUE,eval=FALSE>>=
# Plot ROC curve for Hampel classifier
x11(width=6, height=5)
plot(x=error_rates[, "typeI"], y=1-error_rates[, "typeII"],
     xlab="FALSE positive rate", ylab="TRUE positive rate",
     xlim=c(0, 1), ylim=c(0, 1),
     main="ROC Curve for Hampel Classifier",
     type="l", lwd=3, col="blue")
abline(a=0.0, b=1.0, lwd=3, col="orange")
      @
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Microstructure Noise Filtering}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The filtering of microstructure noise can be improved by calculating the median prices over an interval of only \texttt{3} data points.
      \vskip1ex
      If the \emph{z-score} exceeds the \emph{threshold value} then it's classified as an \emph{outlier} (jump in prices).
      <<echo=TRUE,eval=FALSE>>=
# Calculate centered Hampel filter over 3 data points
medi_an <- roll::roll_median(taq$price, width=3)
medi_an[1:2] <- taq$price[1:2]
medi_an <- rutils::lagit(medi_an, lagg=-1, pad_zeros=FALSE)
madv <- HighFreq::roll_var(matrix(taq$price), look_back=look_back, method="quantile")
madv <- rutils::lagit(madv, lagg=-1, pad_zeros=FALSE)
# Calculate Z-scores
zscores <- ifelse(madv > 0, (taq$price - medi_an)/madv, 0)
range(zscores); mad(zscores)
madv <- mad(zscores[abs(zscores)>0])
hist(zscores, breaks=2000, xlim=c(-5*madv, 5*madv))
      @
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/hf_trade_ticks_good_ticks_dygraph.png}
      <<echo=TRUE,eval=FALSE>>=
# Define discrimination threshold value
threshold <- 6*madv
bad_ticks <- (abs(zscores) > threshold)
good_ticks <- taq[!bad_ticks]
# Calculate number of price jumps
sum(bad_ticks)/NROW(zscores)
# Coerce trade prices to xts
xtes <- xts::xts(good_ticks[, .(price, volume)], good_ticks$index)
colnames(xtes) <- c("XLK.Close", "XLK.Volume")
# Plot dygraph of the clean lots
dygraphs::dygraph(xtes$XLK.Close,
  main="XLK Trade Ticks for 2020-03-16 (Hampel filtered)")
# Plot the large lots
x11(width=6, height=5)
quantmod::chart_Series(x=xtes$XLK.Close,
  name="XLK Trade Ticks for 2020-03-16 (Hampel filtered)")
      @
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Homework Assignment}


%%%%%%%%%%%%%%%
\subsection{Homework Assignment}
\begin{frame}[t]{\secname}
\vspace{-1em}
\begin{block}{Required}
  \begin{itemize}[]
    \item Study all the lecture slides in \emph{FRE7241\_Lecture\3.pdf}, and run all the code in \emph{FRE7241\_Lecture\3.R}
  \end{itemize}
\end{block}
% \begin{block}{Recommended}
%   \begin{itemize}[]
%   \end{itemize}
% \end{block}

\end{frame}


\end{document}
